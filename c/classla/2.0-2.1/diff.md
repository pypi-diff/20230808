# Comparing `tmp/classla-2.0.tar.gz` & `tmp/classla-2.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "classla-2.0.tar", last modified: Thu Feb 16 18:43:24 2023, max compression
+gzip compressed data, was "classla-2.1.tar", last modified: Tue Aug  8 07:48:44 2023, max compression
```

## Comparing `classla-2.0.tar` & `classla-2.1.tar`

### file list

```diff
@@ -1,187 +1,187 @@
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.667933 classla-2.0/
--rw-rw-rw-   0        0        0      616 2022-12-14 15:07:03.000000 classla-2.0/LICENSE
--rw-rw-rw-   0        0        0    19322 2023-02-16 18:43:24.667443 classla-2.0/PKG-INFO
--rw-rw-rw-   0        0        0    17713 2022-12-14 15:07:03.000000 classla-2.0/README.md
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.450192 classla-2.0/classla/
--rw-rw-rw-   0        0        0      976 2022-12-14 15:07:03.000000 classla-2.0/classla/__init__.py
--rw-rw-rw-   0        0        0      105 2023-02-16 18:09:12.000000 classla-2.0/classla/_version.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.469614 classla-2.0/classla/models/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/classla/models/__init__.py
--rw-rw-rw-   0        0        0       87 2022-12-14 15:07:03.000000 classla-2.0/classla/models/_training_logging.py
--rw-rw-rw-   0        0        0    15436 2022-12-14 15:07:03.000000 classla-2.0/classla/models/charlm.py
--rw-rw-rw-   0        0        0    26846 2022-12-14 15:07:03.000000 classla-2.0/classla/models/classifier.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.475535 classla-2.0/classla/models/classifiers/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/classla/models/classifiers/__init__.py
--rw-rw-rw-   0        0        0     2269 2022-12-14 15:07:03.000000 classla-2.0/classla/models/classifiers/classifier_args.py
--rw-rw-rw-   0        0        0    15711 2022-12-14 15:07:03.000000 classla-2.0/classla/models/classifiers/cnn_classifier.py
--rw-rw-rw-   0        0        0     2506 2022-12-14 15:07:03.000000 classla-2.0/classla/models/classifiers/iterate_test.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.509807 classla-2.0/classla/models/common/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/__init__.py
--rw-rw-rw-   0        0        0     4040 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/beam.py
--rw-rw-rw-   0        0        0     3662 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/biaffine.py
--rw-rw-rw-   0        0        0     6864 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/char_model.py
--rw-rw-rw-   0        0        0     8588 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/chuliu_edmonds.py
--rw-rw-rw-   0        0        0     2098 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/constant.py
--rw-rw-rw-   0        0        0     5565 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/crf.py
--rw-rw-rw-   0        0        0     1452 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/data.py
--rw-rw-rw-   0        0        0    37072 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/doc.py
--rw-rw-rw-   0        0        0     2931 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/dropout.py
--rw-rw-rw-   0        0        0     5213 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/hlstm.py
--rw-rw-rw-   0        0        0     2747 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/loss.py
--rw-rw-rw-   0        0        0     4960 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/packed_lstm.py
--rw-rw-rw-   0        0        0     5322 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/pretrain.py
--rw-rw-rw-   0        0        0      238 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/seq2seq_constant.py
--rw-rw-rw-   0        0        0    12119 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/seq2seq_model.py
--rw-rw-rw-   0        0        0     8247 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/seq2seq_modules.py
--rw-rw-rw-   0        0        0     3637 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/seq2seq_utils.py
--rw-rw-rw-   0        0        0      665 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/trainer.py
--rw-rw-rw-   0        0        0     7515 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/utils.py
--rw-rw-rw-   0        0        0     8138 2022-12-14 15:07:03.000000 classla-2.0/classla/models/common/vocab.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.515400 classla-2.0/classla/models/depparse/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/classla/models/depparse/__init__.py
--rw-rw-rw-   0        0        0     9457 2022-12-14 15:07:03.000000 classla-2.0/classla/models/depparse/data.py
--rw-rw-rw-   0        0        0    10380 2022-12-14 15:07:03.000000 classla-2.0/classla/models/depparse/model.py
--rw-rw-rw-   0        0        0      638 2022-12-14 15:07:03.000000 classla-2.0/classla/models/depparse/scorer.py
--rw-rw-rw-   0        0        0     6172 2022-12-14 15:07:03.000000 classla-2.0/classla/models/depparse/trainer.py
--rw-rw-rw-   0        0        0     2310 2022-12-14 15:07:03.000000 classla-2.0/classla/models/identity_lemmatizer.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.523575 classla-2.0/classla/models/lemma/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/classla/models/lemma/__init__.py
--rw-rw-rw-   0        0        0     4671 2022-12-14 15:07:03.000000 classla-2.0/classla/models/lemma/data.py
--rw-rw-rw-   0        0        0      658 2022-12-14 15:07:03.000000 classla-2.0/classla/models/lemma/edit.py
--rw-rw-rw-   0        0        0      474 2022-12-14 15:07:03.000000 classla-2.0/classla/models/lemma/scorer.py
--rw-rw-rw-   0        0        0     9324 2022-12-14 15:07:03.000000 classla-2.0/classla/models/lemma/trainer.py
--rw-rw-rw-   0        0        0      669 2022-12-14 15:07:03.000000 classla-2.0/classla/models/lemma/vocab.py
--rw-rw-rw-   0        0        0    14886 2022-12-14 15:07:03.000000 classla-2.0/classla/models/lemmatizer.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.529180 classla-2.0/classla/models/mwt/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/classla/models/mwt/__init__.py
--rw-rw-rw-   0        0        0     3656 2022-12-14 15:07:03.000000 classla-2.0/classla/models/mwt/data.py
--rw-rw-rw-   0        0        0      370 2022-12-14 15:07:03.000000 classla-2.0/classla/models/mwt/scorer.py
--rw-rw-rw-   0        0        0     5776 2022-12-14 15:07:03.000000 classla-2.0/classla/models/mwt/trainer.py
--rw-rw-rw-   0        0        0      521 2022-12-14 15:07:03.000000 classla-2.0/classla/models/mwt/vocab.py
--rw-rw-rw-   0        0        0    11714 2022-12-14 15:07:03.000000 classla-2.0/classla/models/mwt_expander.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.540099 classla-2.0/classla/models/ner/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/classla/models/ner/__init__.py
--rw-rw-rw-   0        0        0     9042 2022-12-14 15:07:03.000000 classla-2.0/classla/models/ner/data.py
--rw-rw-rw-   0        0        0     6430 2022-12-14 15:07:03.000000 classla-2.0/classla/models/ner/model.py
--rw-rw-rw-   0        0        0     4732 2022-12-14 15:07:03.000000 classla-2.0/classla/models/ner/scorer.py
--rw-rw-rw-   0        0        0     5523 2022-12-14 15:07:03.000000 classla-2.0/classla/models/ner/trainer.py
--rw-rw-rw-   0        0        0     3683 2022-12-14 15:07:03.000000 classla-2.0/classla/models/ner/utils.py
--rw-rw-rw-   0        0        0     1596 2022-12-14 15:07:03.000000 classla-2.0/classla/models/ner/vocab.py
--rw-rw-rw-   0        0        0    11941 2022-12-14 15:07:03.000000 classla-2.0/classla/models/ner_tagger.py
--rw-rw-rw-   0        0        0    12333 2022-12-14 15:07:03.000000 classla-2.0/classla/models/parser.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.551702 classla-2.0/classla/models/pos/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/classla/models/pos/__init__.py
--rw-rw-rw-   0        0        0     4160 2022-12-14 15:07:03.000000 classla-2.0/classla/models/pos/build_xpos_vocab_factory.py
--rw-rw-rw-   0        0        0     6388 2022-12-14 15:07:03.000000 classla-2.0/classla/models/pos/data.py
--rw-rw-rw-   0        0        0     9210 2022-12-14 15:07:03.000000 classla-2.0/classla/models/pos/model.py
--rw-rw-rw-   0        0        0    13908 2022-12-14 15:07:03.000000 classla-2.0/classla/models/pos/postprocessor.py
--rw-rw-rw-   0        0        0      673 2022-12-14 15:07:03.000000 classla-2.0/classla/models/pos/scorer.py
--rw-rw-rw-   0        0        0     7531 2023-02-07 15:40:42.000000 classla-2.0/classla/models/pos/trainer.py
--rw-rw-rw-   0        0        0     3355 2022-12-14 15:07:03.000000 classla-2.0/classla/models/pos/vocab.py
--rw-rw-rw-   0        0        0     2132 2022-12-14 15:07:03.000000 classla-2.0/classla/models/pos/xpos_vocab_factory.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.560299 classla-2.0/classla/models/srl/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/classla/models/srl/__init__.py
--rw-rw-rw-   0        0        0     6297 2022-12-14 15:07:03.000000 classla-2.0/classla/models/srl/data.py
--rw-rw-rw-   0        0        0     7684 2022-12-14 15:07:03.000000 classla-2.0/classla/models/srl/model.py
--rw-rw-rw-   0        0        0     1875 2022-12-14 15:07:03.000000 classla-2.0/classla/models/srl/scorer.py
--rw-rw-rw-   0        0        0     4620 2022-12-14 15:07:03.000000 classla-2.0/classla/models/srl/trainer.py
--rw-rw-rw-   0        0        0     1137 2022-12-14 15:07:03.000000 classla-2.0/classla/models/srl/vocab.py
--rw-rw-rw-   0        0        0    11630 2022-12-14 15:07:03.000000 classla-2.0/classla/models/srl_tagger.py
--rw-rw-rw-   0        0        0    16006 2023-02-07 15:40:42.000000 classla-2.0/classla/models/tagger.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.575199 classla-2.0/classla/pipeline/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/__init__.py
--rw-rw-rw-   0        0        0      217 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/_constants.py
--rw-rw-rw-   0        0        0     8090 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/core.py
--rw-rw-rw-   0        0        0     2101 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/depparse_processor.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.579674 classla-2.0/classla/pipeline/external/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/external/__init__.py
--rw-rw-rw-   0        0        0     2216 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/external/jieba.py
--rw-rw-rw-   0        0        0     2343 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/external/spacy.py
--rw-rw-rw-   0        0        0     2776 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/external/sudachipy.py
--rw-rw-rw-   0        0        0     4184 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/lemma_processor.py
--rw-rw-rw-   0        0        0     1560 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/mwt_processor.py
--rw-rw-rw-   0        0        0     1598 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/ner_processor.py
--rw-rw-rw-   0        0        0     2963 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/pos_processor.py
--rw-rw-rw-   0        0        0     8522 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/processor.py
--rw-rw-rw-   0        0        0      134 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/registry.py
--rw-rw-rw-   0        0        0     1806 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/sentiment_processor.py
--rw-rw-rw-   0        0        0     1679 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/srl_processor.py
--rw-rw-rw-   0        0        0     4404 2022-12-14 15:07:03.000000 classla-2.0/classla/pipeline/tokenize_processor.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.585654 classla-2.0/classla/protobuf/
--rw-rw-rw-   0        0        0   185687 2022-12-14 15:07:03.000000 classla-2.0/classla/protobuf/CoreNLP_pb2.py
--rw-rw-rw-   0        0        0     1753 2022-12-14 15:07:03.000000 classla-2.0/classla/protobuf/__init__.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.591115 classla-2.0/classla/resources/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/classla/resources/__init__.py
--rw-rw-rw-   0        0        0    17887 2022-12-14 15:07:03.000000 classla-2.0/classla/resources/common.py
--rw-rw-rw-   0        0        0     4502 2022-12-14 15:07:03.000000 classla-2.0/classla/resources/installation.py
--rw-rw-rw-   0        0        0     9648 2022-12-14 15:07:03.000000 classla-2.0/classla/resources/prepare_resources.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.598503 classla-2.0/classla/server/
--rw-rw-rw-   0        0        0      642 2022-12-14 15:07:03.000000 classla-2.0/classla/server/__init__.py
--rw-rw-rw-   0        0        0     4810 2022-12-14 15:07:03.000000 classla-2.0/classla/server/annotator.py
--rw-rw-rw-   0        0        0    28534 2022-12-14 15:07:03.000000 classla-2.0/classla/server/client.py
--rw-rw-rw-   0        0        0     2691 2022-12-14 15:07:03.000000 classla-2.0/classla/server/main.py
--rw-rw-rw-   0        0        0     2669 2022-12-14 15:07:03.000000 classla-2.0/classla/server/semgrex.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.616845 classla-2.0/classla/utils/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/classla/utils/__init__.py
--rw-rw-rw-   0        0        0      420 2022-12-14 15:07:03.000000 classla-2.0/classla/utils/avg_sent_len.py
--rw-rw-rw-   0        0        0     8364 2022-12-14 15:07:03.000000 classla-2.0/classla/utils/conll.py
--rw-rw-rw-   0        0        0    28539 2022-12-14 15:07:03.000000 classla-2.0/classla/utils/conll18_ud_eval.py
--rw-rw-rw-   0        0        0     1341 2022-12-14 15:07:03.000000 classla-2.0/classla/utils/contract_mwt.py
--rw-rw-rw-   0        0        0     1301 2022-12-14 15:07:03.000000 classla-2.0/classla/utils/helper_func.py
--rw-rw-rw-   0        0        0      259 2022-12-14 15:07:03.000000 classla-2.0/classla/utils/max_mwt_length.py
--rw-rw-rw-   0        0        0     1843 2022-12-14 15:07:03.000000 classla-2.0/classla/utils/obeliks.py
--rw-rw-rw-   0        0        0     2520 2022-12-14 15:07:03.000000 classla-2.0/classla/utils/postprocess_vietnamese_tokenizer_data.py
--rw-rw-rw-   0        0        0     2296 2022-12-14 15:07:03.000000 classla-2.0/classla/utils/prepare_ner_data.py
--rw-rw-rw-   0        0        0     5215 2022-12-14 15:07:03.000000 classla-2.0/classla/utils/prepare_tokenizer_data.py
--rw-rw-rw-   0        0        0     1824 2022-12-14 15:07:03.000000 classla-2.0/classla/utils/reldi.py
--rw-rw-rw-   0        0        0      464 2022-12-14 15:07:03.000000 classla-2.0/classla/utils/select_backoff.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.452616 classla-2.0/classla.egg-info/
--rw-rw-rw-   0        0        0    19322 2023-02-16 18:43:24.000000 classla-2.0/classla.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0     4927 2023-02-16 18:43:24.000000 classla-2.0/classla.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2023-02-16 18:43:24.000000 classla-2.0/classla.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0      152 2023-02-16 18:43:24.000000 classla-2.0/classla.egg-info/requires.txt
--rw-rw-rw-   0        0        0       40 2023-02-16 18:43:24.000000 classla-2.0/classla.egg-info/top_level.txt
--rw-rw-rw-   0        0        0       42 2023-02-16 18:43:24.667933 classla-2.0/setup.cfg
--rw-rw-rw-   0        0        0     4303 2022-12-14 15:07:03.000000 classla-2.0/setup.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.616845 classla-2.0/stanfordnlp/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/stanfordnlp/__init__.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.617841 classla-2.0/stanfordnlp/models/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/stanfordnlp/models/__init__.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.618838 classla-2.0/stanfordnlp/models/common/
--rw-rw-rw-   0        0        0        0 2022-12-14 15:07:03.000000 classla-2.0/stanfordnlp/models/common/__init__.py
--rw-rw-rw-   0        0        0       46 2022-12-14 15:07:03.000000 classla-2.0/stanfordnlp/models/common/pretrain.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.654073 classla-2.0/tests/
--rw-rw-rw-   0        0        0     4720 2022-12-14 15:07:03.000000 classla-2.0/tests/__init__.py
--rw-rw-rw-   0        0        0     8339 2022-12-14 15:07:03.000000 classla-2.0/tests/test_client.py
--rw-rw-rw-   0        0        0     3448 2022-12-14 15:07:03.000000 classla-2.0/tests/test_data_conversion.py
--rw-rw-rw-   0        0        0     1630 2022-12-14 15:07:03.000000 classla-2.0/tests/test_data_objects.py
--rw-rw-rw-   0        0        0     5011 2022-12-14 15:07:03.000000 classla-2.0/tests/test_decorators.py
--rw-rw-rw-   0        0        0     2703 2022-12-14 15:07:03.000000 classla-2.0/tests/test_depparse.py
--rw-rw-rw-   0        0        0     2529 2022-12-14 15:07:03.000000 classla-2.0/tests/test_depparse_data.py
--rw-rw-rw-   0        0        0     1600 2022-12-14 15:07:03.000000 classla-2.0/tests/test_doc.py
--rw-rw-rw-   0        0        0     7075 2022-12-14 15:07:03.000000 classla-2.0/tests/test_english_pipeline.py
--rw-rw-rw-   0        0        0     1463 2022-12-14 15:07:03.000000 classla-2.0/tests/test_installation.py
--rw-rw-rw-   0        0        0     1177 2022-12-14 15:07:03.000000 classla-2.0/tests/test_lemmatizer.py
--rw-rw-rw-   0        0        0     3442 2022-12-14 15:07:03.000000 classla-2.0/tests/test_mwt_expander.py
--rw-rw-rw-   0        0        0     1554 2022-12-14 15:07:03.000000 classla-2.0/tests/test_ner_tagger.py
--rw-rw-rw-   0        0        0     1903 2022-12-14 15:07:03.000000 classla-2.0/tests/test_pretrain.py
--rw-rw-rw-   0        0        0     4758 2022-12-14 15:07:03.000000 classla-2.0/tests/test_protobuf.py
--rw-rw-rw-   0        0        0     3162 2022-12-14 15:07:03.000000 classla-2.0/tests/test_requirements.py
--rw-rw-rw-   0        0        0     6818 2022-12-14 15:07:03.000000 classla-2.0/tests/test_semgrex.py
--rw-rw-rw-   0        0        0     4335 2022-12-14 15:07:03.000000 classla-2.0/tests/test_server_misc.py
--rw-rw-rw-   0        0        0    10489 2022-12-14 15:07:03.000000 classla-2.0/tests/test_server_request.py
--rw-rw-rw-   0        0        0     9941 2022-12-14 15:07:03.000000 classla-2.0/tests/test_server_start.py
--rw-rw-rw-   0        0        0     1066 2022-12-14 15:07:03.000000 classla-2.0/tests/test_tagger.py
--rw-rw-rw-   0        0        0     8227 2022-12-14 15:07:03.000000 classla-2.0/tests/test_tokenizer.py
--rw-rw-rw-   0        0        0     2834 2022-12-14 15:07:03.000000 classla-2.0/tests/test_utils.py
-drwxrwxrwx   0        0        0        0 2023-02-16 18:43:24.666445 classla-2.0/tests_classla/
--rw-rw-rw-   0        0        0     4693 2022-12-14 15:07:03.000000 classla-2.0/tests_classla/__init__.py
--rw-rw-rw-   0        0        0      596 2022-12-14 15:07:03.000000 classla-2.0/tests_classla/test_downloads.py
--rw-rw-rw-   0        0        0     3124 2022-12-14 15:07:03.000000 classla-2.0/tests_classla/test_lemmatizer.py
--rw-rw-rw-   0        0        0     1018 2022-12-14 15:07:03.000000 classla-2.0/tests_classla/test_ner.py
--rw-rw-rw-   0        0        0     1261 2022-12-14 15:07:03.000000 classla-2.0/tests_classla/test_parser.py
--rw-rw-rw-   0        0        0    11368 2023-02-16 17:18:44.000000 classla-2.0/tests_classla/test_readme_examples.py
--rw-rw-rw-   0        0        0     8910 2023-02-16 17:32:28.000000 classla-2.0/tests_classla/test_slovenian_pipeline.py
--rw-rw-rw-   0        0        0     1127 2022-12-14 15:07:03.000000 classla-2.0/tests_classla/test_srl.py
--rw-rw-rw-   0        0        0     1312 2022-12-14 15:07:03.000000 classla-2.0/tests_classla/test_tagger.py
--rw-rw-rw-   0        0        0     3079 2022-12-14 15:07:03.000000 classla-2.0/tests_classla/test_tokenizer.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.690206 classla-2.1/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      603 2023-08-08 07:48:16.000000 classla-2.1/LICENSE
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    19995 2023-08-08 07:48:44.686206 classla-2.1/PKG-INFO
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    18731 2023-08-08 07:48:16.000000 classla-2.1/README.md
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.630205 classla-2.1/classla/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      950 2023-08-08 07:48:16.000000 classla-2.1/classla/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      101 2023-08-08 07:48:16.000000 classla-2.1/classla/_version.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.638205 classla-2.1/classla/models/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/classla/models/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)       84 2023-08-08 07:48:16.000000 classla-2.1/classla/models/_training_logging.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    15100 2023-08-08 07:48:16.000000 classla-2.1/classla/models/charlm.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    26251 2023-08-08 07:48:16.000000 classla-2.1/classla/models/classifier.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.638205 classla-2.1/classla/models/classifiers/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/classla/models/classifiers/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2224 2023-08-08 07:48:16.000000 classla-2.1/classla/models/classifiers/classifier_args.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    15364 2023-08-08 07:48:16.000000 classla-2.1/classla/models/classifiers/cnn_classifier.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2439 2023-08-08 07:48:16.000000 classla-2.1/classla/models/classifiers/iterate_test.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.646205 classla-2.1/classla/models/common/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     3908 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/beam.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     3582 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/biaffine.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     6728 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/char_model.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     8385 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/chuliu_edmonds.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2012 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/constant.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     5418 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/crf.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1413 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/data.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    36070 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/doc.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2856 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/dropout.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     5089 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/hlstm.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2662 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/loss.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4855 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/packed_lstm.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     5183 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/pretrain.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      221 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/seq2seq_constant.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    11837 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/seq2seq_model.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     8016 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/seq2seq_modules.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     3507 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/seq2seq_utils.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      645 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/trainer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     7287 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/utils.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     7916 2023-08-08 07:48:16.000000 classla-2.1/classla/models/common/vocab.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.646205 classla-2.1/classla/models/depparse/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/classla/models/depparse/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     9232 2023-08-08 07:48:16.000000 classla-2.1/classla/models/depparse/data.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    10188 2023-08-08 07:48:16.000000 classla-2.1/classla/models/depparse/model.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      616 2023-08-08 07:48:16.000000 classla-2.1/classla/models/depparse/scorer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     6031 2023-08-08 07:48:16.000000 classla-2.1/classla/models/depparse/trainer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2248 2023-08-08 07:48:16.000000 classla-2.1/classla/models/identity_lemmatizer.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.650205 classla-2.1/classla/models/lemma/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/classla/models/lemma/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4545 2023-08-08 07:48:16.000000 classla-2.1/classla/models/lemma/data.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      631 2023-08-08 07:48:16.000000 classla-2.1/classla/models/lemma/edit.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      460 2023-08-08 07:48:16.000000 classla-2.1/classla/models/lemma/scorer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     9087 2023-08-08 07:48:16.000000 classla-2.1/classla/models/lemma/trainer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      651 2023-08-08 07:48:16.000000 classla-2.1/classla/models/lemma/vocab.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    14569 2023-08-08 07:48:16.000000 classla-2.1/classla/models/lemmatizer.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.650205 classla-2.1/classla/models/mwt/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/classla/models/mwt/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     3550 2023-08-08 07:48:16.000000 classla-2.1/classla/models/mwt/data.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      358 2023-08-08 07:48:16.000000 classla-2.1/classla/models/mwt/scorer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     5623 2023-08-08 07:48:16.000000 classla-2.1/classla/models/mwt/trainer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      508 2023-08-08 07:48:16.000000 classla-2.1/classla/models/mwt/vocab.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    11456 2023-08-08 07:48:16.000000 classla-2.1/classla/models/mwt_expander.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.654205 classla-2.1/classla/models/ner/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/classla/models/ner/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     8846 2023-08-08 07:48:16.000000 classla-2.1/classla/models/ner/data.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     6299 2023-08-08 07:48:16.000000 classla-2.1/classla/models/ner/model.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4604 2023-08-08 07:48:16.000000 classla-2.1/classla/models/ner/scorer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     5389 2023-08-08 07:48:16.000000 classla-2.1/classla/models/ner/trainer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     3557 2023-08-08 07:48:16.000000 classla-2.1/classla/models/ner/utils.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1557 2023-08-08 07:48:16.000000 classla-2.1/classla/models/ner/vocab.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    11691 2023-08-08 07:48:16.000000 classla-2.1/classla/models/ner_tagger.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    12065 2023-08-08 07:48:16.000000 classla-2.1/classla/models/parser.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.658205 classla-2.1/classla/models/pos/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/classla/models/pos/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4059 2023-08-08 07:48:16.000000 classla-2.1/classla/models/pos/build_xpos_vocab_factory.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     6225 2023-08-08 07:48:16.000000 classla-2.1/classla/models/pos/data.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     9023 2023-08-08 07:48:16.000000 classla-2.1/classla/models/pos/model.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    13663 2023-08-08 07:48:16.000000 classla-2.1/classla/models/pos/postprocessor.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      651 2023-08-08 07:48:16.000000 classla-2.1/classla/models/pos/scorer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     7364 2023-08-08 07:48:16.000000 classla-2.1/classla/models/pos/trainer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     3275 2023-08-08 07:48:16.000000 classla-2.1/classla/models/pos/vocab.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2120 2023-08-08 07:48:16.000000 classla-2.1/classla/models/pos/xpos_vocab_factory.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.658205 classla-2.1/classla/models/srl/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/classla/models/srl/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     6141 2023-08-08 07:48:16.000000 classla-2.1/classla/models/srl/data.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     7526 2023-08-08 07:48:16.000000 classla-2.1/classla/models/srl/model.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1821 2023-08-08 07:48:16.000000 classla-2.1/classla/models/srl/scorer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4509 2023-08-08 07:48:16.000000 classla-2.1/classla/models/srl/trainer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1106 2023-08-08 07:48:16.000000 classla-2.1/classla/models/srl/vocab.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    11382 2023-08-08 07:48:16.000000 classla-2.1/classla/models/srl_tagger.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    15674 2023-08-08 07:48:16.000000 classla-2.1/classla/models/tagger.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.662205 classla-2.1/classla/pipeline/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      206 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/_constants.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     7920 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/core.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2052 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/depparse_processor.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.666206 classla-2.1/classla/pipeline/external/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/external/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2150 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/external/jieba.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2280 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/external/spacy.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2693 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/external/sudachipy.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4093 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/lemma_processor.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1518 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/mwt_processor.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1559 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/ner_processor.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2890 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/pos_processor.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     8296 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/processor.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      129 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/registry.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1763 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/sentiment_processor.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1634 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/srl_processor.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4302 2023-08-08 07:48:16.000000 classla-2.1/classla/pipeline/tokenize_processor.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.666206 classla-2.1/classla/protobuf/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)   182057 2023-08-08 07:48:16.000000 classla-2.1/classla/protobuf/CoreNLP_pb2.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1701 2023-08-08 07:48:16.000000 classla-2.1/classla/protobuf/__init__.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.666206 classla-2.1/classla/resources/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/classla/resources/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    17440 2023-08-08 07:48:16.000000 classla-2.1/classla/resources/common.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4384 2023-08-08 07:48:16.000000 classla-2.1/classla/resources/installation.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     9320 2023-08-08 07:48:16.000000 classla-2.1/classla/resources/prepare_resources.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.670205 classla-2.1/classla/server/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      632 2023-08-08 07:48:16.000000 classla-2.1/classla/server/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4672 2023-08-08 07:48:16.000000 classla-2.1/classla/server/annotator.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    27864 2023-08-08 07:48:16.000000 classla-2.1/classla/server/client.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2619 2023-08-08 07:48:16.000000 classla-2.1/classla/server/main.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2590 2023-08-08 07:48:16.000000 classla-2.1/classla/server/semgrex.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.674206 classla-2.1/classla/utils/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/classla/utils/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      402 2023-08-08 07:48:16.000000 classla-2.1/classla/utils/avg_sent_len.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     8163 2023-08-08 07:48:16.000000 classla-2.1/classla/utils/conll.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    27954 2023-08-08 07:48:16.000000 classla-2.1/classla/utils/conll18_ud_eval.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1301 2023-08-08 07:48:16.000000 classla-2.1/classla/utils/contract_mwt.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1263 2023-08-08 07:48:16.000000 classla-2.1/classla/utils/helper_func.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      246 2023-08-08 07:48:16.000000 classla-2.1/classla/utils/max_mwt_length.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1788 2023-08-08 07:48:16.000000 classla-2.1/classla/utils/obeliks.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2456 2023-08-08 07:48:16.000000 classla-2.1/classla/utils/postprocess_vietnamese_tokenizer_data.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2225 2023-08-08 07:48:16.000000 classla-2.1/classla/utils/prepare_ner_data.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     5072 2023-08-08 07:48:16.000000 classla-2.1/classla/utils/prepare_tokenizer_data.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1777 2023-08-08 07:48:16.000000 classla-2.1/classla/utils/reldi.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      451 2023-08-08 07:48:16.000000 classla-2.1/classla/utils/select_backoff.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.634204 classla-2.1/classla.egg-info/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    19995 2023-08-08 07:48:44.000000 classla-2.1/classla.egg-info/PKG-INFO
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4927 2023-08-08 07:48:44.000000 classla-2.1/classla.egg-info/SOURCES.txt
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        1 2023-08-08 07:48:44.000000 classla-2.1/classla.egg-info/dependency_links.txt
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      152 2023-08-08 07:48:44.000000 classla-2.1/classla.egg-info/requires.txt
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)       40 2023-08-08 07:48:44.000000 classla-2.1/classla.egg-info/top_level.txt
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)       38 2023-08-08 07:48:44.690206 classla-2.1/setup.cfg
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4197 2023-08-08 07:48:16.000000 classla-2.1/setup.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.674206 classla-2.1/stanfordnlp/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/stanfordnlp/__init__.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.674206 classla-2.1/stanfordnlp/models/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/stanfordnlp/models/__init__.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.674206 classla-2.1/stanfordnlp/models/common/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:16.000000 classla-2.1/stanfordnlp/models/common/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)       45 2023-08-08 07:48:16.000000 classla-2.1/stanfordnlp/models/common/pretrain.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.682206 classla-2.1/tests/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4600 2023-08-08 07:48:16.000000 classla-2.1/tests/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     8157 2023-08-08 07:48:16.000000 classla-2.1/tests/test_client.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     3413 2023-08-08 07:48:16.000000 classla-2.1/tests/test_data_conversion.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1580 2023-08-08 07:48:16.000000 classla-2.1/tests/test_data_objects.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4885 2023-08-08 07:48:16.000000 classla-2.1/tests/test_decorators.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2618 2023-08-08 07:48:16.000000 classla-2.1/tests/test_depparse.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2467 2023-08-08 07:48:16.000000 classla-2.1/tests/test_depparse_data.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1543 2023-08-08 07:48:16.000000 classla-2.1/tests/test_doc.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     6940 2023-08-08 07:48:16.000000 classla-2.1/tests/test_english_pipeline.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1424 2023-08-08 07:48:16.000000 classla-2.1/tests/test_installation.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1126 2023-08-08 07:48:16.000000 classla-2.1/tests/test_lemmatizer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     3358 2023-08-08 07:48:16.000000 classla-2.1/tests/test_mwt_expander.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1514 2023-08-08 07:48:16.000000 classla-2.1/tests/test_ner_tagger.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1850 2023-08-08 07:48:16.000000 classla-2.1/tests/test_pretrain.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4607 2023-08-08 07:48:16.000000 classla-2.1/tests/test_protobuf.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     3090 2023-08-08 07:48:16.000000 classla-2.1/tests/test_requirements.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     6581 2023-08-08 07:48:16.000000 classla-2.1/tests/test_semgrex.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4239 2023-08-08 07:48:16.000000 classla-2.1/tests/test_server_misc.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    10266 2023-08-08 07:48:16.000000 classla-2.1/tests/test_server_request.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     9727 2023-08-08 07:48:16.000000 classla-2.1/tests/test_server_start.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1038 2023-08-08 07:48:16.000000 classla-2.1/tests/test_tagger.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     8051 2023-08-08 07:48:16.000000 classla-2.1/tests/test_tokenizer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2757 2023-08-08 07:48:16.000000 classla-2.1/tests/test_utils.py
+drwxrwxr-x   0 decaf     (1000) decaf     (1000)        0 2023-08-08 07:48:44.686206 classla-2.1/tests_classla/
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     4574 2023-08-08 07:48:16.000000 classla-2.1/tests_classla/__init__.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      583 2023-08-08 07:48:16.000000 classla-2.1/tests_classla/test_downloads.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     3067 2023-08-08 07:48:16.000000 classla-2.1/tests_classla/test_lemmatizer.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)      989 2023-08-08 07:48:16.000000 classla-2.1/tests_classla/test_ner.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1231 2023-08-08 07:48:16.000000 classla-2.1/tests_classla/test_parser.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)    11092 2023-08-08 07:48:16.000000 classla-2.1/tests_classla/test_readme_examples.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     8757 2023-08-08 07:48:16.000000 classla-2.1/tests_classla/test_slovenian_pipeline.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1099 2023-08-08 07:48:16.000000 classla-2.1/tests_classla/test_srl.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     1284 2023-08-08 07:48:16.000000 classla-2.1/tests_classla/test_tagger.py
+-rw-rw-r--   0 decaf     (1000) decaf     (1000)     2998 2023-08-08 07:48:16.000000 classla-2.1/tests_classla/test_tokenizer.py
```

### Comparing `classla-2.0/LICENSE` & `classla-2.1/LICENSE`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,13 +1,13 @@
-Copyright 2019 The Board of Trustees of The Leland Stanford Junior University
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
+Copyright 2019 The Board of Trustees of The Leland Stanford Junior University
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
```

### Comparing `classla-2.0/PKG-INFO` & `classla-2.1/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,345 +1,369 @@
-Metadata-Version: 2.1
-Name: classla
-Version: 2.0
-Summary: Adapted Stanford NLP Python Library with improvements for specific languages.
-Home-page: https://github.com/clarinsi/classla-stanfordnlp.git
-Author: CLARIN.SI
-Author-email: info@clarin.si
-License: Apache License 2.0
-Keywords: natural-language-processing nlp natural-language-understanding stanford-nlp deep-learning clarinsi
-Classifier: Development Status :: 4 - Beta
-Classifier: Intended Audience :: Developers
-Classifier: Intended Audience :: Education
-Classifier: Intended Audience :: Science/Research
-Classifier: Intended Audience :: Information Technology
-Classifier: Topic :: Scientific/Engineering
-Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
-Classifier: Topic :: Scientific/Engineering :: Information Analysis
-Classifier: Topic :: Text Processing
-Classifier: Topic :: Text Processing :: Linguistic
-Classifier: Topic :: Software Development
-Classifier: Topic :: Software Development :: Libraries
-Classifier: Programming Language :: Python :: 3.6
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Requires-Python: >=3.6
-Description-Content-Type: text/markdown
-Provides-Extra: dev
-Provides-Extra: test
-License-File: LICENSE
-
-# A [CLASSLA](http://www.clarin.si/info/k-centre/) Fork of [Stanza](https://github.com/stanfordnlp/stanza) for Processing Slovenian, Croatian, Serbian, Macedonian and Bulgarian
-
-## Description
-
-This pipeline allows for processing of standard Slovenian, Croatian, Serbian and Bulgarian on the levels of
-
-- tokenization and sentence splitting
-- part-of-speech tagging
-- lemmatization
-- dependency parsing
-- named entity recognition
-
-It also allows for (alpha) processing of standard Macedonian on the levels of 
-
-- tokenization and sentence splitting
-- part-of-speech tagging
-- lemmatization
-
-Finally, it allows for processing of non-standard (Internet) Slovenian, Croatian and Serbian on the same levels as standard language (all models are tailored to non-standard language except for dependency parsing where the standard module is used).
-
-## Differences to Stanza
-
-The differences of this pipeline to the original Stanza pipeline are the following:
-
-- usage of language-specific rule-based tokenizers and sentence splitters, [obeliks](https://pypi.org/project/obeliks/) for standard Slovenian and [reldi-tokeniser](https://pypi.org/project/reldi-tokeniser/) for the remaining varieties and languages (Stanza uses inferior machine-learning-based tokenization and sentence splitting trained on UD data)
-- default pre-tagging and pre-lemmatization on the level of tokenizers for the following phenomena: punctuation, symbol, e-mail, URL, mention, hashtag, emoticon, emoji (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#usage-of-tagging-control-via-the-tokenizer))
-- optional control of the tagger for Slovenian via an inflectional lexicon on the levels of XPOS, UPOS, FEATS (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#usage-of-inflectional-lexicon))
-- closed class handling depending on the usage of the options described in the last two bullets, as documented [here](https://github.com/clarinsi/classla/blob/master/README.closed_classes.md)
-- usage of external inflectional lexicons for lookup lemmatization, seq2seq being used very infrequently on OOVs only (Stanza uses only UD training data for lookup lemmatization)
-- morphosyntactic tagging models based on larger quantities of training data than is available in UD (training data that are morphosyntactically tagged, but not UD-parsed)
-- lemmatization models based on larger quantities of training data than is available in UD (training data that are lemmatized, but not UD-parsed)
-- optional JOS-project-based parsing of Slovenian (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#jos-dependency-parsing-system))
-- named entity recognition models for all languages except Macedonian (Stanza does not cover named entity recognition for any of the languages supported by classla)
-- Macedonian models (Macedonian is not available in UD yet)
-- non-standard models for Croatian, Slovenian, Serbian (there is no UD data for these varieties)
-
-The above modifications led to some important improvements in the tool’s performance in comparison to original Stanza. For standard Slovenian, for example, running the full classla pipeline increases sentence segmentation F1 scores to 99.52 (94.29% error reduction), lemmatization to 99.17 (68.8% error reduction), XPOS tagging  to 97.38 (46.75% error reduction), UPOS tagging to 98.69 (23.4% error reduction), and LAS to 92.05 (23.56% error reduction).  See official [Stanza performance](https://stanfordnlp.github.io/stanza/performance.html) (evaluated on different data splits) for comparison.
-
-## Installation
-### pip
-We recommend that you install CLASSLA via pip, the Python package manager. To install, run:
-```bash
-pip install classla
-```
-This will also resolve all dependencies.
-
-__NOTE TO EXISTING USERS__: Once you install this classla version, you will HAVE TO re-download the models. All previously downloaded models will not be used anymore. We suggest you delete the old models. Their default location is at `~/classla_resources`.
-
-## Running CLASSLA
-
-### Getting started
-
-To run the CLASSLA pipeline for the first time on processing standard Slovenian, follow these steps:
-
-```
->>> import classla
->>> classla.download('sl')                            # download standard models for Slovenian, use hr for Croatian, sr for Serbian, bg for Bulgarian, mk for Macedonian
->>> nlp = classla.Pipeline('sl')                      # initialize the default Slovenian pipeline, use hr for Croatian, sr for Serbian, bg for Bulgarian, mk for Macedonian
->>> doc = nlp("France Prešeren je rojen v Vrbi.")     # run the pipeline
->>> print(doc.to_conll())                             # print the output in CoNLL-U format
-# newpar id = 1
-# sent_id = 1.1
-# text = France Prešeren je rojen v Vrbi.
-1	France	France	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	4	nsubj	_	NER=B-PER
-2	Prešeren	Prešeren	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat_name	_	NER=I-PER
-3	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	cop	_	NER=O
-4	rojen	rojen	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part	0	root	_	NER=O
-5	v	v	ADP	Sl	Case=Loc	6	case	_	NER=O
-6	Vrbi	Vrba	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
-7	.	.	PUNCT	Z	_	4	punct	_	NER=O
-
-```
-You can find examples of standard language processing for [Croatian](#example-of-standard-croatian), [Serbian](#example-of-standard-serbian), [Macedonian](#example-of-standard-macedonian) and [Bulgarian](#example-of-standard-bulgarian) at the end of this document.
-
-### Processing non-standard language
-
-Processing non-standard Slovenian differs to the above standard example just by an additional argument ```type="nonstandard"```:
-
-```
->>> import classla
->>> classla.download('sl', type='nonstandard')        # download non-standard models for Slovenian, use hr for Croatian and sr for Serbian
->>> nlp = classla.Pipeline('sl', type='nonstandard')  # initialize the default non-standard Slovenian pipeline, use hr for Croatian and sr for Serbian
->>> doc = nlp("kva smo mi zurali zadnje leto v zagrebu...")     # run the pipeline
->>> print(doc.to_conll())                             # print the output in CoNLL-U format 
-1	kva	kaj	PRON	Pq-nsa	Case=Acc|Gender=Neut|Number=Sing|PronType=Int	4	obj	_	NER=O
-2	smo	biti	AUX	Va-r1p-n	Mood=Ind|Number=Plur|Person=1|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
-3	mi	jaz	PRON	Pp1mpn	Case=Nom|Gender=Masc|Number=Plur|Person=1|PronType=Prs	nsubj	_	NER=O
-4	zurali	žurati	VERB	Vmpp-pm	Aspect=Imp|Gender=Masc|Number=Plur|VerbForm=Part	root	_	NER=O
-5	zadnje	zadnji	ADJ	Agpnsa	Case=Acc|Degree=Pos|Gender=Neut|Number=Sing	6	amod	_	NER=O
-6	leto	leto	NOUN	Ncnsa	Case=Acc|Gender=Neut|Number=Sing	4	obl	NER=O
-7	v	v	ADP	Sl	Case=Loc	8	case	_	NER=O
-8	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	4	obl	NER=B-LOC|SpaceAfter=No
-9	...	.	PUNCT	Z	_	4	punct	_	NER=O
-
-```
-
-You can find examples of non-standard language processing for [Croatian](#example-of-non-standard-croatian) and [Serbian](#example-of-non-standard-serbian)  at the end of this document.
-
-For additional usage examples you can also consult the ```pipeline_demo.py``` file.
-
-## Processors
-
-The CLASSLA pipeline is built from multiple units. These units are called processors. By default CLASSLA runs the ```tokenize```, ```ner```, ```pos```, ```lemma``` and ```depparse``` processors.
-
-You can specify which processors CLASSLA should run, via the ```processors``` attribute as in the following example, performing tokenization, named entity recognition, part-of-speech tagging and lemmatization.
-
-```python
->>> nlp = classla.Pipeline('sl', processors='tokenize,ner,pos,lemma')
-```
-
-Another popular option might be to perform tokenization, part-of-speech tagging, lemmatization and dependency parsing.
-
-```python
->>> nlp = classla.Pipeline('sl', processors='tokenize,pos,lemma,depparse')
-```
-
-### Tokenization and sentence splitting
-
-The tokenization and sentence splitting processor ```tokenize``` is the first processor and is required for any further processing.
-
-In case you already have tokenized text, you should separate tokens via spaces and pass the attribute ```tokenize_pretokenized=True```.
-
-By default CLASSLA uses a rule-based tokenizer - [obeliks](https://github.com/clarinsi/obeliks) for Slovenian standard language pipeline. In other cases we use [reldi-tokeniser](https://github.com/clarinsi/reldi-tokeniser).
-
-<!--Most important attributes:
-```
-tokenize_pretokenized   - [boolean]     ignores tokenizer
-```-->
-
-### Part-of-speech tagging
-
-The POS tagging processor ```pos``` will general output that contains morphosyntactic description following the [MULTEXT-East standard](http://nl.ijs.si/ME/V6/msd/html/msd.lang-specific.html) and universal part-of-speech tags and universal features following the [Universal Dependencies standard](https://universaldependencies.org). This processing requires the usage of the ```tokenize``` processor.
-
-<!--Most important attributes:
-```
-pos_model_path          - [str]         alternative path to model file
-pos_pretrain_path       - [str]         alternative path to pretrain file
-```-->
-
-### Lemmatization
-
-The lemmatization processor ```lemma``` will produce lemmas (basic forms) for each token in the input. It requires the usage of both the ```tokenize``` and ```pos``` processors.
-
-### Dependency parsing
-
-The dependency parsing processor ```depparse``` performs syntactic dependency parsing of sentences following the [Universal Dependencies formalism](https://universaldependencies.org/introduction.html#:~:text=Universal%20Dependencies%20(UD)%20is%20a,from%20a%20language%20typology%20perspective.). It requires the ```tokenize``` and ```pos``` processors.
-
-### Named entity recognition
-
-The named entity recognition processor ```ner``` identifies named entities in text following the [IOB2](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)) format. It requires only the ```tokenize``` processor.
-
-## Citing
-
-If you use this tool, please cite the following paper:
-
-```
-@inproceedings{ljubesic-dobrovoljc-2019-neural,
-    title = "What does Neural Bring? Analysing Improvements in Morphosyntactic Annotation and Lemmatisation of {S}lovenian, {C}roatian and {S}erbian",
-    author = "Ljube{\v{s}}i{\'c}, Nikola  and
-      Dobrovoljc, Kaja",
-    booktitle = "Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing",
-    month = aug,
-    year = "2019",
-    address = "Florence, Italy",
-    publisher = "Association for Computational Linguistics",
-    url = "https://www.aclweb.org/anthology/W19-3704",
-    doi = "10.18653/v1/W19-3704",
-    pages = "29--34"
-    }
-```
-
-## Croatian examples
-
-### Example of standard Croatian 
-
-```
->>> import classla
->>> nlp = classla.Pipeline('hr') # run classla.download('hr') beforehand if necessary
->>> doc = nlp("Ante Starčević rođen je u Velikom Žitniku.")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = Ante Starčević rođen je u Velikom Žitniku.
-1	Ante	Ante	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj_pass	_	NER=B-PER
-2	Starčević	Starčević	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
-3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
-4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux_pass	_	NER=O
-5	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
-6	Velikom	velik	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	7	amod	_	NER=B-LOC
-7	Žitniku	Žitnik	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	3	obl	_	NER=I-LOC|SpaceAfter=No
-8	.	.	PUNCT	Z	_	3	punct	_	NER=O
-
-```
-### Example of non-standard Croatian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('hr', type='nonstandard') # run classla.download('hr', type='nonstandard') beforehand if necessary
->>> doc = nlp("kaj sam ja tulumaril jucer u ljubljani...")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = kaj sam ja tulumaril jucer u ljubljani...
-1	kaj	što	PRON	Pi3n-a	Case=Acc|Gender=Neut|PronType=Int,Rel	4	obj	_	NER=O
-2	sam	biti	AUX	Var1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
-3	ja	ja	PRON	Pp1-sn	Case=Nom|Number=Sing|Person=1|PronType=Prs	4	nsubj	_	NER=O
-4	tulumaril	tulumariti	VERB	Vmp-sm	Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	0	root	_	NER=O
-5	jucer	jučer	ADV	Rgp	Degree=Pos	4	advmod	_	NER=O
-6	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
-7	ljubljani	Ljubljana	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
-8	...	.	PUNCT	Z	_	4	punct	_	NER=O
-
-```
-
-## Serbian examples
-
-### Example of standard Serbian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('sr') # run classla.download('sr') beforehand if necessary
->>> doc = nlp("Slobodan Jovanović rođen je u Novom Sadu.")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = Slobodan Jovanović rođen je u Novom Sadu.
-1	Slobodan	Slobodan	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj	_	NER=B-PER
-2	Jovanović	Jovanović	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
-3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
-4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux	_	NER=O
-5	u	u	ADP	Sl	Case=Loc	6	case	_	NER=O
-6	Novom	nov	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	3	obl	_	NER=B-LOC
-7	Sadu	Sad	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	6	flat	_	NER=I-LOC|SpaceAfter=No
-8	.	.	PUNCT	Z	_	3	punct	_	NER=O
-
-```
-
-### Example of non-standard Serbian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('sr', type='nonstandard') # run classla.download('sr', type='nonstandard') beforehand if necessary
->>> doc = nlp("ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu...")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu...
-1	ne	ne	PART	Qz	Polarity=Neg	2	advmod	_	NER=O
-2	mogu	moći	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	root	_	NER=O
-3	da	da	SCONJ	Cs	_	4	mark	_	NER=O
-4	verujem	verovati	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	2	xcomp	_	NER=O
-5	kakvo	kakav	DET	Pi-nsn	Case=Nom|Gender=Neut|Number=Sing|PronType=Int,Rel	ccomp	_	NER=O
-6	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	aux	_	NER=O
-7	zezanje	zezanje	NOUN	Ncnsn	Case=Nom|Gender=Neut|Number=Sing	5	nsubj	NER=O
-8	bilo	biti	AUX	Vap-sn	Gender=Neut|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	5	cop	_	NER=O
-9	prosle	prošli	ADJ	Agpfsgy	Case=Gen|Definite=Def|Degree=Pos|Gender=Fem|Number=Sing	10	amod	_	NER=O
-10	godine	godina	NOUN	Ncfsg	Case=Gen|Gender=Fem|Number=Sing	8	obl	_	NER=O
-11	u	u	ADP	Sl	Case=Loc	12	case	_	NER=O
-12	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	8	obl	NER=B-LOC|SpaceAfter=No
-13	...	.	PUNCT	Z	_	2	punct	_	NER=O
-
-```
-
-## Bulgarian examples
-
-### Example of standard Bulgarian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('bg') # run classla.download('bg') beforehand if necessary
->>> doc = nlp("Алеко Константинов е роден в Свищов.")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = Алеко Константинов е роден в Свищов.
-1	Алеко	алеко	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	nsubj:pass	_	NER=B-PER
-2	Константинов	константинов	PROPN	Hmsi	Definite=Ind|Gender=Masc|Number=Sing	flat	_	NER=I-PER
-3	е	съм	AUX	Vxitf-r3s	Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act	4	aux:pass	_	NER=O
-4	роден	родя-(се)	VERB	Vpptcv--smi	Aspect=Perf|Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
-5	в	в	ADP	R	_	6	case	_	NER=O
-6	Свищов	свищов	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	iobj	NER=B-LOC|SpaceAfter=No
-7	.	.	PUNCT	punct	_	4	punct	_	NER=O
-
-```
-
-## Macedonian examples
-
-### Example of standard Macedonian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('mk') # run classla.download('mk') beforehand if necessary
->>> doc = nlp('Крсте Петков Мисирков е роден во Постол.')
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = Крсте Петков Мисирков е роден во Постол.
-1	Крсте	крсте	ADJ	Afpms-n	Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
-2	Петков	петков	NOUN	Ncmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
-3	Мисирков	мисирков	NOUN	Ncmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
-4	е	сум	AUX	Vapip3s-n	Aspect=Prog|Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres	_	_
-5	роден	роден	ADJ	Ap-ms-n	Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part	_	_	_	_
-6	во	во	ADP	Sps	AdpType=Prep	_	_	_	_
-7	Постол	постол	NOUN	Ncmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	SpaceAfter=No
-8	.	.	PUNCT	Z	_	_	_	_	_
-
-```
-
-## Training instructions
-
-[Training instructions](https://github.com/clarinsi/classla-stanfordnlp/blob/master/README.train.md)
-
-## Superuser instructions
-
-[Superuser instructions](https://github.com/clarinsi/classla-stanfordnlp/blob/master/README.superuser.md)
+Metadata-Version: 2.1
+Name: classla
+Version: 2.1
+Summary: Adapted Stanford NLP Python Library with improvements for specific languages.
+Home-page: https://github.com/clarinsi/classla-stanfordnlp.git
+Author: CLARIN.SI
+Author-email: info@clarin.si
+License: Apache License 2.0
+Keywords: natural-language-processing nlp natural-language-understanding stanford-nlp deep-learning clarinsi
+Classifier: Development Status :: 4 - Beta
+Classifier: Intended Audience :: Developers
+Classifier: Intended Audience :: Education
+Classifier: Intended Audience :: Science/Research
+Classifier: Intended Audience :: Information Technology
+Classifier: Topic :: Scientific/Engineering
+Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
+Classifier: Topic :: Scientific/Engineering :: Information Analysis
+Classifier: Topic :: Text Processing
+Classifier: Topic :: Text Processing :: Linguistic
+Classifier: Topic :: Software Development
+Classifier: Topic :: Software Development :: Libraries
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Requires-Python: >=3.6
+Description-Content-Type: text/markdown
+Provides-Extra: dev
+Provides-Extra: test
+License-File: LICENSE
+
+# A [CLASSLA](http://www.clarin.si/info/k-centre/) Fork of [Stanza](https://github.com/stanfordnlp/stanza) for Processing Slovenian, Croatian, Serbian, Macedonian and Bulgarian
+
+## Description
+
+This pipeline allows for processing of standard Slovenian, Croatian, Serbian and Bulgarian on the levels of
+
+- tokenization and sentence splitting
+- part-of-speech tagging
+- lemmatization
+- dependency parsing
+- named entity recognition
+
+It also allows for (alpha) processing of standard Macedonian on the levels of 
+
+- tokenization and sentence splitting
+- part-of-speech tagging
+- lemmatization
+
+Finally, it allows for processing of non-standard (Internet) Slovenian, Croatian and Serbian on the same levels as standard language (all models are tailored to non-standard language except for dependency parsing where the standard module is used).
+
+## Differences to Stanza
+
+The differences of this pipeline to the original Stanza pipeline are the following:
+
+- usage of language-specific rule-based tokenizers and sentence splitters, [obeliks](https://pypi.org/project/obeliks/) for standard Slovenian and [reldi-tokeniser](https://pypi.org/project/reldi-tokeniser/) for the remaining varieties and languages (Stanza uses inferior machine-learning-based tokenization and sentence splitting trained on UD data)
+- default pre-tagging and pre-lemmatization on the level of tokenizers for the following phenomena: punctuation, symbol, e-mail, URL, mention, hashtag, emoticon, emoji (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#usage-of-tagging-control-via-the-tokenizer))
+- optional control of the tagger for Slovenian via an inflectional lexicon on the levels of XPOS, UPOS, FEATS (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#usage-of-inflectional-lexicon))
+- closed class handling depending on the usage of the options described in the last two bullets, as documented [here](https://github.com/clarinsi/classla/blob/master/README.closed_classes.md)
+- usage of external inflectional lexicons for lookup lemmatization, seq2seq being used very infrequently on OOVs only (Stanza uses only UD training data for lookup lemmatization)
+- morphosyntactic tagging models based on larger quantities of training data than is available in UD (training data that are morphosyntactically tagged, but not UD-parsed)
+- lemmatization models based on larger quantities of training data than is available in UD (training data that are lemmatized, but not UD-parsed)
+- optional JOS-project-based parsing of Slovenian (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#jos-dependency-parsing-system))
+- named entity recognition models for all languages except Macedonian (Stanza does not cover named entity recognition for any of the languages supported by classla)
+- Macedonian models (Macedonian is not available in UD yet)
+- non-standard models for Croatian, Slovenian, Serbian (there is no UD data for these varieties)
+
+The above modifications led to some important improvements in the tool’s performance in comparison to original Stanza. For standard Slovenian, for example, running the full classla pipeline increases sentence segmentation F1 scores to 99.52 (94.29% error reduction), lemmatization to 99.17 (68.8% error reduction), XPOS tagging  to 97.38 (46.75% error reduction), UPOS tagging to 98.69 (23.4% error reduction), and LAS to 92.05 (23.56% error reduction).  See official [Stanza performance](https://stanfordnlp.github.io/stanza/performance.html) (evaluated on different data splits) for comparison.
+
+## Installation
+### pip
+We recommend that you install CLASSLA via pip, the Python package manager. To install, run:
+```bash
+pip install classla
+```
+This will also resolve all dependencies.
+
+__NOTE TO EXISTING USERS__: Once you install this classla version, you will HAVE TO re-download the models. All previously downloaded models will not be used anymore. We suggest you delete the old models. Their default location is at `~/classla_resources`.
+
+## Running CLASSLA
+
+### Getting started
+
+To run the CLASSLA pipeline for the first time on processing standard Slovenian, follow these steps:
+
+```
+>>> import classla
+>>> classla.download('sl')                            # download standard models for Slovenian, use hr for Croatian, sr for Serbian, bg for Bulgarian, mk for Macedonian
+>>> nlp = classla.Pipeline('sl')                      # initialize the default Slovenian pipeline, use hr for Croatian, sr for Serbian, bg for Bulgarian, mk for Macedonian
+>>> doc = nlp("France Prešeren je rojen v Vrbi.")     # run the pipeline
+>>> print(doc.to_conll())                             # print the output in CoNLL-U format
+# newpar id = 1
+# sent_id = 1.1
+# text = France Prešeren je rojen v Vrbi.
+1	France	France	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	4	nsubj	_	NER=B-PER
+2	Prešeren	Prešeren	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat:name	_	NER=I-PER
+3	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	cop	_	NER=O
+4	rojen	rojen	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part	0	root	_	NER=O
+5	v	v	ADP	Sl	Case=Loc	6	case	_	NER=O
+6	Vrbi	Vrba	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
+7	.	.	PUNCT	Z	_	4	punct	_	NER=O
+
+```
+You can find examples of standard language processing for [Croatian](#example-of-standard-croatian), [Serbian](#example-of-standard-serbian), [Macedonian](#example-of-standard-macedonian) and [Bulgarian](#example-of-standard-bulgarian) at the end of this document.
+
+### Processing non-standard language
+
+Processing non-standard Slovenian differs to the above standard example just by an additional argument ```type="nonstandard"```:
+
+```
+>>> import classla
+>>> classla.download('sl', type='nonstandard')        # download non-standard models for Slovenian, use hr for Croatian and sr for Serbian
+>>> nlp = classla.Pipeline('sl', type='nonstandard')  # initialize the default non-standard Slovenian pipeline, use hr for Croatian and sr for Serbian
+>>> doc = nlp("kva smo mi zurali zadnje leto v zagrebu...")     # run the pipeline
+>>> print(doc.to_conll())                             # print the output in CoNLL-U format
+# newpar id = 1
+# sent_id = 1.1
+# text = kva smo mi zurali zadnje leto v zagrebu...
+1	kva	kaj	PRON	Pq-nsa	Case=Acc|Gender=Neut|Number=Sing|PronType=Int	4	obj	_	NER=O
+2	smo	biti	AUX	Va-r1p-n	Mood=Ind|Number=Plur|Person=1|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
+3	mi	jaz	PRON	Pp1mpn	Case=Nom|Gender=Masc|Number=Plur|Person=1|PronType=Prs	4	nsubj	_	NER=O
+4	zurali	zurati	VERB	Vmpp-pm	Aspect=Imp|Gender=Masc|Number=Plur|VerbForm=Part	0	root	_	NER=O
+5	zadnje	zadnji	ADJ	Agpnsa	Case=Acc|Degree=Pos|Gender=Neut|Number=Sing	6	amod	_	NER=O
+6	leto	leto	NOUN	Ncnsa	Case=Acc|Gender=Neut|Number=Sing	4	obl	_	NER=O
+7	v	v	ADP	Sl	Case=Loc	8	case	_	NER=O
+8	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
+9	...	...	PUNCT	Z	_	4	punct	_	NER=O
+
+```
+
+You can find examples of non-standard language processing for [Croatian](#example-of-non-standard-croatian) and [Serbian](#example-of-non-standard-serbian)  at the end of this document.
+
+For additional usage examples you can also consult the ```pipeline_demo.py``` file.
+
+### Processing online texts
+
+A special web processing mode for processing texts obtained from the internet can be activated with the ```type="web"``` argument:
+
+```
+>>> import classla
+>>> classla.download('sl', type='web')        # download web models for Slovenian, use hr for Croatian and sr for Serbian
+>>> nlp = classla.Pipeline('sl', type='web')  # initialize the default Slovenian web pipeline, use hr for Croatian and sr for Serbian
+>>> doc = nlp("Kdor hoce prenesti preko racunalnika http://t.co/LwWyzs0cA0")     # run the pipeline
+>>> print(doc.to_conll())                             # print the output in CoNLL-U format
+# newpar id = 1
+# sent_id = 1.1
+# text = Kdor hoce prenesti preko racunalnika http://t.co/LwWyzs0cA0
+1	Kdor	kdor	PRON	Pr-msn	Case=Nom|Gender=Masc|Number=Sing|PronType=Rel	2	nsubj	_	NER=O
+2	hoce	hoteti	VERB	Vmpr3s-n	Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	0	root	_	NER=O
+3	prenesti	prenesti	VERB	Vmen	Aspect=Perf|VerbForm=Inf	2	xcomp	_	NER=O
+4	preko	preko	ADP	Sg	Case=Gen	5	case	_	NER=O
+5	racunalnika	računalnik	NOUN	Ncmsg	Case=Gen|Gender=Masc|Number=Sing	3	obl	_	NER=O
+6	http://t.co/LwWyzs0cA0	http://t.co/LwWyzs0cA0	SYM	Xw	_	5	nmod	_	NER=O
+```
+
+## Processors
+
+The CLASSLA pipeline is built from multiple units. These units are called processors. By default CLASSLA runs the ```tokenize```, ```ner```, ```pos```, ```lemma``` and ```depparse``` processors.
+
+You can specify which processors CLASSLA should run, via the ```processors``` attribute as in the following example, performing tokenization, named entity recognition, part-of-speech tagging and lemmatization.
+
+```python
+>>> nlp = classla.Pipeline('sl', processors='tokenize,ner,pos,lemma')
+```
+
+Another popular option might be to perform tokenization, part-of-speech tagging, lemmatization and dependency parsing.
+
+```python
+>>> nlp = classla.Pipeline('sl', processors='tokenize,pos,lemma,depparse')
+```
+
+### Tokenization and sentence splitting
+
+The tokenization and sentence splitting processor ```tokenize``` is the first processor and is required for any further processing.
+
+In case you already have tokenized text, you should separate tokens via spaces and pass the attribute ```tokenize_pretokenized=True```.
+
+By default CLASSLA uses a rule-based tokenizer - [obeliks](https://github.com/clarinsi/obeliks) for Slovenian standard language pipeline. In other cases we use [reldi-tokeniser](https://github.com/clarinsi/reldi-tokeniser).
+
+<!--Most important attributes:
+```
+tokenize_pretokenized   - [boolean]     ignores tokenizer
+```-->
+
+### Part-of-speech tagging
+
+The POS tagging processor ```pos``` will general output that contains morphosyntactic description following the [MULTEXT-East standard](http://nl.ijs.si/ME/V6/msd/html/msd.lang-specific.html) and universal part-of-speech tags and universal features following the [Universal Dependencies standard](https://universaldependencies.org). This processing requires the usage of the ```tokenize``` processor.
+
+<!--Most important attributes:
+```
+pos_model_path          - [str]         alternative path to model file
+pos_pretrain_path       - [str]         alternative path to pretrain file
+```-->
+
+### Lemmatization
+
+The lemmatization processor ```lemma``` will produce lemmas (basic forms) for each token in the input. It requires the usage of both the ```tokenize``` and ```pos``` processors.
+
+### Dependency parsing
+
+The dependency parsing processor ```depparse``` performs syntactic dependency parsing of sentences following the [Universal Dependencies formalism](https://universaldependencies.org/introduction.html#:~:text=Universal%20Dependencies%20(UD)%20is%20a,from%20a%20language%20typology%20perspective.). It requires the ```tokenize``` and ```pos``` processors.
+
+### Named entity recognition
+
+The named entity recognition processor ```ner``` identifies named entities in text following the [IOB2](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)) format. It requires only the ```tokenize``` processor.
+
+## Citing
+
+If you use this tool, please cite the following paper:
+
+```
+@inproceedings{ljubesic-dobrovoljc-2019-neural,
+    title = "What does Neural Bring? Analysing Improvements in Morphosyntactic Annotation and Lemmatisation of {S}lovenian, {C}roatian and {S}erbian",
+    author = "Ljube{\v{s}}i{\'c}, Nikola  and
+      Dobrovoljc, Kaja",
+    booktitle = "Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing",
+    month = aug,
+    year = "2019",
+    address = "Florence, Italy",
+    publisher = "Association for Computational Linguistics",
+    url = "https://www.aclweb.org/anthology/W19-3704",
+    doi = "10.18653/v1/W19-3704",
+    pages = "29--34"
+    }
+```
+
+## Croatian examples
+
+### Example of standard Croatian 
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('hr') # run classla.download('hr') beforehand if necessary
+>>> doc = nlp("Ante Starčević rođen je u Velikom Žitniku.")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = Ante Starčević rođen je u Velikom Žitniku.
+1	Ante	Ante	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj	_	NER=B-PER
+2	Starčević	Starčević	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
+3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
+4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux	_	NER=O
+5	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
+6	Velikom	velik	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	7	amod	_	NER=B-LOC
+7	Žitniku	Žitnik	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	3	obl	_	NER=I-LOC|SpaceAfter=No
+8	.	.	PUNCT	Z	_	3	punct	_	NER=O
+
+```
+### Example of non-standard Croatian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('hr', type='nonstandard') # run classla.download('hr', type='nonstandard') beforehand if necessary
+>>> doc = nlp("kaj sam ja tulumaril jucer u ljubljani...")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = kaj sam ja tulumaril jucer u ljubljani...
+1	kaj	što	PRON	Pq3n-a	Case=Acc|Gender=Neut|PronType=Int,Rel	4	obj	_	NER=O
+2	sam	biti	AUX	Var1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
+3	ja	ja	PRON	Pp1-sn	Case=Nom|Number=Sing|Person=1|PronType=Prs	4	nsubj	_	NER=O
+4	tulumaril	tulumariti	VERB	Vmp-sm	Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	0	root	_	NER=O
+5	jucer	jučer	ADV	Rgp	Degree=Pos	4	advmod	_	NER=O
+6	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
+7	ljubljani	Ljubljana	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
+8	...	...	PUNCT	Z	_	4	punct	_	NER=O
+
+```
+
+## Serbian examples
+
+### Example of standard Serbian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('sr') # run classla.download('sr') beforehand if necessary
+>>> doc = nlp("Slobodan Jovanović rođen je u Novom Sadu.")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = Slobodan Jovanović rođen je u Novom Sadu.
+1	Slobodan	Slobodan	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj	_	NER=B-PER
+2	Jovanović	Jovanović	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
+3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
+4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux	_	NER=O
+5	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
+6	Novom	nov	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	7	amod	_	NER=B-LOC
+7	Sadu	Sad	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	3	obl	_	NER=I-LOC|SpaceAfter=No
+8	.	.	PUNCT	Z	_	3	punct	_	NER=O
+
+```
+
+### Example of non-standard Serbian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('sr', type='nonstandard') # run classla.download('sr', type='nonstandard') beforehand if necessary
+>>> doc = nlp("ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu...")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu...
+1	ne	ne	PART	Qz	Polarity=Neg	2	advmod	_	NER=O
+2	mogu	moći	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	0	root	_	NER=O
+3	da	da	SCONJ	Cs	_	4	mark	_	NER=O
+4	verujem	verovati	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	2	xcomp	_	NER=O
+5	kakvo	kakav	DET	Pi-nsn	Case=Nom|Gender=Neut|Number=Sing|PronType=Int,Rel	4	ccomp	_	NER=O
+6	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	5	aux	_	NER=O
+7	zezanje	zezanje	NOUN	Ncnsn	Case=Nom|Gender=Neut|Number=Sing	8	nsubj	_	NER=O
+8	bilo	biti	AUX	Vap-sn	Gender=Neut|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	5	cop	_	NER=O
+9	prosle	prošli	ADJ	Agpfsgy	Case=Gen|Definite=Def|Degree=Pos|Gender=Fem|Number=Sing	10	amod	_	NER=O
+10	godine	godina	NOUN	Ncfsg	Case=Gen|Gender=Fem|Number=Sing	8	obl	_	NER=O
+11	u	u	ADP	Sl	Case=Loc	12	case	_	NER=O
+12	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	8	obl	_	NER=B-LOC|SpaceAfter=No
+13	...	...	PUNCT	Z	_	2	punct	_	NER=O
+
+```
+
+## Bulgarian examples
+
+### Example of standard Bulgarian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('bg') # run classla.download('bg') beforehand if necessary
+>>> doc = nlp("Алеко Константинов е роден в Свищов.")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = Алеко Константинов е роден в Свищов.
+1	Алеко	алеко	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	nsubj:pass	_	NER=B-PER
+2	Константинов	константинов	PROPN	Hmsi	Definite=Ind|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
+3	е	съм	AUX	Vxitf-r3s	Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act	4	aux:pass	_	NER=O
+4	роден	родя-(се)	VERB	Vpptcv--smi	Aspect=Perf|Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
+5	в	в	ADP	R	_	6	case	_	NER=O
+6	Свищов	свищов	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	iobj	_	NER=B-LOC|SpaceAfter=No
+7	.	.	PUNCT	punct	_	4	punct	_	NER=O
+
+```
+
+## Macedonian examples
+
+### Example of standard Macedonian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('mk') # run classla.download('mk') beforehand if necessary
+>>> doc = nlp('Крсте Петков Мисирков е роден во Постол.')
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = Крсте Петков Мисирков е роден во Постол.
+1	Крсте	Крсте	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
+2	Петков	Петков	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
+3	Мисирков	Мисирков	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
+4	е	сум	AUX	Vapip3s-n	Aspect=Prog|Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres	_	_	_	_
+5	роден	роден	ADJ	Ap-ms-n	Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part	_	_	_	_
+6	во	во	ADP	Sps	AdpType=Prep	_	_	_	_
+7	Постол	Постол	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	SpaceAfter=No
+8	.	.	PUNCT	Z	_	_	_	_	_
+
+```
+
+## Training instructions
+
+[Training instructions](https://github.com/clarinsi/classla-stanfordnlp/blob/master/README.train.md)
+
+## Superuser instructions
+
+[Superuser instructions](https://github.com/clarinsi/classla-stanfordnlp/blob/master/README.superuser.md)
```

### Comparing `classla-2.0/README.md` & `classla-2.1/classla.egg-info/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,315 +1,369 @@
-# A [CLASSLA](http://www.clarin.si/info/k-centre/) Fork of [Stanza](https://github.com/stanfordnlp/stanza) for Processing Slovenian, Croatian, Serbian, Macedonian and Bulgarian
-
-## Description
-
-This pipeline allows for processing of standard Slovenian, Croatian, Serbian and Bulgarian on the levels of
-
-- tokenization and sentence splitting
-- part-of-speech tagging
-- lemmatization
-- dependency parsing
-- named entity recognition
-
-It also allows for (alpha) processing of standard Macedonian on the levels of 
-
-- tokenization and sentence splitting
-- part-of-speech tagging
-- lemmatization
-
-Finally, it allows for processing of non-standard (Internet) Slovenian, Croatian and Serbian on the same levels as standard language (all models are tailored to non-standard language except for dependency parsing where the standard module is used).
-
-## Differences to Stanza
-
-The differences of this pipeline to the original Stanza pipeline are the following:
-
-- usage of language-specific rule-based tokenizers and sentence splitters, [obeliks](https://pypi.org/project/obeliks/) for standard Slovenian and [reldi-tokeniser](https://pypi.org/project/reldi-tokeniser/) for the remaining varieties and languages (Stanza uses inferior machine-learning-based tokenization and sentence splitting trained on UD data)
-- default pre-tagging and pre-lemmatization on the level of tokenizers for the following phenomena: punctuation, symbol, e-mail, URL, mention, hashtag, emoticon, emoji (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#usage-of-tagging-control-via-the-tokenizer))
-- optional control of the tagger for Slovenian via an inflectional lexicon on the levels of XPOS, UPOS, FEATS (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#usage-of-inflectional-lexicon))
-- closed class handling depending on the usage of the options described in the last two bullets, as documented [here](https://github.com/clarinsi/classla/blob/master/README.closed_classes.md)
-- usage of external inflectional lexicons for lookup lemmatization, seq2seq being used very infrequently on OOVs only (Stanza uses only UD training data for lookup lemmatization)
-- morphosyntactic tagging models based on larger quantities of training data than is available in UD (training data that are morphosyntactically tagged, but not UD-parsed)
-- lemmatization models based on larger quantities of training data than is available in UD (training data that are lemmatized, but not UD-parsed)
-- optional JOS-project-based parsing of Slovenian (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#jos-dependency-parsing-system))
-- named entity recognition models for all languages except Macedonian (Stanza does not cover named entity recognition for any of the languages supported by classla)
-- Macedonian models (Macedonian is not available in UD yet)
-- non-standard models for Croatian, Slovenian, Serbian (there is no UD data for these varieties)
-
-The above modifications led to some important improvements in the tool’s performance in comparison to original Stanza. For standard Slovenian, for example, running the full classla pipeline increases sentence segmentation F1 scores to 99.52 (94.29% error reduction), lemmatization to 99.17 (68.8% error reduction), XPOS tagging  to 97.38 (46.75% error reduction), UPOS tagging to 98.69 (23.4% error reduction), and LAS to 92.05 (23.56% error reduction).  See official [Stanza performance](https://stanfordnlp.github.io/stanza/performance.html) (evaluated on different data splits) for comparison.
-
-## Installation
-### pip
-We recommend that you install CLASSLA via pip, the Python package manager. To install, run:
-```bash
-pip install classla
-```
-This will also resolve all dependencies.
-
-__NOTE TO EXISTING USERS__: Once you install this classla version, you will HAVE TO re-download the models. All previously downloaded models will not be used anymore. We suggest you delete the old models. Their default location is at `~/classla_resources`.
-
-## Running CLASSLA
-
-### Getting started
-
-To run the CLASSLA pipeline for the first time on processing standard Slovenian, follow these steps:
-
-```
->>> import classla
->>> classla.download('sl')                            # download standard models for Slovenian, use hr for Croatian, sr for Serbian, bg for Bulgarian, mk for Macedonian
->>> nlp = classla.Pipeline('sl')                      # initialize the default Slovenian pipeline, use hr for Croatian, sr for Serbian, bg for Bulgarian, mk for Macedonian
->>> doc = nlp("France Prešeren je rojen v Vrbi.")     # run the pipeline
->>> print(doc.to_conll())                             # print the output in CoNLL-U format
-# newpar id = 1
-# sent_id = 1.1
-# text = France Prešeren je rojen v Vrbi.
-1	France	France	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	4	nsubj	_	NER=B-PER
-2	Prešeren	Prešeren	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat_name	_	NER=I-PER
-3	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	cop	_	NER=O
-4	rojen	rojen	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part	0	root	_	NER=O
-5	v	v	ADP	Sl	Case=Loc	6	case	_	NER=O
-6	Vrbi	Vrba	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
-7	.	.	PUNCT	Z	_	4	punct	_	NER=O
-
-```
-You can find examples of standard language processing for [Croatian](#example-of-standard-croatian), [Serbian](#example-of-standard-serbian), [Macedonian](#example-of-standard-macedonian) and [Bulgarian](#example-of-standard-bulgarian) at the end of this document.
-
-### Processing non-standard language
-
-Processing non-standard Slovenian differs to the above standard example just by an additional argument ```type="nonstandard"```:
-
-```
->>> import classla
->>> classla.download('sl', type='nonstandard')        # download non-standard models for Slovenian, use hr for Croatian and sr for Serbian
->>> nlp = classla.Pipeline('sl', type='nonstandard')  # initialize the default non-standard Slovenian pipeline, use hr for Croatian and sr for Serbian
->>> doc = nlp("kva smo mi zurali zadnje leto v zagrebu...")     # run the pipeline
->>> print(doc.to_conll())                             # print the output in CoNLL-U format 
-1	kva	kaj	PRON	Pq-nsa	Case=Acc|Gender=Neut|Number=Sing|PronType=Int	4	obj	_	NER=O
-2	smo	biti	AUX	Va-r1p-n	Mood=Ind|Number=Plur|Person=1|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
-3	mi	jaz	PRON	Pp1mpn	Case=Nom|Gender=Masc|Number=Plur|Person=1|PronType=Prs	nsubj	_	NER=O
-4	zurali	žurati	VERB	Vmpp-pm	Aspect=Imp|Gender=Masc|Number=Plur|VerbForm=Part	root	_	NER=O
-5	zadnje	zadnji	ADJ	Agpnsa	Case=Acc|Degree=Pos|Gender=Neut|Number=Sing	6	amod	_	NER=O
-6	leto	leto	NOUN	Ncnsa	Case=Acc|Gender=Neut|Number=Sing	4	obl	NER=O
-7	v	v	ADP	Sl	Case=Loc	8	case	_	NER=O
-8	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	4	obl	NER=B-LOC|SpaceAfter=No
-9	...	.	PUNCT	Z	_	4	punct	_	NER=O
-
-```
-
-You can find examples of non-standard language processing for [Croatian](#example-of-non-standard-croatian) and [Serbian](#example-of-non-standard-serbian)  at the end of this document.
-
-For additional usage examples you can also consult the ```pipeline_demo.py``` file.
-
-## Processors
-
-The CLASSLA pipeline is built from multiple units. These units are called processors. By default CLASSLA runs the ```tokenize```, ```ner```, ```pos```, ```lemma``` and ```depparse``` processors.
-
-You can specify which processors CLASSLA should run, via the ```processors``` attribute as in the following example, performing tokenization, named entity recognition, part-of-speech tagging and lemmatization.
-
-```python
->>> nlp = classla.Pipeline('sl', processors='tokenize,ner,pos,lemma')
-```
-
-Another popular option might be to perform tokenization, part-of-speech tagging, lemmatization and dependency parsing.
-
-```python
->>> nlp = classla.Pipeline('sl', processors='tokenize,pos,lemma,depparse')
-```
-
-### Tokenization and sentence splitting
-
-The tokenization and sentence splitting processor ```tokenize``` is the first processor and is required for any further processing.
-
-In case you already have tokenized text, you should separate tokens via spaces and pass the attribute ```tokenize_pretokenized=True```.
-
-By default CLASSLA uses a rule-based tokenizer - [obeliks](https://github.com/clarinsi/obeliks) for Slovenian standard language pipeline. In other cases we use [reldi-tokeniser](https://github.com/clarinsi/reldi-tokeniser).
-
-<!--Most important attributes:
-```
-tokenize_pretokenized   - [boolean]     ignores tokenizer
-```-->
-
-### Part-of-speech tagging
-
-The POS tagging processor ```pos``` will general output that contains morphosyntactic description following the [MULTEXT-East standard](http://nl.ijs.si/ME/V6/msd/html/msd.lang-specific.html) and universal part-of-speech tags and universal features following the [Universal Dependencies standard](https://universaldependencies.org). This processing requires the usage of the ```tokenize``` processor.
-
-<!--Most important attributes:
-```
-pos_model_path          - [str]         alternative path to model file
-pos_pretrain_path       - [str]         alternative path to pretrain file
-```-->
-
-### Lemmatization
-
-The lemmatization processor ```lemma``` will produce lemmas (basic forms) for each token in the input. It requires the usage of both the ```tokenize``` and ```pos``` processors.
-
-### Dependency parsing
-
-The dependency parsing processor ```depparse``` performs syntactic dependency parsing of sentences following the [Universal Dependencies formalism](https://universaldependencies.org/introduction.html#:~:text=Universal%20Dependencies%20(UD)%20is%20a,from%20a%20language%20typology%20perspective.). It requires the ```tokenize``` and ```pos``` processors.
-
-### Named entity recognition
-
-The named entity recognition processor ```ner``` identifies named entities in text following the [IOB2](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)) format. It requires only the ```tokenize``` processor.
-
-## Citing
-
-If you use this tool, please cite the following paper:
-
-```
-@inproceedings{ljubesic-dobrovoljc-2019-neural,
-    title = "What does Neural Bring? Analysing Improvements in Morphosyntactic Annotation and Lemmatisation of {S}lovenian, {C}roatian and {S}erbian",
-    author = "Ljube{\v{s}}i{\'c}, Nikola  and
-      Dobrovoljc, Kaja",
-    booktitle = "Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing",
-    month = aug,
-    year = "2019",
-    address = "Florence, Italy",
-    publisher = "Association for Computational Linguistics",
-    url = "https://www.aclweb.org/anthology/W19-3704",
-    doi = "10.18653/v1/W19-3704",
-    pages = "29--34"
-    }
-```
-
-## Croatian examples
-
-### Example of standard Croatian 
-
-```
->>> import classla
->>> nlp = classla.Pipeline('hr') # run classla.download('hr') beforehand if necessary
->>> doc = nlp("Ante Starčević rođen je u Velikom Žitniku.")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = Ante Starčević rođen je u Velikom Žitniku.
-1	Ante	Ante	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj_pass	_	NER=B-PER
-2	Starčević	Starčević	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
-3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
-4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux_pass	_	NER=O
-5	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
-6	Velikom	velik	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	7	amod	_	NER=B-LOC
-7	Žitniku	Žitnik	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	3	obl	_	NER=I-LOC|SpaceAfter=No
-8	.	.	PUNCT	Z	_	3	punct	_	NER=O
-
-```
-### Example of non-standard Croatian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('hr', type='nonstandard') # run classla.download('hr', type='nonstandard') beforehand if necessary
->>> doc = nlp("kaj sam ja tulumaril jucer u ljubljani...")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = kaj sam ja tulumaril jucer u ljubljani...
-1	kaj	što	PRON	Pi3n-a	Case=Acc|Gender=Neut|PronType=Int,Rel	4	obj	_	NER=O
-2	sam	biti	AUX	Var1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
-3	ja	ja	PRON	Pp1-sn	Case=Nom|Number=Sing|Person=1|PronType=Prs	4	nsubj	_	NER=O
-4	tulumaril	tulumariti	VERB	Vmp-sm	Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	0	root	_	NER=O
-5	jucer	jučer	ADV	Rgp	Degree=Pos	4	advmod	_	NER=O
-6	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
-7	ljubljani	Ljubljana	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
-8	...	.	PUNCT	Z	_	4	punct	_	NER=O
-
-```
-
-## Serbian examples
-
-### Example of standard Serbian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('sr') # run classla.download('sr') beforehand if necessary
->>> doc = nlp("Slobodan Jovanović rođen je u Novom Sadu.")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = Slobodan Jovanović rođen je u Novom Sadu.
-1	Slobodan	Slobodan	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj	_	NER=B-PER
-2	Jovanović	Jovanović	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
-3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
-4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux	_	NER=O
-5	u	u	ADP	Sl	Case=Loc	6	case	_	NER=O
-6	Novom	nov	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	3	obl	_	NER=B-LOC
-7	Sadu	Sad	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	6	flat	_	NER=I-LOC|SpaceAfter=No
-8	.	.	PUNCT	Z	_	3	punct	_	NER=O
-
-```
-
-### Example of non-standard Serbian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('sr', type='nonstandard') # run classla.download('sr', type='nonstandard') beforehand if necessary
->>> doc = nlp("ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu...")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu...
-1	ne	ne	PART	Qz	Polarity=Neg	2	advmod	_	NER=O
-2	mogu	moći	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	root	_	NER=O
-3	da	da	SCONJ	Cs	_	4	mark	_	NER=O
-4	verujem	verovati	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	2	xcomp	_	NER=O
-5	kakvo	kakav	DET	Pi-nsn	Case=Nom|Gender=Neut|Number=Sing|PronType=Int,Rel	ccomp	_	NER=O
-6	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	aux	_	NER=O
-7	zezanje	zezanje	NOUN	Ncnsn	Case=Nom|Gender=Neut|Number=Sing	5	nsubj	NER=O
-8	bilo	biti	AUX	Vap-sn	Gender=Neut|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	5	cop	_	NER=O
-9	prosle	prošli	ADJ	Agpfsgy	Case=Gen|Definite=Def|Degree=Pos|Gender=Fem|Number=Sing	10	amod	_	NER=O
-10	godine	godina	NOUN	Ncfsg	Case=Gen|Gender=Fem|Number=Sing	8	obl	_	NER=O
-11	u	u	ADP	Sl	Case=Loc	12	case	_	NER=O
-12	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	8	obl	NER=B-LOC|SpaceAfter=No
-13	...	.	PUNCT	Z	_	2	punct	_	NER=O
-
-```
-
-## Bulgarian examples
-
-### Example of standard Bulgarian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('bg') # run classla.download('bg') beforehand if necessary
->>> doc = nlp("Алеко Константинов е роден в Свищов.")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = Алеко Константинов е роден в Свищов.
-1	Алеко	алеко	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	nsubj:pass	_	NER=B-PER
-2	Константинов	константинов	PROPN	Hmsi	Definite=Ind|Gender=Masc|Number=Sing	flat	_	NER=I-PER
-3	е	съм	AUX	Vxitf-r3s	Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act	4	aux:pass	_	NER=O
-4	роден	родя-(се)	VERB	Vpptcv--smi	Aspect=Perf|Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
-5	в	в	ADP	R	_	6	case	_	NER=O
-6	Свищов	свищов	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	iobj	NER=B-LOC|SpaceAfter=No
-7	.	.	PUNCT	punct	_	4	punct	_	NER=O
-
-```
-
-## Macedonian examples
-
-### Example of standard Macedonian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('mk') # run classla.download('mk') beforehand if necessary
->>> doc = nlp('Крсте Петков Мисирков е роден во Постол.')
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = Крсте Петков Мисирков е роден во Постол.
-1	Крсте	крсте	ADJ	Afpms-n	Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
-2	Петков	петков	NOUN	Ncmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
-3	Мисирков	мисирков	NOUN	Ncmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
-4	е	сум	AUX	Vapip3s-n	Aspect=Prog|Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres	_	_
-5	роден	роден	ADJ	Ap-ms-n	Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part	_	_	_	_
-6	во	во	ADP	Sps	AdpType=Prep	_	_	_	_
-7	Постол	постол	NOUN	Ncmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	SpaceAfter=No
-8	.	.	PUNCT	Z	_	_	_	_	_
-
-```
-
-## Training instructions
-
-[Training instructions](https://github.com/clarinsi/classla-stanfordnlp/blob/master/README.train.md)
-
-## Superuser instructions
-
-[Superuser instructions](https://github.com/clarinsi/classla-stanfordnlp/blob/master/README.superuser.md)
+Metadata-Version: 2.1
+Name: classla
+Version: 2.1
+Summary: Adapted Stanford NLP Python Library with improvements for specific languages.
+Home-page: https://github.com/clarinsi/classla-stanfordnlp.git
+Author: CLARIN.SI
+Author-email: info@clarin.si
+License: Apache License 2.0
+Keywords: natural-language-processing nlp natural-language-understanding stanford-nlp deep-learning clarinsi
+Classifier: Development Status :: 4 - Beta
+Classifier: Intended Audience :: Developers
+Classifier: Intended Audience :: Education
+Classifier: Intended Audience :: Science/Research
+Classifier: Intended Audience :: Information Technology
+Classifier: Topic :: Scientific/Engineering
+Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
+Classifier: Topic :: Scientific/Engineering :: Information Analysis
+Classifier: Topic :: Text Processing
+Classifier: Topic :: Text Processing :: Linguistic
+Classifier: Topic :: Software Development
+Classifier: Topic :: Software Development :: Libraries
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Requires-Python: >=3.6
+Description-Content-Type: text/markdown
+Provides-Extra: dev
+Provides-Extra: test
+License-File: LICENSE
+
+# A [CLASSLA](http://www.clarin.si/info/k-centre/) Fork of [Stanza](https://github.com/stanfordnlp/stanza) for Processing Slovenian, Croatian, Serbian, Macedonian and Bulgarian
+
+## Description
+
+This pipeline allows for processing of standard Slovenian, Croatian, Serbian and Bulgarian on the levels of
+
+- tokenization and sentence splitting
+- part-of-speech tagging
+- lemmatization
+- dependency parsing
+- named entity recognition
+
+It also allows for (alpha) processing of standard Macedonian on the levels of 
+
+- tokenization and sentence splitting
+- part-of-speech tagging
+- lemmatization
+
+Finally, it allows for processing of non-standard (Internet) Slovenian, Croatian and Serbian on the same levels as standard language (all models are tailored to non-standard language except for dependency parsing where the standard module is used).
+
+## Differences to Stanza
+
+The differences of this pipeline to the original Stanza pipeline are the following:
+
+- usage of language-specific rule-based tokenizers and sentence splitters, [obeliks](https://pypi.org/project/obeliks/) for standard Slovenian and [reldi-tokeniser](https://pypi.org/project/reldi-tokeniser/) for the remaining varieties and languages (Stanza uses inferior machine-learning-based tokenization and sentence splitting trained on UD data)
+- default pre-tagging and pre-lemmatization on the level of tokenizers for the following phenomena: punctuation, symbol, e-mail, URL, mention, hashtag, emoticon, emoji (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#usage-of-tagging-control-via-the-tokenizer))
+- optional control of the tagger for Slovenian via an inflectional lexicon on the levels of XPOS, UPOS, FEATS (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#usage-of-inflectional-lexicon))
+- closed class handling depending on the usage of the options described in the last two bullets, as documented [here](https://github.com/clarinsi/classla/blob/master/README.closed_classes.md)
+- usage of external inflectional lexicons for lookup lemmatization, seq2seq being used very infrequently on OOVs only (Stanza uses only UD training data for lookup lemmatization)
+- morphosyntactic tagging models based on larger quantities of training data than is available in UD (training data that are morphosyntactically tagged, but not UD-parsed)
+- lemmatization models based on larger quantities of training data than is available in UD (training data that are lemmatized, but not UD-parsed)
+- optional JOS-project-based parsing of Slovenian (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#jos-dependency-parsing-system))
+- named entity recognition models for all languages except Macedonian (Stanza does not cover named entity recognition for any of the languages supported by classla)
+- Macedonian models (Macedonian is not available in UD yet)
+- non-standard models for Croatian, Slovenian, Serbian (there is no UD data for these varieties)
+
+The above modifications led to some important improvements in the tool’s performance in comparison to original Stanza. For standard Slovenian, for example, running the full classla pipeline increases sentence segmentation F1 scores to 99.52 (94.29% error reduction), lemmatization to 99.17 (68.8% error reduction), XPOS tagging  to 97.38 (46.75% error reduction), UPOS tagging to 98.69 (23.4% error reduction), and LAS to 92.05 (23.56% error reduction).  See official [Stanza performance](https://stanfordnlp.github.io/stanza/performance.html) (evaluated on different data splits) for comparison.
+
+## Installation
+### pip
+We recommend that you install CLASSLA via pip, the Python package manager. To install, run:
+```bash
+pip install classla
+```
+This will also resolve all dependencies.
+
+__NOTE TO EXISTING USERS__: Once you install this classla version, you will HAVE TO re-download the models. All previously downloaded models will not be used anymore. We suggest you delete the old models. Their default location is at `~/classla_resources`.
+
+## Running CLASSLA
+
+### Getting started
+
+To run the CLASSLA pipeline for the first time on processing standard Slovenian, follow these steps:
+
+```
+>>> import classla
+>>> classla.download('sl')                            # download standard models for Slovenian, use hr for Croatian, sr for Serbian, bg for Bulgarian, mk for Macedonian
+>>> nlp = classla.Pipeline('sl')                      # initialize the default Slovenian pipeline, use hr for Croatian, sr for Serbian, bg for Bulgarian, mk for Macedonian
+>>> doc = nlp("France Prešeren je rojen v Vrbi.")     # run the pipeline
+>>> print(doc.to_conll())                             # print the output in CoNLL-U format
+# newpar id = 1
+# sent_id = 1.1
+# text = France Prešeren je rojen v Vrbi.
+1	France	France	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	4	nsubj	_	NER=B-PER
+2	Prešeren	Prešeren	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat:name	_	NER=I-PER
+3	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	cop	_	NER=O
+4	rojen	rojen	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part	0	root	_	NER=O
+5	v	v	ADP	Sl	Case=Loc	6	case	_	NER=O
+6	Vrbi	Vrba	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
+7	.	.	PUNCT	Z	_	4	punct	_	NER=O
+
+```
+You can find examples of standard language processing for [Croatian](#example-of-standard-croatian), [Serbian](#example-of-standard-serbian), [Macedonian](#example-of-standard-macedonian) and [Bulgarian](#example-of-standard-bulgarian) at the end of this document.
+
+### Processing non-standard language
+
+Processing non-standard Slovenian differs to the above standard example just by an additional argument ```type="nonstandard"```:
+
+```
+>>> import classla
+>>> classla.download('sl', type='nonstandard')        # download non-standard models for Slovenian, use hr for Croatian and sr for Serbian
+>>> nlp = classla.Pipeline('sl', type='nonstandard')  # initialize the default non-standard Slovenian pipeline, use hr for Croatian and sr for Serbian
+>>> doc = nlp("kva smo mi zurali zadnje leto v zagrebu...")     # run the pipeline
+>>> print(doc.to_conll())                             # print the output in CoNLL-U format
+# newpar id = 1
+# sent_id = 1.1
+# text = kva smo mi zurali zadnje leto v zagrebu...
+1	kva	kaj	PRON	Pq-nsa	Case=Acc|Gender=Neut|Number=Sing|PronType=Int	4	obj	_	NER=O
+2	smo	biti	AUX	Va-r1p-n	Mood=Ind|Number=Plur|Person=1|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
+3	mi	jaz	PRON	Pp1mpn	Case=Nom|Gender=Masc|Number=Plur|Person=1|PronType=Prs	4	nsubj	_	NER=O
+4	zurali	zurati	VERB	Vmpp-pm	Aspect=Imp|Gender=Masc|Number=Plur|VerbForm=Part	0	root	_	NER=O
+5	zadnje	zadnji	ADJ	Agpnsa	Case=Acc|Degree=Pos|Gender=Neut|Number=Sing	6	amod	_	NER=O
+6	leto	leto	NOUN	Ncnsa	Case=Acc|Gender=Neut|Number=Sing	4	obl	_	NER=O
+7	v	v	ADP	Sl	Case=Loc	8	case	_	NER=O
+8	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
+9	...	...	PUNCT	Z	_	4	punct	_	NER=O
+
+```
+
+You can find examples of non-standard language processing for [Croatian](#example-of-non-standard-croatian) and [Serbian](#example-of-non-standard-serbian)  at the end of this document.
+
+For additional usage examples you can also consult the ```pipeline_demo.py``` file.
+
+### Processing online texts
+
+A special web processing mode for processing texts obtained from the internet can be activated with the ```type="web"``` argument:
+
+```
+>>> import classla
+>>> classla.download('sl', type='web')        # download web models for Slovenian, use hr for Croatian and sr for Serbian
+>>> nlp = classla.Pipeline('sl', type='web')  # initialize the default Slovenian web pipeline, use hr for Croatian and sr for Serbian
+>>> doc = nlp("Kdor hoce prenesti preko racunalnika http://t.co/LwWyzs0cA0")     # run the pipeline
+>>> print(doc.to_conll())                             # print the output in CoNLL-U format
+# newpar id = 1
+# sent_id = 1.1
+# text = Kdor hoce prenesti preko racunalnika http://t.co/LwWyzs0cA0
+1	Kdor	kdor	PRON	Pr-msn	Case=Nom|Gender=Masc|Number=Sing|PronType=Rel	2	nsubj	_	NER=O
+2	hoce	hoteti	VERB	Vmpr3s-n	Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	0	root	_	NER=O
+3	prenesti	prenesti	VERB	Vmen	Aspect=Perf|VerbForm=Inf	2	xcomp	_	NER=O
+4	preko	preko	ADP	Sg	Case=Gen	5	case	_	NER=O
+5	racunalnika	računalnik	NOUN	Ncmsg	Case=Gen|Gender=Masc|Number=Sing	3	obl	_	NER=O
+6	http://t.co/LwWyzs0cA0	http://t.co/LwWyzs0cA0	SYM	Xw	_	5	nmod	_	NER=O
+```
+
+## Processors
+
+The CLASSLA pipeline is built from multiple units. These units are called processors. By default CLASSLA runs the ```tokenize```, ```ner```, ```pos```, ```lemma``` and ```depparse``` processors.
+
+You can specify which processors CLASSLA should run, via the ```processors``` attribute as in the following example, performing tokenization, named entity recognition, part-of-speech tagging and lemmatization.
+
+```python
+>>> nlp = classla.Pipeline('sl', processors='tokenize,ner,pos,lemma')
+```
+
+Another popular option might be to perform tokenization, part-of-speech tagging, lemmatization and dependency parsing.
+
+```python
+>>> nlp = classla.Pipeline('sl', processors='tokenize,pos,lemma,depparse')
+```
+
+### Tokenization and sentence splitting
+
+The tokenization and sentence splitting processor ```tokenize``` is the first processor and is required for any further processing.
+
+In case you already have tokenized text, you should separate tokens via spaces and pass the attribute ```tokenize_pretokenized=True```.
+
+By default CLASSLA uses a rule-based tokenizer - [obeliks](https://github.com/clarinsi/obeliks) for Slovenian standard language pipeline. In other cases we use [reldi-tokeniser](https://github.com/clarinsi/reldi-tokeniser).
+
+<!--Most important attributes:
+```
+tokenize_pretokenized   - [boolean]     ignores tokenizer
+```-->
+
+### Part-of-speech tagging
+
+The POS tagging processor ```pos``` will general output that contains morphosyntactic description following the [MULTEXT-East standard](http://nl.ijs.si/ME/V6/msd/html/msd.lang-specific.html) and universal part-of-speech tags and universal features following the [Universal Dependencies standard](https://universaldependencies.org). This processing requires the usage of the ```tokenize``` processor.
+
+<!--Most important attributes:
+```
+pos_model_path          - [str]         alternative path to model file
+pos_pretrain_path       - [str]         alternative path to pretrain file
+```-->
+
+### Lemmatization
+
+The lemmatization processor ```lemma``` will produce lemmas (basic forms) for each token in the input. It requires the usage of both the ```tokenize``` and ```pos``` processors.
+
+### Dependency parsing
+
+The dependency parsing processor ```depparse``` performs syntactic dependency parsing of sentences following the [Universal Dependencies formalism](https://universaldependencies.org/introduction.html#:~:text=Universal%20Dependencies%20(UD)%20is%20a,from%20a%20language%20typology%20perspective.). It requires the ```tokenize``` and ```pos``` processors.
+
+### Named entity recognition
+
+The named entity recognition processor ```ner``` identifies named entities in text following the [IOB2](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)) format. It requires only the ```tokenize``` processor.
+
+## Citing
+
+If you use this tool, please cite the following paper:
+
+```
+@inproceedings{ljubesic-dobrovoljc-2019-neural,
+    title = "What does Neural Bring? Analysing Improvements in Morphosyntactic Annotation and Lemmatisation of {S}lovenian, {C}roatian and {S}erbian",
+    author = "Ljube{\v{s}}i{\'c}, Nikola  and
+      Dobrovoljc, Kaja",
+    booktitle = "Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing",
+    month = aug,
+    year = "2019",
+    address = "Florence, Italy",
+    publisher = "Association for Computational Linguistics",
+    url = "https://www.aclweb.org/anthology/W19-3704",
+    doi = "10.18653/v1/W19-3704",
+    pages = "29--34"
+    }
+```
+
+## Croatian examples
+
+### Example of standard Croatian 
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('hr') # run classla.download('hr') beforehand if necessary
+>>> doc = nlp("Ante Starčević rođen je u Velikom Žitniku.")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = Ante Starčević rođen je u Velikom Žitniku.
+1	Ante	Ante	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj	_	NER=B-PER
+2	Starčević	Starčević	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
+3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
+4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux	_	NER=O
+5	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
+6	Velikom	velik	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	7	amod	_	NER=B-LOC
+7	Žitniku	Žitnik	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	3	obl	_	NER=I-LOC|SpaceAfter=No
+8	.	.	PUNCT	Z	_	3	punct	_	NER=O
+
+```
+### Example of non-standard Croatian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('hr', type='nonstandard') # run classla.download('hr', type='nonstandard') beforehand if necessary
+>>> doc = nlp("kaj sam ja tulumaril jucer u ljubljani...")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = kaj sam ja tulumaril jucer u ljubljani...
+1	kaj	što	PRON	Pq3n-a	Case=Acc|Gender=Neut|PronType=Int,Rel	4	obj	_	NER=O
+2	sam	biti	AUX	Var1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
+3	ja	ja	PRON	Pp1-sn	Case=Nom|Number=Sing|Person=1|PronType=Prs	4	nsubj	_	NER=O
+4	tulumaril	tulumariti	VERB	Vmp-sm	Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	0	root	_	NER=O
+5	jucer	jučer	ADV	Rgp	Degree=Pos	4	advmod	_	NER=O
+6	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
+7	ljubljani	Ljubljana	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
+8	...	...	PUNCT	Z	_	4	punct	_	NER=O
+
+```
+
+## Serbian examples
+
+### Example of standard Serbian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('sr') # run classla.download('sr') beforehand if necessary
+>>> doc = nlp("Slobodan Jovanović rođen je u Novom Sadu.")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = Slobodan Jovanović rođen je u Novom Sadu.
+1	Slobodan	Slobodan	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj	_	NER=B-PER
+2	Jovanović	Jovanović	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
+3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
+4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux	_	NER=O
+5	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
+6	Novom	nov	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	7	amod	_	NER=B-LOC
+7	Sadu	Sad	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	3	obl	_	NER=I-LOC|SpaceAfter=No
+8	.	.	PUNCT	Z	_	3	punct	_	NER=O
+
+```
+
+### Example of non-standard Serbian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('sr', type='nonstandard') # run classla.download('sr', type='nonstandard') beforehand if necessary
+>>> doc = nlp("ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu...")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu...
+1	ne	ne	PART	Qz	Polarity=Neg	2	advmod	_	NER=O
+2	mogu	moći	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	0	root	_	NER=O
+3	da	da	SCONJ	Cs	_	4	mark	_	NER=O
+4	verujem	verovati	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	2	xcomp	_	NER=O
+5	kakvo	kakav	DET	Pi-nsn	Case=Nom|Gender=Neut|Number=Sing|PronType=Int,Rel	4	ccomp	_	NER=O
+6	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	5	aux	_	NER=O
+7	zezanje	zezanje	NOUN	Ncnsn	Case=Nom|Gender=Neut|Number=Sing	8	nsubj	_	NER=O
+8	bilo	biti	AUX	Vap-sn	Gender=Neut|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	5	cop	_	NER=O
+9	prosle	prošli	ADJ	Agpfsgy	Case=Gen|Definite=Def|Degree=Pos|Gender=Fem|Number=Sing	10	amod	_	NER=O
+10	godine	godina	NOUN	Ncfsg	Case=Gen|Gender=Fem|Number=Sing	8	obl	_	NER=O
+11	u	u	ADP	Sl	Case=Loc	12	case	_	NER=O
+12	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	8	obl	_	NER=B-LOC|SpaceAfter=No
+13	...	...	PUNCT	Z	_	2	punct	_	NER=O
+
+```
+
+## Bulgarian examples
+
+### Example of standard Bulgarian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('bg') # run classla.download('bg') beforehand if necessary
+>>> doc = nlp("Алеко Константинов е роден в Свищов.")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = Алеко Константинов е роден в Свищов.
+1	Алеко	алеко	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	nsubj:pass	_	NER=B-PER
+2	Константинов	константинов	PROPN	Hmsi	Definite=Ind|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
+3	е	съм	AUX	Vxitf-r3s	Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act	4	aux:pass	_	NER=O
+4	роден	родя-(се)	VERB	Vpptcv--smi	Aspect=Perf|Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
+5	в	в	ADP	R	_	6	case	_	NER=O
+6	Свищов	свищов	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	iobj	_	NER=B-LOC|SpaceAfter=No
+7	.	.	PUNCT	punct	_	4	punct	_	NER=O
+
+```
+
+## Macedonian examples
+
+### Example of standard Macedonian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('mk') # run classla.download('mk') beforehand if necessary
+>>> doc = nlp('Крсте Петков Мисирков е роден во Постол.')
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = Крсте Петков Мисирков е роден во Постол.
+1	Крсте	Крсте	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
+2	Петков	Петков	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
+3	Мисирков	Мисирков	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
+4	е	сум	AUX	Vapip3s-n	Aspect=Prog|Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres	_	_	_	_
+5	роден	роден	ADJ	Ap-ms-n	Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part	_	_	_	_
+6	во	во	ADP	Sps	AdpType=Prep	_	_	_	_
+7	Постол	Постол	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	SpaceAfter=No
+8	.	.	PUNCT	Z	_	_	_	_	_
+
+```
+
+## Training instructions
+
+[Training instructions](https://github.com/clarinsi/classla-stanfordnlp/blob/master/README.train.md)
+
+## Superuser instructions
+
+[Superuser instructions](https://github.com/clarinsi/classla-stanfordnlp/blob/master/README.superuser.md)
```

### Comparing `classla-2.0/classla/models/classifier.py` & `classla-2.1/classla/models/classifier.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,595 +1,595 @@
-import argparse
-import ast
-import collections
-import logging
-import os
-import random
-import re
-from enum import Enum
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-
-from classla.models.common import loss
-from classla.models.common import utils
-from classla.models.common.vocab import PAD, PAD_ID, UNK, UNK_ID
-from classla.models.common.pretrain import Pretrain
-
-import classla.models.classifiers.classifier_args as classifier_args
-import classla.models.classifiers.cnn_classifier as cnn_classifier
-
-class Loss(Enum):
-    CROSS = 1
-    WEIGHTED_CROSS = 2
-    LOG_CROSS = 3
-
-logger = logging.getLogger('classla')
-
-DEFAULT_TRAIN='extern_data/sentiment/sst-processed/fiveclass/train-phrases.txt'
-DEFAULT_DEV='extern_data/sentiment/sst-processed/fiveclass/dev-roots.txt'
-DEFAULT_TEST='extern_data/sentiment/sst-processed/fiveclass/test-roots.txt'
-
-"""A script for training and testing classifier models, especially on the SST.
-
-If you run the script with no arguments, it will start trying to train
-a sentiment model.
-
-python3 -m classla.models.classifier
-
-This requires the sentiment dataset to be in an `extern_data`
-directory, such as by symlinking it from somewhere else.
-
-The default model is a CNN where the word vectors are first mapped to
-channels with filters of a few different widths, those channels are
-maxpooled over the entire sentence, and then the resulting pools have
-fully connected layers until they reach the number of classes in the
-training data.  You can see the defaults in the options below.
-
-https://arxiv.org/abs/1408.5882
-
-(Currently the CNN is the only sentence classifier implemented.)
-
-To train with a more complicated CNN arch:
-
-nohup python3 -u -m classla.models.classifier --max_epochs 400 --filter_channels 1000 --fc_shapes 400,100 > FC41.out 2>&1 &
-
-You can train models with word vectors other than the default word2vec.  For example:
-
- nohup python3 -u -m classla.models.classifier  --wordvec_type google --wordvec_dir extern_data/google --max_epochs 200 --filter_channels 1000 --fc_shapes 200,100 --base_name FC21_google > FC21_google.out 2>&1 &
-
-A model trained on the 5 class dataset can be tested on the 2 class dataset with a command line like this:
-
-python3 -u -m classla.models.classifier  --no_train --load_name saved_models/classifier/sst_en_ewt_FS_3_4_5_C_1000_FC_400_100_classifier.E0165-ACC41.87.pt --test_file extern_data/sentiment/sst-processed/binary/test-binary-roots.txt --test_remap_labels "{0:0, 1:0, 3:1, 4:1}"
-
-python3 -u -m classla.models.classifier  --wordvec_type google --wordvec_dir extern_data/google --no_train --load_name saved_models/classifier/FC21_google_en_ewt_FS_3_4_5_C_1000_FC_200_100_classifier.E0189-ACC45.87.pt --test_file extern_data/sentiment/sst-processed/binary/test-binary-roots.txt --test_remap_labels "{0:0, 1:0, 3:1, 4:1}"
-
-A model trained on the 3 class dataset can be tested on the 2 class dataset with a command line like this:
-
-python3 -u -m classla.models.classifier  --wordvec_type google --wordvec_dir extern_data/google --no_train --load_name saved_models/classifier/FC21_3C_google_en_ewt_FS_3_4_5_C_1000_FC_200_100_classifier.E0101-ACC68.94.pt --test_file extern_data/sentiment/sst-processed/binary/test-binary-roots.txt --test_remap_labels "{0:0, 2:1}"
-
-To train models on combined 3 class datasets:
-
-nohup python3 -u -m classla.models.classifier --max_epochs 400 --filter_channels 1000 --fc_shapes 400,100 --base_name FC41_3class  --train_file extern_data/sentiment/sst-processed/threeclass/train-threeclass-phrases.txt,extern_data/sentiment/MELD/train.txt,extern_data/sentiment/slsd/train.txt,extern_data/sentiment/arguana/train.txt,extern_data/sentiment/airline/train.txt,extern_data/sentiment/sst-processed/threeclass/extra-train-threeclass-phrases.txt,extern_data/sentiment/sst-processed/threeclass/checked-extra-threeclass-phrases.txt --dev_file extern_data/sentiment/sst-processed/threeclass/dev-threeclass-roots.txt --test_file extern_data/sentiment/sst-processed/threeclass/test-threeclass-roots.txt > FC41_3class.out 2>&1 &
-
-This tests that model:
-
-python3 -u -m classla.models.classifier --no_train --load_name en_sstplus.pt --test_file extern_data/sentiment/sst-processed/threeclass/test-threeclass-roots.txt
-
-Here is an example for training a model in a different language:
-
-nohup python3 -u -m classla.models.classifier --max_epochs 400 --filter_channels 1000 --fc_shapes 400,100 --base_name FC41_german  --train_file extern_data/sentiment/german/sb-10k/train.txt,extern_data/sentiment/german/scare/train.txt,extern_data/sentiment/USAGE/de-train.txt --dev_file extern_data/sentiment/german/sb-10k/dev.txt --test_file extern_data/sentiment/german/sb-10k/test.tmp --shorthand de_sb10k --min_train_len 3 --extra_wordvec_method CONCAT --extra_wordvec_dim 100 > de_sb10k.out 2>&1 &
-
-nohup python3 -u -m classla.models.classifier --max_epochs 400 --filter_channels 1000 --fc_shapes 400,100 --base_name FC41_chinese  --train_file extern_data/sentiment/chinese/RenCECps/train.txt --dev_file extern_data/sentiment/chinese/RenCECps/dev.txt --test_file extern_data/sentiment/chinese/RenCECps/test.tmp --shorthand zh_ren --wordvec_type fasttext --extra_wordvec_method SUM > zh_ren.out 2>&1 &
-"""
-
-def convert_fc_shapes(arg):
-    """
-    Returns a tuple of sizes to use in FC layers.
-
-    For examples, converts "100" -> (100,)
-    "100,200" -> (100,200)
-    """
-    arg = arg.strip()
-    if not arg:
-        return ()
-    arg = ast.literal_eval(arg)
-    if isinstance(arg, int):
-        return (arg,)
-    if isinstance(arg, tuple):
-        return arg
-    return tuple(arg)
-
-def parse_args():
-    """
-    Add arguments for building the classifier.
-    Parses command line args and returns the result.
-    """
-    parser = argparse.ArgumentParser()
-
-    classifier_args.add_common_args(parser)
-
-    parser.add_argument('--train', dest='train', default=True, action='store_true', help='Train the model (default)')
-    parser.add_argument('--no_train', dest='train', action='store_false', help="Don't train the model")
-
-    parser.add_argument('--load_name', type=str, default=None, help='Name for loading an existing model')
-
-    parser.add_argument('--save_name', type=str, default=None, help='Name for saving the model')
-    parser.add_argument('--base_name', type=str, default='sst', help="Base name of the model to use when building a model name from args")
-
-
-    parser.add_argument('--train_file', type=str, default=DEFAULT_TRAIN, help='Input file(s) to train a model from.  Each line is an example.  Should go <label> <tokenized sentence>.  Comma separated list.')
-    parser.add_argument('--dev_file', type=str, default=DEFAULT_DEV, help='Input file(s) to use as the dev set.')
-    parser.add_argument('--test_file', type=str, default=DEFAULT_TEST, help='Input file(s) to use as the test set.')
-    parser.add_argument('--max_epochs', type=int, default=100)
-
-    parser.add_argument('--filter_sizes', default=(3,4,5), type=ast.literal_eval, help='Filter sizes for the layer after the word vectors')
-    parser.add_argument('--filter_channels', default=100, type=int, help='Number of channels for layers after the word vectors')
-    parser.add_argument('--fc_shapes', default="100", type=convert_fc_shapes, help='Extra fully connected layers to put after the initial filters.  If set to blank, will FC directly from the max pooling to the output layer.')
-    parser.add_argument('--dropout', default=0.5, type=float, help='Dropout value to use')
-
-    parser.add_argument('--batch_size', default=50, type=int, help='Batch size when training')
-
-    parser.add_argument('--weight_decay', default=0.0001, type=float, help='Weight decay (eg, l2 reg) to use in the optimizer')
-
-    parser.add_argument('--optim', default='Adadelta', help='Optimizer type: SGD or Adadelta')
-
-    parser.add_argument('--test_remap_labels', default=None, type=ast.literal_eval,
-                        help='Map of which label each classifier label should map to.  For example, "{0:0, 1:0, 3:1, 4:1}" to map a 5 class sentiment test to a 2 class.  Any labels not mapped will be considered wrong')
-    parser.add_argument('--forgive_unmapped_labels', dest='forgive_unmapped_labels', default=True, action='store_true',
-                        help='When remapping labels, such as from 5 class to 2 class, pick a different label if the first guess is not remapped.')
-    parser.add_argument('--no_forgive_unmapped_labels', dest='forgive_unmapped_labels', action='store_false',
-                        help="When remapping labels, such as from 5 class to 2 class, DON'T pick a different label if the first guess is not remapped.")
-
-    parser.add_argument('--loss', type=lambda x: Loss[x.upper()], default=Loss.CROSS,
-                        help="Whether to use regular cross entropy or scale it by 1/log(quantity)")
-    parser.add_argument('--min_train_len', type=int, default=0,
-                        help="Filter sentences less than this length")
-
-    args = parser.parse_args()
-    return args
-
-
-def read_dataset(dataset, wordvec_type, min_len):
-    """
-    returns a list where the values of the list are
-      label, [token...]
-    """
-    lines = []
-    for filename in dataset.split(","):
-        try:
-            new_lines = open(filename, encoding="utf-8").readlines()
-        except UnicodeDecodeError:
-            logger.error("Could not read {}".format(filename))
-            raise
-        lines.extend(new_lines)
-    lines = [x.strip() for x in lines]
-    lines = [x.split(maxsplit=1) for x in lines if x]
-    lines = [x for x in lines if len(x) > 1]
-    # TODO: maybe do this processing later, once the model is built.
-    # then move the processing into the model so we can use
-    # overloading to potentially make future model types
-    lines = [(x[0], cnn_classifier.update_text(x[1], wordvec_type)) for x in lines]
-    if min_len:
-        lines = [x for x in lines if len(x[1]) >= min_len]
-    return lines
-
-def dataset_labels(dataset):
-    """
-    Returns a sorted list of label name
-    """
-    labels = set([x[0] for x in dataset])
-    if all(re.match("^[0-9]+$", label) for label in labels):
-        # if all of the labels are integers, sort numerically
-        # maybe not super important, but it would be nicer than having
-        # 10 before 2
-        labels = [str(x) for x in sorted(map(int, list(labels)))]
-    else:
-        labels = sorted(list(labels))
-    return labels
-
-def dataset_vocab(dataset):
-    vocab = set()
-    for line in dataset:
-        for word in line[1]:
-            vocab.add(word)
-    vocab = [PAD, UNK] + list(vocab)
-    if vocab[PAD_ID] != PAD or vocab[UNK_ID] != UNK:
-        raise ValueError("Unexpected values for PAD and UNK!")
-    return vocab
-
-def sort_dataset_by_len(dataset):
-    """
-    returns a dict mapping length -> list of items of that length
-    an OrderedDict is used to that the mapping is sorted from smallest to largest
-    """
-    sorted_dataset = collections.OrderedDict()
-    lengths = sorted(list(set(len(x[1]) for x in dataset)))
-    for l in lengths:
-        sorted_dataset[l] = []
-    for item in dataset:
-        sorted_dataset[len(item[1])].append(item)
-    return sorted_dataset
-
-def shuffle_dataset(sorted_dataset):
-    """
-    Given a dataset sorted by len, sorts within each length to make
-    chunks of roughly the same size.  Returns all items as a single list.
-    """
-    dataset = []
-    for l in sorted_dataset.keys():
-        items = list(sorted_dataset[l])
-        random.shuffle(items)
-        dataset.extend(items)
-    return dataset
-
-def confusion_dataset(model, dataset, device=None):
-    """
-    Returns a confusion matrix
-
-    First key: gold
-    Second key: predicted
-    so: confusion[gold][predicted]
-    """
-    model.eval()
-    index_label_map = {x: y for (x, y) in enumerate(model.labels)}
-    if device is None:
-        device = next(model.parameters()).device
-
-    dataset_lengths = sort_dataset_by_len(dataset)
-
-    confusion = {}
-    for label in model.labels:
-        confusion[label] = {}
-
-    for length in dataset_lengths.keys():
-        batch = dataset_lengths[length]
-        text = [x[1] for x in batch]
-        expected_labels = [x[0] for x in batch]
-
-        output = model(text, device)
-        for i in range(len(expected_labels)):
-            predicted = torch.argmax(output[i])
-            predicted_label = index_label_map[predicted.item()]
-            confusion[expected_labels[i]][predicted_label] = confusion[expected_labels[i]].get(predicted_label, 0) + 1
-
-    return confusion
-
-
-def confusion_to_accuracy(confusion):
-    """
-    Given a confusion dictionary returned by confusion_dataset, return correct, total
-    """
-    correct = 0
-    total = 0
-    for l1 in confusion.keys():
-        for l2 in confusion[l1].keys():
-            if l1 == l2:
-                correct = correct + confusion[l1][l2]
-            else:
-                total = total + confusion[l1][l2]
-    return correct, (correct + total)
-
-def confusion_to_macro_f1(confusion):
-    """
-    Return the macro f1 for a confusion matrix.
-    """
-    keys = set()
-    for k in confusion.keys():
-        keys.add(k)
-        for k2 in confusion.get(k).keys():
-            keys.add(k2)
-
-    sum_f1 = 0
-    for k in keys:
-        tp = 0
-        fn = 0
-        fp = 0
-        for k2 in keys:
-            if k == k2:
-                tp = confusion.get(k, {}).get(k, 0)
-            else:
-                fn = fn + confusion.get(k, {}).get(k2, 0)
-                fp = fp + confusion.get(k2, {}).get(k, 0)
-        precision = tp / (tp + fp)
-        recall = tp / (tp + fn)
-        f1 = 2 * (precision * recall) / (precision + recall)
-        sum_f1 = sum_f1 + f1
-
-    return sum_f1 / len(keys)
-
-
-def format_confusion(confusion, labels, hide_zeroes=False):
-    """
-    pretty print for confusion matrixes
-    adapted from https://gist.github.com/zachguo/10296432
-    """
-    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length
-    empty_cell = " " * columnwidth
-
-    fst_empty_cell = (columnwidth-3)//2 * " " + "t/p" + (columnwidth-3)//2 * " "
-
-    if len(fst_empty_cell) < len(empty_cell):
-        fst_empty_cell = " " * (len(empty_cell) - len(fst_empty_cell)) + fst_empty_cell
-    # Print header
-    header = "    " + fst_empty_cell + " "
-
-    for label in labels:
-        header = header + "%{0}s ".format(columnwidth) % label
-    text = [header]
-
-    # Print rows
-    for i, label1 in enumerate(labels):
-        row = "    %{0}s ".format(columnwidth) % label1
-        for j, label2 in enumerate(labels):
-            confusion_cell = confusion.get(label1, {}).get(label2, 0)
-            cell = "%{0}.1f".format(columnwidth) % confusion_cell
-            if hide_zeroes:
-                cell = cell if confusion_cell else empty_cell
-            row = row + cell + " "
-        text.append(row)
-    return "\n".join(text)
-
-
-def score_dataset(model, dataset, label_map=None, device=None,
-                  remap_labels=None, forgive_unmapped_labels=False):
-    """
-    remap_labels: a dict from old label to new label to use when
-    testing a classifier on a dataset with a simpler label set.
-    For example, a model trained on 5 class sentiment can be tested
-    on a binary distribution with {"0": "0", "1": "0", "3": "1", "4": "1"}
-
-    forgive_unmapped_labels says the following: in the case that the
-    model predicts "2" in the above example for remap_labels, instead
-    treat the model's prediction as whichever label it gave the
-    highest score
-    """
-    model.eval()
-    if label_map is None:
-        label_map = {x: y for (y, x) in enumerate(model.labels)}
-    if device is None:
-        device = next(model.parameters()).device
-    correct = 0
-    dataset_lengths = sort_dataset_by_len(dataset)
-
-    for length in dataset_lengths.keys():
-        # TODO: possibly break this up into smaller batches
-        batch = dataset_lengths[length]
-        text = [x[1] for x in batch]
-        expected_labels = [label_map[x[0]] for x in batch]
-
-        output = model(text, device)
-
-        for i in range(len(expected_labels)):
-            predicted = torch.argmax(output[i])
-            predicted_label = predicted.item()
-            if remap_labels:
-                if predicted_label in remap_labels:
-                    predicted_label = remap_labels[predicted_label]
-                else:
-                    found = False
-                    if forgive_unmapped_labels:
-                        items = []
-                        for j in range(len(output[i])):
-                            items.append((output[i][j].item(), j))
-                        items.sort(key=lambda x: -x[0])
-                        for _, item in items:
-                            if item in remap_labels:
-                                predicted_label = remap_labels[item]
-                                found = True
-                                break
-                    # if slack guesses allowed, none of the existing
-                    # labels matched, so we count it wrong.  if slack
-                    # guesses not allowed, just count it wrong
-                    if not found:
-                        continue
-
-            if predicted_label == expected_labels[i]:
-                correct = correct + 1
-    return correct
-
-def check_labels(labels, dataset):
-    """
-    Check that all of the labels in the dataset are in the known labels.
-    Actually, unknown labels could be acceptable if we just treat the model as always wrong.
-    However, this is a good sanity check to make sure the datasets match
-    """
-    new_labels = dataset_labels(dataset)
-    not_found = [i for i in new_labels if i not in labels]
-    if not_found:
-        raise RuntimeError('Dataset contains labels which the model does not know about:' + str(not_found))
-
-def checkpoint_name(filename, epoch, acc):
-    """
-    Build an informative checkpoint name from a base name, epoch #, and accuracy
-    """
-    root, ext = os.path.splitext(filename)
-    return root + ".E{epoch:04d}-ACC{acc:05.2f}".format(**{"epoch": epoch, "acc": acc * 100}) + ext
-
-def train_model(model, model_file, args, train_set, dev_set, labels):
-    # TODO: separate this into a trainer like the other models.
-    # TODO: possibly reuse the trainer code other models have
-    # TODO: use a (torch) dataloader to possibly speed up the GPU usage
-    device = next(model.parameters()).device
-    logger.info("Current device: %s" % device)
-
-    # TODO: if reloading a model for continued training, the internal
-    # parameters for the optimizer should be reloaded as well
-    # Otherwise this ability is actually not very useful
-    if args.optim.lower() == 'sgd':
-        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9,
-                              weight_decay=args.weight_decay)
-    elif args.optim.lower() == 'adadelta':
-        optimizer = optim.Adadelta(model.parameters(), weight_decay=args.weight_decay)
-    else:
-        raise ValueError("Unknown optimizer: %s" % args.optim)
-
-    label_map = {x: y for (y, x) in enumerate(labels)}
-    label_tensors = {x: torch.tensor(y, requires_grad=False, device=device)
-                     for (y, x) in enumerate(labels)}
-
-    if args.loss == Loss.CROSS:
-        loss_function = nn.CrossEntropyLoss()
-    elif args.loss == Loss.WEIGHTED_CROSS:
-        loss_function = loss.weighted_cross_entropy_loss([label_map[x[0]] for x in train_set], log_dampened=False)
-    elif args.loss == Loss.LOG_CROSS:
-        loss_function = loss.weighted_cross_entropy_loss([label_map[x[0]] for x in train_set], log_dampened=True)
-    else:
-        raise ValueError("Unknown loss function {}".format(args.loss))
-    if args.cuda:
-        loss_function.cuda()
-
-    train_set_by_len = sort_dataset_by_len(train_set)
-
-    if args.load_name:
-        # We reloaded the model, so let's report its current dev set score
-        correct = score_dataset(model, dev_set, label_map, device)
-        logger.info("Reloaded model for continued training.  Dev set: %d correct of %d examples.  Accuracy: %f" %
-                    (correct, len(dev_set), correct / len(dev_set)))
-
-    # https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html
-    batch_starts = list(range(0, len(train_set), args.batch_size))
-    for epoch in range(args.max_epochs):
-        running_loss = 0.0
-        epoch_loss = 0.0
-        shuffled = shuffle_dataset(train_set_by_len)
-        model.train()
-        random.shuffle(batch_starts)
-        for batch_num, start_batch in enumerate(batch_starts):
-            logger.debug("Starting batch: %d" % start_batch)
-            batch = shuffled[start_batch:start_batch+args.batch_size]
-            text = [x[1] for x in batch]
-            label = torch.stack([label_tensors[x[0]] for x in batch])
-
-            # zero the parameter gradients
-            optimizer.zero_grad()
-
-            outputs = model(text, device)
-            batch_loss = loss_function(outputs, label)
-            batch_loss.backward()
-            optimizer.step()
-
-            # print statistics
-            running_loss += batch_loss.item()
-            if ((batch_num + 1) * args.batch_size) % 2000 < args.batch_size: # print every 2000 items
-                logger.info('[%d, %5d] average loss: %.3f' %
-                            (epoch + 1, ((batch_num + 1) * args.batch_size), running_loss / 2000))
-                epoch_loss += running_loss
-                running_loss = 0.0
-        # Add any leftover loss to the epoch_loss
-        epoch_loss += running_loss
-
-        correct = score_dataset(model, dev_set, label_map, device)
-        logger.info("Finished epoch %d.  Dev set: %d correct of %d examples.  Accuracy: %f  Total loss: %f" %
-                    ((epoch + 1), correct, len(dev_set), correct / len(dev_set), epoch_loss))
-
-        checkpoint_file = checkpoint_name(model_file, epoch + 1, correct / len(dev_set))
-        cnn_classifier.save(checkpoint_file, model)
-
-    cnn_classifier.save(model_file, model)
-
-def load_pretrain(args):
-    if args.wordvec_pretrain_file:
-        pretrain_file = args.wordvec_pretrain_file
-    elif args.wordvec_type:
-        pretrain_file = '{}/{}.{}.pretrain.pt'.format(args.save_dir, args.shorthand, args.wordvec_type.name.lower())
-    else:
-        raise Exception("TODO: need to get the wv type back from get_wordvec_file")
-
-    logger.info("Looking for pretrained vectors in {}".format(pretrain_file))
-    if os.path.exists(pretrain_file):
-        vec_file = None
-    elif args.wordvec_raw_file:
-        vec_file = args.wordvec_raw_file
-        logger.info("Pretrain not found.  Looking in {}".format(vec_file))
-    else:
-        vec_file = utils.get_wordvec_file(args.wordvec_dir, args.shorthand, args.wordvec_type.name.lower())
-        logger.info("Pretrain not found.  Looking in {}".format(vec_file))
-    pretrain = Pretrain(pretrain_file, vec_file, args.pretrain_max_vocab)
-    logger.info("Embedding shape: %s" % str(pretrain.emb.shape))
-    return pretrain
-
-
-def print_args(args):
-    """
-    For record keeping purposes, print out the arguments when training
-    """
-    args = vars(args)
-    keys = sorted(args.keys())
-    log_lines = ['%s: %s' % (k, args[k]) for k in keys]
-    logger.info('ARGS USED AT TRAINGING TIME:\n%s\n' % '\n'.join(log_lines))
-
-
-def main():
-    args = parse_args()
-    seed = utils.set_random_seed(args.seed, args.cuda)
-    logger.info("Using random seed: %d" % seed)
-
-    utils.ensure_dir(args.save_dir)
-
-    # TODO: maybe the dataset needs to be in a torch data loader in order to
-    # make cuda operations faster
-    if args.train:
-        train_set = read_dataset(args.train_file, args.wordvec_type, args.min_train_len)
-        logger.info("Using training set: %s" % args.train_file)
-        logger.info("Training set has %d labels" % len(dataset_labels(train_set)))
-    elif not args.load_name:
-        raise ValueError("No model provided and not asked to train a model.  This makes no sense")
-    else:
-        train_set = None
-
-    pretrain = load_pretrain(args)
-
-    if args.load_name:
-        model = cnn_classifier.load(args.load_name, pretrain)
-    else:
-        assert train_set is not None
-        labels = dataset_labels(train_set)
-        extra_vocab = dataset_vocab(train_set)
-        model = cnn_classifier.CNNClassifier(pretrain.emb, pretrain.vocab, extra_vocab, labels, args)
-
-    if args.cuda:
-        model.cuda()
-
-    logger.info("Filter sizes: %s" % str(model.config.filter_sizes))
-    logger.info("Filter channels: %s" % str(model.config.filter_channels))
-    logger.info("Intermediate layers: %s" % str(model.config.fc_shapes))
-
-    save_name = args.save_name
-    if not(save_name):
-        save_name = args.base_name + "_" + args.shorthand + "_"
-        save_name = save_name + "FS_%s_" % "_".join([str(x) for x in model.config.filter_sizes])
-        save_name = save_name + "C_%d_" % model.config.filter_channels
-        if model.config.fc_shapes:
-            save_name = save_name + "FC_%s_" % "_".join([str(x) for x in model.config.fc_shapes])
-        save_name = save_name + "classifier.pt"
-    model_file = os.path.join(args.save_dir, save_name)
-
-    if args.train:
-        print_args(args)
-
-        dev_set = read_dataset(args.dev_file, args.wordvec_type, min_len=None)
-        logger.info("Using dev set: %s" % args.dev_file)
-        check_labels(model.labels, dev_set)
-
-        train_model(model, model_file, args, train_set, dev_set, model.labels)
-
-    test_set = read_dataset(args.test_file, args.wordvec_type, min_len=None)
-    logger.info("Using test set: %s" % args.test_file)
-    check_labels(model.labels, test_set)
-
-    if args.test_remap_labels is None:
-        confusion = confusion_dataset(model, test_set)
-        logger.info("Confusion matrix:\n{}".format(format_confusion(confusion, model.labels)))
-        correct, total = confusion_to_accuracy(confusion)
-        logger.info("Macro f1: {}".format(confusion_to_macro_f1(confusion)))
-    else:
-        correct = score_dataset(model, test_set,
-                                remap_labels=args.test_remap_labels,
-                                forgive_unmapped_labels=args.forgive_unmapped_labels)
-        total = len(test_set)
-    logger.info("Test set: %d correct of %d examples.  Accuracy: %f" %
-                (correct, total, correct / total))
-
-if __name__ == '__main__':
-    main()
+import argparse
+import ast
+import collections
+import logging
+import os
+import random
+import re
+from enum import Enum
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+
+from classla.models.common import loss
+from classla.models.common import utils
+from classla.models.common.vocab import PAD, PAD_ID, UNK, UNK_ID
+from classla.models.common.pretrain import Pretrain
+
+import classla.models.classifiers.classifier_args as classifier_args
+import classla.models.classifiers.cnn_classifier as cnn_classifier
+
+class Loss(Enum):
+    CROSS = 1
+    WEIGHTED_CROSS = 2
+    LOG_CROSS = 3
+
+logger = logging.getLogger('classla')
+
+DEFAULT_TRAIN='extern_data/sentiment/sst-processed/fiveclass/train-phrases.txt'
+DEFAULT_DEV='extern_data/sentiment/sst-processed/fiveclass/dev-roots.txt'
+DEFAULT_TEST='extern_data/sentiment/sst-processed/fiveclass/test-roots.txt'
+
+"""A script for training and testing classifier models, especially on the SST.
+
+If you run the script with no arguments, it will start trying to train
+a sentiment model.
+
+python3 -m classla.models.classifier
+
+This requires the sentiment dataset to be in an `extern_data`
+directory, such as by symlinking it from somewhere else.
+
+The default model is a CNN where the word vectors are first mapped to
+channels with filters of a few different widths, those channels are
+maxpooled over the entire sentence, and then the resulting pools have
+fully connected layers until they reach the number of classes in the
+training data.  You can see the defaults in the options below.
+
+https://arxiv.org/abs/1408.5882
+
+(Currently the CNN is the only sentence classifier implemented.)
+
+To train with a more complicated CNN arch:
+
+nohup python3 -u -m classla.models.classifier --max_epochs 400 --filter_channels 1000 --fc_shapes 400,100 > FC41.out 2>&1 &
+
+You can train models with word vectors other than the default word2vec.  For example:
+
+ nohup python3 -u -m classla.models.classifier  --wordvec_type google --wordvec_dir extern_data/google --max_epochs 200 --filter_channels 1000 --fc_shapes 200,100 --base_name FC21_google > FC21_google.out 2>&1 &
+
+A model trained on the 5 class dataset can be tested on the 2 class dataset with a command line like this:
+
+python3 -u -m classla.models.classifier  --no_train --load_name saved_models/classifier/sst_en_ewt_FS_3_4_5_C_1000_FC_400_100_classifier.E0165-ACC41.87.pt --test_file extern_data/sentiment/sst-processed/binary/test-binary-roots.txt --test_remap_labels "{0:0, 1:0, 3:1, 4:1}"
+
+python3 -u -m classla.models.classifier  --wordvec_type google --wordvec_dir extern_data/google --no_train --load_name saved_models/classifier/FC21_google_en_ewt_FS_3_4_5_C_1000_FC_200_100_classifier.E0189-ACC45.87.pt --test_file extern_data/sentiment/sst-processed/binary/test-binary-roots.txt --test_remap_labels "{0:0, 1:0, 3:1, 4:1}"
+
+A model trained on the 3 class dataset can be tested on the 2 class dataset with a command line like this:
+
+python3 -u -m classla.models.classifier  --wordvec_type google --wordvec_dir extern_data/google --no_train --load_name saved_models/classifier/FC21_3C_google_en_ewt_FS_3_4_5_C_1000_FC_200_100_classifier.E0101-ACC68.94.pt --test_file extern_data/sentiment/sst-processed/binary/test-binary-roots.txt --test_remap_labels "{0:0, 2:1}"
+
+To train models on combined 3 class datasets:
+
+nohup python3 -u -m classla.models.classifier --max_epochs 400 --filter_channels 1000 --fc_shapes 400,100 --base_name FC41_3class  --train_file extern_data/sentiment/sst-processed/threeclass/train-threeclass-phrases.txt,extern_data/sentiment/MELD/train.txt,extern_data/sentiment/slsd/train.txt,extern_data/sentiment/arguana/train.txt,extern_data/sentiment/airline/train.txt,extern_data/sentiment/sst-processed/threeclass/extra-train-threeclass-phrases.txt,extern_data/sentiment/sst-processed/threeclass/checked-extra-threeclass-phrases.txt --dev_file extern_data/sentiment/sst-processed/threeclass/dev-threeclass-roots.txt --test_file extern_data/sentiment/sst-processed/threeclass/test-threeclass-roots.txt > FC41_3class.out 2>&1 &
+
+This tests that model:
+
+python3 -u -m classla.models.classifier --no_train --load_name en_sstplus.pt --test_file extern_data/sentiment/sst-processed/threeclass/test-threeclass-roots.txt
+
+Here is an example for training a model in a different language:
+
+nohup python3 -u -m classla.models.classifier --max_epochs 400 --filter_channels 1000 --fc_shapes 400,100 --base_name FC41_german  --train_file extern_data/sentiment/german/sb-10k/train.txt,extern_data/sentiment/german/scare/train.txt,extern_data/sentiment/USAGE/de-train.txt --dev_file extern_data/sentiment/german/sb-10k/dev.txt --test_file extern_data/sentiment/german/sb-10k/test.tmp --shorthand de_sb10k --min_train_len 3 --extra_wordvec_method CONCAT --extra_wordvec_dim 100 > de_sb10k.out 2>&1 &
+
+nohup python3 -u -m classla.models.classifier --max_epochs 400 --filter_channels 1000 --fc_shapes 400,100 --base_name FC41_chinese  --train_file extern_data/sentiment/chinese/RenCECps/train.txt --dev_file extern_data/sentiment/chinese/RenCECps/dev.txt --test_file extern_data/sentiment/chinese/RenCECps/test.tmp --shorthand zh_ren --wordvec_type fasttext --extra_wordvec_method SUM > zh_ren.out 2>&1 &
+"""
+
+def convert_fc_shapes(arg):
+    """
+    Returns a tuple of sizes to use in FC layers.
+
+    For examples, converts "100" -> (100,)
+    "100,200" -> (100,200)
+    """
+    arg = arg.strip()
+    if not arg:
+        return ()
+    arg = ast.literal_eval(arg)
+    if isinstance(arg, int):
+        return (arg,)
+    if isinstance(arg, tuple):
+        return arg
+    return tuple(arg)
+
+def parse_args():
+    """
+    Add arguments for building the classifier.
+    Parses command line args and returns the result.
+    """
+    parser = argparse.ArgumentParser()
+
+    classifier_args.add_common_args(parser)
+
+    parser.add_argument('--train', dest='train', default=True, action='store_true', help='Train the model (default)')
+    parser.add_argument('--no_train', dest='train', action='store_false', help="Don't train the model")
+
+    parser.add_argument('--load_name', type=str, default=None, help='Name for loading an existing model')
+
+    parser.add_argument('--save_name', type=str, default=None, help='Name for saving the model')
+    parser.add_argument('--base_name', type=str, default='sst', help="Base name of the model to use when building a model name from args")
+
+
+    parser.add_argument('--train_file', type=str, default=DEFAULT_TRAIN, help='Input file(s) to train a model from.  Each line is an example.  Should go <label> <tokenized sentence>.  Comma separated list.')
+    parser.add_argument('--dev_file', type=str, default=DEFAULT_DEV, help='Input file(s) to use as the dev set.')
+    parser.add_argument('--test_file', type=str, default=DEFAULT_TEST, help='Input file(s) to use as the test set.')
+    parser.add_argument('--max_epochs', type=int, default=100)
+
+    parser.add_argument('--filter_sizes', default=(3,4,5), type=ast.literal_eval, help='Filter sizes for the layer after the word vectors')
+    parser.add_argument('--filter_channels', default=100, type=int, help='Number of channels for layers after the word vectors')
+    parser.add_argument('--fc_shapes', default="100", type=convert_fc_shapes, help='Extra fully connected layers to put after the initial filters.  If set to blank, will FC directly from the max pooling to the output layer.')
+    parser.add_argument('--dropout', default=0.5, type=float, help='Dropout value to use')
+
+    parser.add_argument('--batch_size', default=50, type=int, help='Batch size when training')
+
+    parser.add_argument('--weight_decay', default=0.0001, type=float, help='Weight decay (eg, l2 reg) to use in the optimizer')
+
+    parser.add_argument('--optim', default='Adadelta', help='Optimizer type: SGD or Adadelta')
+
+    parser.add_argument('--test_remap_labels', default=None, type=ast.literal_eval,
+                        help='Map of which label each classifier label should map to.  For example, "{0:0, 1:0, 3:1, 4:1}" to map a 5 class sentiment test to a 2 class.  Any labels not mapped will be considered wrong')
+    parser.add_argument('--forgive_unmapped_labels', dest='forgive_unmapped_labels', default=True, action='store_true',
+                        help='When remapping labels, such as from 5 class to 2 class, pick a different label if the first guess is not remapped.')
+    parser.add_argument('--no_forgive_unmapped_labels', dest='forgive_unmapped_labels', action='store_false',
+                        help="When remapping labels, such as from 5 class to 2 class, DON'T pick a different label if the first guess is not remapped.")
+
+    parser.add_argument('--loss', type=lambda x: Loss[x.upper()], default=Loss.CROSS,
+                        help="Whether to use regular cross entropy or scale it by 1/log(quantity)")
+    parser.add_argument('--min_train_len', type=int, default=0,
+                        help="Filter sentences less than this length")
+
+    args = parser.parse_args()
+    return args
+
+
+def read_dataset(dataset, wordvec_type, min_len):
+    """
+    returns a list where the values of the list are
+      label, [token...]
+    """
+    lines = []
+    for filename in dataset.split(","):
+        try:
+            new_lines = open(filename, encoding="utf-8").readlines()
+        except UnicodeDecodeError:
+            logger.error("Could not read {}".format(filename))
+            raise
+        lines.extend(new_lines)
+    lines = [x.strip() for x in lines]
+    lines = [x.split(maxsplit=1) for x in lines if x]
+    lines = [x for x in lines if len(x) > 1]
+    # TODO: maybe do this processing later, once the model is built.
+    # then move the processing into the model so we can use
+    # overloading to potentially make future model types
+    lines = [(x[0], cnn_classifier.update_text(x[1], wordvec_type)) for x in lines]
+    if min_len:
+        lines = [x for x in lines if len(x[1]) >= min_len]
+    return lines
+
+def dataset_labels(dataset):
+    """
+    Returns a sorted list of label name
+    """
+    labels = set([x[0] for x in dataset])
+    if all(re.match("^[0-9]+$", label) for label in labels):
+        # if all of the labels are integers, sort numerically
+        # maybe not super important, but it would be nicer than having
+        # 10 before 2
+        labels = [str(x) for x in sorted(map(int, list(labels)))]
+    else:
+        labels = sorted(list(labels))
+    return labels
+
+def dataset_vocab(dataset):
+    vocab = set()
+    for line in dataset:
+        for word in line[1]:
+            vocab.add(word)
+    vocab = [PAD, UNK] + list(vocab)
+    if vocab[PAD_ID] != PAD or vocab[UNK_ID] != UNK:
+        raise ValueError("Unexpected values for PAD and UNK!")
+    return vocab
+
+def sort_dataset_by_len(dataset):
+    """
+    returns a dict mapping length -> list of items of that length
+    an OrderedDict is used to that the mapping is sorted from smallest to largest
+    """
+    sorted_dataset = collections.OrderedDict()
+    lengths = sorted(list(set(len(x[1]) for x in dataset)))
+    for l in lengths:
+        sorted_dataset[l] = []
+    for item in dataset:
+        sorted_dataset[len(item[1])].append(item)
+    return sorted_dataset
+
+def shuffle_dataset(sorted_dataset):
+    """
+    Given a dataset sorted by len, sorts within each length to make
+    chunks of roughly the same size.  Returns all items as a single list.
+    """
+    dataset = []
+    for l in sorted_dataset.keys():
+        items = list(sorted_dataset[l])
+        random.shuffle(items)
+        dataset.extend(items)
+    return dataset
+
+def confusion_dataset(model, dataset, device=None):
+    """
+    Returns a confusion matrix
+
+    First key: gold
+    Second key: predicted
+    so: confusion[gold][predicted]
+    """
+    model.eval()
+    index_label_map = {x: y for (x, y) in enumerate(model.labels)}
+    if device is None:
+        device = next(model.parameters()).device
+
+    dataset_lengths = sort_dataset_by_len(dataset)
+
+    confusion = {}
+    for label in model.labels:
+        confusion[label] = {}
+
+    for length in dataset_lengths.keys():
+        batch = dataset_lengths[length]
+        text = [x[1] for x in batch]
+        expected_labels = [x[0] for x in batch]
+
+        output = model(text, device)
+        for i in range(len(expected_labels)):
+            predicted = torch.argmax(output[i])
+            predicted_label = index_label_map[predicted.item()]
+            confusion[expected_labels[i]][predicted_label] = confusion[expected_labels[i]].get(predicted_label, 0) + 1
+
+    return confusion
+
+
+def confusion_to_accuracy(confusion):
+    """
+    Given a confusion dictionary returned by confusion_dataset, return correct, total
+    """
+    correct = 0
+    total = 0
+    for l1 in confusion.keys():
+        for l2 in confusion[l1].keys():
+            if l1 == l2:
+                correct = correct + confusion[l1][l2]
+            else:
+                total = total + confusion[l1][l2]
+    return correct, (correct + total)
+
+def confusion_to_macro_f1(confusion):
+    """
+    Return the macro f1 for a confusion matrix.
+    """
+    keys = set()
+    for k in confusion.keys():
+        keys.add(k)
+        for k2 in confusion.get(k).keys():
+            keys.add(k2)
+
+    sum_f1 = 0
+    for k in keys:
+        tp = 0
+        fn = 0
+        fp = 0
+        for k2 in keys:
+            if k == k2:
+                tp = confusion.get(k, {}).get(k, 0)
+            else:
+                fn = fn + confusion.get(k, {}).get(k2, 0)
+                fp = fp + confusion.get(k2, {}).get(k, 0)
+        precision = tp / (tp + fp)
+        recall = tp / (tp + fn)
+        f1 = 2 * (precision * recall) / (precision + recall)
+        sum_f1 = sum_f1 + f1
+
+    return sum_f1 / len(keys)
+
+
+def format_confusion(confusion, labels, hide_zeroes=False):
+    """
+    pretty print for confusion matrixes
+    adapted from https://gist.github.com/zachguo/10296432
+    """
+    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length
+    empty_cell = " " * columnwidth
+
+    fst_empty_cell = (columnwidth-3)//2 * " " + "t/p" + (columnwidth-3)//2 * " "
+
+    if len(fst_empty_cell) < len(empty_cell):
+        fst_empty_cell = " " * (len(empty_cell) - len(fst_empty_cell)) + fst_empty_cell
+    # Print header
+    header = "    " + fst_empty_cell + " "
+
+    for label in labels:
+        header = header + "%{0}s ".format(columnwidth) % label
+    text = [header]
+
+    # Print rows
+    for i, label1 in enumerate(labels):
+        row = "    %{0}s ".format(columnwidth) % label1
+        for j, label2 in enumerate(labels):
+            confusion_cell = confusion.get(label1, {}).get(label2, 0)
+            cell = "%{0}.1f".format(columnwidth) % confusion_cell
+            if hide_zeroes:
+                cell = cell if confusion_cell else empty_cell
+            row = row + cell + " "
+        text.append(row)
+    return "\n".join(text)
+
+
+def score_dataset(model, dataset, label_map=None, device=None,
+                  remap_labels=None, forgive_unmapped_labels=False):
+    """
+    remap_labels: a dict from old label to new label to use when
+    testing a classifier on a dataset with a simpler label set.
+    For example, a model trained on 5 class sentiment can be tested
+    on a binary distribution with {"0": "0", "1": "0", "3": "1", "4": "1"}
+
+    forgive_unmapped_labels says the following: in the case that the
+    model predicts "2" in the above example for remap_labels, instead
+    treat the model's prediction as whichever label it gave the
+    highest score
+    """
+    model.eval()
+    if label_map is None:
+        label_map = {x: y for (y, x) in enumerate(model.labels)}
+    if device is None:
+        device = next(model.parameters()).device
+    correct = 0
+    dataset_lengths = sort_dataset_by_len(dataset)
+
+    for length in dataset_lengths.keys():
+        # TODO: possibly break this up into smaller batches
+        batch = dataset_lengths[length]
+        text = [x[1] for x in batch]
+        expected_labels = [label_map[x[0]] for x in batch]
+
+        output = model(text, device)
+
+        for i in range(len(expected_labels)):
+            predicted = torch.argmax(output[i])
+            predicted_label = predicted.item()
+            if remap_labels:
+                if predicted_label in remap_labels:
+                    predicted_label = remap_labels[predicted_label]
+                else:
+                    found = False
+                    if forgive_unmapped_labels:
+                        items = []
+                        for j in range(len(output[i])):
+                            items.append((output[i][j].item(), j))
+                        items.sort(key=lambda x: -x[0])
+                        for _, item in items:
+                            if item in remap_labels:
+                                predicted_label = remap_labels[item]
+                                found = True
+                                break
+                    # if slack guesses allowed, none of the existing
+                    # labels matched, so we count it wrong.  if slack
+                    # guesses not allowed, just count it wrong
+                    if not found:
+                        continue
+
+            if predicted_label == expected_labels[i]:
+                correct = correct + 1
+    return correct
+
+def check_labels(labels, dataset):
+    """
+    Check that all of the labels in the dataset are in the known labels.
+    Actually, unknown labels could be acceptable if we just treat the model as always wrong.
+    However, this is a good sanity check to make sure the datasets match
+    """
+    new_labels = dataset_labels(dataset)
+    not_found = [i for i in new_labels if i not in labels]
+    if not_found:
+        raise RuntimeError('Dataset contains labels which the model does not know about:' + str(not_found))
+
+def checkpoint_name(filename, epoch, acc):
+    """
+    Build an informative checkpoint name from a base name, epoch #, and accuracy
+    """
+    root, ext = os.path.splitext(filename)
+    return root + ".E{epoch:04d}-ACC{acc:05.2f}".format(**{"epoch": epoch, "acc": acc * 100}) + ext
+
+def train_model(model, model_file, args, train_set, dev_set, labels):
+    # TODO: separate this into a trainer like the other models.
+    # TODO: possibly reuse the trainer code other models have
+    # TODO: use a (torch) dataloader to possibly speed up the GPU usage
+    device = next(model.parameters()).device
+    logger.info("Current device: %s" % device)
+
+    # TODO: if reloading a model for continued training, the internal
+    # parameters for the optimizer should be reloaded as well
+    # Otherwise this ability is actually not very useful
+    if args.optim.lower() == 'sgd':
+        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9,
+                              weight_decay=args.weight_decay)
+    elif args.optim.lower() == 'adadelta':
+        optimizer = optim.Adadelta(model.parameters(), weight_decay=args.weight_decay)
+    else:
+        raise ValueError("Unknown optimizer: %s" % args.optim)
+
+    label_map = {x: y for (y, x) in enumerate(labels)}
+    label_tensors = {x: torch.tensor(y, requires_grad=False, device=device)
+                     for (y, x) in enumerate(labels)}
+
+    if args.loss == Loss.CROSS:
+        loss_function = nn.CrossEntropyLoss()
+    elif args.loss == Loss.WEIGHTED_CROSS:
+        loss_function = loss.weighted_cross_entropy_loss([label_map[x[0]] for x in train_set], log_dampened=False)
+    elif args.loss == Loss.LOG_CROSS:
+        loss_function = loss.weighted_cross_entropy_loss([label_map[x[0]] for x in train_set], log_dampened=True)
+    else:
+        raise ValueError("Unknown loss function {}".format(args.loss))
+    if args.cuda:
+        loss_function.cuda()
+
+    train_set_by_len = sort_dataset_by_len(train_set)
+
+    if args.load_name:
+        # We reloaded the model, so let's report its current dev set score
+        correct = score_dataset(model, dev_set, label_map, device)
+        logger.info("Reloaded model for continued training.  Dev set: %d correct of %d examples.  Accuracy: %f" %
+                    (correct, len(dev_set), correct / len(dev_set)))
+
+    # https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html
+    batch_starts = list(range(0, len(train_set), args.batch_size))
+    for epoch in range(args.max_epochs):
+        running_loss = 0.0
+        epoch_loss = 0.0
+        shuffled = shuffle_dataset(train_set_by_len)
+        model.train()
+        random.shuffle(batch_starts)
+        for batch_num, start_batch in enumerate(batch_starts):
+            logger.debug("Starting batch: %d" % start_batch)
+            batch = shuffled[start_batch:start_batch+args.batch_size]
+            text = [x[1] for x in batch]
+            label = torch.stack([label_tensors[x[0]] for x in batch])
+
+            # zero the parameter gradients
+            optimizer.zero_grad()
+
+            outputs = model(text, device)
+            batch_loss = loss_function(outputs, label)
+            batch_loss.backward()
+            optimizer.step()
+
+            # print statistics
+            running_loss += batch_loss.item()
+            if ((batch_num + 1) * args.batch_size) % 2000 < args.batch_size: # print every 2000 items
+                logger.info('[%d, %5d] average loss: %.3f' %
+                            (epoch + 1, ((batch_num + 1) * args.batch_size), running_loss / 2000))
+                epoch_loss += running_loss
+                running_loss = 0.0
+        # Add any leftover loss to the epoch_loss
+        epoch_loss += running_loss
+
+        correct = score_dataset(model, dev_set, label_map, device)
+        logger.info("Finished epoch %d.  Dev set: %d correct of %d examples.  Accuracy: %f  Total loss: %f" %
+                    ((epoch + 1), correct, len(dev_set), correct / len(dev_set), epoch_loss))
+
+        checkpoint_file = checkpoint_name(model_file, epoch + 1, correct / len(dev_set))
+        cnn_classifier.save(checkpoint_file, model)
+
+    cnn_classifier.save(model_file, model)
+
+def load_pretrain(args):
+    if args.wordvec_pretrain_file:
+        pretrain_file = args.wordvec_pretrain_file
+    elif args.wordvec_type:
+        pretrain_file = '{}/{}.{}.pretrain.pt'.format(args.save_dir, args.shorthand, args.wordvec_type.name.lower())
+    else:
+        raise Exception("TODO: need to get the wv type back from get_wordvec_file")
+
+    logger.info("Looking for pretrained vectors in {}".format(pretrain_file))
+    if os.path.exists(pretrain_file):
+        vec_file = None
+    elif args.wordvec_raw_file:
+        vec_file = args.wordvec_raw_file
+        logger.info("Pretrain not found.  Looking in {}".format(vec_file))
+    else:
+        vec_file = utils.get_wordvec_file(args.wordvec_dir, args.shorthand, args.wordvec_type.name.lower())
+        logger.info("Pretrain not found.  Looking in {}".format(vec_file))
+    pretrain = Pretrain(pretrain_file, vec_file, args.pretrain_max_vocab)
+    logger.info("Embedding shape: %s" % str(pretrain.emb.shape))
+    return pretrain
+
+
+def print_args(args):
+    """
+    For record keeping purposes, print out the arguments when training
+    """
+    args = vars(args)
+    keys = sorted(args.keys())
+    log_lines = ['%s: %s' % (k, args[k]) for k in keys]
+    logger.info('ARGS USED AT TRAINGING TIME:\n%s\n' % '\n'.join(log_lines))
+
+
+def main():
+    args = parse_args()
+    seed = utils.set_random_seed(args.seed, args.cuda)
+    logger.info("Using random seed: %d" % seed)
+
+    utils.ensure_dir(args.save_dir)
+
+    # TODO: maybe the dataset needs to be in a torch data loader in order to
+    # make cuda operations faster
+    if args.train:
+        train_set = read_dataset(args.train_file, args.wordvec_type, args.min_train_len)
+        logger.info("Using training set: %s" % args.train_file)
+        logger.info("Training set has %d labels" % len(dataset_labels(train_set)))
+    elif not args.load_name:
+        raise ValueError("No model provided and not asked to train a model.  This makes no sense")
+    else:
+        train_set = None
+
+    pretrain = load_pretrain(args)
+
+    if args.load_name:
+        model = cnn_classifier.load(args.load_name, pretrain)
+    else:
+        assert train_set is not None
+        labels = dataset_labels(train_set)
+        extra_vocab = dataset_vocab(train_set)
+        model = cnn_classifier.CNNClassifier(pretrain.emb, pretrain.vocab, extra_vocab, labels, args)
+
+    if args.cuda:
+        model.cuda()
+
+    logger.info("Filter sizes: %s" % str(model.config.filter_sizes))
+    logger.info("Filter channels: %s" % str(model.config.filter_channels))
+    logger.info("Intermediate layers: %s" % str(model.config.fc_shapes))
+
+    save_name = args.save_name
+    if not(save_name):
+        save_name = args.base_name + "_" + args.shorthand + "_"
+        save_name = save_name + "FS_%s_" % "_".join([str(x) for x in model.config.filter_sizes])
+        save_name = save_name + "C_%d_" % model.config.filter_channels
+        if model.config.fc_shapes:
+            save_name = save_name + "FC_%s_" % "_".join([str(x) for x in model.config.fc_shapes])
+        save_name = save_name + "classifier.pt"
+    model_file = os.path.join(args.save_dir, save_name)
+
+    if args.train:
+        print_args(args)
+
+        dev_set = read_dataset(args.dev_file, args.wordvec_type, min_len=None)
+        logger.info("Using dev set: %s" % args.dev_file)
+        check_labels(model.labels, dev_set)
+
+        train_model(model, model_file, args, train_set, dev_set, model.labels)
+
+    test_set = read_dataset(args.test_file, args.wordvec_type, min_len=None)
+    logger.info("Using test set: %s" % args.test_file)
+    check_labels(model.labels, test_set)
+
+    if args.test_remap_labels is None:
+        confusion = confusion_dataset(model, test_set)
+        logger.info("Confusion matrix:\n{}".format(format_confusion(confusion, model.labels)))
+        correct, total = confusion_to_accuracy(confusion)
+        logger.info("Macro f1: {}".format(confusion_to_macro_f1(confusion)))
+    else:
+        correct = score_dataset(model, test_set,
+                                remap_labels=args.test_remap_labels,
+                                forgive_unmapped_labels=args.forgive_unmapped_labels)
+        total = len(test_set)
+    logger.info("Test set: %d correct of %d examples.  Accuracy: %f" %
+                (correct, total, correct / total))
+
+if __name__ == '__main__':
+    main()
```

### Comparing `classla-2.0/classla/models/classifiers/classifier_args.py` & `classla-2.1/classla/models/classifiers/classifier_args.py`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-from enum import Enum
-import torch
-
-"""
-Defines some args which are common between the classifier model(s) and tools which use them
-"""
-
-class WVType(Enum):
-    WORD2VEC = 1
-    GOOGLE = 2
-    FASTTEXT = 3
-    OTHER = 4
-
-class ExtraVectors(Enum):
-    NONE = 1
-    CONCAT = 2
-    SUM = 3
-
-# NLP machines:
-# word2vec are in
-# /u/nlp/data/stanfordnlp/model_production/stanfordnlp/extern_data/word2vec
-# google vectors are in
-# /scr/nlp/data/wordvectors/en/google/GoogleNews-vectors-negative300.txt
-
-def add_common_args(parser):
-    parser.add_argument('--seed', default=None, type=int, help='Random seed for model')
-    add_pretrain_args(parser)
-    add_device_args(parser)
-
-def add_pretrain_args(parser):
-    parser.add_argument('--save_dir', type=str, default='saved_models/classifier', help='Root dir for saving models.')
-    parser.add_argument('--pretrain_max_vocab', type=int, default=-1)
-    parser.add_argument('--wordvec_pretrain_file', type=str, default=None, help='Exact name of the pretrain file to read')
-    parser.add_argument('--wordvec_raw_file', type=str, default=None, help='Exact name of the raw wordvec file to read')
-    parser.add_argument('--wordvec_dir', type=str, default='extern_data', help='Directory of word vectors')
-    parser.add_argument('--wordvec_type', type=lambda x: WVType[x.upper()], default='word2vec', help='Different vector types have different options, such as google 300d replacing numbers with #')
-    parser.add_argument('--shorthand', type=str, default='en_ewt', help="Treebank shorthand, eg 'en' for English")
-    parser.add_argument('--extra_wordvec_dim', type=int, default=0, help="Extra dim of word vectors - will be trained")
-    parser.add_argument('--extra_wordvec_method', type=lambda x: ExtraVectors[x.upper()], default='none', help='How to train extra dimensions of word vectors, if at all')
-    parser.add_argument('--extra_wordvec_max_norm', type=float, default=None, help="Max norm for initializing the extra vectors")
-
-def add_device_args(parser):
-    parser.add_argument('--cuda', action='store_true', help='Use CUDA for training/testing', default=torch.cuda.is_available())
-    parser.add_argument('--cpu', action='store_false', help='Ignore CUDA.', dest='cuda')
-
+from enum import Enum
+import torch
+
+"""
+Defines some args which are common between the classifier model(s) and tools which use them
+"""
+
+class WVType(Enum):
+    WORD2VEC = 1
+    GOOGLE = 2
+    FASTTEXT = 3
+    OTHER = 4
+
+class ExtraVectors(Enum):
+    NONE = 1
+    CONCAT = 2
+    SUM = 3
+
+# NLP machines:
+# word2vec are in
+# /u/nlp/data/stanfordnlp/model_production/stanfordnlp/extern_data/word2vec
+# google vectors are in
+# /scr/nlp/data/wordvectors/en/google/GoogleNews-vectors-negative300.txt
+
+def add_common_args(parser):
+    parser.add_argument('--seed', default=None, type=int, help='Random seed for model')
+    add_pretrain_args(parser)
+    add_device_args(parser)
+
+def add_pretrain_args(parser):
+    parser.add_argument('--save_dir', type=str, default='saved_models/classifier', help='Root dir for saving models.')
+    parser.add_argument('--pretrain_max_vocab', type=int, default=-1)
+    parser.add_argument('--wordvec_pretrain_file', type=str, default=None, help='Exact name of the pretrain file to read')
+    parser.add_argument('--wordvec_raw_file', type=str, default=None, help='Exact name of the raw wordvec file to read')
+    parser.add_argument('--wordvec_dir', type=str, default='extern_data', help='Directory of word vectors')
+    parser.add_argument('--wordvec_type', type=lambda x: WVType[x.upper()], default='word2vec', help='Different vector types have different options, such as google 300d replacing numbers with #')
+    parser.add_argument('--shorthand', type=str, default='en_ewt', help="Treebank shorthand, eg 'en' for English")
+    parser.add_argument('--extra_wordvec_dim', type=int, default=0, help="Extra dim of word vectors - will be trained")
+    parser.add_argument('--extra_wordvec_method', type=lambda x: ExtraVectors[x.upper()], default='none', help='How to train extra dimensions of word vectors, if at all')
+    parser.add_argument('--extra_wordvec_max_norm', type=float, default=None, help="Max norm for initializing the extra vectors")
+
+def add_device_args(parser):
+    parser.add_argument('--cuda', action='store_true', help='Use CUDA for training/testing', default=torch.cuda.is_available())
+    parser.add_argument('--cpu', action='store_false', help='Ignore CUDA.', dest='cuda')
+
```

### Comparing `classla-2.0/classla/models/common/biaffine.py` & `classla-2.1/classla/models/common/biaffine.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,80 +1,80 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-class PairwiseBilinear(nn.Module):
-    ''' A bilinear module that deals with broadcasting for efficient memory usage.
-    Input: tensors of sizes (N x L1 x D1) and (N x L2 x D2)
-    Output: tensor of size (N x L1 x L2 x O)'''
-    def __init__(self, input1_size, input2_size, output_size, bias=True):
-        super().__init__()
-
-        self.input1_size = input1_size
-        self.input2_size = input2_size
-        self.output_size = output_size
-
-        self.weight = nn.Parameter(torch.Tensor(input1_size, input2_size, output_size))
-        self.bias = nn.Parameter(torch.Tensor(output_size)) if bias else 0
-
-    def forward(self, input1, input2):
-        input1_size = list(input1.size())
-        input2_size = list(input2.size())
-        output_size = [input1_size[0], input1_size[1], input2_size[1], self.output_size]
-
-        # ((N x L1) x D1) * (D1 x (D2 x O)) -> (N x L1) x (D2 x O)
-        intermediate = torch.mm(input1.view(-1, input1_size[-1]), self.weight.view(-1, self.input2_size * self.output_size))
-        # (N x L2 x D2) -> (N x D2 x L2)
-        input2 = input2.transpose(1, 2)
-        # (N x (L1 x O) x D2) * (N x D2 x L2) -> (N x (L1 x O) x L2)
-        output = intermediate.view(input1_size[0], input1_size[1] * self.output_size, input2_size[2]).bmm(input2)
-        # (N x (L1 x O) x L2) -> (N x L1 x L2 x O)
-        output = output.view(input1_size[0], input1_size[1], self.output_size, input2_size[1]).transpose(2, 3)
-
-        return output
-
-class BiaffineScorer(nn.Module):
-    def __init__(self, input1_size, input2_size, output_size):
-        super().__init__()
-        self.W_bilin = nn.Bilinear(input1_size + 1, input2_size + 1, output_size)
-
-        self.W_bilin.weight.data.zero_()
-        self.W_bilin.bias.data.zero_()
-
-    def forward(self, input1, input2):
-        input1 = torch.cat([input1, input1.new_ones(*input1.size()[:-1], 1)], len(input1.size())-1)
-        input2 = torch.cat([input2, input2.new_ones(*input2.size()[:-1], 1)], len(input2.size())-1)
-        return self.W_bilin(input1, input2)
-
-class PairwiseBiaffineScorer(nn.Module):
-    def __init__(self, input1_size, input2_size, output_size):
-        super().__init__()
-        self.W_bilin = PairwiseBilinear(input1_size + 1, input2_size + 1, output_size)
-
-        self.W_bilin.weight.data.zero_()
-        self.W_bilin.bias.data.zero_()
-
-    def forward(self, input1, input2):
-        input1 = torch.cat([input1, input1.new_ones(*input1.size()[:-1], 1)], len(input1.size())-1)
-        input2 = torch.cat([input2, input2.new_ones(*input2.size()[:-1], 1)], len(input2.size())-1)
-        return self.W_bilin(input1, input2)
-
-class DeepBiaffineScorer(nn.Module):
-    def __init__(self, input1_size, input2_size, hidden_size, output_size, hidden_func=F.relu, dropout=0, pairwise=True):
-        super().__init__()
-        self.W1 = nn.Linear(input1_size, hidden_size)
-        self.W2 = nn.Linear(input2_size, hidden_size)
-        self.hidden_func = hidden_func
-        if pairwise:
-            self.scorer = PairwiseBiaffineScorer(hidden_size, hidden_size, output_size)
-        else:
-            self.scorer = BiaffineScorer(hidden_size, hidden_size, output_size)
-        self.dropout = nn.Dropout(dropout)
-
-    def forward(self, input1, input2):
-        return self.scorer(self.dropout(self.hidden_func(self.W1(input1))), self.dropout(self.hidden_func(self.W2(input2))))
-
-if __name__ == "__main__":
-    x1 = torch.randn(3,4)
-    x2 = torch.randn(3,5)
-    scorer = DeepBiaffineScorer(4, 5, 6, 7)
-    print(scorer(x1, x2))
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+class PairwiseBilinear(nn.Module):
+    ''' A bilinear module that deals with broadcasting for efficient memory usage.
+    Input: tensors of sizes (N x L1 x D1) and (N x L2 x D2)
+    Output: tensor of size (N x L1 x L2 x O)'''
+    def __init__(self, input1_size, input2_size, output_size, bias=True):
+        super().__init__()
+
+        self.input1_size = input1_size
+        self.input2_size = input2_size
+        self.output_size = output_size
+
+        self.weight = nn.Parameter(torch.Tensor(input1_size, input2_size, output_size))
+        self.bias = nn.Parameter(torch.Tensor(output_size)) if bias else 0
+
+    def forward(self, input1, input2):
+        input1_size = list(input1.size())
+        input2_size = list(input2.size())
+        output_size = [input1_size[0], input1_size[1], input2_size[1], self.output_size]
+
+        # ((N x L1) x D1) * (D1 x (D2 x O)) -> (N x L1) x (D2 x O)
+        intermediate = torch.mm(input1.view(-1, input1_size[-1]), self.weight.view(-1, self.input2_size * self.output_size))
+        # (N x L2 x D2) -> (N x D2 x L2)
+        input2 = input2.transpose(1, 2)
+        # (N x (L1 x O) x D2) * (N x D2 x L2) -> (N x (L1 x O) x L2)
+        output = intermediate.view(input1_size[0], input1_size[1] * self.output_size, input2_size[2]).bmm(input2)
+        # (N x (L1 x O) x L2) -> (N x L1 x L2 x O)
+        output = output.view(input1_size[0], input1_size[1], self.output_size, input2_size[1]).transpose(2, 3)
+
+        return output
+
+class BiaffineScorer(nn.Module):
+    def __init__(self, input1_size, input2_size, output_size):
+        super().__init__()
+        self.W_bilin = nn.Bilinear(input1_size + 1, input2_size + 1, output_size)
+
+        self.W_bilin.weight.data.zero_()
+        self.W_bilin.bias.data.zero_()
+
+    def forward(self, input1, input2):
+        input1 = torch.cat([input1, input1.new_ones(*input1.size()[:-1], 1)], len(input1.size())-1)
+        input2 = torch.cat([input2, input2.new_ones(*input2.size()[:-1], 1)], len(input2.size())-1)
+        return self.W_bilin(input1, input2)
+
+class PairwiseBiaffineScorer(nn.Module):
+    def __init__(self, input1_size, input2_size, output_size):
+        super().__init__()
+        self.W_bilin = PairwiseBilinear(input1_size + 1, input2_size + 1, output_size)
+
+        self.W_bilin.weight.data.zero_()
+        self.W_bilin.bias.data.zero_()
+
+    def forward(self, input1, input2):
+        input1 = torch.cat([input1, input1.new_ones(*input1.size()[:-1], 1)], len(input1.size())-1)
+        input2 = torch.cat([input2, input2.new_ones(*input2.size()[:-1], 1)], len(input2.size())-1)
+        return self.W_bilin(input1, input2)
+
+class DeepBiaffineScorer(nn.Module):
+    def __init__(self, input1_size, input2_size, hidden_size, output_size, hidden_func=F.relu, dropout=0, pairwise=True):
+        super().__init__()
+        self.W1 = nn.Linear(input1_size, hidden_size)
+        self.W2 = nn.Linear(input2_size, hidden_size)
+        self.hidden_func = hidden_func
+        if pairwise:
+            self.scorer = PairwiseBiaffineScorer(hidden_size, hidden_size, output_size)
+        else:
+            self.scorer = BiaffineScorer(hidden_size, hidden_size, output_size)
+        self.dropout = nn.Dropout(dropout)
+
+    def forward(self, input1, input2):
+        return self.scorer(self.dropout(self.hidden_func(self.W1(input1))), self.dropout(self.hidden_func(self.W2(input2))))
+
+if __name__ == "__main__":
+    x1 = torch.randn(3,4)
+    x2 = torch.randn(3,5)
+    scorer = DeepBiaffineScorer(4, 5, 6, 7)
+    print(scorer(x1, x2))
```

### Comparing `classla-2.0/classla/models/common/char_model.py` & `classla-2.1/classla/models/common/char_model.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,136 +1,136 @@
-import torch
-import torch.nn as nn
-from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence, pack_padded_sequence, PackedSequence
-
-from classla.models.common.packed_lstm import PackedLSTM
-from classla.models.common.utils import tensor_unsort, unsort
-from classla.models.common.dropout import SequenceUnitDropout
-from classla.models.common.vocab import UNK_ID
-from classla.models.pos.vocab import CharVocab
-
-class CharacterModel(nn.Module):
-    def __init__(self, args, vocab, pad=False, bidirectional=False, attention=True):
-        super().__init__()
-        self.args = args
-        self.pad = pad
-        self.num_dir = 2 if bidirectional else 1
-        self.attn = attention
-
-        # char embeddings
-        self.char_emb = nn.Embedding(len(vocab['char']), self.args['char_emb_dim'], padding_idx=0)
-        if self.attn: 
-            self.char_attn = nn.Linear(self.num_dir * self.args['char_hidden_dim'], 1, bias=False)
-            self.char_attn.weight.data.zero_()
-
-        # modules
-        self.charlstm = PackedLSTM(self.args['char_emb_dim'], self.args['char_hidden_dim'], self.args['char_num_layers'], batch_first=True, \
-                dropout=0 if self.args['char_num_layers'] == 1 else args['dropout'], rec_dropout = self.args['char_rec_dropout'], bidirectional=bidirectional)
-        self.charlstm_h_init = nn.Parameter(torch.zeros(self.num_dir * self.args['char_num_layers'], 1, self.args['char_hidden_dim']))
-        self.charlstm_c_init = nn.Parameter(torch.zeros(self.num_dir * self.args['char_num_layers'], 1, self.args['char_hidden_dim']))
-
-        self.dropout = nn.Dropout(args['dropout'])
-
-    def forward(self, chars, chars_mask, word_orig_idx, sentlens, wordlens):
-        embs = self.dropout(self.char_emb(chars))
-        batch_size = embs.size(0)
-        embs = pack_padded_sequence(embs, wordlens, batch_first=True)
-        output = self.charlstm(embs, wordlens, hx=(\
-                self.charlstm_h_init.expand(self.num_dir * self.args['char_num_layers'], batch_size, self.args['char_hidden_dim']).contiguous(), \
-                self.charlstm_c_init.expand(self.num_dir * self.args['char_num_layers'], batch_size, self.args['char_hidden_dim']).contiguous()))
-         
-        # apply attention, otherwise take final states
-        if self.attn:
-            char_reps = output[0]
-            weights = torch.sigmoid(self.char_attn(self.dropout(char_reps.data)))
-            char_reps = PackedSequence(char_reps.data * weights, char_reps.batch_sizes)
-            char_reps, _ = pad_packed_sequence(char_reps, batch_first=True)
-            res = char_reps.sum(1)
-        else:
-            h, c = output[1]
-            res = h[-2:].transpose(0,1).contiguous().view(batch_size, -1)
-
-        # recover character order and word separation
-        res = tensor_unsort(res, word_orig_idx)
-        res = pack_sequence(res.split(sentlens))
-        if self.pad:
-            res = pad_packed_sequence(res, batch_first=True)[0]
-
-        return res
-
-class CharacterLanguageModel(nn.Module):
-
-    def __init__(self, args, vocab, pad=False, is_forward_lm=True):
-        super().__init__()
-        self.args = args
-        self.vocab = vocab
-        self.is_forward_lm = is_forward_lm
-        self.pad = pad
-        self.finetune = True # always finetune unless otherwise specified
-
-        # char embeddings
-        self.char_emb = nn.Embedding(len(self.vocab['char']), self.args['char_emb_dim'], padding_idx=None) # we use space as padding, so padding_idx is not necessary
-        
-        # modules
-        self.charlstm = PackedLSTM(self.args['char_emb_dim'], self.args['char_hidden_dim'], self.args['char_num_layers'], batch_first=True, \
-                dropout=0 if self.args['char_num_layers'] == 1 else args['char_dropout'], rec_dropout = self.args['char_rec_dropout'], bidirectional=False)
-        self.charlstm_h_init = nn.Parameter(torch.zeros(self.args['char_num_layers'], 1, self.args['char_hidden_dim']))
-        self.charlstm_c_init = nn.Parameter(torch.zeros(self.args['char_num_layers'], 1, self.args['char_hidden_dim']))
-
-        # decoder
-        self.decoder = nn.Linear(self.args['char_hidden_dim'], len(self.vocab['char']))
-        self.dropout = nn.Dropout(args['char_dropout'])
-        self.char_dropout = SequenceUnitDropout(args.get('char_unit_dropout', 0), UNK_ID)
-
-    def forward(self, chars, charlens, hidden=None):
-        chars = self.char_dropout(chars)
-        embs = self.dropout(self.char_emb(chars))
-        batch_size = embs.size(0)
-        embs = pack_padded_sequence(embs, charlens, batch_first=True)
-        if hidden is None: 
-            hidden = (self.charlstm_h_init.expand(self.args['char_num_layers'], batch_size, self.args['char_hidden_dim']).contiguous(),
-                      self.charlstm_c_init.expand(self.args['char_num_layers'], batch_size, self.args['char_hidden_dim']).contiguous())
-        output, hidden = self.charlstm(embs, charlens, hx=hidden)
-        output = self.dropout(pad_packed_sequence(output, batch_first=True)[0])
-        decoded = self.decoder(output)
-        return output, hidden, decoded
-
-    def get_representation(self, chars, charoffsets, charlens, char_orig_idx):
-        with torch.no_grad():
-            output, _, _ = self.forward(chars, charlens)
-            res = [output[i, offsets] for i, offsets in enumerate(charoffsets)]
-            res = unsort(res, char_orig_idx)
-            res = pack_sequence(res)
-            if self.pad:
-                res = pad_packed_sequence(res, batch_first=True)[0]
-        return res
-    
-    def train(self, mode=True):
-        """
-        Override the default train() function, so that when self.finetune == False, the training mode 
-        won't be impacted by the parent models' status change.
-        """
-        if not mode: # eval() is always allowed, regardless of finetune status
-            super().train(mode)
-        else:
-            if self.finetune: # only set to training mode in finetune status
-                super().train(mode)
-
-    def save(self, filename):
-        state = {
-            'vocab': self.vocab['char'].state_dict(),
-            'args': self.args,
-            'state_dict': self.state_dict(),
-            'pad': self.pad,
-            'is_forward_lm': self.is_forward_lm
-        }
-        torch.save(state, filename)
-
-    @classmethod
-    def load(cls, filename, finetune=False):
-        state = torch.load(filename, lambda storage, loc: storage)
-        vocab = {'char': CharVocab.load_state_dict(state['vocab'])}
-        model = cls(state['args'], vocab, state['pad'], state['is_forward_lm'])
-        model.load_state_dict(state['state_dict'])
-        model.eval()
-        model.finetune = finetune # set finetune status
-        return model
+import torch
+import torch.nn as nn
+from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence, pack_padded_sequence, PackedSequence
+
+from classla.models.common.packed_lstm import PackedLSTM
+from classla.models.common.utils import tensor_unsort, unsort
+from classla.models.common.dropout import SequenceUnitDropout
+from classla.models.common.vocab import UNK_ID
+from classla.models.pos.vocab import CharVocab
+
+class CharacterModel(nn.Module):
+    def __init__(self, args, vocab, pad=False, bidirectional=False, attention=True):
+        super().__init__()
+        self.args = args
+        self.pad = pad
+        self.num_dir = 2 if bidirectional else 1
+        self.attn = attention
+
+        # char embeddings
+        self.char_emb = nn.Embedding(len(vocab['char']), self.args['char_emb_dim'], padding_idx=0)
+        if self.attn: 
+            self.char_attn = nn.Linear(self.num_dir * self.args['char_hidden_dim'], 1, bias=False)
+            self.char_attn.weight.data.zero_()
+
+        # modules
+        self.charlstm = PackedLSTM(self.args['char_emb_dim'], self.args['char_hidden_dim'], self.args['char_num_layers'], batch_first=True, \
+                dropout=0 if self.args['char_num_layers'] == 1 else args['dropout'], rec_dropout = self.args['char_rec_dropout'], bidirectional=bidirectional)
+        self.charlstm_h_init = nn.Parameter(torch.zeros(self.num_dir * self.args['char_num_layers'], 1, self.args['char_hidden_dim']))
+        self.charlstm_c_init = nn.Parameter(torch.zeros(self.num_dir * self.args['char_num_layers'], 1, self.args['char_hidden_dim']))
+
+        self.dropout = nn.Dropout(args['dropout'])
+
+    def forward(self, chars, chars_mask, word_orig_idx, sentlens, wordlens):
+        embs = self.dropout(self.char_emb(chars))
+        batch_size = embs.size(0)
+        embs = pack_padded_sequence(embs, wordlens, batch_first=True)
+        output = self.charlstm(embs, wordlens, hx=(\
+                self.charlstm_h_init.expand(self.num_dir * self.args['char_num_layers'], batch_size, self.args['char_hidden_dim']).contiguous(), \
+                self.charlstm_c_init.expand(self.num_dir * self.args['char_num_layers'], batch_size, self.args['char_hidden_dim']).contiguous()))
+         
+        # apply attention, otherwise take final states
+        if self.attn:
+            char_reps = output[0]
+            weights = torch.sigmoid(self.char_attn(self.dropout(char_reps.data)))
+            char_reps = PackedSequence(char_reps.data * weights, char_reps.batch_sizes)
+            char_reps, _ = pad_packed_sequence(char_reps, batch_first=True)
+            res = char_reps.sum(1)
+        else:
+            h, c = output[1]
+            res = h[-2:].transpose(0,1).contiguous().view(batch_size, -1)
+
+        # recover character order and word separation
+        res = tensor_unsort(res, word_orig_idx)
+        res = pack_sequence(res.split(sentlens))
+        if self.pad:
+            res = pad_packed_sequence(res, batch_first=True)[0]
+
+        return res
+
+class CharacterLanguageModel(nn.Module):
+
+    def __init__(self, args, vocab, pad=False, is_forward_lm=True):
+        super().__init__()
+        self.args = args
+        self.vocab = vocab
+        self.is_forward_lm = is_forward_lm
+        self.pad = pad
+        self.finetune = True # always finetune unless otherwise specified
+
+        # char embeddings
+        self.char_emb = nn.Embedding(len(self.vocab['char']), self.args['char_emb_dim'], padding_idx=None) # we use space as padding, so padding_idx is not necessary
+        
+        # modules
+        self.charlstm = PackedLSTM(self.args['char_emb_dim'], self.args['char_hidden_dim'], self.args['char_num_layers'], batch_first=True, \
+                dropout=0 if self.args['char_num_layers'] == 1 else args['char_dropout'], rec_dropout = self.args['char_rec_dropout'], bidirectional=False)
+        self.charlstm_h_init = nn.Parameter(torch.zeros(self.args['char_num_layers'], 1, self.args['char_hidden_dim']))
+        self.charlstm_c_init = nn.Parameter(torch.zeros(self.args['char_num_layers'], 1, self.args['char_hidden_dim']))
+
+        # decoder
+        self.decoder = nn.Linear(self.args['char_hidden_dim'], len(self.vocab['char']))
+        self.dropout = nn.Dropout(args['char_dropout'])
+        self.char_dropout = SequenceUnitDropout(args.get('char_unit_dropout', 0), UNK_ID)
+
+    def forward(self, chars, charlens, hidden=None):
+        chars = self.char_dropout(chars)
+        embs = self.dropout(self.char_emb(chars))
+        batch_size = embs.size(0)
+        embs = pack_padded_sequence(embs, charlens, batch_first=True)
+        if hidden is None: 
+            hidden = (self.charlstm_h_init.expand(self.args['char_num_layers'], batch_size, self.args['char_hidden_dim']).contiguous(),
+                      self.charlstm_c_init.expand(self.args['char_num_layers'], batch_size, self.args['char_hidden_dim']).contiguous())
+        output, hidden = self.charlstm(embs, charlens, hx=hidden)
+        output = self.dropout(pad_packed_sequence(output, batch_first=True)[0])
+        decoded = self.decoder(output)
+        return output, hidden, decoded
+
+    def get_representation(self, chars, charoffsets, charlens, char_orig_idx):
+        with torch.no_grad():
+            output, _, _ = self.forward(chars, charlens)
+            res = [output[i, offsets] for i, offsets in enumerate(charoffsets)]
+            res = unsort(res, char_orig_idx)
+            res = pack_sequence(res)
+            if self.pad:
+                res = pad_packed_sequence(res, batch_first=True)[0]
+        return res
+    
+    def train(self, mode=True):
+        """
+        Override the default train() function, so that when self.finetune == False, the training mode 
+        won't be impacted by the parent models' status change.
+        """
+        if not mode: # eval() is always allowed, regardless of finetune status
+            super().train(mode)
+        else:
+            if self.finetune: # only set to training mode in finetune status
+                super().train(mode)
+
+    def save(self, filename):
+        state = {
+            'vocab': self.vocab['char'].state_dict(),
+            'args': self.args,
+            'state_dict': self.state_dict(),
+            'pad': self.pad,
+            'is_forward_lm': self.is_forward_lm
+        }
+        torch.save(state, filename)
+
+    @classmethod
+    def load(cls, filename, finetune=False):
+        state = torch.load(filename, lambda storage, loc: storage)
+        vocab = {'char': CharVocab.load_state_dict(state['vocab'])}
+        model = cls(state['args'], vocab, state['pad'], state['is_forward_lm'])
+        model.load_state_dict(state['state_dict'])
+        model.eval()
+        model.finetune = finetune # set finetune status
+        return model
```

### Comparing `classla-2.0/classla/models/common/constant.py` & `classla-2.1/classla/models/common/constant.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,86 +1,86 @@
-"""
-Global constants.
-"""
-
-lcode2lang = {
-    "af": "Afrikaans",
-    "grc": "Ancient_Greek",
-    "ar": "Arabic",
-    "hy": "Armenian",
-    "eu": "Basque",
-    "be": "Belarusian",
-    "br": "Breton",
-    "bg": "Bulgarian",
-    "bxr": "Buryat",
-    "ca": "Catalan",
-    "zh-hant": "Traditional_Chinese",
-    "lzh": "Classical_Chinese",
-    "cop": "Coptic",
-    "hr": "Croatian",
-    "cs": "Czech",
-    "da": "Danish",
-    "nl": "Dutch",
-    "en": "English",
-    "et": "Estonian",
-    "fo": "Faroese",
-    "fi": "Finnish",
-    "fr": "French",
-    "gl": "Galician",
-    "de": "German",
-    "got": "Gothic",
-    "el": "Greek",
-    "he": "Hebrew",
-    "hi": "Hindi",
-    "hu": "Hungarian",
-    "id": "Indonesian",
-    "ga": "Irish",
-    "it": "Italian",
-    "ja": "Japanese",
-    "kk": "Kazakh",
-    "ko": "Korean",
-    "kmr": "Kurmanji",
-    "lt": "Lithuanian",
-    "olo": "Livvi",
-    "la": "Latin",
-    "lv": "Latvian",
-    "mt": "Maltese",
-    "mr": "Marathi",
-    "pcm": "Naija",
-    "sme": "North_Sami",
-    "nb": "Norwegian_Bokmaal",
-    "nn": "Norwegian_Nynorsk",
-    "cu": "Old_Church_Slavonic",
-    "fro": "Old_French",
-    "orv": "Old_Russian",
-    "fa": "Persian",
-    "pl": "Polish",
-    "pt": "Portuguese",
-    "ro": "Romanian",
-    "ru": "Russian",
-    "gd": "Scottish_Gaelic",
-    "sr": "Serbian",
-    "zh-hans": "Simplified_Chinese",
-    "sk": "Slovak",
-    "sl": "Slovenian",
-    "es": "Spanish",
-    "sv": "Swedish",
-    "swl": "Swedish_Sign_Language",
-    "ta": "Tamil",
-    "te": "Telugu",
-    "th": "Thai",
-    "tr": "Turkish",
-    "uk": "Ukrainian",
-    "hsb": "Upper_Sorbian",
-    "ur": "Urdu",
-    "ug": "Uyghur",
-    "vi": "Vietnamese",
-    "wo": "Wolof"
-}
-
-lang2lcode = {lcode2lang[k]: k for k in lcode2lang}
-langlower2lcode = {lcode2lang[k].lower(): k.lower() for k in lcode2lang}
-
-# additional useful code to language mapping
-# added after dict invert to avoid conflict
-lcode2lang['nb'] = 'Norwegian' # Norwegian Bokmall mapped to default norwegian
-lcode2lang['zh'] = 'Simplified_Chinese'
+"""
+Global constants.
+"""
+
+lcode2lang = {
+    "af": "Afrikaans",
+    "grc": "Ancient_Greek",
+    "ar": "Arabic",
+    "hy": "Armenian",
+    "eu": "Basque",
+    "be": "Belarusian",
+    "br": "Breton",
+    "bg": "Bulgarian",
+    "bxr": "Buryat",
+    "ca": "Catalan",
+    "zh-hant": "Traditional_Chinese",
+    "lzh": "Classical_Chinese",
+    "cop": "Coptic",
+    "hr": "Croatian",
+    "cs": "Czech",
+    "da": "Danish",
+    "nl": "Dutch",
+    "en": "English",
+    "et": "Estonian",
+    "fo": "Faroese",
+    "fi": "Finnish",
+    "fr": "French",
+    "gl": "Galician",
+    "de": "German",
+    "got": "Gothic",
+    "el": "Greek",
+    "he": "Hebrew",
+    "hi": "Hindi",
+    "hu": "Hungarian",
+    "id": "Indonesian",
+    "ga": "Irish",
+    "it": "Italian",
+    "ja": "Japanese",
+    "kk": "Kazakh",
+    "ko": "Korean",
+    "kmr": "Kurmanji",
+    "lt": "Lithuanian",
+    "olo": "Livvi",
+    "la": "Latin",
+    "lv": "Latvian",
+    "mt": "Maltese",
+    "mr": "Marathi",
+    "pcm": "Naija",
+    "sme": "North_Sami",
+    "nb": "Norwegian_Bokmaal",
+    "nn": "Norwegian_Nynorsk",
+    "cu": "Old_Church_Slavonic",
+    "fro": "Old_French",
+    "orv": "Old_Russian",
+    "fa": "Persian",
+    "pl": "Polish",
+    "pt": "Portuguese",
+    "ro": "Romanian",
+    "ru": "Russian",
+    "gd": "Scottish_Gaelic",
+    "sr": "Serbian",
+    "zh-hans": "Simplified_Chinese",
+    "sk": "Slovak",
+    "sl": "Slovenian",
+    "es": "Spanish",
+    "sv": "Swedish",
+    "swl": "Swedish_Sign_Language",
+    "ta": "Tamil",
+    "te": "Telugu",
+    "th": "Thai",
+    "tr": "Turkish",
+    "uk": "Ukrainian",
+    "hsb": "Upper_Sorbian",
+    "ur": "Urdu",
+    "ug": "Uyghur",
+    "vi": "Vietnamese",
+    "wo": "Wolof"
+}
+
+lang2lcode = {lcode2lang[k]: k for k in lcode2lang}
+langlower2lcode = {lcode2lang[k].lower(): k.lower() for k in lcode2lang}
+
+# additional useful code to language mapping
+# added after dict invert to avoid conflict
+lcode2lang['nb'] = 'Norwegian' # Norwegian Bokmall mapped to default norwegian
+lcode2lang['zh'] = 'Simplified_Chinese'
```

### Comparing `classla-2.0/classla/models/common/crf.py` & `classla-2.1/classla/models/common/crf.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,147 +1,147 @@
-"""
-CRF loss and viterbi decoding.
-"""
-
-import math
-from numbers import Number
-import numpy as np
-import torch
-from torch import nn
-import torch.nn.init as init
-
-class CRFLoss(nn.Module):
-    """
-    Calculate log-space crf loss, given unary potentials, a transition matrix
-    and gold tag sequences.
-    """
-    def __init__(self, num_tag, batch_average=True):
-        super().__init__()
-        self._transitions = nn.Parameter(torch.zeros(num_tag, num_tag))
-        self._batch_average = batch_average # if not batch average, average on all tokens
-
-    def forward(self, inputs, masks, tag_indices):
-        """
-        inputs: batch_size x seq_len x num_tags
-        masks: batch_size x seq_len
-        tag_indices: batch_size x seq_len
-        
-        @return:
-            loss: CRF negative log likelihood on all instances.
-            transitions: the transition matrix
-        """
-        # TODO: handle <start> and <end> tags
-        self.bs, self.sl, self.nc = inputs.size()
-        unary_scores = self.crf_unary_score(inputs, masks, tag_indices)
-        binary_scores = self.crf_binary_score(inputs, masks, tag_indices)
-        log_norm = self.crf_log_norm(inputs, masks, tag_indices)
-        log_likelihood = unary_scores + binary_scores - log_norm # batch_size
-        loss = torch.sum(-log_likelihood)
-        if self._batch_average:
-            loss = loss / self.bs
-        else:
-            total = masks.eq(0).sum()
-            loss = loss / (total + 1e-8)
-        return loss, self._transitions
-
-    def crf_unary_score(self, inputs, masks, tag_indices):
-        """
-        @return:
-            unary_scores: batch_size
-        """
-        flat_inputs = inputs.view(self.bs, -1)
-        flat_tag_indices = tag_indices + \
-                set_cuda(torch.arange(self.sl).long().unsqueeze(0) * self.nc, tag_indices.is_cuda)
-        unary_scores = torch.gather(flat_inputs, 1, flat_tag_indices).view(self.bs, -1)
-        unary_scores.masked_fill_(masks, 0)
-        return unary_scores.sum(dim=1)
-    
-    def crf_binary_score(self, inputs, masks, tag_indices):
-        """
-        @return:
-            binary_scores: batch_size
-        """
-        # get number of transitions
-        nt = tag_indices.size(-1) - 1
-        start_indices = tag_indices[:, :nt]
-        end_indices = tag_indices[:, 1:]
-        # flat matrices
-        flat_transition_indices = start_indices * self.nc + end_indices
-        flat_transition_indices = flat_transition_indices.view(-1)
-        flat_transition_matrix = self._transitions.view(-1)
-        binary_scores = torch.gather(flat_transition_matrix, 0, flat_transition_indices)\
-                .view(self.bs, -1)
-        score_masks = masks[:, 1:]
-        binary_scores.masked_fill_(score_masks, 0)
-        return binary_scores.sum(dim=1)
-
-    def crf_log_norm(self, inputs, masks, tag_indices):
-        """
-        Calculate the CRF partition in log space for each instance, following:
-            http://www.cs.columbia.edu/~mcollins/fb.pdf
-        @return:
-            log_norm: batch_size
-        """
-        start_inputs = inputs[:,0,:] # bs x nc
-        rest_inputs = inputs[:,1:,:]
-        rest_masks = masks[:,1:]
-        alphas = start_inputs # bs x nc
-        trans = self._transitions.unsqueeze(0) # 1 x nc x nc
-        # accumulate alphas in log space
-        for i in range(rest_inputs.size(1)):
-            transition_scores = alphas.unsqueeze(2) + trans # bs x nc x nc
-            new_alphas = rest_inputs[:,i,:] + log_sum_exp(transition_scores, dim=1)
-            m = rest_masks[:,i].unsqueeze(1).expand_as(new_alphas) # bs x nc, 1 for padding idx
-            # apply masks
-            new_alphas.masked_scatter_(m, alphas.masked_select(m))
-            alphas = new_alphas
-        log_norm = log_sum_exp(alphas, dim=1)
-        return log_norm
-
-def viterbi_decode(scores, transition_params):
-    """
-    Decode a tag sequence with viterbi algorithm.
-    scores: seq_len x num_tags (numpy array)
-    transition_params: num_tags x num_tags (numpy array)
-    @return:
-        viterbi: a list of tag ids with highest score
-        viterbi_score: the highest score
-    """
-    trellis = np.zeros_like(scores)
-    backpointers = np.zeros_like(scores, dtype=np.int32)
-    trellis[0] = scores[0]
-
-    for t in range(1, scores.shape[0]):
-        v = np.expand_dims(trellis[t-1], 1) + transition_params
-        trellis[t] = scores[t] + np.max(v, 0)
-        backpointers[t] = np.argmax(v, 0)
-
-    viterbi = [np.argmax(trellis[-1])]
-    for bp in reversed(backpointers[1:]):
-        viterbi.append(bp[viterbi[-1]])
-    viterbi.reverse()
-    viterbi_score = np.max(trellis[-1])
-    return viterbi, viterbi_score
-
-def log_sum_exp(value, dim=None, keepdim=False):
-    """Numerically stable implementation of the operation
-    value.exp().sum(dim, keepdim).log()
-    """
-    if dim is not None:
-        m, _ = torch.max(value, dim=dim, keepdim=True)
-        value0 = value - m
-        if keepdim is False:
-            m = m.squeeze(dim)
-        return m + torch.log(torch.sum(torch.exp(value0),
-                                       dim=dim, keepdim=keepdim))
-    else:
-        m = torch.max(value)
-        sum_exp = torch.sum(torch.exp(value - m))
-        if isinstance(sum_exp, Number):
-            return m + math.log(sum_exp)
-        else:
-            return m + torch.log(sum_exp)
-
-def set_cuda(var, cuda):
-    if cuda:
-        return var.cuda()
-    return var
+"""
+CRF loss and viterbi decoding.
+"""
+
+import math
+from numbers import Number
+import numpy as np
+import torch
+from torch import nn
+import torch.nn.init as init
+
+class CRFLoss(nn.Module):
+    """
+    Calculate log-space crf loss, given unary potentials, a transition matrix
+    and gold tag sequences.
+    """
+    def __init__(self, num_tag, batch_average=True):
+        super().__init__()
+        self._transitions = nn.Parameter(torch.zeros(num_tag, num_tag))
+        self._batch_average = batch_average # if not batch average, average on all tokens
+
+    def forward(self, inputs, masks, tag_indices):
+        """
+        inputs: batch_size x seq_len x num_tags
+        masks: batch_size x seq_len
+        tag_indices: batch_size x seq_len
+        
+        @return:
+            loss: CRF negative log likelihood on all instances.
+            transitions: the transition matrix
+        """
+        # TODO: handle <start> and <end> tags
+        self.bs, self.sl, self.nc = inputs.size()
+        unary_scores = self.crf_unary_score(inputs, masks, tag_indices)
+        binary_scores = self.crf_binary_score(inputs, masks, tag_indices)
+        log_norm = self.crf_log_norm(inputs, masks, tag_indices)
+        log_likelihood = unary_scores + binary_scores - log_norm # batch_size
+        loss = torch.sum(-log_likelihood)
+        if self._batch_average:
+            loss = loss / self.bs
+        else:
+            total = masks.eq(0).sum()
+            loss = loss / (total + 1e-8)
+        return loss, self._transitions
+
+    def crf_unary_score(self, inputs, masks, tag_indices):
+        """
+        @return:
+            unary_scores: batch_size
+        """
+        flat_inputs = inputs.view(self.bs, -1)
+        flat_tag_indices = tag_indices + \
+                set_cuda(torch.arange(self.sl).long().unsqueeze(0) * self.nc, tag_indices.is_cuda)
+        unary_scores = torch.gather(flat_inputs, 1, flat_tag_indices).view(self.bs, -1)
+        unary_scores.masked_fill_(masks, 0)
+        return unary_scores.sum(dim=1)
+    
+    def crf_binary_score(self, inputs, masks, tag_indices):
+        """
+        @return:
+            binary_scores: batch_size
+        """
+        # get number of transitions
+        nt = tag_indices.size(-1) - 1
+        start_indices = tag_indices[:, :nt]
+        end_indices = tag_indices[:, 1:]
+        # flat matrices
+        flat_transition_indices = start_indices * self.nc + end_indices
+        flat_transition_indices = flat_transition_indices.view(-1)
+        flat_transition_matrix = self._transitions.view(-1)
+        binary_scores = torch.gather(flat_transition_matrix, 0, flat_transition_indices)\
+                .view(self.bs, -1)
+        score_masks = masks[:, 1:]
+        binary_scores.masked_fill_(score_masks, 0)
+        return binary_scores.sum(dim=1)
+
+    def crf_log_norm(self, inputs, masks, tag_indices):
+        """
+        Calculate the CRF partition in log space for each instance, following:
+            http://www.cs.columbia.edu/~mcollins/fb.pdf
+        @return:
+            log_norm: batch_size
+        """
+        start_inputs = inputs[:,0,:] # bs x nc
+        rest_inputs = inputs[:,1:,:]
+        rest_masks = masks[:,1:]
+        alphas = start_inputs # bs x nc
+        trans = self._transitions.unsqueeze(0) # 1 x nc x nc
+        # accumulate alphas in log space
+        for i in range(rest_inputs.size(1)):
+            transition_scores = alphas.unsqueeze(2) + trans # bs x nc x nc
+            new_alphas = rest_inputs[:,i,:] + log_sum_exp(transition_scores, dim=1)
+            m = rest_masks[:,i].unsqueeze(1).expand_as(new_alphas) # bs x nc, 1 for padding idx
+            # apply masks
+            new_alphas.masked_scatter_(m, alphas.masked_select(m))
+            alphas = new_alphas
+        log_norm = log_sum_exp(alphas, dim=1)
+        return log_norm
+
+def viterbi_decode(scores, transition_params):
+    """
+    Decode a tag sequence with viterbi algorithm.
+    scores: seq_len x num_tags (numpy array)
+    transition_params: num_tags x num_tags (numpy array)
+    @return:
+        viterbi: a list of tag ids with highest score
+        viterbi_score: the highest score
+    """
+    trellis = np.zeros_like(scores)
+    backpointers = np.zeros_like(scores, dtype=np.int32)
+    trellis[0] = scores[0]
+
+    for t in range(1, scores.shape[0]):
+        v = np.expand_dims(trellis[t-1], 1) + transition_params
+        trellis[t] = scores[t] + np.max(v, 0)
+        backpointers[t] = np.argmax(v, 0)
+
+    viterbi = [np.argmax(trellis[-1])]
+    for bp in reversed(backpointers[1:]):
+        viterbi.append(bp[viterbi[-1]])
+    viterbi.reverse()
+    viterbi_score = np.max(trellis[-1])
+    return viterbi, viterbi_score
+
+def log_sum_exp(value, dim=None, keepdim=False):
+    """Numerically stable implementation of the operation
+    value.exp().sum(dim, keepdim).log()
+    """
+    if dim is not None:
+        m, _ = torch.max(value, dim=dim, keepdim=True)
+        value0 = value - m
+        if keepdim is False:
+            m = m.squeeze(dim)
+        return m + torch.log(torch.sum(torch.exp(value0),
+                                       dim=dim, keepdim=keepdim))
+    else:
+        m = torch.max(value)
+        sum_exp = torch.sum(torch.exp(value - m))
+        if isinstance(sum_exp, Number):
+            return m + math.log(sum_exp)
+        else:
+            return m + torch.log(sum_exp)
+
+def set_cuda(var, cuda):
+    if cuda:
+        return var.cuda()
+    return var
```

### Comparing `classla-2.0/classla/models/common/doc.py` & `classla-2.1/classla/models/common/doc.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,1002 +1,1002 @@
-"""
-Basic data structures
-"""
-
-import io
-import re
-import json
-import pickle
-
-from classla.models.ner.utils import decode_from_bioes
-from classla.utils.conll import CoNLL
-
-multi_word_token_id = re.compile(r"([0-9]+)-([0-9]+)")
-multi_word_token_misc = re.compile(r".*MWT=Yes.*")
-
-ID = 'id'
-TEXT = 'text'
-LEMMA = 'lemma'
-UPOS = 'upos'
-XPOS = 'xpos'
-FEATS = 'feats'
-HEAD = 'head'
-DEPREL = 'deprel'
-DEPS = 'deps'
-MISC = 'misc'
-NER = 'ner'
-START_CHAR = 'start_char'
-END_CHAR = 'end_char'
-TYPE = 'type'
-SENTIMENT = 'sentiment'
-SRL = 'srl'
-
-def _readonly_setter(self, name):
-    full_classname = self.__class__.__module__
-    if full_classname is None:
-        full_classname = self.__class__.__qualname__
-    else:
-        full_classname += '.' + self.__class__.__qualname__
-    raise ValueError(f'Property "{name}" of "{full_classname}" is read-only.')
-
-class StanzaObject(object):
-    """
-    Base class for all Stanza data objects that allows for some flexibility handling annotations
-    """
-
-    @classmethod
-    def add_property(cls, name, default=None, getter=None, setter=None):
-        """
-        Add a property accessible through self.{name} with underlying variable self._{name}.
-        Optionally setup a setter as well.
-        """
-
-        if hasattr(cls, name):
-            raise ValueError(f'Property by the name of {name} already exists in {cls}. Maybe you want to find another name?')
-
-        setattr(cls, f'_{name}', default)
-        if getter is None:
-            getter = lambda self: getattr(self, f'_{name}')
-        if setter is None:
-            setter = lambda self, value: _readonly_setter(self, name)
-
-        setattr(cls, name, property(getter, setter))
-
-class Document(StanzaObject):
-    """ A document class that stores attributes of a document and carries a list of sentences.
-    """
-
-    def __init__(self, sentences, text=None, metasentences=None):
-        """ Construct a document given a list of sentences in the form of lists of CoNLL-U dicts.
-
-        Args:
-            sentences: a list of sentences, which being a list of token entry, in the form of a CoNLL-U dict.
-            text: the raw text of the document.
-        """
-        self._sentences = []
-        self._text = None
-        self._num_tokens = 0
-        self._num_words = 0
-
-        self.text = text
-        self._process_sentences(sentences, metasentences=metasentences)
-        self._ents = []
-
-    @property
-    def text(self):
-        """ Access the raw text for this document. """
-        return self._text
-
-    @text.setter
-    def text(self, value):
-        """ Set the raw text for this document. """
-        self._text = value
-
-    @property
-    def sentences(self):
-        """ Access the list of sentences for this document. """
-        return self._sentences
-
-    @sentences.setter
-    def sentences(self, value):
-        """ Set the list of tokens for this document. """
-        self._sentences = value
-
-    @property
-    def num_tokens(self):
-        """ Access the number of tokens for this document. """
-        return self._num_tokens
-
-    @num_tokens.setter
-    def num_tokens(self, value):
-        """ Set the number of tokens for this document. """
-        self._num_tokens = value
-
-    @property
-    def num_words(self):
-        """ Access the number of words for this document. """
-        return self._num_words
-
-    @num_words.setter
-    def num_words(self, value):
-        """ Set the number of words for this document. """
-        self._num_words = value
-
-    @property
-    def ents(self):
-        """ Access the list of entities in this document. """
-        return self._ents
-
-    @ents.setter
-    def ents(self, value):
-        """ Set the list of entities in this document. """
-        self._ents = value
-
-    @property
-    def entities(self):
-        """ Access the list of entities. This is just an alias of `ents`. """
-        return self._ents
-
-    @entities.setter
-    def entities(self, value):
-        """ Set the list of entities in this document. """
-        self._ents = value
-
-    def _process_sentences(self, sentences, metasentences=None):
-        self.sentences = []
-        metasentences = metasentences if metasentences else [None] * len(sentences)
-        for tokens, metadata in zip(sentences, metasentences):
-            self.sentences.append(Sentence(tokens, doc=self, metadata=metadata))
-            sentence_text = ''
-            if metadata:
-                split_metadata = metadata.split('\n')
-                for l in split_metadata:
-                    if len(l) > 9 and l[:9] == '# text = ':
-                        sentence_text = l[9:]
-                        self.sentences[-1].text = sentence_text
-            if not sentence_text:
-                begin_idx, end_idx = self.sentences[-1].tokens[0].start_char, self.sentences[-1].tokens[-1].end_char
-                if all([self.text is not None, begin_idx is not None, end_idx is not None]): self.sentences[-1].text = self.text[begin_idx: end_idx]
-
-        self.num_tokens = sum([len(sentence.tokens) for sentence in self.sentences])
-        self.num_words = sum([len(sentence.words) for sentence in self.sentences])
-
-    def get(self, fields, as_sentences=False, from_token=False):
-        """ Get fields from a list of field names.
-        If only one field name (string or singleton list) is provided,
-        return a list of that field; if more than one, return a list of list.
-        Note that all returned fields are after multi-word expansion.
-
-        Args:
-            fields: name of the fields as a list or a single string
-            as_sentences: if True, return the fields as a list of sentences; otherwise as a whole list
-            from_token: if True, get the fields from Token; otherwise from Word
-
-        Returns:
-            All requested fields.
-        """
-        if isinstance(fields, str):
-            fields = [fields]
-        assert isinstance(fields, list), "Must provide field names as a list."
-        assert len(fields) >= 1, "Must have at least one field."
-
-        results = []
-        for sentence in self.sentences:
-            cursent = []
-            # decide word or token
-            if from_token:
-                units = sentence.tokens
-            else:
-                units = sentence.words
-            if 'srl' in fields:
-                for unit in units:
-                    misc = {uni.split('=')[0]: uni.split('=')[1] for uni in unit.misc.split('|')} if unit.misc is not None else None
-                    unit.srl = misc['SRL'] if misc is not None and 'SRL' in misc else None
-            for unit in units:
-                if len(fields) == 1:
-                    cursent += [getattr(unit, fields[0])]
-                else:
-                    cursent += [[getattr(unit, field) for field in fields]]
-
-            # decide whether append the results as a sentence or a whole list
-            if as_sentences:
-                results.append(cursent)
-            else:
-                results += cursent
-        return results
-
-    def set(self, fields, contents, to_token=False, to_sentence=False):
-        """Set fields based on contents. If only one field (string or
-        singleton list) is provided, then a list of content will be
-        expected; otherwise a list of list of contents will be expected.
-
-        Args:
-            fields: name of the fields as a list or a single string
-            contents: field values to set; total length should be equal to number of words/tokens
-            to_token: if True, set field values to tokens; otherwise to words
-
-        """
-        if isinstance(fields, str):
-            fields = [fields]
-        assert isinstance(fields, (tuple, list)), "Must provide field names as a list."
-        assert isinstance(contents, (tuple, list)), "Must provide contents as a list (one item per line)."
-        assert len(fields) >= 1, "Must have at least one field."
-
-        assert not to_sentence or not to_token, "Both to_token and to_sentence set to True, which is very confusing"
-
-        if to_sentence:
-            assert len(self.sentences) == len(contents), \
-                "Contents must have the same length as the sentences"
-            for sentence, content in zip(self.sentences, contents):
-                if len(fields) == 1:
-                    setattr(sentence, fields[0], content)
-                else:
-                    for field, piece in zip(fields, content):
-                        setattr(sentence, field, piece)
-        else:
-            assert (to_token and self.num_tokens == len(contents)) or self.num_words == len(contents), \
-                "Contents must have the same length as the original file."
-
-            cidx = 0
-            for sentence in self.sentences:
-                # decide word or token
-                if to_token:
-                    units = sentence.tokens
-                else:
-                    units = sentence.words
-                for unit in units:
-                    if len(fields) == 1:
-                        setattr(unit, fields[0], contents[cidx])
-                    else:
-                        for field, content in zip(fields, contents[cidx]):
-                            setattr(unit, field, content)
-                    cidx += 1
-
-    def set_mwt_expansions(self, expansions):
-        """ Extend the multi-word tokens annotated by tokenizer. A list of list of expansions
-        will be expected for each multi-word token.
-        """
-        idx_e = 0
-        for sentence in self.sentences:
-            idx_w = 0
-            for token in sentence.tokens:
-                idx_w += 1
-                m = (len(token.id) > 1)
-                n = multi_word_token_misc.match(token.misc) if token.misc is not None else None
-                if not m and not n:
-                    for word in token.words:
-                        word.id = idx_w
-                        word.head, word.deprel = None, None # delete dependency information
-                else:
-                    expanded = [x for x in expansions[idx_e].split(' ') if len(x) > 0]
-                    idx_e += 1
-                    idx_w_end = idx_w + len(expanded) - 1
-                    token.misc = None if token.misc == 'MWT=Yes' else '|'.join([x for x in token.misc.split('|') if x != 'MWT=Yes'])
-                    token.id = (idx_w, idx_w_end)
-                    token.words = []
-                    for i, e_word in enumerate(expanded):
-                        token.words.append(Word({ID: idx_w + i, TEXT: e_word}))
-                    idx_w = idx_w_end
-            sentence._process_tokens(sentence.to_dict()) # reprocess to update sentence.words and sentence.dependencies
-        self._process_sentences(self.to_dict()) # reprocess to update number of words
-        assert idx_e == len(expansions), "{} {}".format(idx_e, len(expansions))
-        return
-
-    def get_mwt_expansions(self, evaluation=False):
-        """ Get the multi-word tokens. For training, return a list of
-        (multi-word token, extended multi-word token); otherwise, return a list of
-        multi-word token only.
-        """
-        expansions = []
-        for sentence in self.sentences:
-            for token in sentence.tokens:
-                m = (len(token.id) > 1)
-                n = multi_word_token_misc.match(token.misc) if token.misc is not None else None
-                if m or n:
-                    src = token.text
-                    dst = ' '.join([word.text for word in token.words])
-                    expansions.append([src, dst])
-        if evaluation: expansions = [e[0] for e in expansions]
-        return expansions
-
-    def build_ents(self):
-        """ Build the list of entities by iterating over all words. Return all entities as a list. """
-        self.ents = []
-        for s in self.sentences:
-            s_ents = s.build_ents()
-            self.ents += s_ents
-        return self.ents
-
-    def iter_words(self):
-        """ An iterator that returns all of the words in this Document. """
-        for s in self.sentences:
-            yield from s.words
-
-    def iter_tokens(self):
-        """ An iterator that returns all of the tokens in this Document. """
-        for s in self.sentences:
-            yield from s.tokens
-
-    def to_dict(self):
-        """ Dumps the whole document into a list of list of dictionary for each token in each sentence in the doc.
-        """
-        return [sentence.to_dict() for sentence in self.sentences]
-
-    def to_conll(self):
-        """ Produces string output of CoNLLu type.
-        """
-        return CoNLL.conll_as_string(CoNLL.convert_dict(self.to_dict()))
-
-    def __repr__(self):
-        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)
-
-    def to_serialized(self):
-        """ Dumps the whole document including text to a byte array containing a list of list of dictionaries for each token in each sentence in the doc.
-        """
-        return pickle.dumps((self.text, self.to_dict()))
-
-    @classmethod
-    def from_serialized(cls, serialized_string):
-        """ Create and initialize a new document from a serialized string generated by Document.to_serialized_string():
-        """
-        try:
-            text, sentences = pickle.loads(serialized_string)
-            doc = cls(sentences, text)
-            doc.build_ents()
-            return doc
-        except:
-            raise Exception(f"Could not create new Document from serialised string.")
-
-
-class Sentence(StanzaObject):
-    """ A sentence class that stores attributes of a sentence and carries a list of tokens.
-    """
-
-    def __init__(self, tokens, doc=None, metadata=None):
-        """ Construct a setence given a list of tokens in the form of CoNLL-U dicts.
-        """
-        self._tokens = []
-        self._words = []
-        self._dependencies = []
-        self._text = None
-        self._ents = []
-        self._doc = doc
-        self._metadata = metadata
-
-        self._process_tokens(tokens)
-
-    def _process_tokens(self, tokens):
-        st, en = -1, -1
-        self.tokens, self.words = [], []
-        for i, entry in enumerate(tokens):
-            if ID not in entry: # manually set a 1-based id for word if not exist
-                entry[ID] = (i+1, )
-            if isinstance(entry[ID], int):
-                entry[ID] = (entry[ID], )
-            m = (len(entry.get(ID)) > 1)
-            n = multi_word_token_misc.match(entry.get(MISC)) if entry.get(MISC, None) is not None else None
-            if m or n: # if this token is a multi-word token
-                if m: st, en = entry[ID]
-                self.tokens.append(Token(entry))
-            else: # else this token is a word
-                new_word = Word(entry)
-                self.words.append(new_word)
-                idx = entry.get(ID)[0]
-                if idx <= en:
-                    self.tokens[-1].words.append(new_word)
-                else:
-                    self.tokens.append(Token(entry, words=[new_word]))
-                new_word.parent = self.tokens[-1]
-
-        # check if there is dependency info
-        is_complete_dependencies = all(word.head is not None and word.deprel is not None for word in self.words)
-        is_complete_words = (len(self.words) >= len(self.tokens)) and (len(self.words) == self.words[-1].id)
-        if is_complete_dependencies and is_complete_words: self.build_dependencies()
-
-    @property
-    def doc(self):
-        """ Access the parent doc of this span. """
-        return self._doc
-
-    @doc.setter
-    def doc(self, value):
-        """ Set the parent doc of this span. """
-        self._doc = value
-
-    @property
-    def text(self):
-        """ Access the raw text for this sentence. """
-        return self._text
-
-    @text.setter
-    def text(self, value):
-        """ Set the raw text for this sentence. """
-        self._text = value
-
-    @property
-    def dependencies(self):
-        """ Access list of dependencies for this sentence. """
-        return self._dependencies
-
-    @dependencies.setter
-    def dependencies(self, value):
-        """ Set the list of dependencies for this sentence. """
-        self._dependencies = value
-
-    @property
-    def tokens(self):
-        """ Access the list of tokens for this sentence. """
-        return self._tokens
-
-    @tokens.setter
-    def tokens(self, value):
-        """ Set the list of tokens for this sentence. """
-        self._tokens = value
-
-    @property
-    def words(self):
-        """ Access the list of words for this sentence. """
-        return self._words
-
-    @words.setter
-    def words(self, value):
-        """ Set the list of words for this sentence. """
-        self._words = value
-
-    @property
-    def ents(self):
-        """ Access the list of entities in this sentence. """
-        return self._ents
-
-    @ents.setter
-    def ents(self, value):
-        """ Set the list of entities in this sentence. """
-        self._ents = value
-
-    @property
-    def entities(self):
-        """ Access the list of entities. This is just an alias of `ents`. """
-        return self._ents
-
-    @entities.setter
-    def entities(self, value):
-        """ Set the list of entities in this sentence. """
-        self._ents = value
-
-    def build_ents(self):
-        """ Build the list of entities by iterating over all tokens. Return all entities as a list.
-
-        Note that unlike other attributes, since NER requires raw text, the actual tagging are always
-        performed at and attached to the `Token`s, instead of `Word`s.
-        """
-        self.ents = []
-        tags = [w.ner for w in self.tokens]
-        decoded = decode_from_bioes(tags)
-        for e in decoded:
-            ent_tokens = self.tokens[e['start']:e['end']+1]
-            self.ents.append(Span(tokens=ent_tokens, type=e['type'], doc=self.doc, sent=self))
-        return self.ents
-
-    @property
-    def sentiment(self):
-        """ Returns the sentiment value for this sentence """
-        return self._sentiment
-
-    @sentiment.setter
-    def sentiment(self, value):
-        """ Set the sentiment value """
-        self._sentiment = value
-
-    def build_dependencies(self):
-        """ Build the dependency graph for this sentence. Each dependency graph entry is
-        a list of (head, deprel, word).
-        """
-        self.dependencies = []
-        for word in self.words:
-            if word.head == 0:
-                # make a word for the ROOT
-                word_entry = {ID: 0, TEXT: "ROOT"}
-                head = Word(word_entry)
-            else:
-                # id is index in words list + 1
-                head = self.words[word.head - 1]
-                assert(word.head == head.id)
-            self.dependencies.append((head, word.deprel, word))
-
-    def print_dependencies(self, file=None):
-        """ Print the dependencies for this sentence. """
-        for dep_edge in self.dependencies:
-            print((dep_edge[2].text, dep_edge[0].id, dep_edge[1]), file=file)
-
-    def dependencies_string(self):
-        """ Dump the dependencies for this sentence into string. """
-        dep_string = io.StringIO()
-        self.print_dependencies(file=dep_string)
-        return dep_string.getvalue().strip()
-
-    def print_tokens(self, file=None):
-        """ Print the tokens for this sentence. """
-        for tok in self.tokens:
-            print(tok.pretty_print(), file=file)
-
-    def tokens_string(self):
-        """ Dump the tokens for this sentence into string. """
-        toks_string = io.StringIO()
-        self.print_tokens(file=toks_string)
-        return toks_string.getvalue().strip()
-
-    def print_words(self, file=None):
-        """ Print the words for this sentence. """
-        for word in self.words:
-            print(word.pretty_print(), file=file)
-
-    def words_string(self):
-        """ Dump the words for this sentence into string. """
-        wrds_string = io.StringIO()
-        self.print_words(file=wrds_string)
-        return wrds_string.getvalue().strip()
-
-    def to_dict(self):
-        """ Dumps the sentence into a list of dictionary for each token in the sentence.
-        """
-        ret = []
-        for token in self.tokens:
-            ret += token.to_dict()
-        return (ret, self._metadata)
-
-    def __repr__(self):
-        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)
-
-
-class Token(StanzaObject):
-    """ A token class that stores attributes of a token and carries a list of words. A token corresponds to a unit in the raw
-    text. In some languages such as English, a token has a one-to-one mapping to a word, while in other languages such as French,
-    a (multi-word) token might be expanded into multiple words that carry syntactic annotations.
-    """
-
-    def __init__(self, token_entry, words=None):
-        """ Construct a token given a dictionary format token entry. Optionally link itself to the corresponding words.
-        """
-        self._id = token_entry.get(ID)
-        self._text = token_entry.get(TEXT)
-        assert self._id and self._text, 'id and text should be included for the token'
-        self._misc = token_entry.get(MISC, None)
-        self._ner = token_entry.get(NER, None)
-        self._srl = token_entry.get(SRL, None)
-        self._words = words if words is not None else []
-        self._start_char = token_entry.get(START_CHAR, None)
-        self._end_char = token_entry.get(END_CHAR, None)
-
-        if self._misc is not None:
-            self.init_from_misc()
-
-    def init_from_misc(self):
-        """ Create attributes by parsing from the `misc` field.
-        """
-        for item in self._misc.split('|'):
-            key_value = item.split('=', 1)
-            if len(key_value) == 1: continue # some key_value can not be splited
-            key, value = key_value
-            if key in (START_CHAR, END_CHAR):
-                value = int(value)
-            # set attribute
-            attr = f'_{key}'
-            if hasattr(self, attr):
-                setattr(self, attr, value)
-
-    @property
-    def id(self):
-        """ Access the index of this token. """
-        return self._id
-
-    @id.setter
-    def id(self, value):
-        """ Set the token's id value. """
-        self._id = value
-
-    @property
-    def text(self):
-        """ Access the text of this token. Example: 'The' """
-        return self._text
-
-    @text.setter
-    def text(self, value):
-        """ Set the token's text value. Example: 'The' """
-        self._text = value
-
-    @property
-    def misc(self):
-        """ Access the miscellaneousness of this token. """
-        return self._misc
-
-    @misc.setter
-    def misc(self, value):
-        """ Set the token's miscellaneousness value. """
-        self._misc = value if self._is_null(value) == False else None
-
-    @property
-    def words(self):
-        """ Access the list of syntactic words underlying this token. """
-        return self._words
-
-    @words.setter
-    def words(self, value):
-        """ Set this token's list of underlying syntactic words. """
-        self._words = value
-        for w in self._words:
-            w.parent = self
-
-    @property
-    def start_char(self):
-        """ Access the start character index for this token in the raw text. """
-        return self._start_char
-
-    @property
-    def end_char(self):
-        """ Access the end character index for this token in the raw text. """
-        return self._end_char
-
-    @property
-    def ner(self):
-        """ Access the NER tag of this token. Example: 'B-ORG'"""
-        return self._ner
-
-    @ner.setter
-    def ner(self, value):
-        """ Set the token's NER tag. Example: 'B-ORG'"""
-        self._ner = value if self._is_null(value) == False else None
-
-    @property
-    def srl(self):
-        """ Access the SRL tag of this token. Example: 'ACT'"""
-        return self._srl
-
-    @srl.setter
-    def srl(self, value):
-        """ Set the token's SRL tag. Example: 'ACT'"""
-        self._srl = value if self._is_null(value) == False else None
-
-    def __repr__(self):
-        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)
-
-    def to_dict(self, fields=[ID, TEXT, NER, SRL, MISC]):
-        """ Dumps the token into a list of dictionary for this token with its extended words
-        if the token is a multi-word token.
-        """
-        ret = []
-        if len(self.id) > 1:
-            token_dict = {}
-            for field in fields:
-                if getattr(self, field) is not None:
-                    token_dict[field] = getattr(self, field)
-            ret.append(token_dict)
-        for word in self.words:
-            word_dict = word.to_dict()
-            if len(self.id) == 1 and NER in fields and getattr(self, NER) is not None: # propagate NER label to Word if it is a single-word token
-                word_dict[NER] = getattr(self, NER)
-            if len(self.id) == 1 and SRL in fields and getattr(self, SRL) is not None: # propagate SRL label to Word if it is a single-word token
-                word_dict[SRL] = getattr(self, SRL)
-            ret.append(word_dict)
-        return ret
-
-    def pretty_print(self):
-        """ Print this token with its extended words in one line. """
-        return f"<{self.__class__.__name__} id={'-'.join([str(x) for x in self.id])};words=[{', '.join([word.pretty_print() for word in self.words])}]>"
-
-    def _is_null(self, value):
-        return (value is None) or (value == '_')
-
-class Word(StanzaObject):
-    """ A word class that stores attributes of a word.
-    """
-
-    def __init__(self, word_entry):
-        """ Construct a word given a dictionary format word entry.
-        """
-        self._id = word_entry.get(ID, None)
-        if isinstance(self._id, tuple):
-            assert len(self._id) == 1
-            self._id = self._id[0]
-        self._text = word_entry.get(TEXT, None)
-
-        assert self._id is not None and self._text is not None, 'id and text should be included for the word. {}'.format(word_entry)
-
-        self._lemma = word_entry.get(LEMMA, None)
-        self._upos = word_entry.get(UPOS, None)
-        self._xpos = word_entry.get(XPOS, None)
-        self._feats = word_entry.get(FEATS, None)
-        self._head = word_entry.get(HEAD, None)
-        self._deprel = word_entry.get(DEPREL, None)
-        self._deps = word_entry.get(DEPS, None)
-        self._misc = word_entry.get(MISC, None)
-        self._start_char = word_entry.get(START_CHAR, None)
-        self._end_char = word_entry.get(END_CHAR, None)
-        self._parent = None
-
-        if self._misc is not None:
-            self.init_from_misc()
-
-    def init_from_misc(self):
-        """ Create attributes by parsing from the `misc` field.
-        """
-        for item in self._misc.split('|'):
-            key_value = item.split('=', 1)
-            if len(key_value) == 1: continue # some key_value can not be splited
-            key, value = key_value
-            # set attribute
-            attr = f'_{key}'
-            if hasattr(self, attr):
-                setattr(self, attr, value)
-
-    @property
-    def id(self):
-        """ Access the index of this word. """
-        return self._id
-
-    @id.setter
-    def id(self, value):
-        """ Set the word's index value. """
-        self._id = value
-
-    @property
-    def text(self):
-        """ Access the text of this word. Example: 'The'"""
-        return self._text
-
-    @text.setter
-    def text(self, value):
-        """ Set the word's text value. Example: 'The'"""
-        self._text = value
-
-    @property
-    def lemma(self):
-        """ Access the lemma of this word. """
-        return self._lemma
-
-    @lemma.setter
-    def lemma(self, value):
-        """ Set the word's lemma value. """
-        self._lemma = value if self._is_null(value) == False or self._text == '_' else None
-
-    @property
-    def upos(self):
-        """ Access the universal part-of-speech of this word. Example: 'NOUN'"""
-        return self._upos
-
-    @upos.setter
-    def upos(self, value):
-        """ Set the word's universal part-of-speech value. Example: 'NOUN'"""
-        self._upos = value if self._is_null(value) == False else None
-
-    @property
-    def xpos(self):
-        """ Access the treebank-specific part-of-speech of this word. Example: 'NNP'"""
-        return self._xpos
-
-    @xpos.setter
-    def xpos(self, value):
-        """ Set the word's treebank-specific part-of-speech value. Example: 'NNP'"""
-        self._xpos = value if self._is_null(value) == False else None
-
-    @property
-    def feats(self):
-        """ Access the morphological features of this word. Example: 'Gender=Fem'"""
-        return self._feats
-
-    @feats.setter
-    def feats(self, value):
-        """ Set this word's morphological features. Example: 'Gender=Fem'"""
-        self._feats = value if self._is_null(value) == False else None
-
-    @property
-    def head(self):
-        """ Access the id of the governer of this word. """
-        return self._head
-
-    @head.setter
-    def head(self, value):
-        """ Set the word's governor id value. """
-        self._head = int(value) if self._is_null(value) == False else None
-
-    @property
-    def deprel(self):
-        """ Access the dependency relation of this word. Example: 'nmod'"""
-        return self._deprel
-
-    @deprel.setter
-    def deprel(self, value):
-        """ Set the word's dependency relation value. Example: 'nmod'"""
-        self._deprel = value if self._is_null(value) == False else None
-
-    @property
-    def deps(self):
-        """ Access the dependencies of this word. """
-        return self._deps
-
-    @deps.setter
-    def deps(self, value):
-        """ Set the word's dependencies value. """
-        self._deps = value if self._is_null(value) == False else None
-
-    @property
-    def misc(self):
-        """ Access the miscellaneousness of this word. """
-        return self._misc
-
-    @misc.setter
-    def misc(self, value):
-        """ Set the word's miscellaneousness value. """
-        self._misc = value if self._is_null(value) == False else None
-
-    @property
-    def parent(self):
-        """ Access the parent token of this word. In the case of a multi-word token, a token can be the parent of
-        multiple words. Note that this should return a reference to the parent token object.
-        """
-        return self._parent
-
-    @parent.setter
-    def parent(self, value):
-        """ Set this word's parent token. In the case of a multi-word token, a token can be the parent of
-        multiple words. Note that value here should be a reference to the parent token object.
-        """
-        self._parent = value
-
-    @property
-    def pos(self):
-        """ Access the universal part-of-speech of this word. Example: 'NOUN'"""
-        return self._upos
-
-    @pos.setter
-    def pos(self, value):
-        """ Set the word's universal part-of-speech value. Example: 'NOUN'"""
-        self._upos = value if self._is_null(value) == False else None
-
-    def __repr__(self):
-        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)
-
-    def to_dict(self, fields=[ID, TEXT, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC]):
-        """ Dumps the word into a dictionary.
-        """
-        word_dict = {}
-        for field in fields:
-            if getattr(self, field) is not None:
-                word_dict[field] = getattr(self, field)
-        return word_dict
-
-    def pretty_print(self):
-        """ Print the word in one line. """
-        features = [ID, TEXT, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL]
-        feature_str = ";".join(["{}={}".format(k, getattr(self, k)) for k in features if getattr(self, k) is not None])
-        return f"<{self.__class__.__name__} {feature_str}>"
-
-    def _is_null(self, value):
-        return (value is None) or (value == '_')
-
-
-class Span(StanzaObject):
-    """ A span class that stores attributes of a textual span. A span can be typed.
-    A range of objects (e.g., entity mentions) can be represented as spans.
-    """
-
-    def __init__(self, span_entry=None, tokens=None, type=None, doc=None, sent=None):
-        """ Construct a span given a span entry or a list of tokens. A valid reference to a doc
-        must be provided to construct a span (otherwise the text of the span cannot be initialized).
-        """
-        assert span_entry is not None or (tokens is not None and type is not None), \
-                'Either a span_entry or a token list needs to be provided to construct a span.'
-        assert doc is not None, 'A parent doc must be provided to construct a span.'
-        self._text, self._type, self._start_char, self._end_char = [None] * 4
-        self._tokens = []
-        self._words = []
-        self._doc = doc
-        self._sent = sent
-
-        if span_entry is not None:
-            self.init_from_entry(span_entry)
-
-        if tokens is not None:
-            self.init_from_tokens(tokens, type)
-
-    def init_from_entry(self, span_entry):
-        self.text = span_entry.get(TEXT, None)
-        self.type = span_entry.get(TYPE, None)
-        self.start_char = span_entry.get(START_CHAR, None)
-        self.end_char = span_entry.get(END_CHAR, None)
-
-    def init_from_tokens(self, tokens, type):
-        assert isinstance(tokens, list), 'Tokens must be provided as a list to construct a span.'
-        assert len(tokens) > 0, "Tokens of a span cannot be an empty list."
-        self.tokens = tokens
-        self.type = type
-        # load start and end char offsets from tokens
-        self.start_char = self.tokens[0].start_char
-        self.end_char = self.tokens[-1].end_char
-        # assume doc is already provided and not None
-        self.text = self.doc.text[self.start_char:self.end_char]
-        # collect the words of the span following tokens
-        self.words = [w for t in tokens for w in t.words]
-
-    @property
-    def doc(self):
-        """ Access the parent doc of this span. """
-        return self._doc
-
-    @doc.setter
-    def doc(self, value):
-        """ Set the parent doc of this span. """
-        self._doc = value
-
-    @property
-    def text(self):
-        """ Access the text of this span. Example: 'Stanford University'"""
-        return self._text
-
-    @text.setter
-    def text(self, value):
-        """ Set the span's text value. Example: 'Stanford University'"""
-        self._text = value
-
-    @property
-    def tokens(self):
-        """ Access reference to a list of tokens that correspond to this span. """
-        return self._tokens
-
-    @tokens.setter
-    def tokens(self, value):
-        """ Set the span's list of tokens. """
-        self._tokens = value
-
-    @property
-    def words(self):
-        """ Access reference to a list of words that correspond to this span. """
-        return self._words
-
-    @words.setter
-    def words(self, value):
-        """ Set the span's list of words. """
-        self._words = value
-
-    @property
-    def type(self):
-        """ Access the type of this span. Example: 'PERSON'"""
-        return self._type
-
-    @type.setter
-    def type(self, value):
-        """ Set the type of this span. """
-        self._type = value
-
-    @property
-    def start_char(self):
-        """ Access the start character offset of this span. """
-        return self._start_char
-
-    @start_char.setter
-    def start_char(self, value):
-        """ Set the start character offset of this span. """
-        self._start_char = value
-
-    @property
-    def end_char(self):
-        """ Access the end character offset of this span. """
-        return self._end_char
-
-    @end_char.setter
-    def end_char(self, value):
-        """ Set the end character offset of this span. """
-        self._end_char = value
-
-    def to_dict(self):
-        """ Dumps the span into a dictionary. """
-        attrs = ['text', 'type', 'start_char', 'end_char']
-        span_dict = dict([(attr_name, getattr(self, attr_name)) for attr_name in attrs])
-        return span_dict
-
-    def __repr__(self):
-        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)
-
-    def pretty_print(self):
-        """ Print the span in one line. """
-        span_dict = self.to_dict()
-        feature_str = ";".join(["{}={}".format(k,v) for k,v in span_dict.items()])
-        return f"<{self.__class__.__name__} {feature_str}>"
+"""
+Basic data structures
+"""
+
+import io
+import re
+import json
+import pickle
+
+from classla.models.ner.utils import decode_from_bioes
+from classla.utils.conll import CoNLL
+
+multi_word_token_id = re.compile(r"([0-9]+)-([0-9]+)")
+multi_word_token_misc = re.compile(r".*MWT=Yes.*")
+
+ID = 'id'
+TEXT = 'text'
+LEMMA = 'lemma'
+UPOS = 'upos'
+XPOS = 'xpos'
+FEATS = 'feats'
+HEAD = 'head'
+DEPREL = 'deprel'
+DEPS = 'deps'
+MISC = 'misc'
+NER = 'ner'
+START_CHAR = 'start_char'
+END_CHAR = 'end_char'
+TYPE = 'type'
+SENTIMENT = 'sentiment'
+SRL = 'srl'
+
+def _readonly_setter(self, name):
+    full_classname = self.__class__.__module__
+    if full_classname is None:
+        full_classname = self.__class__.__qualname__
+    else:
+        full_classname += '.' + self.__class__.__qualname__
+    raise ValueError(f'Property "{name}" of "{full_classname}" is read-only.')
+
+class StanzaObject(object):
+    """
+    Base class for all Stanza data objects that allows for some flexibility handling annotations
+    """
+
+    @classmethod
+    def add_property(cls, name, default=None, getter=None, setter=None):
+        """
+        Add a property accessible through self.{name} with underlying variable self._{name}.
+        Optionally setup a setter as well.
+        """
+
+        if hasattr(cls, name):
+            raise ValueError(f'Property by the name of {name} already exists in {cls}. Maybe you want to find another name?')
+
+        setattr(cls, f'_{name}', default)
+        if getter is None:
+            getter = lambda self: getattr(self, f'_{name}')
+        if setter is None:
+            setter = lambda self, value: _readonly_setter(self, name)
+
+        setattr(cls, name, property(getter, setter))
+
+class Document(StanzaObject):
+    """ A document class that stores attributes of a document and carries a list of sentences.
+    """
+
+    def __init__(self, sentences, text=None, metasentences=None):
+        """ Construct a document given a list of sentences in the form of lists of CoNLL-U dicts.
+
+        Args:
+            sentences: a list of sentences, which being a list of token entry, in the form of a CoNLL-U dict.
+            text: the raw text of the document.
+        """
+        self._sentences = []
+        self._text = None
+        self._num_tokens = 0
+        self._num_words = 0
+
+        self.text = text
+        self._process_sentences(sentences, metasentences=metasentences)
+        self._ents = []
+
+    @property
+    def text(self):
+        """ Access the raw text for this document. """
+        return self._text
+
+    @text.setter
+    def text(self, value):
+        """ Set the raw text for this document. """
+        self._text = value
+
+    @property
+    def sentences(self):
+        """ Access the list of sentences for this document. """
+        return self._sentences
+
+    @sentences.setter
+    def sentences(self, value):
+        """ Set the list of tokens for this document. """
+        self._sentences = value
+
+    @property
+    def num_tokens(self):
+        """ Access the number of tokens for this document. """
+        return self._num_tokens
+
+    @num_tokens.setter
+    def num_tokens(self, value):
+        """ Set the number of tokens for this document. """
+        self._num_tokens = value
+
+    @property
+    def num_words(self):
+        """ Access the number of words for this document. """
+        return self._num_words
+
+    @num_words.setter
+    def num_words(self, value):
+        """ Set the number of words for this document. """
+        self._num_words = value
+
+    @property
+    def ents(self):
+        """ Access the list of entities in this document. """
+        return self._ents
+
+    @ents.setter
+    def ents(self, value):
+        """ Set the list of entities in this document. """
+        self._ents = value
+
+    @property
+    def entities(self):
+        """ Access the list of entities. This is just an alias of `ents`. """
+        return self._ents
+
+    @entities.setter
+    def entities(self, value):
+        """ Set the list of entities in this document. """
+        self._ents = value
+
+    def _process_sentences(self, sentences, metasentences=None):
+        self.sentences = []
+        metasentences = metasentences if metasentences else [None] * len(sentences)
+        for tokens, metadata in zip(sentences, metasentences):
+            self.sentences.append(Sentence(tokens, doc=self, metadata=metadata))
+            sentence_text = ''
+            if metadata:
+                split_metadata = metadata.split('\n')
+                for l in split_metadata:
+                    if len(l) > 9 and l[:9] == '# text = ':
+                        sentence_text = l[9:]
+                        self.sentences[-1].text = sentence_text
+            if not sentence_text:
+                begin_idx, end_idx = self.sentences[-1].tokens[0].start_char, self.sentences[-1].tokens[-1].end_char
+                if all([self.text is not None, begin_idx is not None, end_idx is not None]): self.sentences[-1].text = self.text[begin_idx: end_idx]
+
+        self.num_tokens = sum([len(sentence.tokens) for sentence in self.sentences])
+        self.num_words = sum([len(sentence.words) for sentence in self.sentences])
+
+    def get(self, fields, as_sentences=False, from_token=False):
+        """ Get fields from a list of field names.
+        If only one field name (string or singleton list) is provided,
+        return a list of that field; if more than one, return a list of list.
+        Note that all returned fields are after multi-word expansion.
+
+        Args:
+            fields: name of the fields as a list or a single string
+            as_sentences: if True, return the fields as a list of sentences; otherwise as a whole list
+            from_token: if True, get the fields from Token; otherwise from Word
+
+        Returns:
+            All requested fields.
+        """
+        if isinstance(fields, str):
+            fields = [fields]
+        assert isinstance(fields, list), "Must provide field names as a list."
+        assert len(fields) >= 1, "Must have at least one field."
+
+        results = []
+        for sentence in self.sentences:
+            cursent = []
+            # decide word or token
+            if from_token:
+                units = sentence.tokens
+            else:
+                units = sentence.words
+            if 'srl' in fields:
+                for unit in units:
+                    misc = {uni.split('=')[0]: uni.split('=')[1] for uni in unit.misc.split('|')} if unit.misc is not None else None
+                    unit.srl = misc['SRL'] if misc is not None and 'SRL' in misc else None
+            for unit in units:
+                if len(fields) == 1:
+                    cursent += [getattr(unit, fields[0])]
+                else:
+                    cursent += [[getattr(unit, field) for field in fields]]
+
+            # decide whether append the results as a sentence or a whole list
+            if as_sentences:
+                results.append(cursent)
+            else:
+                results += cursent
+        return results
+
+    def set(self, fields, contents, to_token=False, to_sentence=False):
+        """Set fields based on contents. If only one field (string or
+        singleton list) is provided, then a list of content will be
+        expected; otherwise a list of list of contents will be expected.
+
+        Args:
+            fields: name of the fields as a list or a single string
+            contents: field values to set; total length should be equal to number of words/tokens
+            to_token: if True, set field values to tokens; otherwise to words
+
+        """
+        if isinstance(fields, str):
+            fields = [fields]
+        assert isinstance(fields, (tuple, list)), "Must provide field names as a list."
+        assert isinstance(contents, (tuple, list)), "Must provide contents as a list (one item per line)."
+        assert len(fields) >= 1, "Must have at least one field."
+
+        assert not to_sentence or not to_token, "Both to_token and to_sentence set to True, which is very confusing"
+
+        if to_sentence:
+            assert len(self.sentences) == len(contents), \
+                "Contents must have the same length as the sentences"
+            for sentence, content in zip(self.sentences, contents):
+                if len(fields) == 1:
+                    setattr(sentence, fields[0], content)
+                else:
+                    for field, piece in zip(fields, content):
+                        setattr(sentence, field, piece)
+        else:
+            assert (to_token and self.num_tokens == len(contents)) or self.num_words == len(contents), \
+                "Contents must have the same length as the original file."
+
+            cidx = 0
+            for sentence in self.sentences:
+                # decide word or token
+                if to_token:
+                    units = sentence.tokens
+                else:
+                    units = sentence.words
+                for unit in units:
+                    if len(fields) == 1:
+                        setattr(unit, fields[0], contents[cidx])
+                    else:
+                        for field, content in zip(fields, contents[cidx]):
+                            setattr(unit, field, content)
+                    cidx += 1
+
+    def set_mwt_expansions(self, expansions):
+        """ Extend the multi-word tokens annotated by tokenizer. A list of list of expansions
+        will be expected for each multi-word token.
+        """
+        idx_e = 0
+        for sentence in self.sentences:
+            idx_w = 0
+            for token in sentence.tokens:
+                idx_w += 1
+                m = (len(token.id) > 1)
+                n = multi_word_token_misc.match(token.misc) if token.misc is not None else None
+                if not m and not n:
+                    for word in token.words:
+                        word.id = idx_w
+                        word.head, word.deprel = None, None # delete dependency information
+                else:
+                    expanded = [x for x in expansions[idx_e].split(' ') if len(x) > 0]
+                    idx_e += 1
+                    idx_w_end = idx_w + len(expanded) - 1
+                    token.misc = None if token.misc == 'MWT=Yes' else '|'.join([x for x in token.misc.split('|') if x != 'MWT=Yes'])
+                    token.id = (idx_w, idx_w_end)
+                    token.words = []
+                    for i, e_word in enumerate(expanded):
+                        token.words.append(Word({ID: idx_w + i, TEXT: e_word}))
+                    idx_w = idx_w_end
+            sentence._process_tokens(sentence.to_dict()) # reprocess to update sentence.words and sentence.dependencies
+        self._process_sentences(self.to_dict()) # reprocess to update number of words
+        assert idx_e == len(expansions), "{} {}".format(idx_e, len(expansions))
+        return
+
+    def get_mwt_expansions(self, evaluation=False):
+        """ Get the multi-word tokens. For training, return a list of
+        (multi-word token, extended multi-word token); otherwise, return a list of
+        multi-word token only.
+        """
+        expansions = []
+        for sentence in self.sentences:
+            for token in sentence.tokens:
+                m = (len(token.id) > 1)
+                n = multi_word_token_misc.match(token.misc) if token.misc is not None else None
+                if m or n:
+                    src = token.text
+                    dst = ' '.join([word.text for word in token.words])
+                    expansions.append([src, dst])
+        if evaluation: expansions = [e[0] for e in expansions]
+        return expansions
+
+    def build_ents(self):
+        """ Build the list of entities by iterating over all words. Return all entities as a list. """
+        self.ents = []
+        for s in self.sentences:
+            s_ents = s.build_ents()
+            self.ents += s_ents
+        return self.ents
+
+    def iter_words(self):
+        """ An iterator that returns all of the words in this Document. """
+        for s in self.sentences:
+            yield from s.words
+
+    def iter_tokens(self):
+        """ An iterator that returns all of the tokens in this Document. """
+        for s in self.sentences:
+            yield from s.tokens
+
+    def to_dict(self):
+        """ Dumps the whole document into a list of list of dictionary for each token in each sentence in the doc.
+        """
+        return [sentence.to_dict() for sentence in self.sentences]
+
+    def to_conll(self):
+        """ Produces string output of CoNLLu type.
+        """
+        return CoNLL.conll_as_string(CoNLL.convert_dict(self.to_dict()))
+
+    def __repr__(self):
+        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)
+
+    def to_serialized(self):
+        """ Dumps the whole document including text to a byte array containing a list of list of dictionaries for each token in each sentence in the doc.
+        """
+        return pickle.dumps((self.text, self.to_dict()))
+
+    @classmethod
+    def from_serialized(cls, serialized_string):
+        """ Create and initialize a new document from a serialized string generated by Document.to_serialized_string():
+        """
+        try:
+            text, sentences = pickle.loads(serialized_string)
+            doc = cls(sentences, text)
+            doc.build_ents()
+            return doc
+        except:
+            raise Exception(f"Could not create new Document from serialised string.")
+
+
+class Sentence(StanzaObject):
+    """ A sentence class that stores attributes of a sentence and carries a list of tokens.
+    """
+
+    def __init__(self, tokens, doc=None, metadata=None):
+        """ Construct a setence given a list of tokens in the form of CoNLL-U dicts.
+        """
+        self._tokens = []
+        self._words = []
+        self._dependencies = []
+        self._text = None
+        self._ents = []
+        self._doc = doc
+        self._metadata = metadata
+
+        self._process_tokens(tokens)
+
+    def _process_tokens(self, tokens):
+        st, en = -1, -1
+        self.tokens, self.words = [], []
+        for i, entry in enumerate(tokens):
+            if ID not in entry: # manually set a 1-based id for word if not exist
+                entry[ID] = (i+1, )
+            if isinstance(entry[ID], int):
+                entry[ID] = (entry[ID], )
+            m = (len(entry.get(ID)) > 1)
+            n = multi_word_token_misc.match(entry.get(MISC)) if entry.get(MISC, None) is not None else None
+            if m or n: # if this token is a multi-word token
+                if m: st, en = entry[ID]
+                self.tokens.append(Token(entry))
+            else: # else this token is a word
+                new_word = Word(entry)
+                self.words.append(new_word)
+                idx = entry.get(ID)[0]
+                if idx <= en:
+                    self.tokens[-1].words.append(new_word)
+                else:
+                    self.tokens.append(Token(entry, words=[new_word]))
+                new_word.parent = self.tokens[-1]
+
+        # check if there is dependency info
+        is_complete_dependencies = all(word.head is not None and word.deprel is not None for word in self.words)
+        is_complete_words = (len(self.words) >= len(self.tokens)) and (len(self.words) == self.words[-1].id)
+        if is_complete_dependencies and is_complete_words: self.build_dependencies()
+
+    @property
+    def doc(self):
+        """ Access the parent doc of this span. """
+        return self._doc
+
+    @doc.setter
+    def doc(self, value):
+        """ Set the parent doc of this span. """
+        self._doc = value
+
+    @property
+    def text(self):
+        """ Access the raw text for this sentence. """
+        return self._text
+
+    @text.setter
+    def text(self, value):
+        """ Set the raw text for this sentence. """
+        self._text = value
+
+    @property
+    def dependencies(self):
+        """ Access list of dependencies for this sentence. """
+        return self._dependencies
+
+    @dependencies.setter
+    def dependencies(self, value):
+        """ Set the list of dependencies for this sentence. """
+        self._dependencies = value
+
+    @property
+    def tokens(self):
+        """ Access the list of tokens for this sentence. """
+        return self._tokens
+
+    @tokens.setter
+    def tokens(self, value):
+        """ Set the list of tokens for this sentence. """
+        self._tokens = value
+
+    @property
+    def words(self):
+        """ Access the list of words for this sentence. """
+        return self._words
+
+    @words.setter
+    def words(self, value):
+        """ Set the list of words for this sentence. """
+        self._words = value
+
+    @property
+    def ents(self):
+        """ Access the list of entities in this sentence. """
+        return self._ents
+
+    @ents.setter
+    def ents(self, value):
+        """ Set the list of entities in this sentence. """
+        self._ents = value
+
+    @property
+    def entities(self):
+        """ Access the list of entities. This is just an alias of `ents`. """
+        return self._ents
+
+    @entities.setter
+    def entities(self, value):
+        """ Set the list of entities in this sentence. """
+        self._ents = value
+
+    def build_ents(self):
+        """ Build the list of entities by iterating over all tokens. Return all entities as a list.
+
+        Note that unlike other attributes, since NER requires raw text, the actual tagging are always
+        performed at and attached to the `Token`s, instead of `Word`s.
+        """
+        self.ents = []
+        tags = [w.ner for w in self.tokens]
+        decoded = decode_from_bioes(tags)
+        for e in decoded:
+            ent_tokens = self.tokens[e['start']:e['end']+1]
+            self.ents.append(Span(tokens=ent_tokens, type=e['type'], doc=self.doc, sent=self))
+        return self.ents
+
+    @property
+    def sentiment(self):
+        """ Returns the sentiment value for this sentence """
+        return self._sentiment
+
+    @sentiment.setter
+    def sentiment(self, value):
+        """ Set the sentiment value """
+        self._sentiment = value
+
+    def build_dependencies(self):
+        """ Build the dependency graph for this sentence. Each dependency graph entry is
+        a list of (head, deprel, word).
+        """
+        self.dependencies = []
+        for word in self.words:
+            if word.head == 0:
+                # make a word for the ROOT
+                word_entry = {ID: 0, TEXT: "ROOT"}
+                head = Word(word_entry)
+            else:
+                # id is index in words list + 1
+                head = self.words[word.head - 1]
+                assert(word.head == head.id)
+            self.dependencies.append((head, word.deprel, word))
+
+    def print_dependencies(self, file=None):
+        """ Print the dependencies for this sentence. """
+        for dep_edge in self.dependencies:
+            print((dep_edge[2].text, dep_edge[0].id, dep_edge[1]), file=file)
+
+    def dependencies_string(self):
+        """ Dump the dependencies for this sentence into string. """
+        dep_string = io.StringIO()
+        self.print_dependencies(file=dep_string)
+        return dep_string.getvalue().strip()
+
+    def print_tokens(self, file=None):
+        """ Print the tokens for this sentence. """
+        for tok in self.tokens:
+            print(tok.pretty_print(), file=file)
+
+    def tokens_string(self):
+        """ Dump the tokens for this sentence into string. """
+        toks_string = io.StringIO()
+        self.print_tokens(file=toks_string)
+        return toks_string.getvalue().strip()
+
+    def print_words(self, file=None):
+        """ Print the words for this sentence. """
+        for word in self.words:
+            print(word.pretty_print(), file=file)
+
+    def words_string(self):
+        """ Dump the words for this sentence into string. """
+        wrds_string = io.StringIO()
+        self.print_words(file=wrds_string)
+        return wrds_string.getvalue().strip()
+
+    def to_dict(self):
+        """ Dumps the sentence into a list of dictionary for each token in the sentence.
+        """
+        ret = []
+        for token in self.tokens:
+            ret += token.to_dict()
+        return (ret, self._metadata)
+
+    def __repr__(self):
+        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)
+
+
+class Token(StanzaObject):
+    """ A token class that stores attributes of a token and carries a list of words. A token corresponds to a unit in the raw
+    text. In some languages such as English, a token has a one-to-one mapping to a word, while in other languages such as French,
+    a (multi-word) token might be expanded into multiple words that carry syntactic annotations.
+    """
+
+    def __init__(self, token_entry, words=None):
+        """ Construct a token given a dictionary format token entry. Optionally link itself to the corresponding words.
+        """
+        self._id = token_entry.get(ID)
+        self._text = token_entry.get(TEXT)
+        assert self._id and self._text, 'id and text should be included for the token'
+        self._misc = token_entry.get(MISC, None)
+        self._ner = token_entry.get(NER, None)
+        self._srl = token_entry.get(SRL, None)
+        self._words = words if words is not None else []
+        self._start_char = token_entry.get(START_CHAR, None)
+        self._end_char = token_entry.get(END_CHAR, None)
+
+        if self._misc is not None:
+            self.init_from_misc()
+
+    def init_from_misc(self):
+        """ Create attributes by parsing from the `misc` field.
+        """
+        for item in self._misc.split('|'):
+            key_value = item.split('=', 1)
+            if len(key_value) == 1: continue # some key_value can not be splited
+            key, value = key_value
+            if key in (START_CHAR, END_CHAR):
+                value = int(value)
+            # set attribute
+            attr = f'_{key}'
+            if hasattr(self, attr):
+                setattr(self, attr, value)
+
+    @property
+    def id(self):
+        """ Access the index of this token. """
+        return self._id
+
+    @id.setter
+    def id(self, value):
+        """ Set the token's id value. """
+        self._id = value
+
+    @property
+    def text(self):
+        """ Access the text of this token. Example: 'The' """
+        return self._text
+
+    @text.setter
+    def text(self, value):
+        """ Set the token's text value. Example: 'The' """
+        self._text = value
+
+    @property
+    def misc(self):
+        """ Access the miscellaneousness of this token. """
+        return self._misc
+
+    @misc.setter
+    def misc(self, value):
+        """ Set the token's miscellaneousness value. """
+        self._misc = value if self._is_null(value) == False else None
+
+    @property
+    def words(self):
+        """ Access the list of syntactic words underlying this token. """
+        return self._words
+
+    @words.setter
+    def words(self, value):
+        """ Set this token's list of underlying syntactic words. """
+        self._words = value
+        for w in self._words:
+            w.parent = self
+
+    @property
+    def start_char(self):
+        """ Access the start character index for this token in the raw text. """
+        return self._start_char
+
+    @property
+    def end_char(self):
+        """ Access the end character index for this token in the raw text. """
+        return self._end_char
+
+    @property
+    def ner(self):
+        """ Access the NER tag of this token. Example: 'B-ORG'"""
+        return self._ner
+
+    @ner.setter
+    def ner(self, value):
+        """ Set the token's NER tag. Example: 'B-ORG'"""
+        self._ner = value if self._is_null(value) == False else None
+
+    @property
+    def srl(self):
+        """ Access the SRL tag of this token. Example: 'ACT'"""
+        return self._srl
+
+    @srl.setter
+    def srl(self, value):
+        """ Set the token's SRL tag. Example: 'ACT'"""
+        self._srl = value if self._is_null(value) == False else None
+
+    def __repr__(self):
+        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)
+
+    def to_dict(self, fields=[ID, TEXT, NER, SRL, MISC]):
+        """ Dumps the token into a list of dictionary for this token with its extended words
+        if the token is a multi-word token.
+        """
+        ret = []
+        if len(self.id) > 1:
+            token_dict = {}
+            for field in fields:
+                if getattr(self, field) is not None:
+                    token_dict[field] = getattr(self, field)
+            ret.append(token_dict)
+        for word in self.words:
+            word_dict = word.to_dict()
+            if len(self.id) == 1 and NER in fields and getattr(self, NER) is not None: # propagate NER label to Word if it is a single-word token
+                word_dict[NER] = getattr(self, NER)
+            if len(self.id) == 1 and SRL in fields and getattr(self, SRL) is not None: # propagate SRL label to Word if it is a single-word token
+                word_dict[SRL] = getattr(self, SRL)
+            ret.append(word_dict)
+        return ret
+
+    def pretty_print(self):
+        """ Print this token with its extended words in one line. """
+        return f"<{self.__class__.__name__} id={'-'.join([str(x) for x in self.id])};words=[{', '.join([word.pretty_print() for word in self.words])}]>"
+
+    def _is_null(self, value):
+        return (value is None) or (value == '_')
+
+class Word(StanzaObject):
+    """ A word class that stores attributes of a word.
+    """
+
+    def __init__(self, word_entry):
+        """ Construct a word given a dictionary format word entry.
+        """
+        self._id = word_entry.get(ID, None)
+        if isinstance(self._id, tuple):
+            assert len(self._id) == 1
+            self._id = self._id[0]
+        self._text = word_entry.get(TEXT, None)
+
+        assert self._id is not None and self._text is not None, 'id and text should be included for the word. {}'.format(word_entry)
+
+        self._lemma = word_entry.get(LEMMA, None)
+        self._upos = word_entry.get(UPOS, None)
+        self._xpos = word_entry.get(XPOS, None)
+        self._feats = word_entry.get(FEATS, None)
+        self._head = word_entry.get(HEAD, None)
+        self._deprel = word_entry.get(DEPREL, None)
+        self._deps = word_entry.get(DEPS, None)
+        self._misc = word_entry.get(MISC, None)
+        self._start_char = word_entry.get(START_CHAR, None)
+        self._end_char = word_entry.get(END_CHAR, None)
+        self._parent = None
+
+        if self._misc is not None:
+            self.init_from_misc()
+
+    def init_from_misc(self):
+        """ Create attributes by parsing from the `misc` field.
+        """
+        for item in self._misc.split('|'):
+            key_value = item.split('=', 1)
+            if len(key_value) == 1: continue # some key_value can not be splited
+            key, value = key_value
+            # set attribute
+            attr = f'_{key}'
+            if hasattr(self, attr):
+                setattr(self, attr, value)
+
+    @property
+    def id(self):
+        """ Access the index of this word. """
+        return self._id
+
+    @id.setter
+    def id(self, value):
+        """ Set the word's index value. """
+        self._id = value
+
+    @property
+    def text(self):
+        """ Access the text of this word. Example: 'The'"""
+        return self._text
+
+    @text.setter
+    def text(self, value):
+        """ Set the word's text value. Example: 'The'"""
+        self._text = value
+
+    @property
+    def lemma(self):
+        """ Access the lemma of this word. """
+        return self._lemma
+
+    @lemma.setter
+    def lemma(self, value):
+        """ Set the word's lemma value. """
+        self._lemma = value if self._is_null(value) == False or self._text == '_' else None
+
+    @property
+    def upos(self):
+        """ Access the universal part-of-speech of this word. Example: 'NOUN'"""
+        return self._upos
+
+    @upos.setter
+    def upos(self, value):
+        """ Set the word's universal part-of-speech value. Example: 'NOUN'"""
+        self._upos = value if self._is_null(value) == False else None
+
+    @property
+    def xpos(self):
+        """ Access the treebank-specific part-of-speech of this word. Example: 'NNP'"""
+        return self._xpos
+
+    @xpos.setter
+    def xpos(self, value):
+        """ Set the word's treebank-specific part-of-speech value. Example: 'NNP'"""
+        self._xpos = value if self._is_null(value) == False else None
+
+    @property
+    def feats(self):
+        """ Access the morphological features of this word. Example: 'Gender=Fem'"""
+        return self._feats
+
+    @feats.setter
+    def feats(self, value):
+        """ Set this word's morphological features. Example: 'Gender=Fem'"""
+        self._feats = value if self._is_null(value) == False else None
+
+    @property
+    def head(self):
+        """ Access the id of the governer of this word. """
+        return self._head
+
+    @head.setter
+    def head(self, value):
+        """ Set the word's governor id value. """
+        self._head = int(value) if self._is_null(value) == False else None
+
+    @property
+    def deprel(self):
+        """ Access the dependency relation of this word. Example: 'nmod'"""
+        return self._deprel
+
+    @deprel.setter
+    def deprel(self, value):
+        """ Set the word's dependency relation value. Example: 'nmod'"""
+        self._deprel = value if self._is_null(value) == False else None
+
+    @property
+    def deps(self):
+        """ Access the dependencies of this word. """
+        return self._deps
+
+    @deps.setter
+    def deps(self, value):
+        """ Set the word's dependencies value. """
+        self._deps = value if self._is_null(value) == False else None
+
+    @property
+    def misc(self):
+        """ Access the miscellaneousness of this word. """
+        return self._misc
+
+    @misc.setter
+    def misc(self, value):
+        """ Set the word's miscellaneousness value. """
+        self._misc = value if self._is_null(value) == False else None
+
+    @property
+    def parent(self):
+        """ Access the parent token of this word. In the case of a multi-word token, a token can be the parent of
+        multiple words. Note that this should return a reference to the parent token object.
+        """
+        return self._parent
+
+    @parent.setter
+    def parent(self, value):
+        """ Set this word's parent token. In the case of a multi-word token, a token can be the parent of
+        multiple words. Note that value here should be a reference to the parent token object.
+        """
+        self._parent = value
+
+    @property
+    def pos(self):
+        """ Access the universal part-of-speech of this word. Example: 'NOUN'"""
+        return self._upos
+
+    @pos.setter
+    def pos(self, value):
+        """ Set the word's universal part-of-speech value. Example: 'NOUN'"""
+        self._upos = value if self._is_null(value) == False else None
+
+    def __repr__(self):
+        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)
+
+    def to_dict(self, fields=[ID, TEXT, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC]):
+        """ Dumps the word into a dictionary.
+        """
+        word_dict = {}
+        for field in fields:
+            if getattr(self, field) is not None:
+                word_dict[field] = getattr(self, field)
+        return word_dict
+
+    def pretty_print(self):
+        """ Print the word in one line. """
+        features = [ID, TEXT, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL]
+        feature_str = ";".join(["{}={}".format(k, getattr(self, k)) for k in features if getattr(self, k) is not None])
+        return f"<{self.__class__.__name__} {feature_str}>"
+
+    def _is_null(self, value):
+        return (value is None) or (value == '_')
+
+
+class Span(StanzaObject):
+    """ A span class that stores attributes of a textual span. A span can be typed.
+    A range of objects (e.g., entity mentions) can be represented as spans.
+    """
+
+    def __init__(self, span_entry=None, tokens=None, type=None, doc=None, sent=None):
+        """ Construct a span given a span entry or a list of tokens. A valid reference to a doc
+        must be provided to construct a span (otherwise the text of the span cannot be initialized).
+        """
+        assert span_entry is not None or (tokens is not None and type is not None), \
+                'Either a span_entry or a token list needs to be provided to construct a span.'
+        assert doc is not None, 'A parent doc must be provided to construct a span.'
+        self._text, self._type, self._start_char, self._end_char = [None] * 4
+        self._tokens = []
+        self._words = []
+        self._doc = doc
+        self._sent = sent
+
+        if span_entry is not None:
+            self.init_from_entry(span_entry)
+
+        if tokens is not None:
+            self.init_from_tokens(tokens, type)
+
+    def init_from_entry(self, span_entry):
+        self.text = span_entry.get(TEXT, None)
+        self.type = span_entry.get(TYPE, None)
+        self.start_char = span_entry.get(START_CHAR, None)
+        self.end_char = span_entry.get(END_CHAR, None)
+
+    def init_from_tokens(self, tokens, type):
+        assert isinstance(tokens, list), 'Tokens must be provided as a list to construct a span.'
+        assert len(tokens) > 0, "Tokens of a span cannot be an empty list."
+        self.tokens = tokens
+        self.type = type
+        # load start and end char offsets from tokens
+        self.start_char = self.tokens[0].start_char
+        self.end_char = self.tokens[-1].end_char
+        # assume doc is already provided and not None
+        self.text = self.doc.text[self.start_char:self.end_char]
+        # collect the words of the span following tokens
+        self.words = [w for t in tokens for w in t.words]
+
+    @property
+    def doc(self):
+        """ Access the parent doc of this span. """
+        return self._doc
+
+    @doc.setter
+    def doc(self, value):
+        """ Set the parent doc of this span. """
+        self._doc = value
+
+    @property
+    def text(self):
+        """ Access the text of this span. Example: 'Stanford University'"""
+        return self._text
+
+    @text.setter
+    def text(self, value):
+        """ Set the span's text value. Example: 'Stanford University'"""
+        self._text = value
+
+    @property
+    def tokens(self):
+        """ Access reference to a list of tokens that correspond to this span. """
+        return self._tokens
+
+    @tokens.setter
+    def tokens(self, value):
+        """ Set the span's list of tokens. """
+        self._tokens = value
+
+    @property
+    def words(self):
+        """ Access reference to a list of words that correspond to this span. """
+        return self._words
+
+    @words.setter
+    def words(self, value):
+        """ Set the span's list of words. """
+        self._words = value
+
+    @property
+    def type(self):
+        """ Access the type of this span. Example: 'PERSON'"""
+        return self._type
+
+    @type.setter
+    def type(self, value):
+        """ Set the type of this span. """
+        self._type = value
+
+    @property
+    def start_char(self):
+        """ Access the start character offset of this span. """
+        return self._start_char
+
+    @start_char.setter
+    def start_char(self, value):
+        """ Set the start character offset of this span. """
+        self._start_char = value
+
+    @property
+    def end_char(self):
+        """ Access the end character offset of this span. """
+        return self._end_char
+
+    @end_char.setter
+    def end_char(self, value):
+        """ Set the end character offset of this span. """
+        self._end_char = value
+
+    def to_dict(self):
+        """ Dumps the span into a dictionary. """
+        attrs = ['text', 'type', 'start_char', 'end_char']
+        span_dict = dict([(attr_name, getattr(self, attr_name)) for attr_name in attrs])
+        return span_dict
+
+    def __repr__(self):
+        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)
+
+    def pretty_print(self):
+        """ Print the span in one line. """
+        span_dict = self.to_dict()
+        feature_str = ";".join(["{}={}".format(k,v) for k,v in span_dict.items()])
+        return f"<{self.__class__.__name__} {feature_str}>"
```

### Comparing `classla-2.0/classla/models/common/hlstm.py` & `classla-2.1/classla/models/common/hlstm.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,124 +1,124 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence
-
-from classla.models.common.packed_lstm import PackedLSTM
-
-class HLSTMCell(nn.modules.rnn.RNNCellBase):
-    """
-    A Highway LSTM Cell as proposed in Zhang et al. (2018) Highway Long Short-Term Memory RNNs for 
-    Distant Speech Recognition.
-    """
-    def __init__(self, input_size, hidden_size, bias=True):
-        super(HLSTMCell, self).__init__()
-        self.input_size = input_size
-        self.hidden_size = hidden_size
-
-        # LSTM parameters
-        self.Wi = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)
-        self.Wf = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)
-        self.Wo = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)
-        self.Wg = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)
-
-        # highway gate parameters
-        self.gate = nn.Linear(input_size + 2 * hidden_size, hidden_size, bias=bias)
-
-    def forward(self, input, c_l_minus_one=None, hx=None):
-        self.check_forward_input(input)
-        if hx is None:
-            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)
-            hx = (hx, hx)
-        if c_l_minus_one is None:
-            c_l_minus_one = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)
-
-        self.check_forward_hidden(input, hx[0], '[0]')
-        self.check_forward_hidden(input, hx[1], '[1]')
-        self.check_forward_hidden(input, c_l_minus_one, 'c_l_minus_one')
-
-        # vanilla LSTM computation
-        rec_input = torch.cat([input, hx[0]], 1)
-        i = F.sigmoid(self.Wi(rec_input))
-        f = F.sigmoid(self.Wf(rec_input))
-        o = F.sigmoid(self.Wo(rec_input))
-        g = F.tanh(self.Wg(rec_input))
-
-        # highway gates
-        gate = F.sigmoid(self.gate(torch.cat([c_l_minus_one, hx[1], input], 1)))
-
-        c = gate * c_l_minus_one + f * hx[1] + i * g
-        h = o * F.tanh(c)
-
-        return h, c
-
-# Highway LSTM network, does NOT use the HLSTMCell above
-class HighwayLSTM(nn.Module):
-    """
-    A Highway LSTM network, as used in the original Tensorflow version of the Dozat parser. Note that this
-    is independent from the HLSTMCell above.
-    """
-    def __init__(self, input_size, hidden_size,
-                 num_layers=1, bias=True, batch_first=False,
-                 dropout=0, bidirectional=False, rec_dropout=0, highway_func=None, pad=False):
-        super(HighwayLSTM, self).__init__()
-        self.input_size = input_size
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.bias = bias
-        self.batch_first = batch_first
-        self.dropout = dropout
-        self.dropout_state = {}
-        self.bidirectional = bidirectional
-        self.num_directions = 2 if bidirectional else 1
-        self.highway_func = highway_func
-        self.pad = pad
-
-        self.lstm = nn.ModuleList()
-        self.highway = nn.ModuleList()
-        self.gate = nn.ModuleList()
-        self.drop = nn.Dropout(dropout, inplace=True)
-
-        in_size = input_size
-        for l in range(num_layers):
-            self.lstm.append(PackedLSTM(in_size, hidden_size, num_layers=1, bias=bias,
-                batch_first=batch_first, dropout=0, bidirectional=bidirectional, rec_dropout=rec_dropout))
-            self.highway.append(nn.Linear(in_size, hidden_size * self.num_directions))
-            self.gate.append(nn.Linear(in_size, hidden_size * self.num_directions))
-            self.highway[-1].bias.data.zero_()
-            self.gate[-1].bias.data.zero_()
-            in_size = hidden_size * self.num_directions
-
-    def forward(self, input, seqlens, hx=None):
-        highway_func = (lambda x: x) if self.highway_func is None else self.highway_func
-
-        hs = []
-        cs = []
-
-        if not isinstance(input, PackedSequence):
-            input = pack_padded_sequence(input, seqlens, batch_first=self.batch_first)
-
-        for l in range(self.num_layers):
-            if l > 0:
-                input = PackedSequence(self.drop(input.data), input.batch_sizes)
-            layer_hx = (hx[0][l * self.num_directions:(l+1)*self.num_directions], hx[1][l * self.num_directions:(l+1)*self.num_directions]) if hx is not None else None
-            h, (ht, ct) = self.lstm[l](input, seqlens, layer_hx)
-
-            hs.append(ht)
-            cs.append(ct)
-
-            input = PackedSequence(h.data + torch.sigmoid(self.gate[l](input.data)) * highway_func(self.highway[l](input.data)), input.batch_sizes)
-
-        if self.pad:
-            input = pad_packed_sequence(input, batch_first=self.batch_first)[0]
-        return input, (torch.cat(hs, 0), torch.cat(cs, 0))
-
-if __name__ == "__main__":
-    T = 10
-    bidir = True
-    num_dir = 2 if bidir else 1
-    rnn = HighwayLSTM(10, 20, num_layers=2, bidirectional=True)
-    input = torch.randn(T, 3, 10)
-    hx = torch.randn(2 * num_dir, 3, 20)
-    cx = torch.randn(2 * num_dir, 3, 20)
-    output = rnn(input, (hx, cx))
-    print(output)
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence
+
+from classla.models.common.packed_lstm import PackedLSTM
+
+class HLSTMCell(nn.modules.rnn.RNNCellBase):
+    """
+    A Highway LSTM Cell as proposed in Zhang et al. (2018) Highway Long Short-Term Memory RNNs for 
+    Distant Speech Recognition.
+    """
+    def __init__(self, input_size, hidden_size, bias=True):
+        super(HLSTMCell, self).__init__()
+        self.input_size = input_size
+        self.hidden_size = hidden_size
+
+        # LSTM parameters
+        self.Wi = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)
+        self.Wf = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)
+        self.Wo = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)
+        self.Wg = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)
+
+        # highway gate parameters
+        self.gate = nn.Linear(input_size + 2 * hidden_size, hidden_size, bias=bias)
+
+    def forward(self, input, c_l_minus_one=None, hx=None):
+        self.check_forward_input(input)
+        if hx is None:
+            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)
+            hx = (hx, hx)
+        if c_l_minus_one is None:
+            c_l_minus_one = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)
+
+        self.check_forward_hidden(input, hx[0], '[0]')
+        self.check_forward_hidden(input, hx[1], '[1]')
+        self.check_forward_hidden(input, c_l_minus_one, 'c_l_minus_one')
+
+        # vanilla LSTM computation
+        rec_input = torch.cat([input, hx[0]], 1)
+        i = F.sigmoid(self.Wi(rec_input))
+        f = F.sigmoid(self.Wf(rec_input))
+        o = F.sigmoid(self.Wo(rec_input))
+        g = F.tanh(self.Wg(rec_input))
+
+        # highway gates
+        gate = F.sigmoid(self.gate(torch.cat([c_l_minus_one, hx[1], input], 1)))
+
+        c = gate * c_l_minus_one + f * hx[1] + i * g
+        h = o * F.tanh(c)
+
+        return h, c
+
+# Highway LSTM network, does NOT use the HLSTMCell above
+class HighwayLSTM(nn.Module):
+    """
+    A Highway LSTM network, as used in the original Tensorflow version of the Dozat parser. Note that this
+    is independent from the HLSTMCell above.
+    """
+    def __init__(self, input_size, hidden_size,
+                 num_layers=1, bias=True, batch_first=False,
+                 dropout=0, bidirectional=False, rec_dropout=0, highway_func=None, pad=False):
+        super(HighwayLSTM, self).__init__()
+        self.input_size = input_size
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.bias = bias
+        self.batch_first = batch_first
+        self.dropout = dropout
+        self.dropout_state = {}
+        self.bidirectional = bidirectional
+        self.num_directions = 2 if bidirectional else 1
+        self.highway_func = highway_func
+        self.pad = pad
+
+        self.lstm = nn.ModuleList()
+        self.highway = nn.ModuleList()
+        self.gate = nn.ModuleList()
+        self.drop = nn.Dropout(dropout, inplace=True)
+
+        in_size = input_size
+        for l in range(num_layers):
+            self.lstm.append(PackedLSTM(in_size, hidden_size, num_layers=1, bias=bias,
+                batch_first=batch_first, dropout=0, bidirectional=bidirectional, rec_dropout=rec_dropout))
+            self.highway.append(nn.Linear(in_size, hidden_size * self.num_directions))
+            self.gate.append(nn.Linear(in_size, hidden_size * self.num_directions))
+            self.highway[-1].bias.data.zero_()
+            self.gate[-1].bias.data.zero_()
+            in_size = hidden_size * self.num_directions
+
+    def forward(self, input, seqlens, hx=None):
+        highway_func = (lambda x: x) if self.highway_func is None else self.highway_func
+
+        hs = []
+        cs = []
+
+        if not isinstance(input, PackedSequence):
+            input = pack_padded_sequence(input, seqlens, batch_first=self.batch_first)
+
+        for l in range(self.num_layers):
+            if l > 0:
+                input = PackedSequence(self.drop(input.data), input.batch_sizes)
+            layer_hx = (hx[0][l * self.num_directions:(l+1)*self.num_directions], hx[1][l * self.num_directions:(l+1)*self.num_directions]) if hx is not None else None
+            h, (ht, ct) = self.lstm[l](input, seqlens, layer_hx)
+
+            hs.append(ht)
+            cs.append(ct)
+
+            input = PackedSequence(h.data + torch.sigmoid(self.gate[l](input.data)) * highway_func(self.highway[l](input.data)), input.batch_sizes)
+
+        if self.pad:
+            input = pad_packed_sequence(input, batch_first=self.batch_first)[0]
+        return input, (torch.cat(hs, 0), torch.cat(cs, 0))
+
+if __name__ == "__main__":
+    T = 10
+    bidir = True
+    num_dir = 2 if bidir else 1
+    rnn = HighwayLSTM(10, 20, num_layers=2, bidirectional=True)
+    input = torch.randn(T, 3, 10)
+    hx = torch.randn(2 * num_dir, 3, 20)
+    cx = torch.randn(2 * num_dir, 3, 20)
+    output = rnn(input, (hx, cx))
+    print(output)
```

### Comparing `classla-2.0/classla/models/common/loss.py` & `classla-2.1/classla/models/common/loss.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,85 +1,85 @@
-"""
-Different loss functions.
-"""
-
-import logging
-import numpy as np
-import torch
-import torch.nn as nn
-
-import classla.models.common.seq2seq_constant as constant
-
-logger = logging.getLogger('classla')
-
-def SequenceLoss(vocab_size):
-    weight = torch.ones(vocab_size)
-    weight[constant.PAD_ID] = 0
-    crit = nn.NLLLoss(weight)
-    return crit
-
-def weighted_cross_entropy_loss(labels, log_dampened=False):
-    """
-    Either return a loss function which reweights all examples so the
-    classes have the same effective weight, or dampened reweighting
-    using log() so that the biggest class has some priority
-    """
-    if isinstance(labels, list):
-        all_labels = np.array(labels)
-    _, weights = np.unique(labels, return_counts=True)
-    weights = weights / float(np.sum(weights))
-    weights = np.sum(weights) / weights
-    if log_dampened:
-        weights = 1 + np.log(weights)
-    logger.debug("Reweighting cross entropy by {}".format(weights))
-    loss = nn.CrossEntropyLoss(
-        weight=torch.from_numpy(weights).type('torch.FloatTensor')
-    )
-    return loss
-
-class MixLoss(nn.Module):
-    """
-    A mixture of SequenceLoss and CrossEntropyLoss.
-    Loss = SequenceLoss + alpha * CELoss
-    """
-    def __init__(self, vocab_size, alpha):
-        super().__init__()
-        self.seq_loss = SequenceLoss(vocab_size)
-        self.ce_loss = nn.CrossEntropyLoss()
-        assert alpha >= 0
-        self.alpha = alpha
-
-    def forward(self, seq_inputs, seq_targets, class_inputs, class_targets):
-        sl = self.seq_loss(seq_inputs, seq_targets)
-        cel = self.ce_loss(class_inputs, class_targets)
-        loss = sl + self.alpha * cel
-        return loss
-
-class MaxEntropySequenceLoss(nn.Module):
-    """
-    A max entropy loss that encourage the model to have large entropy,
-    therefore giving more diverse outputs.
-
-    Loss = NLLLoss + alpha * EntropyLoss
-    """
-    def __init__(self, vocab_size, alpha):
-        super().__init__()
-        weight = torch.ones(vocab_size)
-        weight[constant.PAD_ID] = 0
-        self.nll = nn.NLLLoss(weight)
-        self.alpha = alpha
-
-    def forward(self, inputs, targets):
-        """
-        inputs: [N, C]
-        targets: [N]
-        """
-        assert inputs.size(0) == targets.size(0)
-        nll_loss = self.nll(inputs, targets)
-        # entropy loss
-        mask = targets.eq(constant.PAD_ID).unsqueeze(1).expand_as(inputs)
-        masked_inputs = inputs.clone().masked_fill_(mask, 0.0)
-        p = torch.exp(masked_inputs)
-        ent_loss = p.mul(masked_inputs).sum() / inputs.size(0) # average over minibatch
-        loss = nll_loss + self.alpha * ent_loss
-        return loss
-
+"""
+Different loss functions.
+"""
+
+import logging
+import numpy as np
+import torch
+import torch.nn as nn
+
+import classla.models.common.seq2seq_constant as constant
+
+logger = logging.getLogger('classla')
+
+def SequenceLoss(vocab_size):
+    weight = torch.ones(vocab_size)
+    weight[constant.PAD_ID] = 0
+    crit = nn.NLLLoss(weight)
+    return crit
+
+def weighted_cross_entropy_loss(labels, log_dampened=False):
+    """
+    Either return a loss function which reweights all examples so the
+    classes have the same effective weight, or dampened reweighting
+    using log() so that the biggest class has some priority
+    """
+    if isinstance(labels, list):
+        all_labels = np.array(labels)
+    _, weights = np.unique(labels, return_counts=True)
+    weights = weights / float(np.sum(weights))
+    weights = np.sum(weights) / weights
+    if log_dampened:
+        weights = 1 + np.log(weights)
+    logger.debug("Reweighting cross entropy by {}".format(weights))
+    loss = nn.CrossEntropyLoss(
+        weight=torch.from_numpy(weights).type('torch.FloatTensor')
+    )
+    return loss
+
+class MixLoss(nn.Module):
+    """
+    A mixture of SequenceLoss and CrossEntropyLoss.
+    Loss = SequenceLoss + alpha * CELoss
+    """
+    def __init__(self, vocab_size, alpha):
+        super().__init__()
+        self.seq_loss = SequenceLoss(vocab_size)
+        self.ce_loss = nn.CrossEntropyLoss()
+        assert alpha >= 0
+        self.alpha = alpha
+
+    def forward(self, seq_inputs, seq_targets, class_inputs, class_targets):
+        sl = self.seq_loss(seq_inputs, seq_targets)
+        cel = self.ce_loss(class_inputs, class_targets)
+        loss = sl + self.alpha * cel
+        return loss
+
+class MaxEntropySequenceLoss(nn.Module):
+    """
+    A max entropy loss that encourage the model to have large entropy,
+    therefore giving more diverse outputs.
+
+    Loss = NLLLoss + alpha * EntropyLoss
+    """
+    def __init__(self, vocab_size, alpha):
+        super().__init__()
+        weight = torch.ones(vocab_size)
+        weight[constant.PAD_ID] = 0
+        self.nll = nn.NLLLoss(weight)
+        self.alpha = alpha
+
+    def forward(self, inputs, targets):
+        """
+        inputs: [N, C]
+        targets: [N]
+        """
+        assert inputs.size(0) == targets.size(0)
+        nll_loss = self.nll(inputs, targets)
+        # entropy loss
+        mask = targets.eq(constant.PAD_ID).unsqueeze(1).expand_as(inputs)
+        masked_inputs = inputs.clone().masked_fill_(mask, 0.0)
+        p = torch.exp(masked_inputs)
+        ent_loss = p.mul(masked_inputs).sum() / inputs.size(0) # average over minibatch
+        loss = nll_loss + self.alpha * ent_loss
+        return loss
+
```

### Comparing `classla-2.0/classla/models/common/packed_lstm.py` & `classla-2.1/classla/models/common/packed_lstm.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,105 +1,105 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence
-
-class PackedLSTM(nn.Module):
-    def __init__(self, input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, pad=False, rec_dropout=0):
-        super().__init__()
-
-        self.batch_first = batch_first
-        self.pad = pad
-        if rec_dropout == 0:
-            # use the fast, native LSTM implementation
-            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)
-        else:
-            self.lstm = LSTMwRecDropout(input_size, hidden_size, num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional, rec_dropout=rec_dropout)
-
-    def forward(self, input, lengths, hx=None):
-        if not isinstance(input, PackedSequence):
-            input = pack_padded_sequence(input, lengths, batch_first=self.batch_first)
-
-        res = self.lstm(input, hx)
-        if self.pad:
-            res = (pad_packed_sequence(res[0], batch_first=self.batch_first)[0], res[1])
-        return res
-
-class LSTMwRecDropout(nn.Module):
-    """ An LSTM implementation that supports recurrent dropout """
-    def __init__(self, input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, pad=False, rec_dropout=0):
-        super().__init__()
-        self.batch_first = batch_first
-        self.pad = pad
-        self.num_layers = num_layers
-        self.hidden_size = hidden_size
-
-        self.dropout = dropout
-        self.drop = nn.Dropout(dropout, inplace=True)
-        self.rec_drop = nn.Dropout(rec_dropout, inplace=True)
-
-        self.num_directions = 2 if bidirectional else 1
-
-        self.cells = nn.ModuleList()
-        for l in range(num_layers):
-            in_size = input_size if l == 0 else self.num_directions * hidden_size
-            for d in range(self.num_directions):
-                self.cells.append(nn.LSTMCell(in_size, hidden_size, bias=bias))
-
-    def forward(self, input, hx=None):
-        def rnn_loop(x, batch_sizes, cell, inits, reverse=False):
-            # RNN loop for one layer in one direction with recurrent dropout
-            # Assumes input is PackedSequence, returns PackedSequence as well
-            batch_size = batch_sizes[0].item()
-            states = [list(init.split([1] * batch_size)) for init in inits]
-            h_drop_mask = x.new_ones(batch_size, self.hidden_size)
-            h_drop_mask = self.rec_drop(h_drop_mask)
-            resh = []
-
-            if not reverse:
-                st = 0
-                for bs in batch_sizes:
-                    s1 = cell(x[st:st+bs], (torch.cat(states[0][:bs], 0) * h_drop_mask[:bs], torch.cat(states[1][:bs], 0)))
-                    resh.append(s1[0])
-                    for j in range(bs):
-                        states[0][j] = s1[0][j].unsqueeze(0)
-                        states[1][j] = s1[1][j].unsqueeze(0)
-                    st += bs
-            else:
-                en = x.size(0)
-                for i in range(batch_sizes.size(0)-1, -1, -1):
-                    bs = batch_sizes[i]
-                    s1 = cell(x[en-bs:en], (torch.cat(states[0][:bs], 0) * h_drop_mask[:bs], torch.cat(states[1][:bs], 0)))
-                    resh.append(s1[0])
-                    for j in range(bs):
-                        states[0][j] = s1[0][j].unsqueeze(0)
-                        states[1][j] = s1[1][j].unsqueeze(0)
-                    en -= bs
-                resh = list(reversed(resh))
-
-            return torch.cat(resh, 0), tuple(torch.cat(s, 0) for s in states)
-
-        all_states = [[], []]
-        inputdata, batch_sizes = input.data, input.batch_sizes
-        for l in range(self.num_layers):
-            new_input = []
-
-            if self.dropout > 0 and l > 0:
-                inputdata = self.drop(inputdata)
-            for d in range(self.num_directions):
-                idx = l * self.num_directions + d
-                cell = self.cells[idx]
-                out, states = rnn_loop(inputdata, batch_sizes, cell, (hx[i][idx] for i in range(2)) if hx is not None else (input.data.new_zeros(input.batch_sizes[0].item(), self.hidden_size, requires_grad=False) for _ in range(2)), reverse=(d == 1))
-
-                new_input.append(out)
-                all_states[0].append(states[0].unsqueeze(0))
-                all_states[1].append(states[1].unsqueeze(0))
-
-            if self.num_directions > 1:
-                # concatenate both directions
-                inputdata = torch.cat(new_input, 1)
-            else:
-                inputdata = new_input[0]
-
-        input = PackedSequence(inputdata, batch_sizes)
-
-        return input, tuple(torch.cat(x, 0) for x in all_states)
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence
+
+class PackedLSTM(nn.Module):
+    def __init__(self, input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, pad=False, rec_dropout=0):
+        super().__init__()
+
+        self.batch_first = batch_first
+        self.pad = pad
+        if rec_dropout == 0:
+            # use the fast, native LSTM implementation
+            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)
+        else:
+            self.lstm = LSTMwRecDropout(input_size, hidden_size, num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional, rec_dropout=rec_dropout)
+
+    def forward(self, input, lengths, hx=None):
+        if not isinstance(input, PackedSequence):
+            input = pack_padded_sequence(input, lengths, batch_first=self.batch_first)
+
+        res = self.lstm(input, hx)
+        if self.pad:
+            res = (pad_packed_sequence(res[0], batch_first=self.batch_first)[0], res[1])
+        return res
+
+class LSTMwRecDropout(nn.Module):
+    """ An LSTM implementation that supports recurrent dropout """
+    def __init__(self, input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, pad=False, rec_dropout=0):
+        super().__init__()
+        self.batch_first = batch_first
+        self.pad = pad
+        self.num_layers = num_layers
+        self.hidden_size = hidden_size
+
+        self.dropout = dropout
+        self.drop = nn.Dropout(dropout, inplace=True)
+        self.rec_drop = nn.Dropout(rec_dropout, inplace=True)
+
+        self.num_directions = 2 if bidirectional else 1
+
+        self.cells = nn.ModuleList()
+        for l in range(num_layers):
+            in_size = input_size if l == 0 else self.num_directions * hidden_size
+            for d in range(self.num_directions):
+                self.cells.append(nn.LSTMCell(in_size, hidden_size, bias=bias))
+
+    def forward(self, input, hx=None):
+        def rnn_loop(x, batch_sizes, cell, inits, reverse=False):
+            # RNN loop for one layer in one direction with recurrent dropout
+            # Assumes input is PackedSequence, returns PackedSequence as well
+            batch_size = batch_sizes[0].item()
+            states = [list(init.split([1] * batch_size)) for init in inits]
+            h_drop_mask = x.new_ones(batch_size, self.hidden_size)
+            h_drop_mask = self.rec_drop(h_drop_mask)
+            resh = []
+
+            if not reverse:
+                st = 0
+                for bs in batch_sizes:
+                    s1 = cell(x[st:st+bs], (torch.cat(states[0][:bs], 0) * h_drop_mask[:bs], torch.cat(states[1][:bs], 0)))
+                    resh.append(s1[0])
+                    for j in range(bs):
+                        states[0][j] = s1[0][j].unsqueeze(0)
+                        states[1][j] = s1[1][j].unsqueeze(0)
+                    st += bs
+            else:
+                en = x.size(0)
+                for i in range(batch_sizes.size(0)-1, -1, -1):
+                    bs = batch_sizes[i]
+                    s1 = cell(x[en-bs:en], (torch.cat(states[0][:bs], 0) * h_drop_mask[:bs], torch.cat(states[1][:bs], 0)))
+                    resh.append(s1[0])
+                    for j in range(bs):
+                        states[0][j] = s1[0][j].unsqueeze(0)
+                        states[1][j] = s1[1][j].unsqueeze(0)
+                    en -= bs
+                resh = list(reversed(resh))
+
+            return torch.cat(resh, 0), tuple(torch.cat(s, 0) for s in states)
+
+        all_states = [[], []]
+        inputdata, batch_sizes = input.data, input.batch_sizes
+        for l in range(self.num_layers):
+            new_input = []
+
+            if self.dropout > 0 and l > 0:
+                inputdata = self.drop(inputdata)
+            for d in range(self.num_directions):
+                idx = l * self.num_directions + d
+                cell = self.cells[idx]
+                out, states = rnn_loop(inputdata, batch_sizes, cell, (hx[i][idx] for i in range(2)) if hx is not None else (input.data.new_zeros(input.batch_sizes[0].item(), self.hidden_size, requires_grad=False) for _ in range(2)), reverse=(d == 1))
+
+                new_input.append(out)
+                all_states[0].append(states[0].unsqueeze(0))
+                all_states[1].append(states[1].unsqueeze(0))
+
+            if self.num_directions > 1:
+                # concatenate both directions
+                inputdata = torch.cat(new_input, 1)
+            else:
+                inputdata = new_input[0]
+
+        input = PackedSequence(inputdata, batch_sizes)
+
+        return input, tuple(torch.cat(x, 0) for x in all_states)
```

### Comparing `classla-2.0/classla/models/common/pretrain.py` & `classla-2.1/classla/models/common/pretrain.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,139 +1,139 @@
-"""
-Supports for pretrained data.
-"""
-import os
-import re
-
-import lzma
-import logging
-from collections import OrderedDict
-
-import numpy as np
-import torch
-
-from .vocab import BaseVocab, VOCAB_PREFIX
-
-logger = logging.getLogger('classla')
-
-class PretrainedWordVocab(BaseVocab):
-    def build_vocab(self):
-        self._id2unit = VOCAB_PREFIX + self.data
-        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}
-
-class Pretrain:
-    """ A loader and saver for pretrained embeddings. """
-
-    def __init__(self, filename=None, vec_filename=None, max_vocab=-1, save_to_file=True):
-        self.filename = filename
-        self._vec_filename = vec_filename
-        self._max_vocab = max_vocab
-        self._save_to_file = save_to_file
-
-    @property
-    def vocab(self):
-        if not hasattr(self, '_vocab'):
-            self._vocab, self._emb = self.load()
-        return self._vocab
-
-    @property
-    def emb(self):
-        if not hasattr(self, '_emb'):
-            self._vocab, self._emb = self.load()
-        return self._emb
-
-    def load(self):
-        if self.filename is not None and os.path.exists(self.filename):
-            try:
-                data = torch.load(self.filename, lambda storage, loc: storage)
-                logger.debug("Loaded pretrain from {}".format(self.filename))
-            except (KeyboardInterrupt, SystemExit):
-                raise
-            except BaseException as e:
-                logger.warning("Pretrained file exists but cannot be loaded from {}, due to the following exception:\n\t{}".format(self.filename, e))
-                return self.read_pretrain()
-            return PretrainedWordVocab.load_state_dict(data['vocab']), data['emb']
-        else:
-            return self.read_pretrain()
-
-    def read_pretrain(self):
-        # load from pretrained filename
-        if self._vec_filename is None:
-            raise Exception("Vector file is not provided.")
-        logger.info("Reading pretrained vectors from {}...".format(self._vec_filename))
-
-        # first try reading as xz file, if failed retry as text file
-        try:
-            words, emb, failed = self.read_from_file(self._vec_filename, open_func=lzma.open)
-        except lzma.LZMAError as err:
-            logger.warning("Cannot decode vector file %s as xz file. Retrying as text file..." % self._vec_filename)
-            words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)
-
-        if failed > 0: # recover failure
-            emb = emb[:-failed]
-        if len(emb) - len(VOCAB_PREFIX) != len(words):
-            raise Exception("Loaded number of vectors does not match number of words.")
-        
-        # Use a fixed vocab size
-        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words):
-            words = words[:self._max_vocab - len(VOCAB_PREFIX)]
-            emb = emb[:self._max_vocab]
-
-        vocab = PretrainedWordVocab(words, lower=True)
-        
-        if self._save_to_file:
-            assert self.filename is not None, "Filename must be provided to save pretrained vector to file."
-            # save to file
-            data = {'vocab': vocab.state_dict(), 'emb': emb}
-            try:
-                torch.save(data, self.filename)
-                logger.info("Saved pretrained vocab and vectors to {}".format(self.filename))
-            except (KeyboardInterrupt, SystemExit):
-                raise
-            except BaseException as e:
-                logger.warning("Saving pretrained data failed due to the following exception... continuing anyway.\n\t{}".format(e))
-
-        return vocab, emb
-
-    def read_from_file(self, filename, open_func=open):
-        """
-        Open a vector file using the provided function and read from it.
-        """
-        # some vector files, such as Google News, use tabs
-        tab_space_pattern = re.compile(r"[ \t]+")
-        first = True
-        words = []
-        failed = 0
-        with open_func(filename, 'rb') as f:
-            for i, line in enumerate(f):
-                try:
-                    line = line.decode()
-                except UnicodeDecodeError:
-                    failed += 1
-                    continue
-                if first:
-                    # the first line contains the number of word vectors and the dimensionality
-                    first = False
-                    line = line.strip().split(' ')
-                    rows, cols = [int(x) for x in line]
-                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)
-                    continue
-
-                line = tab_space_pattern.split((line.rstrip()))
-                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]
-                words.append(' '.join(line[:-cols]))
-        return words, emb, failed
-
-
-if __name__ == '__main__':
-    with open('test.tmp', 'w') as fout:
-        fout.write('3 2\na 1 1\nb -1 -1\nc 0 0\n')
-    # 1st load: save to pt file
-    pretrain = Pretrain('test.pt', 'test.tmp')
-    print(pretrain.emb)
-    # verify pt file
-    x = torch.load('test.pt')
-    print(x)
-    # 2nd load: load saved pt file
-    pretrain = Pretrain('test.pt', 'test.tmp')
-    print(pretrain.emb)
-
+"""
+Supports for pretrained data.
+"""
+import os
+import re
+
+import lzma
+import logging
+from collections import OrderedDict
+
+import numpy as np
+import torch
+
+from .vocab import BaseVocab, VOCAB_PREFIX
+
+logger = logging.getLogger('classla')
+
+class PretrainedWordVocab(BaseVocab):
+    def build_vocab(self):
+        self._id2unit = VOCAB_PREFIX + self.data
+        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}
+
+class Pretrain:
+    """ A loader and saver for pretrained embeddings. """
+
+    def __init__(self, filename=None, vec_filename=None, max_vocab=-1, save_to_file=True):
+        self.filename = filename
+        self._vec_filename = vec_filename
+        self._max_vocab = max_vocab
+        self._save_to_file = save_to_file
+
+    @property
+    def vocab(self):
+        if not hasattr(self, '_vocab'):
+            self._vocab, self._emb = self.load()
+        return self._vocab
+
+    @property
+    def emb(self):
+        if not hasattr(self, '_emb'):
+            self._vocab, self._emb = self.load()
+        return self._emb
+
+    def load(self):
+        if self.filename is not None and os.path.exists(self.filename):
+            try:
+                data = torch.load(self.filename, lambda storage, loc: storage)
+                logger.debug("Loaded pretrain from {}".format(self.filename))
+            except (KeyboardInterrupt, SystemExit):
+                raise
+            except BaseException as e:
+                logger.warning("Pretrained file exists but cannot be loaded from {}, due to the following exception:\n\t{}".format(self.filename, e))
+                return self.read_pretrain()
+            return PretrainedWordVocab.load_state_dict(data['vocab']), data['emb']
+        else:
+            return self.read_pretrain()
+
+    def read_pretrain(self):
+        # load from pretrained filename
+        if self._vec_filename is None:
+            raise Exception("Vector file is not provided.")
+        logger.info("Reading pretrained vectors from {}...".format(self._vec_filename))
+
+        # first try reading as xz file, if failed retry as text file
+        try:
+            words, emb, failed = self.read_from_file(self._vec_filename, open_func=lzma.open)
+        except lzma.LZMAError as err:
+            logger.warning("Cannot decode vector file %s as xz file. Retrying as text file..." % self._vec_filename)
+            words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)
+
+        if failed > 0: # recover failure
+            emb = emb[:-failed]
+        if len(emb) - len(VOCAB_PREFIX) != len(words):
+            raise Exception("Loaded number of vectors does not match number of words.")
+        
+        # Use a fixed vocab size
+        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words):
+            words = words[:self._max_vocab - len(VOCAB_PREFIX)]
+            emb = emb[:self._max_vocab]
+
+        vocab = PretrainedWordVocab(words, lower=True)
+        
+        if self._save_to_file:
+            assert self.filename is not None, "Filename must be provided to save pretrained vector to file."
+            # save to file
+            data = {'vocab': vocab.state_dict(), 'emb': emb}
+            try:
+                torch.save(data, self.filename)
+                logger.info("Saved pretrained vocab and vectors to {}".format(self.filename))
+            except (KeyboardInterrupt, SystemExit):
+                raise
+            except BaseException as e:
+                logger.warning("Saving pretrained data failed due to the following exception... continuing anyway.\n\t{}".format(e))
+
+        return vocab, emb
+
+    def read_from_file(self, filename, open_func=open):
+        """
+        Open a vector file using the provided function and read from it.
+        """
+        # some vector files, such as Google News, use tabs
+        tab_space_pattern = re.compile(r"[ \t]+")
+        first = True
+        words = []
+        failed = 0
+        with open_func(filename, 'rb') as f:
+            for i, line in enumerate(f):
+                try:
+                    line = line.decode()
+                except UnicodeDecodeError:
+                    failed += 1
+                    continue
+                if first:
+                    # the first line contains the number of word vectors and the dimensionality
+                    first = False
+                    line = line.strip().split(' ')
+                    rows, cols = [int(x) for x in line]
+                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)
+                    continue
+
+                line = tab_space_pattern.split((line.rstrip()))
+                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]
+                words.append(' '.join(line[:-cols]))
+        return words, emb, failed
+
+
+if __name__ == '__main__':
+    with open('test.tmp', 'w') as fout:
+        fout.write('3 2\na 1 1\nb -1 -1\nc 0 0\n')
+    # 1st load: save to pt file
+    pretrain = Pretrain('test.pt', 'test.tmp')
+    print(pretrain.emb)
+    # verify pt file
+    x = torch.load('test.pt')
+    print(x)
+    # 2nd load: load saved pt file
+    pretrain = Pretrain('test.pt', 'test.tmp')
+    print(pretrain.emb)
+
```

### Comparing `classla-2.0/classla/models/common/seq2seq_model.py` & `classla-2.1/classla/models/common/seq2seq_model.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,282 +1,282 @@
-"""
-The full encoder-decoder model, built on top of the base seq2seq modules.
-"""
-
-import logging
-import torch
-from torch import nn
-import torch.nn.functional as F
-import numpy as np
-
-import classla.models.common.seq2seq_constant as constant
-from classla.models.common import utils
-from classla.models.common.seq2seq_modules import LSTMAttention
-from classla.models.common.beam import Beam
-
-logger = logging.getLogger('classla')
-
-class Seq2SeqModel(nn.Module):
-    """
-    A complete encoder-decoder model, with optional attention.
-    """
-    def __init__(self, args, emb_matrix=None, use_cuda=False):
-        super().__init__()
-        self.vocab_size = args['vocab_size']
-        self.emb_dim = args['emb_dim']
-        self.hidden_dim = args['hidden_dim']
-        self.nlayers = args['num_layers'] # encoder layers, decoder layers = 1
-        self.emb_dropout = args.get('emb_dropout', 0.0)
-        self.dropout = args['dropout']
-        self.pad_token = constant.PAD_ID
-        self.max_dec_len = args['max_dec_len']
-        self.use_cuda = use_cuda
-        self.top = args.get('top', 1e10)
-        self.args = args
-        self.emb_matrix = emb_matrix
-
-        logger.debug("Building an attentional Seq2Seq model...")
-        logger.debug("Using a Bi-LSTM encoder")
-        self.num_directions = 2
-        self.enc_hidden_dim = self.hidden_dim // 2
-        self.dec_hidden_dim = self.hidden_dim
-
-        self.use_pos = args.get('pos', False)
-        self.pos_dim = args.get('pos_dim', 0)
-        self.pos_vocab_size = args.get('pos_vocab_size', 0)
-        self.pos_dropout = args.get('pos_dropout', 0)
-        self.edit = args.get('edit', False)
-        self.num_edit = args.get('num_edit', 0)
-
-        self.emb_drop = nn.Dropout(self.emb_dropout)
-        self.drop = nn.Dropout(self.dropout)
-        self.embedding = nn.Embedding(self.vocab_size, self.emb_dim, self.pad_token)
-        self.encoder = nn.LSTM(self.emb_dim, self.enc_hidden_dim, self.nlayers, \
-                bidirectional=True, batch_first=True, dropout=self.dropout if self.nlayers > 1 else 0)
-        self.decoder = LSTMAttention(self.emb_dim, self.dec_hidden_dim, \
-                batch_first=True, attn_type=self.args['attn_type'])
-        self.dec2vocab = nn.Linear(self.dec_hidden_dim, self.vocab_size)
-        if self.use_pos and self.pos_dim > 0:
-            logger.debug("Using POS in encoder")
-            self.pos_embedding = nn.Embedding(self.pos_vocab_size, self.pos_dim, self.pad_token)
-            self.pos_drop = nn.Dropout(self.pos_dropout)
-        if self.edit:
-            edit_hidden = self.hidden_dim//2
-            self.edit_clf = nn.Sequential(
-                    nn.Linear(self.hidden_dim, edit_hidden),
-                    nn.ReLU(),
-                    nn.Linear(edit_hidden, self.num_edit))
-
-        self.SOS_tensor = torch.LongTensor([constant.SOS_ID])
-        self.SOS_tensor = self.SOS_tensor.cuda() if self.use_cuda else self.SOS_tensor
-
-        self.init_weights()
-
-    def init_weights(self):
-        # initialize embeddings
-        init_range = constant.EMB_INIT_RANGE
-        if self.emb_matrix is not None:
-            if isinstance(self.emb_matrix, np.ndarray):
-                self.emb_matrix = torch.from_numpy(self.emb_matrix)
-            assert self.emb_matrix.size() == (self.vocab_size, self.emb_dim), \
-                    "Input embedding matrix must match size: {} x {}".format(self.vocab_size, self.emb_dim)
-            self.embedding.weight.data.copy_(self.emb_matrix)
-        else:
-            self.embedding.weight.data.uniform_(-init_range, init_range)
-        # decide finetuning
-        if self.top <= 0:
-            logger.debug("Do not finetune embedding layer.")
-            self.embedding.weight.requires_grad = False
-        elif self.top < self.vocab_size:
-            logger.debug("Finetune top {} embeddings.".format(self.top))
-            self.embedding.weight.register_hook(lambda x: utils.keep_partial_grad(x, self.top))
-        else:
-            logger.debug("Finetune all embeddings.")
-        # initialize pos embeddings
-        if self.use_pos:
-            self.pos_embedding.weight.data.uniform_(-init_range, init_range)
-
-    def cuda(self):
-        super().cuda()
-        self.use_cuda = True
-
-    def cpu(self):
-        super().cpu()
-        self.use_cuda = False
-
-    def zero_state(self, inputs):
-        batch_size = inputs.size(0)
-        h0 = torch.zeros(self.encoder.num_layers*2, batch_size, self.enc_hidden_dim, requires_grad=False)
-        c0 = torch.zeros(self.encoder.num_layers*2, batch_size, self.enc_hidden_dim, requires_grad=False)
-        if self.use_cuda:
-            return h0.cuda(), c0.cuda()
-        return h0, c0
-
-    def encode(self, enc_inputs, lens):
-        """ Encode source sequence. """
-        self.h0, self.c0 = self.zero_state(enc_inputs)
-
-        packed_inputs = nn.utils.rnn.pack_padded_sequence(enc_inputs, lens, batch_first=True)
-        packed_h_in, (hn, cn) = self.encoder(packed_inputs, (self.h0, self.c0))
-        h_in, _ = nn.utils.rnn.pad_packed_sequence(packed_h_in, batch_first=True)
-        hn = torch.cat((hn[-1], hn[-2]), 1)
-        cn = torch.cat((cn[-1], cn[-2]), 1)
-        return h_in, (hn, cn)
-
-    def decode(self, dec_inputs, hn, cn, ctx, ctx_mask=None):
-        """ Decode a step, based on context encoding and source context states."""
-        dec_hidden = (hn, cn)
-        h_out, dec_hidden = self.decoder(dec_inputs, dec_hidden, ctx, ctx_mask)
-
-        h_out_reshape = h_out.contiguous().view(h_out.size(0) * h_out.size(1), -1)
-        decoder_logits = self.dec2vocab(h_out_reshape)
-        decoder_logits = decoder_logits.view(h_out.size(0), h_out.size(1), -1)
-        log_probs = self.get_log_prob(decoder_logits)
-        return log_probs, dec_hidden
-
-    def forward(self, src, src_mask, tgt_in, pos=None):
-        # prepare for encoder/decoder
-        batch_size = src.size(0)
-        enc_inputs = self.emb_drop(self.embedding(src))
-        dec_inputs = self.emb_drop(self.embedding(tgt_in))
-        if self.use_pos:
-            assert pos is not None, "Missing POS input for seq2seq lemmatizer."
-            pos_inputs = self.pos_drop(self.pos_embedding(pos))
-            enc_inputs = torch.cat([pos_inputs.unsqueeze(1), enc_inputs], dim=1)
-            pos_src_mask = src_mask.new_zeros([batch_size, 1])
-            src_mask = torch.cat([pos_src_mask, src_mask], dim=1)
-        src_lens = list(src_mask.data.eq(0).long().sum(1))
-
-        h_in, (hn, cn) = self.encode(enc_inputs, src_lens)
-
-        if self.edit:
-            edit_logits = self.edit_clf(hn)
-        else:
-            edit_logits = None
-
-        log_probs, _ = self.decode(dec_inputs, hn, cn, h_in, src_mask)
-        return log_probs, edit_logits
-
-    def get_log_prob(self, logits):
-        logits_reshape = logits.view(-1, self.vocab_size)
-        log_probs = F.log_softmax(logits_reshape, dim=1)
-        if logits.dim() == 2:
-            return log_probs
-        return log_probs.view(logits.size(0), logits.size(1), logits.size(2))
-
-    def predict_greedy(self, src, src_mask, pos=None):
-        """ Predict with greedy decoding. """
-        enc_inputs = self.embedding(src)
-        batch_size = enc_inputs.size(0)
-        if self.use_pos:
-            assert pos is not None, "Missing POS input for seq2seq lemmatizer."
-            pos_inputs = self.pos_drop(self.pos_embedding(pos))
-            enc_inputs = torch.cat([pos_inputs.unsqueeze(1), enc_inputs], dim=1)
-            pos_src_mask = src_mask.new_zeros([batch_size, 1])
-            src_mask = torch.cat([pos_src_mask, src_mask], dim=1)
-        src_lens = list(src_mask.data.eq(constant.PAD_ID).long().sum(1))
-
-        # encode source
-        h_in, (hn, cn) = self.encode(enc_inputs, src_lens)
-
-        if self.edit:
-            edit_logits = self.edit_clf(hn)
-        else:
-            edit_logits = None
-
-        # greedy decode by step
-        dec_inputs = self.embedding(self.SOS_tensor)
-        dec_inputs = dec_inputs.expand(batch_size, dec_inputs.size(0), dec_inputs.size(1))
-
-        done = [False for _ in range(batch_size)]
-        total_done = 0
-        max_len = 0
-        output_seqs = [[] for _ in range(batch_size)]
-
-        while total_done < batch_size and max_len < self.max_dec_len:
-            log_probs, (hn, cn) = self.decode(dec_inputs, hn, cn, h_in, src_mask)
-            assert log_probs.size(1) == 1, "Output must have 1-step of output."
-            _, preds = log_probs.squeeze(1).max(1, keepdim=True)
-            dec_inputs = self.embedding(preds) # update decoder inputs
-            max_len += 1
-            for i in range(batch_size):
-                if not done[i]:
-                    token = preds.data[i][0].item()
-                    if token == constant.EOS_ID:
-                        done[i] = True
-                        total_done += 1
-                    else:
-                        output_seqs[i].append(token)
-        return output_seqs, edit_logits
-
-    def predict(self, src, src_mask, pos=None, beam_size=5):
-        """ Predict with beam search. """
-        if beam_size == 1:
-            return self.predict_greedy(src, src_mask, pos=pos)
-
-        enc_inputs = self.embedding(src)
-        batch_size = enc_inputs.size(0)
-        if self.use_pos:
-            assert pos is not None, "Missing POS input for seq2seq lemmatizer."
-            pos_inputs = self.pos_drop(self.pos_embedding(pos))
-            enc_inputs = torch.cat([pos_inputs.unsqueeze(1), enc_inputs], dim=1)
-            pos_src_mask = src_mask.new_zeros([batch_size, 1])
-            src_mask = torch.cat([pos_src_mask, src_mask], dim=1)
-        src_lens = list(src_mask.data.eq(constant.PAD_ID).long().sum(1))
-
-        # (1) encode source
-        h_in, (hn, cn) = self.encode(enc_inputs, src_lens)
-
-        if self.edit:
-            edit_logits = self.edit_clf(hn)
-        else:
-            edit_logits = None
-
-        # (2) set up beam
-        with torch.no_grad():
-            h_in = h_in.data.repeat(beam_size, 1, 1) # repeat data for beam search
-            src_mask = src_mask.repeat(beam_size, 1)
-            # repeat decoder hidden states
-            hn = hn.data.repeat(beam_size, 1)
-            cn = cn.data.repeat(beam_size, 1)
-        beam = [Beam(beam_size, self.use_cuda) for _ in range(batch_size)]
-
-        def update_state(states, idx, positions, beam_size):
-            """ Select the states according to back pointers. """
-            for e in states:
-                br, d = e.size()
-                s = e.contiguous().view(beam_size, br // beam_size, d)[:,idx]
-                s.data.copy_(s.data.index_select(0, positions))
-
-        # (3) main loop
-        for i in range(self.max_dec_len):
-            dec_inputs = torch.stack([b.get_current_state() for b in beam]).t().contiguous().view(-1, 1)
-            dec_inputs = self.embedding(dec_inputs)
-            log_probs, (hn, cn) = self.decode(dec_inputs, hn, cn, h_in, src_mask)
-            log_probs = log_probs.view(beam_size, batch_size, -1).transpose(0,1)\
-                    .contiguous() # [batch, beam, V]
-
-            # advance each beam
-            done = []
-            for b in range(batch_size):
-                is_done = beam[b].advance(log_probs.data[b])
-                if is_done:
-                    done += [b]
-                # update beam state
-                update_state((hn, cn), b, beam[b].get_current_origin(), beam_size)
-
-            if len(done) == batch_size:
-                break
-
-        # back trace and find hypothesis
-        all_hyp, all_scores = [], []
-        for b in range(batch_size):
-            scores, ks = beam[b].sort_best()
-            all_scores += [scores[0]]
-            k = ks[0]
-            hyp = beam[b].get_hyp(k)
-            hyp = utils.prune_hyp(hyp)
-            hyp = [i.item() for i in hyp]
-            all_hyp += [hyp]
-
-        return all_hyp, edit_logits
-
+"""
+The full encoder-decoder model, built on top of the base seq2seq modules.
+"""
+
+import logging
+import torch
+from torch import nn
+import torch.nn.functional as F
+import numpy as np
+
+import classla.models.common.seq2seq_constant as constant
+from classla.models.common import utils
+from classla.models.common.seq2seq_modules import LSTMAttention
+from classla.models.common.beam import Beam
+
+logger = logging.getLogger('classla')
+
+class Seq2SeqModel(nn.Module):
+    """
+    A complete encoder-decoder model, with optional attention.
+    """
+    def __init__(self, args, emb_matrix=None, use_cuda=False):
+        super().__init__()
+        self.vocab_size = args['vocab_size']
+        self.emb_dim = args['emb_dim']
+        self.hidden_dim = args['hidden_dim']
+        self.nlayers = args['num_layers'] # encoder layers, decoder layers = 1
+        self.emb_dropout = args.get('emb_dropout', 0.0)
+        self.dropout = args['dropout']
+        self.pad_token = constant.PAD_ID
+        self.max_dec_len = args['max_dec_len']
+        self.use_cuda = use_cuda
+        self.top = args.get('top', 1e10)
+        self.args = args
+        self.emb_matrix = emb_matrix
+
+        logger.debug("Building an attentional Seq2Seq model...")
+        logger.debug("Using a Bi-LSTM encoder")
+        self.num_directions = 2
+        self.enc_hidden_dim = self.hidden_dim // 2
+        self.dec_hidden_dim = self.hidden_dim
+
+        self.use_pos = args.get('pos', False)
+        self.pos_dim = args.get('pos_dim', 0)
+        self.pos_vocab_size = args.get('pos_vocab_size', 0)
+        self.pos_dropout = args.get('pos_dropout', 0)
+        self.edit = args.get('edit', False)
+        self.num_edit = args.get('num_edit', 0)
+
+        self.emb_drop = nn.Dropout(self.emb_dropout)
+        self.drop = nn.Dropout(self.dropout)
+        self.embedding = nn.Embedding(self.vocab_size, self.emb_dim, self.pad_token)
+        self.encoder = nn.LSTM(self.emb_dim, self.enc_hidden_dim, self.nlayers, \
+                bidirectional=True, batch_first=True, dropout=self.dropout if self.nlayers > 1 else 0)
+        self.decoder = LSTMAttention(self.emb_dim, self.dec_hidden_dim, \
+                batch_first=True, attn_type=self.args['attn_type'])
+        self.dec2vocab = nn.Linear(self.dec_hidden_dim, self.vocab_size)
+        if self.use_pos and self.pos_dim > 0:
+            logger.debug("Using POS in encoder")
+            self.pos_embedding = nn.Embedding(self.pos_vocab_size, self.pos_dim, self.pad_token)
+            self.pos_drop = nn.Dropout(self.pos_dropout)
+        if self.edit:
+            edit_hidden = self.hidden_dim//2
+            self.edit_clf = nn.Sequential(
+                    nn.Linear(self.hidden_dim, edit_hidden),
+                    nn.ReLU(),
+                    nn.Linear(edit_hidden, self.num_edit))
+
+        self.SOS_tensor = torch.LongTensor([constant.SOS_ID])
+        self.SOS_tensor = self.SOS_tensor.cuda() if self.use_cuda else self.SOS_tensor
+
+        self.init_weights()
+
+    def init_weights(self):
+        # initialize embeddings
+        init_range = constant.EMB_INIT_RANGE
+        if self.emb_matrix is not None:
+            if isinstance(self.emb_matrix, np.ndarray):
+                self.emb_matrix = torch.from_numpy(self.emb_matrix)
+            assert self.emb_matrix.size() == (self.vocab_size, self.emb_dim), \
+                    "Input embedding matrix must match size: {} x {}".format(self.vocab_size, self.emb_dim)
+            self.embedding.weight.data.copy_(self.emb_matrix)
+        else:
+            self.embedding.weight.data.uniform_(-init_range, init_range)
+        # decide finetuning
+        if self.top <= 0:
+            logger.debug("Do not finetune embedding layer.")
+            self.embedding.weight.requires_grad = False
+        elif self.top < self.vocab_size:
+            logger.debug("Finetune top {} embeddings.".format(self.top))
+            self.embedding.weight.register_hook(lambda x: utils.keep_partial_grad(x, self.top))
+        else:
+            logger.debug("Finetune all embeddings.")
+        # initialize pos embeddings
+        if self.use_pos:
+            self.pos_embedding.weight.data.uniform_(-init_range, init_range)
+
+    def cuda(self):
+        super().cuda()
+        self.use_cuda = True
+
+    def cpu(self):
+        super().cpu()
+        self.use_cuda = False
+
+    def zero_state(self, inputs):
+        batch_size = inputs.size(0)
+        h0 = torch.zeros(self.encoder.num_layers*2, batch_size, self.enc_hidden_dim, requires_grad=False)
+        c0 = torch.zeros(self.encoder.num_layers*2, batch_size, self.enc_hidden_dim, requires_grad=False)
+        if self.use_cuda:
+            return h0.cuda(), c0.cuda()
+        return h0, c0
+
+    def encode(self, enc_inputs, lens):
+        """ Encode source sequence. """
+        self.h0, self.c0 = self.zero_state(enc_inputs)
+
+        packed_inputs = nn.utils.rnn.pack_padded_sequence(enc_inputs, lens, batch_first=True)
+        packed_h_in, (hn, cn) = self.encoder(packed_inputs, (self.h0, self.c0))
+        h_in, _ = nn.utils.rnn.pad_packed_sequence(packed_h_in, batch_first=True)
+        hn = torch.cat((hn[-1], hn[-2]), 1)
+        cn = torch.cat((cn[-1], cn[-2]), 1)
+        return h_in, (hn, cn)
+
+    def decode(self, dec_inputs, hn, cn, ctx, ctx_mask=None):
+        """ Decode a step, based on context encoding and source context states."""
+        dec_hidden = (hn, cn)
+        h_out, dec_hidden = self.decoder(dec_inputs, dec_hidden, ctx, ctx_mask)
+
+        h_out_reshape = h_out.contiguous().view(h_out.size(0) * h_out.size(1), -1)
+        decoder_logits = self.dec2vocab(h_out_reshape)
+        decoder_logits = decoder_logits.view(h_out.size(0), h_out.size(1), -1)
+        log_probs = self.get_log_prob(decoder_logits)
+        return log_probs, dec_hidden
+
+    def forward(self, src, src_mask, tgt_in, pos=None):
+        # prepare for encoder/decoder
+        batch_size = src.size(0)
+        enc_inputs = self.emb_drop(self.embedding(src))
+        dec_inputs = self.emb_drop(self.embedding(tgt_in))
+        if self.use_pos:
+            assert pos is not None, "Missing POS input for seq2seq lemmatizer."
+            pos_inputs = self.pos_drop(self.pos_embedding(pos))
+            enc_inputs = torch.cat([pos_inputs.unsqueeze(1), enc_inputs], dim=1)
+            pos_src_mask = src_mask.new_zeros([batch_size, 1])
+            src_mask = torch.cat([pos_src_mask, src_mask], dim=1)
+        src_lens = list(src_mask.data.eq(0).long().sum(1))
+
+        h_in, (hn, cn) = self.encode(enc_inputs, src_lens)
+
+        if self.edit:
+            edit_logits = self.edit_clf(hn)
+        else:
+            edit_logits = None
+
+        log_probs, _ = self.decode(dec_inputs, hn, cn, h_in, src_mask)
+        return log_probs, edit_logits
+
+    def get_log_prob(self, logits):
+        logits_reshape = logits.view(-1, self.vocab_size)
+        log_probs = F.log_softmax(logits_reshape, dim=1)
+        if logits.dim() == 2:
+            return log_probs
+        return log_probs.view(logits.size(0), logits.size(1), logits.size(2))
+
+    def predict_greedy(self, src, src_mask, pos=None):
+        """ Predict with greedy decoding. """
+        enc_inputs = self.embedding(src)
+        batch_size = enc_inputs.size(0)
+        if self.use_pos:
+            assert pos is not None, "Missing POS input for seq2seq lemmatizer."
+            pos_inputs = self.pos_drop(self.pos_embedding(pos))
+            enc_inputs = torch.cat([pos_inputs.unsqueeze(1), enc_inputs], dim=1)
+            pos_src_mask = src_mask.new_zeros([batch_size, 1])
+            src_mask = torch.cat([pos_src_mask, src_mask], dim=1)
+        src_lens = list(src_mask.data.eq(constant.PAD_ID).long().sum(1))
+
+        # encode source
+        h_in, (hn, cn) = self.encode(enc_inputs, src_lens)
+
+        if self.edit:
+            edit_logits = self.edit_clf(hn)
+        else:
+            edit_logits = None
+
+        # greedy decode by step
+        dec_inputs = self.embedding(self.SOS_tensor)
+        dec_inputs = dec_inputs.expand(batch_size, dec_inputs.size(0), dec_inputs.size(1))
+
+        done = [False for _ in range(batch_size)]
+        total_done = 0
+        max_len = 0
+        output_seqs = [[] for _ in range(batch_size)]
+
+        while total_done < batch_size and max_len < self.max_dec_len:
+            log_probs, (hn, cn) = self.decode(dec_inputs, hn, cn, h_in, src_mask)
+            assert log_probs.size(1) == 1, "Output must have 1-step of output."
+            _, preds = log_probs.squeeze(1).max(1, keepdim=True)
+            dec_inputs = self.embedding(preds) # update decoder inputs
+            max_len += 1
+            for i in range(batch_size):
+                if not done[i]:
+                    token = preds.data[i][0].item()
+                    if token == constant.EOS_ID:
+                        done[i] = True
+                        total_done += 1
+                    else:
+                        output_seqs[i].append(token)
+        return output_seqs, edit_logits
+
+    def predict(self, src, src_mask, pos=None, beam_size=5):
+        """ Predict with beam search. """
+        if beam_size == 1:
+            return self.predict_greedy(src, src_mask, pos=pos)
+
+        enc_inputs = self.embedding(src)
+        batch_size = enc_inputs.size(0)
+        if self.use_pos:
+            assert pos is not None, "Missing POS input for seq2seq lemmatizer."
+            pos_inputs = self.pos_drop(self.pos_embedding(pos))
+            enc_inputs = torch.cat([pos_inputs.unsqueeze(1), enc_inputs], dim=1)
+            pos_src_mask = src_mask.new_zeros([batch_size, 1])
+            src_mask = torch.cat([pos_src_mask, src_mask], dim=1)
+        src_lens = list(src_mask.data.eq(constant.PAD_ID).long().sum(1))
+
+        # (1) encode source
+        h_in, (hn, cn) = self.encode(enc_inputs, src_lens)
+
+        if self.edit:
+            edit_logits = self.edit_clf(hn)
+        else:
+            edit_logits = None
+
+        # (2) set up beam
+        with torch.no_grad():
+            h_in = h_in.data.repeat(beam_size, 1, 1) # repeat data for beam search
+            src_mask = src_mask.repeat(beam_size, 1)
+            # repeat decoder hidden states
+            hn = hn.data.repeat(beam_size, 1)
+            cn = cn.data.repeat(beam_size, 1)
+        beam = [Beam(beam_size, self.use_cuda) for _ in range(batch_size)]
+
+        def update_state(states, idx, positions, beam_size):
+            """ Select the states according to back pointers. """
+            for e in states:
+                br, d = e.size()
+                s = e.contiguous().view(beam_size, br // beam_size, d)[:,idx]
+                s.data.copy_(s.data.index_select(0, positions))
+
+        # (3) main loop
+        for i in range(self.max_dec_len):
+            dec_inputs = torch.stack([b.get_current_state() for b in beam]).t().contiguous().view(-1, 1)
+            dec_inputs = self.embedding(dec_inputs)
+            log_probs, (hn, cn) = self.decode(dec_inputs, hn, cn, h_in, src_mask)
+            log_probs = log_probs.view(beam_size, batch_size, -1).transpose(0,1)\
+                    .contiguous() # [batch, beam, V]
+
+            # advance each beam
+            done = []
+            for b in range(batch_size):
+                is_done = beam[b].advance(log_probs.data[b])
+                if is_done:
+                    done += [b]
+                # update beam state
+                update_state((hn, cn), b, beam[b].get_current_origin(), beam_size)
+
+            if len(done) == batch_size:
+                break
+
+        # back trace and find hypothesis
+        all_hyp, all_scores = [], []
+        for b in range(batch_size):
+            scores, ks = beam[b].sort_best()
+            all_scores += [scores[0]]
+            k = ks[0]
+            hyp = beam[b].get_hyp(k)
+            hyp = utils.prune_hyp(hyp)
+            hyp = [i.item() for i in hyp]
+            all_hyp += [hyp]
+
+        return all_hyp, edit_logits
+
```

### Comparing `classla-2.0/classla/models/common/seq2seq_modules.py` & `classla-2.1/classla/models/common/seq2seq_modules.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,231 +1,231 @@
-"""
-Pytorch implementation of basic sequence to Sequence modules.
-"""
-
-import logging
-import torch
-import torch.nn as nn
-import math
-import numpy as np
-
-import classla.models.common.seq2seq_constant as constant
-
-logger = logging.getLogger('classla')
-
-class BasicAttention(nn.Module):
-    """
-    A basic MLP attention layer.
-    """
-    def __init__(self, dim):
-        super(BasicAttention, self).__init__()
-        self.linear_in = nn.Linear(dim, dim, bias=False)
-        self.linear_c = nn.Linear(dim, dim)
-        self.linear_v = nn.Linear(dim, 1, bias=False)
-        self.linear_out = nn.Linear(dim * 2, dim, bias=False)
-        self.tanh = nn.Tanh()
-        self.sm = nn.Softmax(dim=1)
-
-    def forward(self, input, context, mask=None, attn_only=False):
-        """
-        input: batch x dim
-        context: batch x sourceL x dim
-        """
-        batch_size = context.size(0)
-        source_len = context.size(1)
-        dim = context.size(2)
-        target = self.linear_in(input) # batch x dim
-        source = self.linear_c(context.contiguous().view(-1, dim)).view(batch_size, source_len, dim)
-        attn = target.unsqueeze(1).expand_as(context) + source
-        attn = self.tanh(attn) # batch x sourceL x dim
-        attn = self.linear_v(attn.view(-1, dim)).view(batch_size, source_len)
-
-        if mask is not None:
-            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)
-
-        attn = self.sm(attn)
-        if attn_only:
-            return attn
-
-        weighted_context = torch.bmm(attn.unsqueeze(1), context).squeeze(1)
-        h_tilde = torch.cat((weighted_context, input), 1)
-        h_tilde = self.tanh(self.linear_out(h_tilde))
-
-        return h_tilde, attn
-
-class SoftDotAttention(nn.Module):
-    """Soft Dot Attention.
-
-    Ref: http://www.aclweb.org/anthology/D15-1166
-    Adapted from PyTorch OPEN NMT.
-    """
-
-    def __init__(self, dim):
-        """Initialize layer."""
-        super(SoftDotAttention, self).__init__()
-        self.linear_in = nn.Linear(dim, dim, bias=False)
-        self.sm = nn.Softmax(dim=1)
-        self.linear_out = nn.Linear(dim * 2, dim, bias=False)
-        self.tanh = nn.Tanh()
-        self.mask = None
-
-    def forward(self, input, context, mask=None, attn_only=False):
-        """Propogate input through the network.
-
-        input: batch x dim
-        context: batch x sourceL x dim
-        """
-        target = self.linear_in(input).unsqueeze(2)  # batch x dim x 1
-
-        # Get attention
-        attn = torch.bmm(context, target).squeeze(2)  # batch x sourceL
-
-        if mask is not None:
-            # sett the padding attention logits to -inf
-            assert mask.size() == attn.size(), "Mask size must match the attention size!"
-            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)
-
-        attn = self.sm(attn)
-        if attn_only:
-            return attn
-
-        attn3 = attn.view(attn.size(0), 1, attn.size(1))  # batch x 1 x sourceL
-
-        weighted_context = torch.bmm(attn3, context).squeeze(1)  # batch x dim
-        h_tilde = torch.cat((weighted_context, input), 1)
-
-        h_tilde = self.tanh(self.linear_out(h_tilde))
-
-        return h_tilde, attn
-
-
-class LinearAttention(nn.Module):
-    """ A linear attention form, inspired by BiDAF:
-        a = W (u; v; u o v)
-    """
-
-    def __init__(self, dim):
-        super(LinearAttention, self).__init__()
-        self.linear = nn.Linear(dim*3, 1, bias=False)
-        self.linear_out = nn.Linear(dim * 2, dim, bias=False)
-        self.sm = nn.Softmax(dim=1)
-        self.tanh = nn.Tanh()
-        self.mask = None
-
-    def forward(self, input, context, mask=None, attn_only=False):
-        """
-        input: batch x dim
-        context: batch x sourceL x dim
-        """
-        batch_size = context.size(0)
-        source_len = context.size(1)
-        dim = context.size(2)
-        u = input.unsqueeze(1).expand_as(context).contiguous().view(-1, dim)  # batch*sourceL x dim
-        v = context.contiguous().view(-1, dim)
-        attn_in = torch.cat((u, v, u.mul(v)), 1)
-        attn = self.linear(attn_in).view(batch_size, source_len)
-
-        if mask is not None:
-            # sett the padding attention logits to -inf
-            assert mask.size() == attn.size(), "Mask size must match the attention size!"
-            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)
-
-        attn = self.sm(attn)
-        if attn_only:
-            return attn
-
-        attn3 = attn.view(batch_size, 1, source_len)  # batch x 1 x sourceL
-
-        weighted_context = torch.bmm(attn3, context).squeeze(1)  # batch x dim
-        h_tilde = torch.cat((weighted_context, input), 1)
-        h_tilde = self.tanh(self.linear_out(h_tilde))
-        return h_tilde, attn
-
-class DeepAttention(nn.Module):
-    """ A deep attention form, invented by Robert:
-        u = ReLU(Wx)
-        v = ReLU(Wy)
-        a = V.(u o v)
-    """
-
-    def __init__(self, dim):
-        super(DeepAttention, self).__init__()
-        self.linear_in = nn.Linear(dim, dim, bias=False)
-        self.linear_v = nn.Linear(dim, 1, bias=False)
-        self.linear_out = nn.Linear(dim * 2, dim, bias=False)
-        self.relu = nn.ReLU()
-        self.sm = nn.Softmax(dim=1)
-        self.tanh = nn.Tanh()
-        self.mask = None
-
-    def forward(self, input, context, mask=None, attn_only=False):
-        """
-        input: batch x dim
-        context: batch x sourceL x dim
-        """
-        batch_size = context.size(0)
-        source_len = context.size(1)
-        dim = context.size(2)
-        u = input.unsqueeze(1).expand_as(context).contiguous().view(-1, dim)  # batch*sourceL x dim
-        u = self.relu(self.linear_in(u))
-        v = self.relu(self.linear_in(context.contiguous().view(-1, dim)))
-        attn = self.linear_v(u.mul(v)).view(batch_size, source_len)
-
-        if mask is not None:
-            # sett the padding attention logits to -inf
-            assert mask.size() == attn.size(), "Mask size must match the attention size!"
-            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)
-
-        attn = self.sm(attn)
-        if attn_only:
-            return attn
-
-        attn3 = attn.view(batch_size, 1, source_len)  # batch x 1 x sourceL
-
-        weighted_context = torch.bmm(attn3, context).squeeze(1)  # batch x dim
-        h_tilde = torch.cat((weighted_context, input), 1)
-        h_tilde = self.tanh(self.linear_out(h_tilde))
-        return h_tilde, attn
-
-class LSTMAttention(nn.Module):
-    r"""A long short-term memory (LSTM) cell with attention."""
-
-    def __init__(self, input_size, hidden_size, batch_first=True, attn_type='soft'):
-        """Initialize params."""
-        super(LSTMAttention, self).__init__()
-        self.input_size = input_size
-        self.hidden_size = hidden_size
-        self.batch_first = batch_first
-
-        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)
-
-        if attn_type == 'soft':
-            self.attention_layer = SoftDotAttention(hidden_size)
-        elif attn_type == 'mlp':
-            self.attention_layer = BasicAttention(hidden_size)
-        elif attn_type == 'linear':
-            self.attention_layer = LinearAttention(hidden_size)
-        elif attn_type == 'deep':
-            self.attention_layer = DeepAttention(hidden_size)
-        else:
-            raise Exception("Unsupported LSTM attention type: {}".format(attn_type))
-        logger.debug("Using {} attention for LSTM.".format(attn_type))
-
-    def forward(self, input, hidden, ctx, ctx_mask=None):
-        """Propogate input through the network."""
-        if self.batch_first:
-            input = input.transpose(0,1)
-
-        output = []
-        steps = range(input.size(0))
-        for i in steps:
-            hidden = self.lstm_cell(input[i], hidden)
-            hy, cy = hidden
-            h_tilde, alpha = self.attention_layer(hy, ctx, mask=ctx_mask)
-            output.append(h_tilde)
-        output = torch.cat(output, 0).view(input.size(0), *output[0].size())
-
-        if self.batch_first:
-            output = output.transpose(0,1)
-
-        return output, hidden
-
+"""
+Pytorch implementation of basic sequence to Sequence modules.
+"""
+
+import logging
+import torch
+import torch.nn as nn
+import math
+import numpy as np
+
+import classla.models.common.seq2seq_constant as constant
+
+logger = logging.getLogger('classla')
+
+class BasicAttention(nn.Module):
+    """
+    A basic MLP attention layer.
+    """
+    def __init__(self, dim):
+        super(BasicAttention, self).__init__()
+        self.linear_in = nn.Linear(dim, dim, bias=False)
+        self.linear_c = nn.Linear(dim, dim)
+        self.linear_v = nn.Linear(dim, 1, bias=False)
+        self.linear_out = nn.Linear(dim * 2, dim, bias=False)
+        self.tanh = nn.Tanh()
+        self.sm = nn.Softmax(dim=1)
+
+    def forward(self, input, context, mask=None, attn_only=False):
+        """
+        input: batch x dim
+        context: batch x sourceL x dim
+        """
+        batch_size = context.size(0)
+        source_len = context.size(1)
+        dim = context.size(2)
+        target = self.linear_in(input) # batch x dim
+        source = self.linear_c(context.contiguous().view(-1, dim)).view(batch_size, source_len, dim)
+        attn = target.unsqueeze(1).expand_as(context) + source
+        attn = self.tanh(attn) # batch x sourceL x dim
+        attn = self.linear_v(attn.view(-1, dim)).view(batch_size, source_len)
+
+        if mask is not None:
+            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)
+
+        attn = self.sm(attn)
+        if attn_only:
+            return attn
+
+        weighted_context = torch.bmm(attn.unsqueeze(1), context).squeeze(1)
+        h_tilde = torch.cat((weighted_context, input), 1)
+        h_tilde = self.tanh(self.linear_out(h_tilde))
+
+        return h_tilde, attn
+
+class SoftDotAttention(nn.Module):
+    """Soft Dot Attention.
+
+    Ref: http://www.aclweb.org/anthology/D15-1166
+    Adapted from PyTorch OPEN NMT.
+    """
+
+    def __init__(self, dim):
+        """Initialize layer."""
+        super(SoftDotAttention, self).__init__()
+        self.linear_in = nn.Linear(dim, dim, bias=False)
+        self.sm = nn.Softmax(dim=1)
+        self.linear_out = nn.Linear(dim * 2, dim, bias=False)
+        self.tanh = nn.Tanh()
+        self.mask = None
+
+    def forward(self, input, context, mask=None, attn_only=False):
+        """Propogate input through the network.
+
+        input: batch x dim
+        context: batch x sourceL x dim
+        """
+        target = self.linear_in(input).unsqueeze(2)  # batch x dim x 1
+
+        # Get attention
+        attn = torch.bmm(context, target).squeeze(2)  # batch x sourceL
+
+        if mask is not None:
+            # sett the padding attention logits to -inf
+            assert mask.size() == attn.size(), "Mask size must match the attention size!"
+            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)
+
+        attn = self.sm(attn)
+        if attn_only:
+            return attn
+
+        attn3 = attn.view(attn.size(0), 1, attn.size(1))  # batch x 1 x sourceL
+
+        weighted_context = torch.bmm(attn3, context).squeeze(1)  # batch x dim
+        h_tilde = torch.cat((weighted_context, input), 1)
+
+        h_tilde = self.tanh(self.linear_out(h_tilde))
+
+        return h_tilde, attn
+
+
+class LinearAttention(nn.Module):
+    """ A linear attention form, inspired by BiDAF:
+        a = W (u; v; u o v)
+    """
+
+    def __init__(self, dim):
+        super(LinearAttention, self).__init__()
+        self.linear = nn.Linear(dim*3, 1, bias=False)
+        self.linear_out = nn.Linear(dim * 2, dim, bias=False)
+        self.sm = nn.Softmax(dim=1)
+        self.tanh = nn.Tanh()
+        self.mask = None
+
+    def forward(self, input, context, mask=None, attn_only=False):
+        """
+        input: batch x dim
+        context: batch x sourceL x dim
+        """
+        batch_size = context.size(0)
+        source_len = context.size(1)
+        dim = context.size(2)
+        u = input.unsqueeze(1).expand_as(context).contiguous().view(-1, dim)  # batch*sourceL x dim
+        v = context.contiguous().view(-1, dim)
+        attn_in = torch.cat((u, v, u.mul(v)), 1)
+        attn = self.linear(attn_in).view(batch_size, source_len)
+
+        if mask is not None:
+            # sett the padding attention logits to -inf
+            assert mask.size() == attn.size(), "Mask size must match the attention size!"
+            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)
+
+        attn = self.sm(attn)
+        if attn_only:
+            return attn
+
+        attn3 = attn.view(batch_size, 1, source_len)  # batch x 1 x sourceL
+
+        weighted_context = torch.bmm(attn3, context).squeeze(1)  # batch x dim
+        h_tilde = torch.cat((weighted_context, input), 1)
+        h_tilde = self.tanh(self.linear_out(h_tilde))
+        return h_tilde, attn
+
+class DeepAttention(nn.Module):
+    """ A deep attention form, invented by Robert:
+        u = ReLU(Wx)
+        v = ReLU(Wy)
+        a = V.(u o v)
+    """
+
+    def __init__(self, dim):
+        super(DeepAttention, self).__init__()
+        self.linear_in = nn.Linear(dim, dim, bias=False)
+        self.linear_v = nn.Linear(dim, 1, bias=False)
+        self.linear_out = nn.Linear(dim * 2, dim, bias=False)
+        self.relu = nn.ReLU()
+        self.sm = nn.Softmax(dim=1)
+        self.tanh = nn.Tanh()
+        self.mask = None
+
+    def forward(self, input, context, mask=None, attn_only=False):
+        """
+        input: batch x dim
+        context: batch x sourceL x dim
+        """
+        batch_size = context.size(0)
+        source_len = context.size(1)
+        dim = context.size(2)
+        u = input.unsqueeze(1).expand_as(context).contiguous().view(-1, dim)  # batch*sourceL x dim
+        u = self.relu(self.linear_in(u))
+        v = self.relu(self.linear_in(context.contiguous().view(-1, dim)))
+        attn = self.linear_v(u.mul(v)).view(batch_size, source_len)
+
+        if mask is not None:
+            # sett the padding attention logits to -inf
+            assert mask.size() == attn.size(), "Mask size must match the attention size!"
+            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)
+
+        attn = self.sm(attn)
+        if attn_only:
+            return attn
+
+        attn3 = attn.view(batch_size, 1, source_len)  # batch x 1 x sourceL
+
+        weighted_context = torch.bmm(attn3, context).squeeze(1)  # batch x dim
+        h_tilde = torch.cat((weighted_context, input), 1)
+        h_tilde = self.tanh(self.linear_out(h_tilde))
+        return h_tilde, attn
+
+class LSTMAttention(nn.Module):
+    r"""A long short-term memory (LSTM) cell with attention."""
+
+    def __init__(self, input_size, hidden_size, batch_first=True, attn_type='soft'):
+        """Initialize params."""
+        super(LSTMAttention, self).__init__()
+        self.input_size = input_size
+        self.hidden_size = hidden_size
+        self.batch_first = batch_first
+
+        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)
+
+        if attn_type == 'soft':
+            self.attention_layer = SoftDotAttention(hidden_size)
+        elif attn_type == 'mlp':
+            self.attention_layer = BasicAttention(hidden_size)
+        elif attn_type == 'linear':
+            self.attention_layer = LinearAttention(hidden_size)
+        elif attn_type == 'deep':
+            self.attention_layer = DeepAttention(hidden_size)
+        else:
+            raise Exception("Unsupported LSTM attention type: {}".format(attn_type))
+        logger.debug("Using {} attention for LSTM.".format(attn_type))
+
+    def forward(self, input, hidden, ctx, ctx_mask=None):
+        """Propogate input through the network."""
+        if self.batch_first:
+            input = input.transpose(0,1)
+
+        output = []
+        steps = range(input.size(0))
+        for i in steps:
+            hidden = self.lstm_cell(input[i], hidden)
+            hy, cy = hidden
+            h_tilde, alpha = self.attention_layer(hy, ctx, mask=ctx_mask)
+            output.append(h_tilde)
+        output = torch.cat(output, 0).view(input.size(0), *output[0].size())
+
+        if self.batch_first:
+            output = output.transpose(0,1)
+
+        return output, hidden
+
```

### Comparing `classla-2.0/classla/models/common/seq2seq_utils.py` & `classla-2.1/classla/models/common/seq2seq_utils.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,130 +1,130 @@
-"""
-Utils for seq2seq models.
-"""
-from collections import Counter
-import random
-import json
-import unicodedata
-import torch
-
-import classla.models.common.seq2seq_constant as constant
-
-# torch utils
-def get_optimizer(name, parameters, lr):
-    if name == 'sgd':
-        return torch.optim.SGD(parameters, lr=lr)
-    elif name == 'adagrad':
-        return torch.optim.Adagrad(parameters, lr=lr)
-    elif name == 'adam':
-        return torch.optim.Adam(parameters) # use default lr
-    elif name == 'adamax':
-        return torch.optim.Adamax(parameters) # use default lr
-    else:
-        raise Exception("Unsupported optimizer: {}".format(name))
-
-def change_lr(optimizer, new_lr):
-    for param_group in optimizer.param_groups:
-        param_group['lr'] = new_lr
-
-def flatten_indices(seq_lens, width):
-    flat = []
-    for i, l in enumerate(seq_lens):
-        for j in range(l):
-            flat.append(i * width + j)
-    return flat
-
-def set_cuda(var, cuda):
-    if cuda:
-        return var.cuda()
-    return var
-
-def keep_partial_grad(grad, topk):
-    """
-    Keep only the topk rows of grads.
-    """
-    assert topk < grad.size(0)
-    grad.data[topk:].zero_()
-    return grad
-
-# other utils
-def save_config(config, path, verbose=True):
-    with open(path, 'w') as outfile:
-        json.dump(config, outfile, indent=2)
-    if verbose:
-        print("Config saved to file {}".format(path))
-    return config
-
-def load_config(path, verbose=True):
-    with open(path) as f:
-        config = json.load(f)
-    if verbose:
-        print("Config loaded from file {}".format(path))
-    return config
-
-def normalize_text(text):
-    return unicodedata.normalize('NFD', text)
-
-def unmap_with_copy(indices, src_tokens, vocab):
-    """
-    Unmap a list of list of indices, by optionally copying from src_tokens.
-    """
-    result = []
-    for ind, tokens in zip(indices, src_tokens):
-        words = []
-        for idx in ind:
-            if idx >= 0:
-                words.append(vocab.id2word[idx])
-            else:
-                idx = -idx - 1 # flip and minus 1
-                words.append(tokens[idx])
-        result += [words]
-    return result
-
-def prune_decoded_seqs(seqs):
-    """
-    Prune decoded sequences after EOS token.
-    """
-    out = []
-    for s in seqs:
-        if constant.EOS in s:
-            idx = s.index(constant.EOS_TOKEN)
-            out += [s[:idx]]
-        else:
-            out += [s]
-    return out
-
-def prune_hyp(hyp):
-    """
-    Prune a decoded hypothesis
-    """
-    if constant.EOS_ID in hyp:
-        idx = hyp.index(constant.EOS_ID)
-        return hyp[:idx]
-    else:
-        return hyp
-
-def prune(data_list, lens):
-    assert len(data_list) == len(lens)
-    nl = []
-    for d, l in zip(data_list, lens):
-        nl.append(d[:l])
-    return nl
-
-def sort(packed, ref, reverse=True):
-    """
-    Sort a series of packed list, according to a ref list.
-    Also return the original index before the sort.
-    """
-    assert (isinstance(packed, tuple) or isinstance(packed, list)) and isinstance(ref, list)
-    packed = [ref] + [range(len(ref))] + list(packed)
-    sorted_packed = [list(t) for t in zip(*sorted(zip(*packed), reverse=reverse))]
-    return tuple(sorted_packed[1:])
-
-def unsort(sorted_list, oidx):
-    """
-    Unsort a sorted list, based on the original idx.
-    """
-    assert len(sorted_list) == len(oidx), "Number of list elements must match with original indices."
-    _, unsorted = [list(t) for t in zip(*sorted(zip(oidx, sorted_list)))]
-    return unsorted
-
+"""
+Utils for seq2seq models.
+"""
+from collections import Counter
+import random
+import json
+import unicodedata
+import torch
+
+import classla.models.common.seq2seq_constant as constant
+
+# torch utils
+def get_optimizer(name, parameters, lr):
+    if name == 'sgd':
+        return torch.optim.SGD(parameters, lr=lr)
+    elif name == 'adagrad':
+        return torch.optim.Adagrad(parameters, lr=lr)
+    elif name == 'adam':
+        return torch.optim.Adam(parameters) # use default lr
+    elif name == 'adamax':
+        return torch.optim.Adamax(parameters) # use default lr
+    else:
+        raise Exception("Unsupported optimizer: {}".format(name))
+
+def change_lr(optimizer, new_lr):
+    for param_group in optimizer.param_groups:
+        param_group['lr'] = new_lr
+
+def flatten_indices(seq_lens, width):
+    flat = []
+    for i, l in enumerate(seq_lens):
+        for j in range(l):
+            flat.append(i * width + j)
+    return flat
+
+def set_cuda(var, cuda):
+    if cuda:
+        return var.cuda()
+    return var
+
+def keep_partial_grad(grad, topk):
+    """
+    Keep only the topk rows of grads.
+    """
+    assert topk < grad.size(0)
+    grad.data[topk:].zero_()
+    return grad
+
+# other utils
+def save_config(config, path, verbose=True):
+    with open(path, 'w') as outfile:
+        json.dump(config, outfile, indent=2)
+    if verbose:
+        print("Config saved to file {}".format(path))
+    return config
+
+def load_config(path, verbose=True):
+    with open(path) as f:
+        config = json.load(f)
+    if verbose:
+        print("Config loaded from file {}".format(path))
+    return config
+
+def normalize_text(text):
+    return unicodedata.normalize('NFD', text)
+
+def unmap_with_copy(indices, src_tokens, vocab):
+    """
+    Unmap a list of list of indices, by optionally copying from src_tokens.
+    """
+    result = []
+    for ind, tokens in zip(indices, src_tokens):
+        words = []
+        for idx in ind:
+            if idx >= 0:
+                words.append(vocab.id2word[idx])
+            else:
+                idx = -idx - 1 # flip and minus 1
+                words.append(tokens[idx])
+        result += [words]
+    return result
+
+def prune_decoded_seqs(seqs):
+    """
+    Prune decoded sequences after EOS token.
+    """
+    out = []
+    for s in seqs:
+        if constant.EOS in s:
+            idx = s.index(constant.EOS_TOKEN)
+            out += [s[:idx]]
+        else:
+            out += [s]
+    return out
+
+def prune_hyp(hyp):
+    """
+    Prune a decoded hypothesis
+    """
+    if constant.EOS_ID in hyp:
+        idx = hyp.index(constant.EOS_ID)
+        return hyp[:idx]
+    else:
+        return hyp
+
+def prune(data_list, lens):
+    assert len(data_list) == len(lens)
+    nl = []
+    for d, l in zip(data_list, lens):
+        nl.append(d[:l])
+    return nl
+
+def sort(packed, ref, reverse=True):
+    """
+    Sort a series of packed list, according to a ref list.
+    Also return the original index before the sort.
+    """
+    assert (isinstance(packed, tuple) or isinstance(packed, list)) and isinstance(ref, list)
+    packed = [ref] + [range(len(ref))] + list(packed)
+    sorted_packed = [list(t) for t in zip(*sorted(zip(*packed), reverse=reverse))]
+    return tuple(sorted_packed[1:])
+
+def unsort(sorted_list, oidx):
+    """
+    Unsort a sorted list, based on the original idx.
+    """
+    assert len(sorted_list) == len(oidx), "Number of list elements must match with original indices."
+    _, unsorted = [list(t) for t in zip(*sorted(zip(oidx, sorted_list)))]
+    return unsorted
+
```

### Comparing `classla-2.0/classla/models/common/vocab.py` & `classla-2.1/classla/models/common/vocab.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,222 +1,222 @@
-from copy import copy
-from collections import Counter, OrderedDict
-import os
-import pickle
-
-PAD = '<PAD>'
-PAD_ID = 0
-UNK = '<UNK>'
-UNK_ID = 1
-EMPTY = '<EMPTY>'
-EMPTY_ID = 2
-ROOT = '<ROOT>'
-ROOT_ID = 3
-VOCAB_PREFIX = [PAD, UNK, EMPTY, ROOT]
-
-class BaseVocab:
-    """ A base class for common vocabulary operations. Each subclass should at least 
-    implement its own build_vocab() function."""
-    def __init__(self, data=None, lang="", idx=0, cutoff=0, lower=False):
-        self.data = data
-        self.lang = lang
-        self.idx = idx
-        self.cutoff = cutoff
-        self.lower = lower
-        if data is not None:
-            self.build_vocab()
-        self.state_attrs = ['lang', 'idx', 'cutoff', 'lower', '_unit2id', '_id2unit']
-        del self.data
-
-    def build_vocab(self):
-        raise NotImplementedError()
-
-    def state_dict(self):
-        """ Returns a dictionary containing all states that are necessary to recover
-        this vocab. Useful for serialization."""
-        state = OrderedDict()
-        for attr in self.state_attrs:
-            if hasattr(self, attr):
-                state[attr] = getattr(self, attr)
-        return state
-
-    @classmethod
-    def load_state_dict(cls, state_dict):
-        """ Returns a new Vocab instance constructed from a state dict. """
-        new = cls()
-        for attr, value in state_dict.items():
-            setattr(new, attr, value)
-        return new
-
-    def normalize_unit(self, unit):
-        if self.lower:
-            return unit.lower()
-        return unit
-
-    def unit2id(self, unit):
-        unit = self.normalize_unit(unit)
-        if unit in self._unit2id:
-            return self._unit2id[unit]
-        else:
-            return self._unit2id[UNK]
-
-    def id2unit(self, id):
-        return self._id2unit[id]
-
-    def map(self, units):
-        return [self.unit2id(x) for x in units]
-
-    def unmap(self, ids):
-        return [self.id2unit(x) for x in ids]
-
-    def __len__(self):
-        return len(self._id2unit)
-
-    def __getitem__(self, key):
-        if isinstance(key, str):
-            return self.unit2id(key)
-        elif isinstance(key, int) or isinstance(key, list):
-            return self.id2unit(key)
-        else:
-            raise TypeError("Vocab key must be one of str, list, or int")
-
-    def __contains__(self, key):
-        return key in self._unit2id
-
-    @property
-    def size(self):
-        return len(self)
-
-class CompositeVocab(BaseVocab):
-    ''' Vocabulary class that handles parsing and printing composite values such as
-    compositional XPOS and universal morphological features (UFeats).
-
-    Two key options are `keyed` and `sep`. `sep` specifies the separator used between
-    different parts of the composite values, which is `|` for UFeats, for example.
-    If `keyed` is `True`, then the incoming value is treated similarly to UFeats, where
-    each part is a key/value pair separated by an equal sign (`=`). There are no inherit
-    order to the keys, and we sort them alphabetically for serialization and deserialization.
-    Whenever a part is absent, its internal value is a special `<EMPTY>` symbol that will
-    be treated accordingly when generating the output. If `keyed` is `False`, then the parts
-    are treated as positioned values, and `<EMPTY>` is used to pad parts at the end when the
-    incoming value is not long enough.'''
-
-    def __init__(self, data=None, lang="", idx=0, sep="", keyed=False):
-        self.sep = sep
-        self.keyed = keyed
-        super().__init__(data, lang, idx=idx)
-        self.state_attrs += ['sep', 'keyed']
-
-    def unit2parts(self, unit):
-        # unpack parts of a unit
-        if self.sep == "":
-            parts = [x for x in unit]
-        else:
-            parts = unit.split(self.sep)
-        if self.keyed:
-            if len(parts) == 1 and parts[0] == '_':
-                return dict()
-            parts = [x.split('=') for x in parts]
-
-            # Just treat multi-valued properties values as one possible value
-            parts = dict(parts)
-        elif unit == '_':
-            parts = []
-        return parts
-
-    def unit2id(self, unit):
-        parts = self.unit2parts(unit)
-        if self.keyed:
-            # treat multi-valued properties as singletons
-            return [self._unit2id[k].get(parts[k], UNK_ID) if k in parts else EMPTY_ID for k in self._unit2id]
-        else:
-            return [self._unit2id[i].get(parts[i], UNK_ID) if i < len(parts) else EMPTY_ID for i in range(len(self._unit2id))]
-
-    def id2unit(self, id):
-        items = []
-        for v, k in zip(id, self._id2unit.keys()):
-            if v == EMPTY_ID: continue
-            if self.keyed:
-                items.append("{}={}".format(k, self._id2unit[k][v]))
-            else:
-                items.append(self._id2unit[k][v])
-        res = self.sep.join(sorted(items, key=lambda x: x.lower()))
-        if res == "":
-            res = "_"
-        return res
-
-    def build_vocab(self):
-        allunits = [w[self.idx] for sent in self.data for w in sent]
-        if self.keyed:
-            self._id2unit = dict()
-
-            for u in allunits:
-                parts = self.unit2parts(u)
-                for key in parts:
-                    if key not in self._id2unit:
-                        self._id2unit[key] = copy(VOCAB_PREFIX)
-
-                    # treat multi-valued properties as singletons
-                    if parts[key] not in self._id2unit[key]:
-                        self._id2unit[key].append(parts[key])
-
-            # special handle for the case where upos/xpos/ufeats are always empty
-            if len(self._id2unit) == 0:
-                self._id2unit['_'] = copy(VOCAB_PREFIX) # use an arbitrary key
-
-        else:
-            self._id2unit = dict()
-
-            allparts = [self.unit2parts(u) for u in allunits]
-            maxlen = max([len(p) for p in allparts])
-
-            for parts in allparts:
-                for i, p in enumerate(parts):
-                    if i not in self._id2unit:
-                        self._id2unit[i] = copy(VOCAB_PREFIX)
-                    if i < len(parts) and p not in self._id2unit[i]:
-                        self._id2unit[i].append(p)
-
-            # special handle for the case where upos/xpos/ufeats are always empty
-            if len(self._id2unit) == 0:
-                self._id2unit[0] = copy(VOCAB_PREFIX) # use an arbitrary key
-
-        self._id2unit = OrderedDict([(k, self._id2unit[k]) for k in sorted(self._id2unit.keys(), key=lambda x: x.casefold())])
-        self._unit2id = {k: {w:i for i, w in enumerate(self._id2unit[k])} for k in self._id2unit}
-
-    def lens(self):
-        return [len(self._unit2id[k]) for k in self._unit2id]
-
-class BaseMultiVocab:
-    """ A convenient vocab container that can store multiple BaseVocab instances, and support 
-    safe serialization of all instances via state dicts. Each subclass of this base class 
-    should implement the load_state_dict() function to specify how a saved state dict 
-    should be loaded back."""
-    def __init__(self, vocab_dict=None):
-        self._vocabs = OrderedDict()
-        if vocab_dict is None:
-            return
-        # check all values provided must be a subclass of the Vocab base class
-        assert all([isinstance(v, BaseVocab) for v in vocab_dict.values()])
-        for k, v in vocab_dict.items():
-            self._vocabs[k] = v
-
-    def __setitem__(self, key, item):
-        self._vocabs[key] = item
-
-    def __getitem__(self, key):
-        return self._vocabs[key]
-
-    def state_dict(self):
-        """ Build a state dict by iteratively calling state_dict() of all vocabs. """
-        state = OrderedDict()
-        for k, v in self._vocabs.items():
-            state[k] = v.state_dict()
-        return state
-
-    @classmethod
-    def load_state_dict(cls, state_dict):
-        """ Construct a MultiVocab by reading from a state dict."""
-        raise NotImplementedError
-
-
-
+from copy import copy
+from collections import Counter, OrderedDict
+import os
+import pickle
+
+PAD = '<PAD>'
+PAD_ID = 0
+UNK = '<UNK>'
+UNK_ID = 1
+EMPTY = '<EMPTY>'
+EMPTY_ID = 2
+ROOT = '<ROOT>'
+ROOT_ID = 3
+VOCAB_PREFIX = [PAD, UNK, EMPTY, ROOT]
+
+class BaseVocab:
+    """ A base class for common vocabulary operations. Each subclass should at least 
+    implement its own build_vocab() function."""
+    def __init__(self, data=None, lang="", idx=0, cutoff=0, lower=False):
+        self.data = data
+        self.lang = lang
+        self.idx = idx
+        self.cutoff = cutoff
+        self.lower = lower
+        if data is not None:
+            self.build_vocab()
+        self.state_attrs = ['lang', 'idx', 'cutoff', 'lower', '_unit2id', '_id2unit']
+        del self.data
+
+    def build_vocab(self):
+        raise NotImplementedError()
+
+    def state_dict(self):
+        """ Returns a dictionary containing all states that are necessary to recover
+        this vocab. Useful for serialization."""
+        state = OrderedDict()
+        for attr in self.state_attrs:
+            if hasattr(self, attr):
+                state[attr] = getattr(self, attr)
+        return state
+
+    @classmethod
+    def load_state_dict(cls, state_dict):
+        """ Returns a new Vocab instance constructed from a state dict. """
+        new = cls()
+        for attr, value in state_dict.items():
+            setattr(new, attr, value)
+        return new
+
+    def normalize_unit(self, unit):
+        if self.lower:
+            return unit.lower()
+        return unit
+
+    def unit2id(self, unit):
+        unit = self.normalize_unit(unit)
+        if unit in self._unit2id:
+            return self._unit2id[unit]
+        else:
+            return self._unit2id[UNK]
+
+    def id2unit(self, id):
+        return self._id2unit[id]
+
+    def map(self, units):
+        return [self.unit2id(x) for x in units]
+
+    def unmap(self, ids):
+        return [self.id2unit(x) for x in ids]
+
+    def __len__(self):
+        return len(self._id2unit)
+
+    def __getitem__(self, key):
+        if isinstance(key, str):
+            return self.unit2id(key)
+        elif isinstance(key, int) or isinstance(key, list):
+            return self.id2unit(key)
+        else:
+            raise TypeError("Vocab key must be one of str, list, or int")
+
+    def __contains__(self, key):
+        return key in self._unit2id
+
+    @property
+    def size(self):
+        return len(self)
+
+class CompositeVocab(BaseVocab):
+    ''' Vocabulary class that handles parsing and printing composite values such as
+    compositional XPOS and universal morphological features (UFeats).
+
+    Two key options are `keyed` and `sep`. `sep` specifies the separator used between
+    different parts of the composite values, which is `|` for UFeats, for example.
+    If `keyed` is `True`, then the incoming value is treated similarly to UFeats, where
+    each part is a key/value pair separated by an equal sign (`=`). There are no inherit
+    order to the keys, and we sort them alphabetically for serialization and deserialization.
+    Whenever a part is absent, its internal value is a special `<EMPTY>` symbol that will
+    be treated accordingly when generating the output. If `keyed` is `False`, then the parts
+    are treated as positioned values, and `<EMPTY>` is used to pad parts at the end when the
+    incoming value is not long enough.'''
+
+    def __init__(self, data=None, lang="", idx=0, sep="", keyed=False):
+        self.sep = sep
+        self.keyed = keyed
+        super().__init__(data, lang, idx=idx)
+        self.state_attrs += ['sep', 'keyed']
+
+    def unit2parts(self, unit):
+        # unpack parts of a unit
+        if self.sep == "":
+            parts = [x for x in unit]
+        else:
+            parts = unit.split(self.sep)
+        if self.keyed:
+            if len(parts) == 1 and parts[0] == '_':
+                return dict()
+            parts = [x.split('=') for x in parts]
+
+            # Just treat multi-valued properties values as one possible value
+            parts = dict(parts)
+        elif unit == '_':
+            parts = []
+        return parts
+
+    def unit2id(self, unit):
+        parts = self.unit2parts(unit)
+        if self.keyed:
+            # treat multi-valued properties as singletons
+            return [self._unit2id[k].get(parts[k], UNK_ID) if k in parts else EMPTY_ID for k in self._unit2id]
+        else:
+            return [self._unit2id[i].get(parts[i], UNK_ID) if i < len(parts) else EMPTY_ID for i in range(len(self._unit2id))]
+
+    def id2unit(self, id):
+        items = []
+        for v, k in zip(id, self._id2unit.keys()):
+            if v == EMPTY_ID: continue
+            if self.keyed:
+                items.append("{}={}".format(k, self._id2unit[k][v]))
+            else:
+                items.append(self._id2unit[k][v])
+        res = self.sep.join(sorted(items, key=lambda x: x.lower()))
+        if res == "":
+            res = "_"
+        return res
+
+    def build_vocab(self):
+        allunits = [w[self.idx] for sent in self.data for w in sent]
+        if self.keyed:
+            self._id2unit = dict()
+
+            for u in allunits:
+                parts = self.unit2parts(u)
+                for key in parts:
+                    if key not in self._id2unit:
+                        self._id2unit[key] = copy(VOCAB_PREFIX)
+
+                    # treat multi-valued properties as singletons
+                    if parts[key] not in self._id2unit[key]:
+                        self._id2unit[key].append(parts[key])
+
+            # special handle for the case where upos/xpos/ufeats are always empty
+            if len(self._id2unit) == 0:
+                self._id2unit['_'] = copy(VOCAB_PREFIX) # use an arbitrary key
+
+        else:
+            self._id2unit = dict()
+
+            allparts = [self.unit2parts(u) for u in allunits]
+            maxlen = max([len(p) for p in allparts])
+
+            for parts in allparts:
+                for i, p in enumerate(parts):
+                    if i not in self._id2unit:
+                        self._id2unit[i] = copy(VOCAB_PREFIX)
+                    if i < len(parts) and p not in self._id2unit[i]:
+                        self._id2unit[i].append(p)
+
+            # special handle for the case where upos/xpos/ufeats are always empty
+            if len(self._id2unit) == 0:
+                self._id2unit[0] = copy(VOCAB_PREFIX) # use an arbitrary key
+
+        self._id2unit = OrderedDict([(k, self._id2unit[k]) for k in sorted(self._id2unit.keys(), key=lambda x: x.casefold())])
+        self._unit2id = {k: {w:i for i, w in enumerate(self._id2unit[k])} for k in self._id2unit}
+
+    def lens(self):
+        return [len(self._unit2id[k]) for k in self._unit2id]
+
+class BaseMultiVocab:
+    """ A convenient vocab container that can store multiple BaseVocab instances, and support 
+    safe serialization of all instances via state dicts. Each subclass of this base class 
+    should implement the load_state_dict() function to specify how a saved state dict 
+    should be loaded back."""
+    def __init__(self, vocab_dict=None):
+        self._vocabs = OrderedDict()
+        if vocab_dict is None:
+            return
+        # check all values provided must be a subclass of the Vocab base class
+        assert all([isinstance(v, BaseVocab) for v in vocab_dict.values()])
+        for k, v in vocab_dict.items():
+            self._vocabs[k] = v
+
+    def __setitem__(self, key, item):
+        self._vocabs[key] = item
+
+    def __getitem__(self, key):
+        return self._vocabs[key]
+
+    def state_dict(self):
+        """ Build a state dict by iteratively calling state_dict() of all vocabs. """
+        state = OrderedDict()
+        for k, v in self._vocabs.items():
+            state[k] = v.state_dict()
+        return state
+
+    @classmethod
+    def load_state_dict(cls, state_dict):
+        """ Construct a MultiVocab by reading from a state dict."""
+        raise NotImplementedError
+
+
+
```

### Comparing `classla-2.0/classla/models/depparse/data.py` & `classla-2.1/classla/models/depparse/data.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,225 +1,225 @@
-import random
-import logging
-import torch
-
-from classla.models.common.data import map_to_ids, get_long_tensor, get_float_tensor, sort_all
-from classla.models.common.vocab import PAD_ID, VOCAB_PREFIX, ROOT_ID, CompositeVocab
-from classla.models.pos.vocab import CharVocab, WordVocab, XPOSVocab, FeatureVocab, MultiVocab
-from classla.models.pos.xpos_vocab_factory import xpos_vocab_factory
-from classla.models.common.doc import *
-
-logger = logging.getLogger('classla')
-
-def data_to_batches(data, batch_size, eval_mode, sort_during_eval, max_sentence_size):
-    """
-    Given a list of lists, where the first element of each sublist
-    represents the sentence, group the sentences into batches.
-
-    During training mode (not eval_mode) the sentences are sorted by
-    length with a bit of random shuffling.  During eval mode, the
-    sentences are sorted by length if sort_during_eval is true.
-
-    Refactored from the data structure in case other models could use
-    it and for ease of testing.
-
-    Returns (batches, original_order), where original_order is None
-    when in train mode or when unsorted and represents the original
-    location of each sentence in the sort
-    """
-    res = []
-
-    if not data:
-        return res, None
-
-    if not eval_mode:
-        # sort sentences (roughly) by length for better memory utilization
-        data = sorted(data, key = lambda x: len(x[0]), reverse=random.random() > .5)
-        data_orig_idx = None
-    elif sort_during_eval:
-        (data, ), data_orig_idx = sort_all([data], [len(x[0]) for x in data])
-    else:
-        data_orig_idx = None
-
-    current = []
-    currentlen = 0
-    for x in data:
-        if max_sentence_size is not None and len(x[0]) > max_sentence_size:
-            if currentlen > 0:
-                res.append(current)
-                current = []
-                currentlen = 0
-            res.append([x])
-        else:
-            if len(x[0]) + currentlen > batch_size and currentlen > 0:
-                res.append(current)
-                current = []
-                currentlen = 0
-            current.append(x)
-            currentlen += len(x[0])
-
-    if currentlen > 0:
-        res.append(current)
-
-    return res, data_orig_idx
-
-
-class DataLoader:
-
-    def __init__(self, doc, batch_size, args, pretrain, vocab=None, evaluation=False, sort_during_eval=False, max_sentence_size=None):
-        self.batch_size = batch_size
-        self.max_sentence_size=max_sentence_size
-        self.args = args
-        self.eval = evaluation
-        self.shuffled = not self.eval
-        self.sort_during_eval = sort_during_eval
-        self.doc = doc
-        data = self.load_doc(doc)
-
-        # handle vocab
-        if vocab is None:
-            self.vocab = self.init_vocab(data)
-        else:
-            self.vocab = vocab
-        
-        # handle pretrain; pretrain vocab is used when args['pretrain'] == True and pretrain is not None
-        self.pretrain_vocab = None
-        if pretrain is not None and args['pretrain']:
-            self.pretrain_vocab = pretrain.vocab
-
-        # filter and sample data
-        if args.get('sample_train', 1.0) < 1.0 and not self.eval:
-            keep = int(args['sample_train'] * len(data))
-            data = random.sample(data, keep)
-            logger.debug("Subsample training set with rate {:g}".format(args['sample_train']))
-
-        data = self.preprocess(data, self.vocab, self.pretrain_vocab, args)
-        # shuffle for training
-        if self.shuffled:
-            random.shuffle(data)
-        self.num_examples = len(data)
-
-        # chunk into batches
-        self.data = self.chunk_batches(data)
-        logger.debug("{} batches created.".format(len(self.data)))
-
-    def init_vocab(self, data):
-        assert self.eval == False # for eval vocab must exist
-        charvocab = CharVocab(data, self.args['shorthand'])
-        wordvocab = WordVocab(data, self.args['shorthand'], cutoff=7, lower=True)
-        uposvocab = WordVocab(data, self.args['shorthand'], idx=1)
-        xposvocab = xpos_vocab_factory(data, self.args['shorthand'])
-        featsvocab = FeatureVocab(data, self.args['shorthand'], idx=3)
-        lemmavocab = WordVocab(data, self.args['shorthand'], cutoff=7, idx=4, lower=True)
-        deprelvocab = WordVocab(data, self.args['shorthand'], idx=6)
-        vocab = MultiVocab({'char': charvocab,
-                            'word': wordvocab,
-                            'upos': uposvocab,
-                            'xpos': xposvocab,
-                            'feats': featsvocab,
-                            'lemma': lemmavocab,
-                            'deprel': deprelvocab})
-        return vocab
-
-    def preprocess(self, data, vocab, pretrain_vocab, args):
-        processed = []
-        xpos_replacement = [[ROOT_ID] * len(vocab['xpos'])] if isinstance(vocab['xpos'], CompositeVocab) else [ROOT_ID]
-        feats_replacement = [[ROOT_ID] * len(vocab['feats'])]
-        for sent in data:
-            processed_sent = [[ROOT_ID] + vocab['word'].map([w[0] for w in sent])]
-            processed_sent += [[[ROOT_ID]] + [vocab['char'].map([x for x in w[0]]) for w in sent]]
-            processed_sent += [[ROOT_ID] + vocab['upos'].map([w[1] for w in sent])]
-            processed_sent += [xpos_replacement + vocab['xpos'].map([w[2] for w in sent])]
-            processed_sent += [feats_replacement + vocab['feats'].map([w[3] for w in sent])]
-            if pretrain_vocab is not None:
-                # always use lowercase lookup in pretrained vocab
-                processed_sent += [[ROOT_ID] + pretrain_vocab.map([w[0].lower() for w in sent])]
-            else:
-                processed_sent += [[ROOT_ID] + [PAD_ID] * len(sent)]
-            processed_sent += [[ROOT_ID] + vocab['lemma'].map([w[4] for w in sent])]
-            processed_sent += [[to_int(w[5], ignore_error=self.eval) for w in sent]]
-            processed_sent += [vocab['deprel'].map([w[6] for w in sent])]
-            processed.append(processed_sent)
-        return processed
-
-    def __len__(self):
-        return len(self.data)
-
-    def __getitem__(self, key):
-        """ Get a batch with index. """
-        if not isinstance(key, int):
-            raise TypeError
-        if key < 0 or key >= len(self.data):
-            raise IndexError
-        batch = self.data[key]
-        batch_size = len(batch)
-        batch = list(zip(*batch))
-        assert len(batch) == 9
-
-        # sort sentences by lens for easy RNN operations
-        lens = [len(x) for x in batch[0]]
-        batch, orig_idx = sort_all(batch, lens)
-
-        # sort words by lens for easy char-RNN operations
-        batch_words = [w for sent in batch[1] for w in sent]
-        word_lens = [len(x) for x in batch_words]
-        batch_words, word_orig_idx = sort_all([batch_words], word_lens)
-        batch_words = batch_words[0]
-        word_lens = [len(x) for x in batch_words]
-
-        # convert to tensors
-        words = batch[0]
-        words = get_long_tensor(words, batch_size)
-        words_mask = torch.eq(words, PAD_ID)
-        wordchars = get_long_tensor(batch_words, len(word_lens))
-        wordchars_mask = torch.eq(wordchars, PAD_ID)
-
-        upos = get_long_tensor(batch[2], batch_size)
-        xpos = get_long_tensor(batch[3], batch_size)
-        ufeats = get_long_tensor(batch[4], batch_size)
-        pretrained = get_long_tensor(batch[5], batch_size)
-        sentlens = [len(x) for x in batch[0]]
-        lemma = get_long_tensor(batch[6], batch_size)
-        head = get_long_tensor(batch[7], batch_size)
-        deprel = get_long_tensor(batch[8], batch_size)
-        return words, words_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, orig_idx, word_orig_idx, sentlens, word_lens
-
-    def load_doc(self, doc):
-        data = doc.get([TEXT, UPOS, XPOS, FEATS, LEMMA, HEAD, DEPREL], as_sentences=True)
-        data = self.resolve_none(data)
-        return data
-
-    def resolve_none(self, data):
-        # replace None to '_'
-        for sent_idx in range(len(data)):
-            for tok_idx in range(len(data[sent_idx])):
-                for feat_idx in range(len(data[sent_idx][tok_idx])):
-                    if data[sent_idx][tok_idx][feat_idx] is None:
-                        data[sent_idx][tok_idx][feat_idx] = '_'
-        return data
-
-    def __iter__(self):
-        for i in range(self.__len__()):
-            yield self.__getitem__(i)
-
-    def reshuffle(self):
-        data = [y for x in self.data for y in x]
-        self.data = self.chunk_batches(data)
-        random.shuffle(self.data)
-
-    def chunk_batches(self, data):
-        batches, data_orig_idx = data_to_batches(data=data, batch_size=self.batch_size,
-                                                 eval_mode=self.eval, sort_during_eval=self.sort_during_eval,
-                                                 max_sentence_size=self.max_sentence_size)
-        # data_orig_idx might be None at train time, since we don't anticipate unsorting
-        self.data_orig_idx = data_orig_idx
-        return batches
-
-def to_int(string, ignore_error=False):
-    try:
-        res = int(string)
-    except ValueError as err:
-        if ignore_error:
-            return 0
-        else:
-            raise err
-    return res
+import random
+import logging
+import torch
+
+from classla.models.common.data import map_to_ids, get_long_tensor, get_float_tensor, sort_all
+from classla.models.common.vocab import PAD_ID, VOCAB_PREFIX, ROOT_ID, CompositeVocab
+from classla.models.pos.vocab import CharVocab, WordVocab, XPOSVocab, FeatureVocab, MultiVocab
+from classla.models.pos.xpos_vocab_factory import xpos_vocab_factory
+from classla.models.common.doc import *
+
+logger = logging.getLogger('classla')
+
+def data_to_batches(data, batch_size, eval_mode, sort_during_eval, max_sentence_size):
+    """
+    Given a list of lists, where the first element of each sublist
+    represents the sentence, group the sentences into batches.
+
+    During training mode (not eval_mode) the sentences are sorted by
+    length with a bit of random shuffling.  During eval mode, the
+    sentences are sorted by length if sort_during_eval is true.
+
+    Refactored from the data structure in case other models could use
+    it and for ease of testing.
+
+    Returns (batches, original_order), where original_order is None
+    when in train mode or when unsorted and represents the original
+    location of each sentence in the sort
+    """
+    res = []
+
+    if not data:
+        return res, None
+
+    if not eval_mode:
+        # sort sentences (roughly) by length for better memory utilization
+        data = sorted(data, key = lambda x: len(x[0]), reverse=random.random() > .5)
+        data_orig_idx = None
+    elif sort_during_eval:
+        (data, ), data_orig_idx = sort_all([data], [len(x[0]) for x in data])
+    else:
+        data_orig_idx = None
+
+    current = []
+    currentlen = 0
+    for x in data:
+        if max_sentence_size is not None and len(x[0]) > max_sentence_size:
+            if currentlen > 0:
+                res.append(current)
+                current = []
+                currentlen = 0
+            res.append([x])
+        else:
+            if len(x[0]) + currentlen > batch_size and currentlen > 0:
+                res.append(current)
+                current = []
+                currentlen = 0
+            current.append(x)
+            currentlen += len(x[0])
+
+    if currentlen > 0:
+        res.append(current)
+
+    return res, data_orig_idx
+
+
+class DataLoader:
+
+    def __init__(self, doc, batch_size, args, pretrain, vocab=None, evaluation=False, sort_during_eval=False, max_sentence_size=None):
+        self.batch_size = batch_size
+        self.max_sentence_size=max_sentence_size
+        self.args = args
+        self.eval = evaluation
+        self.shuffled = not self.eval
+        self.sort_during_eval = sort_during_eval
+        self.doc = doc
+        data = self.load_doc(doc)
+
+        # handle vocab
+        if vocab is None:
+            self.vocab = self.init_vocab(data)
+        else:
+            self.vocab = vocab
+        
+        # handle pretrain; pretrain vocab is used when args['pretrain'] == True and pretrain is not None
+        self.pretrain_vocab = None
+        if pretrain is not None and args['pretrain']:
+            self.pretrain_vocab = pretrain.vocab
+
+        # filter and sample data
+        if args.get('sample_train', 1.0) < 1.0 and not self.eval:
+            keep = int(args['sample_train'] * len(data))
+            data = random.sample(data, keep)
+            logger.debug("Subsample training set with rate {:g}".format(args['sample_train']))
+
+        data = self.preprocess(data, self.vocab, self.pretrain_vocab, args)
+        # shuffle for training
+        if self.shuffled:
+            random.shuffle(data)
+        self.num_examples = len(data)
+
+        # chunk into batches
+        self.data = self.chunk_batches(data)
+        logger.debug("{} batches created.".format(len(self.data)))
+
+    def init_vocab(self, data):
+        assert self.eval == False # for eval vocab must exist
+        charvocab = CharVocab(data, self.args['shorthand'])
+        wordvocab = WordVocab(data, self.args['shorthand'], cutoff=7, lower=True)
+        uposvocab = WordVocab(data, self.args['shorthand'], idx=1)
+        xposvocab = xpos_vocab_factory(data, self.args['shorthand'])
+        featsvocab = FeatureVocab(data, self.args['shorthand'], idx=3)
+        lemmavocab = WordVocab(data, self.args['shorthand'], cutoff=7, idx=4, lower=True)
+        deprelvocab = WordVocab(data, self.args['shorthand'], idx=6)
+        vocab = MultiVocab({'char': charvocab,
+                            'word': wordvocab,
+                            'upos': uposvocab,
+                            'xpos': xposvocab,
+                            'feats': featsvocab,
+                            'lemma': lemmavocab,
+                            'deprel': deprelvocab})
+        return vocab
+
+    def preprocess(self, data, vocab, pretrain_vocab, args):
+        processed = []
+        xpos_replacement = [[ROOT_ID] * len(vocab['xpos'])] if isinstance(vocab['xpos'], CompositeVocab) else [ROOT_ID]
+        feats_replacement = [[ROOT_ID] * len(vocab['feats'])]
+        for sent in data:
+            processed_sent = [[ROOT_ID] + vocab['word'].map([w[0] for w in sent])]
+            processed_sent += [[[ROOT_ID]] + [vocab['char'].map([x for x in w[0]]) for w in sent]]
+            processed_sent += [[ROOT_ID] + vocab['upos'].map([w[1] for w in sent])]
+            processed_sent += [xpos_replacement + vocab['xpos'].map([w[2] for w in sent])]
+            processed_sent += [feats_replacement + vocab['feats'].map([w[3] for w in sent])]
+            if pretrain_vocab is not None:
+                # always use lowercase lookup in pretrained vocab
+                processed_sent += [[ROOT_ID] + pretrain_vocab.map([w[0].lower() for w in sent])]
+            else:
+                processed_sent += [[ROOT_ID] + [PAD_ID] * len(sent)]
+            processed_sent += [[ROOT_ID] + vocab['lemma'].map([w[4] for w in sent])]
+            processed_sent += [[to_int(w[5], ignore_error=self.eval) for w in sent]]
+            processed_sent += [vocab['deprel'].map([w[6] for w in sent])]
+            processed.append(processed_sent)
+        return processed
+
+    def __len__(self):
+        return len(self.data)
+
+    def __getitem__(self, key):
+        """ Get a batch with index. """
+        if not isinstance(key, int):
+            raise TypeError
+        if key < 0 or key >= len(self.data):
+            raise IndexError
+        batch = self.data[key]
+        batch_size = len(batch)
+        batch = list(zip(*batch))
+        assert len(batch) == 9
+
+        # sort sentences by lens for easy RNN operations
+        lens = [len(x) for x in batch[0]]
+        batch, orig_idx = sort_all(batch, lens)
+
+        # sort words by lens for easy char-RNN operations
+        batch_words = [w for sent in batch[1] for w in sent]
+        word_lens = [len(x) for x in batch_words]
+        batch_words, word_orig_idx = sort_all([batch_words], word_lens)
+        batch_words = batch_words[0]
+        word_lens = [len(x) for x in batch_words]
+
+        # convert to tensors
+        words = batch[0]
+        words = get_long_tensor(words, batch_size)
+        words_mask = torch.eq(words, PAD_ID)
+        wordchars = get_long_tensor(batch_words, len(word_lens))
+        wordchars_mask = torch.eq(wordchars, PAD_ID)
+
+        upos = get_long_tensor(batch[2], batch_size)
+        xpos = get_long_tensor(batch[3], batch_size)
+        ufeats = get_long_tensor(batch[4], batch_size)
+        pretrained = get_long_tensor(batch[5], batch_size)
+        sentlens = [len(x) for x in batch[0]]
+        lemma = get_long_tensor(batch[6], batch_size)
+        head = get_long_tensor(batch[7], batch_size)
+        deprel = get_long_tensor(batch[8], batch_size)
+        return words, words_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, orig_idx, word_orig_idx, sentlens, word_lens
+
+    def load_doc(self, doc):
+        data = doc.get([TEXT, UPOS, XPOS, FEATS, LEMMA, HEAD, DEPREL], as_sentences=True)
+        data = self.resolve_none(data)
+        return data
+
+    def resolve_none(self, data):
+        # replace None to '_'
+        for sent_idx in range(len(data)):
+            for tok_idx in range(len(data[sent_idx])):
+                for feat_idx in range(len(data[sent_idx][tok_idx])):
+                    if data[sent_idx][tok_idx][feat_idx] is None:
+                        data[sent_idx][tok_idx][feat_idx] = '_'
+        return data
+
+    def __iter__(self):
+        for i in range(self.__len__()):
+            yield self.__getitem__(i)
+
+    def reshuffle(self):
+        data = [y for x in self.data for y in x]
+        self.data = self.chunk_batches(data)
+        random.shuffle(self.data)
+
+    def chunk_batches(self, data):
+        batches, data_orig_idx = data_to_batches(data=data, batch_size=self.batch_size,
+                                                 eval_mode=self.eval, sort_during_eval=self.sort_during_eval,
+                                                 max_sentence_size=self.max_sentence_size)
+        # data_orig_idx might be None at train time, since we don't anticipate unsorting
+        self.data_orig_idx = data_orig_idx
+        return batches
+
+def to_int(string, ignore_error=False):
+    try:
+        res = int(string)
+    except ValueError as err:
+        if ignore_error:
+            return 0
+        else:
+            raise err
+    return res
```

### Comparing `classla-2.0/classla/models/depparse/model.py` & `classla-2.1/classla/models/depparse/model.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,192 +1,192 @@
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence
-
-from classla.models.common.biaffine import DeepBiaffineScorer
-from classla.models.common.hlstm import HighwayLSTM
-from classla.models.common.dropout import WordDropout
-from classla.models.common.vocab import CompositeVocab
-from classla.models.common.char_model import CharacterModel
-
-class Parser(nn.Module):
-    def __init__(self, args, vocab, emb_matrix=None, share_hid=False):
-        super().__init__()
-
-        self.vocab = vocab
-        self.args = args
-        self.share_hid = share_hid
-        self.unsaved_modules = []
-
-        def add_unsaved_module(name, module):
-            self.unsaved_modules += [name]
-            setattr(self, name, module)
-
-        # input layers
-        input_size = 0
-        if self.args['word_emb_dim'] > 0:
-            # frequent word embeddings
-            self.word_emb = nn.Embedding(len(vocab['word']), self.args['word_emb_dim'], padding_idx=0)
-            self.lemma_emb = nn.Embedding(len(vocab['lemma']), self.args['word_emb_dim'], padding_idx=0)
-            input_size += self.args['word_emb_dim'] * 2
-
-        if self.args['tag_emb_dim'] > 0:
-            self.upos_emb = nn.Embedding(len(vocab['upos']), self.args['tag_emb_dim'], padding_idx=0)
-
-            if not isinstance(vocab['xpos'], CompositeVocab):
-                self.xpos_emb = nn.Embedding(len(vocab['xpos']), self.args['tag_emb_dim'], padding_idx=0)
-            else:
-                self.xpos_emb = nn.ModuleList()
-
-                for l in vocab['xpos'].lens():
-                    self.xpos_emb.append(nn.Embedding(l, self.args['tag_emb_dim'], padding_idx=0))
-
-            self.ufeats_emb = nn.ModuleList()
-
-            for l in vocab['feats'].lens():
-                self.ufeats_emb.append(nn.Embedding(l, self.args['tag_emb_dim'], padding_idx=0))
-
-            input_size += self.args['tag_emb_dim'] * 2
-
-        if self.args['char'] and self.args['char_emb_dim'] > 0:
-            self.charmodel = CharacterModel(args, vocab)
-            self.trans_char = nn.Linear(self.args['char_hidden_dim'], self.args['transformed_dim'], bias=False)
-            input_size += self.args['transformed_dim']
-
-        if self.args['pretrain']:
-            # pretrained embeddings, by default this won't be saved into model file
-            add_unsaved_module('pretrained_emb', nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True))
-            self.trans_pretrained = nn.Linear(emb_matrix.shape[1], self.args['transformed_dim'], bias=False)
-            input_size += self.args['transformed_dim']
-
-        # recurrent layers
-        self.parserlstm = HighwayLSTM(input_size, self.args['hidden_dim'], self.args['num_layers'], batch_first=True, bidirectional=True, dropout=self.args['dropout'], rec_dropout=self.args['rec_dropout'], highway_func=torch.tanh)
-        self.drop_replacement = nn.Parameter(torch.randn(input_size) / np.sqrt(input_size))
-        self.parserlstm_h_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))
-        self.parserlstm_c_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))
-
-        # classifiers
-        self.unlabeled = DeepBiaffineScorer(2 * self.args['hidden_dim'], 2 * self.args['hidden_dim'], self.args['deep_biaff_hidden_dim'], 1, pairwise=True, dropout=args['dropout'])
-        self.deprel = DeepBiaffineScorer(2 * self.args['hidden_dim'], 2 * self.args['hidden_dim'], self.args['deep_biaff_hidden_dim'], len(vocab['deprel']), pairwise=True, dropout=args['dropout'])
-        if args['linearization']:
-            self.linearization = DeepBiaffineScorer(2 * self.args['hidden_dim'], 2 * self.args['hidden_dim'], self.args['deep_biaff_hidden_dim'], 1, pairwise=True, dropout=args['dropout'])
-        if args['distance']:
-            self.distance = DeepBiaffineScorer(2 * self.args['hidden_dim'], 2 * self.args['hidden_dim'], self.args['deep_biaff_hidden_dim'], 1, pairwise=True, dropout=args['dropout'])
-
-        # criterion
-        self.crit = nn.CrossEntropyLoss(ignore_index=-1, reduction='sum') # ignore padding
-
-        self.drop = nn.Dropout(args['dropout'])
-        self.worddrop = WordDropout(args['word_dropout'])
-
-    def forward(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, word_orig_idx, sentlens, wordlens):
-        def pack(x):
-            return pack_padded_sequence(x, sentlens, batch_first=True)
-
-        inputs = []
-        if self.args['pretrain']:
-            pretrained_emb = self.pretrained_emb(pretrained)
-            pretrained_emb = self.trans_pretrained(pretrained_emb)
-            pretrained_emb = pack(pretrained_emb)
-            inputs += [pretrained_emb]
-
-        #def pad(x):
-        #    return pad_packed_sequence(PackedSequence(x, pretrained_emb.batch_sizes), batch_first=True)[0]
-
-        if self.args['word_emb_dim'] > 0:
-            word_emb = self.word_emb(word)
-            word_emb = pack(word_emb)
-            lemma_emb = self.lemma_emb(lemma)
-            lemma_emb = pack(lemma_emb)
-            inputs += [word_emb, lemma_emb]
-
-        if self.args['tag_emb_dim'] > 0:
-            pos_emb = self.upos_emb(upos)
-
-            if isinstance(self.vocab['xpos'], CompositeVocab):
-                for i in range(len(self.vocab['xpos'])):
-                    pos_emb += self.xpos_emb[i](xpos[:, :, i])
-            else:
-                pos_emb += self.xpos_emb(xpos)
-            pos_emb = pack(pos_emb)
-
-            feats_emb = 0
-            for i in range(len(self.vocab['feats'])):
-                feats_emb += self.ufeats_emb[i](ufeats[:, :, i])
-            feats_emb = pack(feats_emb)
-
-            inputs += [pos_emb, feats_emb]
-
-        if self.args['char'] and self.args['char_emb_dim'] > 0:
-            char_reps = self.charmodel(wordchars, wordchars_mask, word_orig_idx, sentlens, wordlens)
-            char_reps = PackedSequence(self.trans_char(self.drop(char_reps.data)), char_reps.batch_sizes)
-            inputs += [char_reps]
-
-        lstm_inputs = torch.cat([x.data for x in inputs], 1)
-
-        lstm_inputs = self.worddrop(lstm_inputs, self.drop_replacement)
-        lstm_inputs = self.drop(lstm_inputs)
-
-        lstm_inputs = PackedSequence(lstm_inputs, inputs[0].batch_sizes)
-
-        lstm_outputs, _ = self.parserlstm(lstm_inputs, sentlens, hx=(self.parserlstm_h_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous(), self.parserlstm_c_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous()))
-        lstm_outputs, _ = pad_packed_sequence(lstm_outputs, batch_first=True)
-
-        unlabeled_scores = self.unlabeled(self.drop(lstm_outputs), self.drop(lstm_outputs)).squeeze(3)
-        deprel_scores = self.deprel(self.drop(lstm_outputs), self.drop(lstm_outputs))
-
-        #goldmask = head.new_zeros(*head.size(), head.size(-1)+1, dtype=torch.uint8)
-        #goldmask.scatter_(2, head.unsqueeze(2), 1)
-
-        if self.args['linearization'] or self.args['distance']:
-            head_offset = torch.arange(word.size(1), device=head.device).view(1, 1, -1).expand(word.size(0), -1, -1) - torch.arange(word.size(1), device=head.device).view(1, -1, 1).expand(word.size(0), -1, -1)
-
-        if self.args['linearization']:
-            lin_scores = self.linearization(self.drop(lstm_outputs), self.drop(lstm_outputs)).squeeze(3)
-            unlabeled_scores += F.logsigmoid(lin_scores * torch.sign(head_offset).float()).detach()
-
-        if self.args['distance']:
-            dist_scores = self.distance(self.drop(lstm_outputs), self.drop(lstm_outputs)).squeeze(3)
-            dist_pred = 1 + F.softplus(dist_scores)
-            dist_target = torch.abs(head_offset)
-            dist_kld = -torch.log((dist_target.float() - dist_pred)**2/2 + 1)
-            unlabeled_scores += dist_kld.detach()
-
-        diag = torch.eye(head.size(-1)+1, dtype=torch.bool, device=head.device).unsqueeze(0)
-        unlabeled_scores.masked_fill_(diag, -float('inf'))
-
-        preds = []
-
-        if self.training:
-            unlabeled_scores = unlabeled_scores[:, 1:, :] # exclude attachment for the root symbol
-            unlabeled_scores = unlabeled_scores.masked_fill(word_mask.unsqueeze(1), -float('inf'))
-            unlabeled_target = head.masked_fill(word_mask[:, 1:], -1)
-            loss = self.crit(unlabeled_scores.contiguous().view(-1, unlabeled_scores.size(2)), unlabeled_target.view(-1))
-
-            deprel_scores = deprel_scores[:, 1:] # exclude attachment for the root symbol
-            #deprel_scores = deprel_scores.masked_select(goldmask.unsqueeze(3)).view(-1, len(self.vocab['deprel']))
-            deprel_scores = torch.gather(deprel_scores, 2, head.unsqueeze(2).unsqueeze(3).expand(-1, -1, -1, len(self.vocab['deprel']))).view(-1, len(self.vocab['deprel']))
-            deprel_target = deprel.masked_fill(word_mask[:, 1:], -1)
-            loss += self.crit(deprel_scores.contiguous(), deprel_target.view(-1))
-
-            if self.args['linearization']:
-                #lin_scores = lin_scores[:, 1:].masked_select(goldmask)
-                lin_scores = torch.gather(lin_scores[:, 1:], 2, head.unsqueeze(2)).view(-1)
-                lin_scores = torch.cat([-lin_scores.unsqueeze(1)/2, lin_scores.unsqueeze(1)/2], 1)
-                #lin_target = (head_offset[:, 1:] > 0).long().masked_select(goldmask)
-                lin_target = torch.gather((head_offset[:, 1:] > 0).long(), 2, head.unsqueeze(2))
-                loss += self.crit(lin_scores.contiguous(), lin_target.view(-1))
-
-            if self.args['distance']:
-                #dist_kld = dist_kld[:, 1:].masked_select(goldmask)
-                dist_kld = torch.gather(dist_kld[:, 1:], 2, head.unsqueeze(2))
-                loss -= dist_kld.sum()
-
-            loss /= wordchars.size(0) # number of words
-        else:
-            loss = 0
-            preds.append(F.log_softmax(unlabeled_scores, 2).detach().cpu().numpy())
-            preds.append(deprel_scores.max(3)[1].detach().cpu().numpy())
-
-        return loss, preds
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence
+
+from classla.models.common.biaffine import DeepBiaffineScorer
+from classla.models.common.hlstm import HighwayLSTM
+from classla.models.common.dropout import WordDropout
+from classla.models.common.vocab import CompositeVocab
+from classla.models.common.char_model import CharacterModel
+
+class Parser(nn.Module):
+    def __init__(self, args, vocab, emb_matrix=None, share_hid=False):
+        super().__init__()
+
+        self.vocab = vocab
+        self.args = args
+        self.share_hid = share_hid
+        self.unsaved_modules = []
+
+        def add_unsaved_module(name, module):
+            self.unsaved_modules += [name]
+            setattr(self, name, module)
+
+        # input layers
+        input_size = 0
+        if self.args['word_emb_dim'] > 0:
+            # frequent word embeddings
+            self.word_emb = nn.Embedding(len(vocab['word']), self.args['word_emb_dim'], padding_idx=0)
+            self.lemma_emb = nn.Embedding(len(vocab['lemma']), self.args['word_emb_dim'], padding_idx=0)
+            input_size += self.args['word_emb_dim'] * 2
+
+        if self.args['tag_emb_dim'] > 0:
+            self.upos_emb = nn.Embedding(len(vocab['upos']), self.args['tag_emb_dim'], padding_idx=0)
+
+            if not isinstance(vocab['xpos'], CompositeVocab):
+                self.xpos_emb = nn.Embedding(len(vocab['xpos']), self.args['tag_emb_dim'], padding_idx=0)
+            else:
+                self.xpos_emb = nn.ModuleList()
+
+                for l in vocab['xpos'].lens():
+                    self.xpos_emb.append(nn.Embedding(l, self.args['tag_emb_dim'], padding_idx=0))
+
+            self.ufeats_emb = nn.ModuleList()
+
+            for l in vocab['feats'].lens():
+                self.ufeats_emb.append(nn.Embedding(l, self.args['tag_emb_dim'], padding_idx=0))
+
+            input_size += self.args['tag_emb_dim'] * 2
+
+        if self.args['char'] and self.args['char_emb_dim'] > 0:
+            self.charmodel = CharacterModel(args, vocab)
+            self.trans_char = nn.Linear(self.args['char_hidden_dim'], self.args['transformed_dim'], bias=False)
+            input_size += self.args['transformed_dim']
+
+        if self.args['pretrain']:
+            # pretrained embeddings, by default this won't be saved into model file
+            add_unsaved_module('pretrained_emb', nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True))
+            self.trans_pretrained = nn.Linear(emb_matrix.shape[1], self.args['transformed_dim'], bias=False)
+            input_size += self.args['transformed_dim']
+
+        # recurrent layers
+        self.parserlstm = HighwayLSTM(input_size, self.args['hidden_dim'], self.args['num_layers'], batch_first=True, bidirectional=True, dropout=self.args['dropout'], rec_dropout=self.args['rec_dropout'], highway_func=torch.tanh)
+        self.drop_replacement = nn.Parameter(torch.randn(input_size) / np.sqrt(input_size))
+        self.parserlstm_h_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))
+        self.parserlstm_c_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))
+
+        # classifiers
+        self.unlabeled = DeepBiaffineScorer(2 * self.args['hidden_dim'], 2 * self.args['hidden_dim'], self.args['deep_biaff_hidden_dim'], 1, pairwise=True, dropout=args['dropout'])
+        self.deprel = DeepBiaffineScorer(2 * self.args['hidden_dim'], 2 * self.args['hidden_dim'], self.args['deep_biaff_hidden_dim'], len(vocab['deprel']), pairwise=True, dropout=args['dropout'])
+        if args['linearization']:
+            self.linearization = DeepBiaffineScorer(2 * self.args['hidden_dim'], 2 * self.args['hidden_dim'], self.args['deep_biaff_hidden_dim'], 1, pairwise=True, dropout=args['dropout'])
+        if args['distance']:
+            self.distance = DeepBiaffineScorer(2 * self.args['hidden_dim'], 2 * self.args['hidden_dim'], self.args['deep_biaff_hidden_dim'], 1, pairwise=True, dropout=args['dropout'])
+
+        # criterion
+        self.crit = nn.CrossEntropyLoss(ignore_index=-1, reduction='sum') # ignore padding
+
+        self.drop = nn.Dropout(args['dropout'])
+        self.worddrop = WordDropout(args['word_dropout'])
+
+    def forward(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, word_orig_idx, sentlens, wordlens):
+        def pack(x):
+            return pack_padded_sequence(x, sentlens, batch_first=True)
+
+        inputs = []
+        if self.args['pretrain']:
+            pretrained_emb = self.pretrained_emb(pretrained)
+            pretrained_emb = self.trans_pretrained(pretrained_emb)
+            pretrained_emb = pack(pretrained_emb)
+            inputs += [pretrained_emb]
+
+        #def pad(x):
+        #    return pad_packed_sequence(PackedSequence(x, pretrained_emb.batch_sizes), batch_first=True)[0]
+
+        if self.args['word_emb_dim'] > 0:
+            word_emb = self.word_emb(word)
+            word_emb = pack(word_emb)
+            lemma_emb = self.lemma_emb(lemma)
+            lemma_emb = pack(lemma_emb)
+            inputs += [word_emb, lemma_emb]
+
+        if self.args['tag_emb_dim'] > 0:
+            pos_emb = self.upos_emb(upos)
+
+            if isinstance(self.vocab['xpos'], CompositeVocab):
+                for i in range(len(self.vocab['xpos'])):
+                    pos_emb += self.xpos_emb[i](xpos[:, :, i])
+            else:
+                pos_emb += self.xpos_emb(xpos)
+            pos_emb = pack(pos_emb)
+
+            feats_emb = 0
+            for i in range(len(self.vocab['feats'])):
+                feats_emb += self.ufeats_emb[i](ufeats[:, :, i])
+            feats_emb = pack(feats_emb)
+
+            inputs += [pos_emb, feats_emb]
+
+        if self.args['char'] and self.args['char_emb_dim'] > 0:
+            char_reps = self.charmodel(wordchars, wordchars_mask, word_orig_idx, sentlens, wordlens)
+            char_reps = PackedSequence(self.trans_char(self.drop(char_reps.data)), char_reps.batch_sizes)
+            inputs += [char_reps]
+
+        lstm_inputs = torch.cat([x.data for x in inputs], 1)
+
+        lstm_inputs = self.worddrop(lstm_inputs, self.drop_replacement)
+        lstm_inputs = self.drop(lstm_inputs)
+
+        lstm_inputs = PackedSequence(lstm_inputs, inputs[0].batch_sizes)
+
+        lstm_outputs, _ = self.parserlstm(lstm_inputs, sentlens, hx=(self.parserlstm_h_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous(), self.parserlstm_c_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous()))
+        lstm_outputs, _ = pad_packed_sequence(lstm_outputs, batch_first=True)
+
+        unlabeled_scores = self.unlabeled(self.drop(lstm_outputs), self.drop(lstm_outputs)).squeeze(3)
+        deprel_scores = self.deprel(self.drop(lstm_outputs), self.drop(lstm_outputs))
+
+        #goldmask = head.new_zeros(*head.size(), head.size(-1)+1, dtype=torch.uint8)
+        #goldmask.scatter_(2, head.unsqueeze(2), 1)
+
+        if self.args['linearization'] or self.args['distance']:
+            head_offset = torch.arange(word.size(1), device=head.device).view(1, 1, -1).expand(word.size(0), -1, -1) - torch.arange(word.size(1), device=head.device).view(1, -1, 1).expand(word.size(0), -1, -1)
+
+        if self.args['linearization']:
+            lin_scores = self.linearization(self.drop(lstm_outputs), self.drop(lstm_outputs)).squeeze(3)
+            unlabeled_scores += F.logsigmoid(lin_scores * torch.sign(head_offset).float()).detach()
+
+        if self.args['distance']:
+            dist_scores = self.distance(self.drop(lstm_outputs), self.drop(lstm_outputs)).squeeze(3)
+            dist_pred = 1 + F.softplus(dist_scores)
+            dist_target = torch.abs(head_offset)
+            dist_kld = -torch.log((dist_target.float() - dist_pred)**2/2 + 1)
+            unlabeled_scores += dist_kld.detach()
+
+        diag = torch.eye(head.size(-1)+1, dtype=torch.bool, device=head.device).unsqueeze(0)
+        unlabeled_scores.masked_fill_(diag, -float('inf'))
+
+        preds = []
+
+        if self.training:
+            unlabeled_scores = unlabeled_scores[:, 1:, :] # exclude attachment for the root symbol
+            unlabeled_scores = unlabeled_scores.masked_fill(word_mask.unsqueeze(1), -float('inf'))
+            unlabeled_target = head.masked_fill(word_mask[:, 1:], -1)
+            loss = self.crit(unlabeled_scores.contiguous().view(-1, unlabeled_scores.size(2)), unlabeled_target.view(-1))
+
+            deprel_scores = deprel_scores[:, 1:] # exclude attachment for the root symbol
+            #deprel_scores = deprel_scores.masked_select(goldmask.unsqueeze(3)).view(-1, len(self.vocab['deprel']))
+            deprel_scores = torch.gather(deprel_scores, 2, head.unsqueeze(2).unsqueeze(3).expand(-1, -1, -1, len(self.vocab['deprel']))).view(-1, len(self.vocab['deprel']))
+            deprel_target = deprel.masked_fill(word_mask[:, 1:], -1)
+            loss += self.crit(deprel_scores.contiguous(), deprel_target.view(-1))
+
+            if self.args['linearization']:
+                #lin_scores = lin_scores[:, 1:].masked_select(goldmask)
+                lin_scores = torch.gather(lin_scores[:, 1:], 2, head.unsqueeze(2)).view(-1)
+                lin_scores = torch.cat([-lin_scores.unsqueeze(1)/2, lin_scores.unsqueeze(1)/2], 1)
+                #lin_target = (head_offset[:, 1:] > 0).long().masked_select(goldmask)
+                lin_target = torch.gather((head_offset[:, 1:] > 0).long(), 2, head.unsqueeze(2))
+                loss += self.crit(lin_scores.contiguous(), lin_target.view(-1))
+
+            if self.args['distance']:
+                #dist_kld = dist_kld[:, 1:].masked_select(goldmask)
+                dist_kld = torch.gather(dist_kld[:, 1:], 2, head.unsqueeze(2))
+                loss -= dist_kld.sum()
+
+            loss /= wordchars.size(0) # number of words
+        else:
+            loss = 0
+            preds.append(F.log_softmax(unlabeled_scores, 2).detach().cpu().numpy())
+            preds.append(deprel_scores.max(3)[1].detach().cpu().numpy())
+
+        return loss, preds
```

### Comparing `classla-2.0/classla/models/depparse/scorer.py` & `classla-2.1/classla/models/depparse/scorer.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-"""
-Utils and wrappers for scoring parsers.
-"""
-import logging
-
-from classla.models.common.utils import ud_scores
-
-logger = logging.getLogger('classla')
-
-def score(system_conllu_file, gold_conllu_file, verbose=True):
-    """ Wrapper for UD parser scorer. """
-    evaluation = ud_scores(gold_conllu_file, system_conllu_file)
-    el = evaluation['LAS']
-    p = el.precision
-    r = el.recall
-    f = el.f1
-    if verbose:
-        scores = [evaluation[k].f1 * 100 for k in ['LAS', 'MLAS', 'BLEX']]
-        logger.info("LAS\tMLAS\tBLEX")
-        logger.info("{:.2f}\t{:.2f}\t{:.2f}".format(*scores))
-    return p, r, f
-
+"""
+Utils and wrappers for scoring parsers.
+"""
+import logging
+
+from classla.models.common.utils import ud_scores
+
+logger = logging.getLogger('classla')
+
+def score(system_conllu_file, gold_conllu_file, verbose=True):
+    """ Wrapper for UD parser scorer. """
+    evaluation = ud_scores(gold_conllu_file, system_conllu_file)
+    el = evaluation['LAS']
+    p = el.precision
+    r = el.recall
+    f = el.f1
+    if verbose:
+        scores = [evaluation[k].f1 * 100 for k in ['LAS', 'MLAS', 'BLEX']]
+        logger.info("LAS\tMLAS\tBLEX")
+        logger.info("{:.2f}\t{:.2f}\t{:.2f}".format(*scores))
+    return p, r, f
+
```

### Comparing `classla-2.0/classla/models/depparse/trainer.py` & `classla-2.1/classla/models/depparse/trainer.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,141 +1,141 @@
-"""
-A trainer class to handle training and testing of models.
-"""
-
-import sys
-import logging
-import torch
-from torch import nn
-
-from classla.models.common.trainer import Trainer as BaseTrainer
-from classla.models.common import utils, loss
-from classla.models.common.chuliu_edmonds import chuliu_edmonds, chuliu_edmonds_one_root
-from classla.models.depparse.model import Parser
-from classla.models.pos.vocab import MultiVocab
-
-logger = logging.getLogger('classla')
-
-def unpack_batch(batch, use_cuda):
-    """ Unpack a batch from the data loader. """
-    if use_cuda:
-        inputs = [b.cuda() if b is not None else None for b in batch[:11]]
-    else:
-        inputs = batch[:11]
-    orig_idx = batch[11]
-    word_orig_idx = batch[12]
-    sentlens = batch[13]
-    wordlens = batch[14]
-    return inputs, orig_idx, word_orig_idx, sentlens, wordlens
-
-class Trainer(BaseTrainer):
-    """ A trainer for training models. """
-    def __init__(self, args=None, vocab=None, pretrain=None, model_file=None, use_cuda=False):
-        self.use_cuda = use_cuda
-        if model_file is not None:
-            # load everything from file
-            self.load(model_file, pretrain)
-
-            # ugly fix for <PAD> outputs
-            self.vocab._vocabs['deprel']._id2unit[0] = 'dep'
-        else:
-            assert all(var is not None for var in [args, vocab, pretrain])
-            # build model from scratch
-            self.args = args
-            self.vocab = vocab
-            self.model = Parser(args, vocab, emb_matrix=pretrain.emb if pretrain is not None else None)
-
-        # update deprel vocab
-        new_id2unit = []
-        new_unit2id = {}
-        for i, v in enumerate(self.vocab._vocabs['deprel']._id2unit):
-            if '_' in v:
-                v = v.replace('_', ':')
-
-            # else:
-            new_id2unit.append(v)
-            new_unit2id[v] = i
-
-        self.vocab._vocabs['deprel']._id2unit = new_id2unit
-        self.vocab._vocabs['deprel']._unit2id = new_unit2id
-
-        self.parameters = [p for p in self.model.parameters() if p.requires_grad]
-        if self.use_cuda:
-            self.model.cuda()
-        else:
-            self.model.cpu()
-        self.optimizer = utils.get_optimizer(self.args['optim'], self.parameters, self.args['lr'], betas=(0.9, self.args['beta2']), eps=1e-6)
-
-    def update(self, batch, eval=False):
-        inputs, orig_idx, word_orig_idx, sentlens, wordlens = unpack_batch(batch, self.use_cuda)
-        word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel = inputs
-
-        if eval:
-            self.model.eval()
-        else:
-            self.model.train()
-            self.optimizer.zero_grad()
-        loss, _ = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, word_orig_idx, sentlens, wordlens)
-        loss_val = loss.data.item()
-        if eval:
-            return loss_val
-
-        loss.backward()
-        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args['max_grad_norm'])
-        self.optimizer.step()
-        return loss_val
-
-    def predict(self, batch, unsort=True):
-        inputs, orig_idx, word_orig_idx, sentlens, wordlens = unpack_batch(batch, self.use_cuda)
-        word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel = inputs
-
-        self.model.eval()
-        batch_size = word.size(0)
-        _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, word_orig_idx, sentlens, wordlens)
-        if 'multi_root' in self.args and self.args['multi_root']:
-            head_seqs = [chuliu_edmonds(adj[:l, :l])[1:] for adj, l in zip(preds[0], sentlens)]
-        else:
-            head_seqs = [chuliu_edmonds_one_root(adj[:l, :l])[1:] for adj, l in zip(preds[0], sentlens)] # remove attachment for the root
-        deprel_seqs = [self.vocab['deprel'].unmap([preds[1][i][j+1][h] for j, h in enumerate(hs)]) for i, hs in enumerate(head_seqs)]
-
-        pred_tokens = [[[str(head_seqs[i][j]), deprel_seqs[i][j]] for j in range(sentlens[i]-1)] for i in range(batch_size)]
-        if unsort:
-            pred_tokens = utils.unsort(pred_tokens, orig_idx)
-        return pred_tokens
-
-    def save(self, filename, skip_modules=True):
-        model_state = self.model.state_dict()
-        # skip saving modules like pretrained embeddings, because they are large and will be saved in a separate file
-        if skip_modules:
-            skipped = [k for k in model_state.keys() if k.split('.')[0] in self.model.unsaved_modules]
-            for k in skipped:
-                del model_state[k]
-        params = {
-                'model': model_state,
-                'vocab': self.vocab.state_dict(),
-                'config': self.args
-                }
-        try:
-            torch.save(params, filename)
-            logger.info("Model saved to {}".format(filename))
-        except BaseException:
-            logger.warning("Saving failed... continuing anyway.")
-
-    def load(self, filename, pretrain):
-        """
-        Load a model from file, with preloaded pretrain embeddings. Here we allow the pretrain to be None or a dummy input,
-        and the actual use of pretrain embeddings will depend on the boolean config "pretrain" in the loaded args.
-        """
-        try:
-            checkpoint = torch.load(filename, lambda storage, loc: storage)
-        except BaseException:
-            logger.error("Cannot load model from {}".format(filename))
-            raise
-        self.args = checkpoint['config']
-        self.vocab = MultiVocab.load_state_dict(checkpoint['vocab'])
-        # load model
-        emb_matrix = None
-        if self.args['pretrain'] and pretrain is not None: # we use pretrain only if args['pretrain'] == True and pretrain is not None
-            emb_matrix = pretrain.emb
-        self.model = Parser(self.args, self.vocab, emb_matrix=emb_matrix)
-        self.model.load_state_dict(checkpoint['model'], strict=False)
-
+"""
+A trainer class to handle training and testing of models.
+"""
+
+import sys
+import logging
+import torch
+from torch import nn
+
+from classla.models.common.trainer import Trainer as BaseTrainer
+from classla.models.common import utils, loss
+from classla.models.common.chuliu_edmonds import chuliu_edmonds, chuliu_edmonds_one_root
+from classla.models.depparse.model import Parser
+from classla.models.pos.vocab import MultiVocab
+
+logger = logging.getLogger('classla')
+
+def unpack_batch(batch, use_cuda):
+    """ Unpack a batch from the data loader. """
+    if use_cuda:
+        inputs = [b.cuda() if b is not None else None for b in batch[:11]]
+    else:
+        inputs = batch[:11]
+    orig_idx = batch[11]
+    word_orig_idx = batch[12]
+    sentlens = batch[13]
+    wordlens = batch[14]
+    return inputs, orig_idx, word_orig_idx, sentlens, wordlens
+
+class Trainer(BaseTrainer):
+    """ A trainer for training models. """
+    def __init__(self, args=None, vocab=None, pretrain=None, model_file=None, use_cuda=False):
+        self.use_cuda = use_cuda
+        if model_file is not None:
+            # load everything from file
+            self.load(model_file, pretrain)
+
+            # ugly fix for <PAD> outputs
+            self.vocab._vocabs['deprel']._id2unit[0] = 'dep'
+        else:
+            assert all(var is not None for var in [args, vocab, pretrain])
+            # build model from scratch
+            self.args = args
+            self.vocab = vocab
+            self.model = Parser(args, vocab, emb_matrix=pretrain.emb if pretrain is not None else None)
+
+        # update deprel vocab
+        new_id2unit = []
+        new_unit2id = {}
+        for i, v in enumerate(self.vocab._vocabs['deprel']._id2unit):
+            if '_' in v:
+                v = v.replace('_', ':')
+
+            # else:
+            new_id2unit.append(v)
+            new_unit2id[v] = i
+
+        self.vocab._vocabs['deprel']._id2unit = new_id2unit
+        self.vocab._vocabs['deprel']._unit2id = new_unit2id
+
+        self.parameters = [p for p in self.model.parameters() if p.requires_grad]
+        if self.use_cuda:
+            self.model.cuda()
+        else:
+            self.model.cpu()
+        self.optimizer = utils.get_optimizer(self.args['optim'], self.parameters, self.args['lr'], betas=(0.9, self.args['beta2']), eps=1e-6)
+
+    def update(self, batch, eval=False):
+        inputs, orig_idx, word_orig_idx, sentlens, wordlens = unpack_batch(batch, self.use_cuda)
+        word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel = inputs
+
+        if eval:
+            self.model.eval()
+        else:
+            self.model.train()
+            self.optimizer.zero_grad()
+        loss, _ = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, word_orig_idx, sentlens, wordlens)
+        loss_val = loss.data.item()
+        if eval:
+            return loss_val
+
+        loss.backward()
+        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args['max_grad_norm'])
+        self.optimizer.step()
+        return loss_val
+
+    def predict(self, batch, unsort=True):
+        inputs, orig_idx, word_orig_idx, sentlens, wordlens = unpack_batch(batch, self.use_cuda)
+        word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel = inputs
+
+        self.model.eval()
+        batch_size = word.size(0)
+        _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, word_orig_idx, sentlens, wordlens)
+        if 'multi_root' in self.args and self.args['multi_root']:
+            head_seqs = [chuliu_edmonds(adj[:l, :l])[1:] for adj, l in zip(preds[0], sentlens)]
+        else:
+            head_seqs = [chuliu_edmonds_one_root(adj[:l, :l])[1:] for adj, l in zip(preds[0], sentlens)] # remove attachment for the root
+        deprel_seqs = [self.vocab['deprel'].unmap([preds[1][i][j+1][h] for j, h in enumerate(hs)]) for i, hs in enumerate(head_seqs)]
+
+        pred_tokens = [[[str(head_seqs[i][j]), deprel_seqs[i][j]] for j in range(sentlens[i]-1)] for i in range(batch_size)]
+        if unsort:
+            pred_tokens = utils.unsort(pred_tokens, orig_idx)
+        return pred_tokens
+
+    def save(self, filename, skip_modules=True):
+        model_state = self.model.state_dict()
+        # skip saving modules like pretrained embeddings, because they are large and will be saved in a separate file
+        if skip_modules:
+            skipped = [k for k in model_state.keys() if k.split('.')[0] in self.model.unsaved_modules]
+            for k in skipped:
+                del model_state[k]
+        params = {
+                'model': model_state,
+                'vocab': self.vocab.state_dict(),
+                'config': self.args
+                }
+        try:
+            torch.save(params, filename)
+            logger.info("Model saved to {}".format(filename))
+        except BaseException:
+            logger.warning("Saving failed... continuing anyway.")
+
+    def load(self, filename, pretrain):
+        """
+        Load a model from file, with preloaded pretrain embeddings. Here we allow the pretrain to be None or a dummy input,
+        and the actual use of pretrain embeddings will depend on the boolean config "pretrain" in the loaded args.
+        """
+        try:
+            checkpoint = torch.load(filename, lambda storage, loc: storage)
+        except BaseException:
+            logger.error("Cannot load model from {}".format(filename))
+            raise
+        self.args = checkpoint['config']
+        self.vocab = MultiVocab.load_state_dict(checkpoint['vocab'])
+        # load model
+        emb_matrix = None
+        if self.args['pretrain'] and pretrain is not None: # we use pretrain only if args['pretrain'] == True and pretrain is not None
+            emb_matrix = pretrain.emb
+        self.model = Parser(self.args, self.vocab, emb_matrix=emb_matrix)
+        self.model.load_state_dict(checkpoint['model'], strict=False)
+
```

### Comparing `classla-2.0/classla/models/lemma/data.py` & `classla-2.1/classla/models/lemma/data.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,127 +1,127 @@
-import random
-import numpy as np
-import os
-from collections import Counter
-import logging
-import torch
-
-import classla.models.common.seq2seq_constant as constant
-from classla.models.common.data import map_to_ids, get_long_tensor, get_float_tensor, sort_all
-from classla.models.lemma.vocab import Vocab, MultiVocab
-from classla.models.lemma import edit
-from classla.models.common.doc import *
-
-logger = logging.getLogger('classla')
-
-class DataLoader:
-    def __init__(self, doc, batch_size, args, vocab=None, evaluation=False, conll_only=False, skip=None):
-        self.batch_size = batch_size
-        self.args = args
-        self.eval = evaluation
-        self.shuffled = not self.eval
-        self.doc = doc
-
-        data = self.load_doc(self.doc)
-
-        if conll_only: # only load conll file
-            return
-
-        if skip is not None:
-            assert len(data) == len(skip)
-            data = [x for x, y in zip(data, skip) if not y]
-
-        # handle vocab
-        if vocab is not None:
-            self.vocab = vocab
-        else:
-            self.vocab = dict()
-            char_vocab, pos_vocab = self.init_vocab(data)
-            self.vocab = MultiVocab({'char': char_vocab, 'pos': pos_vocab})
-
-        # filter and sample data
-        if args.get('sample_train', 1.0) < 1.0 and not self.eval:
-            keep = int(args['sample_train'] * len(data))
-            data = random.sample(data, keep)
-            logger.debug("Subsample training set with rate {:g}".format(args['sample_train']))
-
-        data = self.preprocess(data, self.vocab['char'], self.vocab['pos'], args)
-        # shuffle for training
-        if self.shuffled:
-            indices = list(range(len(data)))
-            random.shuffle(indices)
-            data = [data[i] for i in indices]
-        self.num_examples = len(data)
-
-        # chunk into batches
-        data = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]
-        self.data = data
-        logger.debug("{} batches created.".format(len(data)))
-
-    def init_vocab(self, data):
-        assert self.eval is False, "Vocab file must exist for evaluation"
-        char_data = "".join(d[0] + d[2] for d in data)
-        char_vocab = Vocab(char_data, self.args['lang'])
-        pos_data = [d[1] for d in data]
-        pos_vocab = Vocab(pos_data, self.args['lang'])
-        return char_vocab, pos_vocab
-
-    def preprocess(self, data, char_vocab, pos_vocab, args):
-        processed = []
-        for d in data:
-            edit_type = edit.EDIT_TO_ID[edit.get_edit_type(d[0], d[2])]
-            src = list(d[0])
-            src = [constant.SOS] + src + [constant.EOS]
-            src = char_vocab.map(src)
-            pos = d[1]
-            pos = pos_vocab.unit2id(pos)
-            tgt = list(d[2])
-            tgt_in = char_vocab.map([constant.SOS] + tgt)
-            tgt_out = char_vocab.map(tgt + [constant.EOS])
-            processed += [[src, tgt_in, tgt_out, pos, edit_type]]
-        return processed
-
-    def __len__(self):
-        return len(self.data)
-
-    def __getitem__(self, key):
-        """ Get a batch with index. """
-        if not isinstance(key, int):
-            raise TypeError
-        if key < 0 or key >= len(self.data):
-            raise IndexError
-        batch = self.data[key]
-        batch_size = len(batch)
-        batch = list(zip(*batch))
-        assert len(batch) == 5
-
-        # sort all fields by lens for easy RNN operations
-        lens = [len(x) for x in batch[0]]
-        batch, orig_idx = sort_all(batch, lens)
-
-        # convert to tensors
-        src = batch[0]
-        src = get_long_tensor(src, batch_size)
-        src_mask = torch.eq(src, constant.PAD_ID)
-        tgt_in = get_long_tensor(batch[1], batch_size)
-        tgt_out = get_long_tensor(batch[2], batch_size)
-        pos = torch.LongTensor(batch[3])
-        edits = torch.LongTensor(batch[4])
-        assert tgt_in.size(1) == tgt_out.size(1), "Target input and output sequence sizes do not match."
-        return src, src_mask, tgt_in, tgt_out, pos, edits, orig_idx
-
-    def __iter__(self):
-        for i in range(self.__len__()):
-            yield self.__getitem__(i)
-
-    def load_doc(self, doc):
-        data = doc.get([TEXT, XPOS, LEMMA])
-        data = self.resolve_none(data)
-        return data
-
-    def resolve_none(self, data):
-        # replace None to '_'
-        for tok_idx in range(len(data)):
-            for feat_idx in range(len(data[tok_idx])):
-                if data[tok_idx][feat_idx] is None:
-                    data[tok_idx][feat_idx] = '_'
+import random
+import numpy as np
+import os
+from collections import Counter
+import logging
+import torch
+
+import classla.models.common.seq2seq_constant as constant
+from classla.models.common.data import map_to_ids, get_long_tensor, get_float_tensor, sort_all
+from classla.models.lemma.vocab import Vocab, MultiVocab
+from classla.models.lemma import edit
+from classla.models.common.doc import *
+
+logger = logging.getLogger('classla')
+
+class DataLoader:
+    def __init__(self, doc, batch_size, args, vocab=None, evaluation=False, conll_only=False, skip=None):
+        self.batch_size = batch_size
+        self.args = args
+        self.eval = evaluation
+        self.shuffled = not self.eval
+        self.doc = doc
+
+        data = self.load_doc(self.doc)
+
+        if conll_only: # only load conll file
+            return
+
+        if skip is not None:
+            assert len(data) == len(skip)
+            data = [x for x, y in zip(data, skip) if not y]
+
+        # handle vocab
+        if vocab is not None:
+            self.vocab = vocab
+        else:
+            self.vocab = dict()
+            char_vocab, pos_vocab = self.init_vocab(data)
+            self.vocab = MultiVocab({'char': char_vocab, 'pos': pos_vocab})
+
+        # filter and sample data
+        if args.get('sample_train', 1.0) < 1.0 and not self.eval:
+            keep = int(args['sample_train'] * len(data))
+            data = random.sample(data, keep)
+            logger.debug("Subsample training set with rate {:g}".format(args['sample_train']))
+
+        data = self.preprocess(data, self.vocab['char'], self.vocab['pos'], args)
+        # shuffle for training
+        if self.shuffled:
+            indices = list(range(len(data)))
+            random.shuffle(indices)
+            data = [data[i] for i in indices]
+        self.num_examples = len(data)
+
+        # chunk into batches
+        data = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]
+        self.data = data
+        logger.debug("{} batches created.".format(len(data)))
+
+    def init_vocab(self, data):
+        assert self.eval is False, "Vocab file must exist for evaluation"
+        char_data = "".join(d[0] + d[2] for d in data)
+        char_vocab = Vocab(char_data, self.args['lang'])
+        pos_data = [d[1] for d in data]
+        pos_vocab = Vocab(pos_data, self.args['lang'])
+        return char_vocab, pos_vocab
+
+    def preprocess(self, data, char_vocab, pos_vocab, args):
+        processed = []
+        for d in data:
+            edit_type = edit.EDIT_TO_ID[edit.get_edit_type(d[0], d[2])]
+            src = list(d[0])
+            src = [constant.SOS] + src + [constant.EOS]
+            src = char_vocab.map(src)
+            pos = d[1]
+            pos = pos_vocab.unit2id(pos)
+            tgt = list(d[2])
+            tgt_in = char_vocab.map([constant.SOS] + tgt)
+            tgt_out = char_vocab.map(tgt + [constant.EOS])
+            processed += [[src, tgt_in, tgt_out, pos, edit_type]]
+        return processed
+
+    def __len__(self):
+        return len(self.data)
+
+    def __getitem__(self, key):
+        """ Get a batch with index. """
+        if not isinstance(key, int):
+            raise TypeError
+        if key < 0 or key >= len(self.data):
+            raise IndexError
+        batch = self.data[key]
+        batch_size = len(batch)
+        batch = list(zip(*batch))
+        assert len(batch) == 5
+
+        # sort all fields by lens for easy RNN operations
+        lens = [len(x) for x in batch[0]]
+        batch, orig_idx = sort_all(batch, lens)
+
+        # convert to tensors
+        src = batch[0]
+        src = get_long_tensor(src, batch_size)
+        src_mask = torch.eq(src, constant.PAD_ID)
+        tgt_in = get_long_tensor(batch[1], batch_size)
+        tgt_out = get_long_tensor(batch[2], batch_size)
+        pos = torch.LongTensor(batch[3])
+        edits = torch.LongTensor(batch[4])
+        assert tgt_in.size(1) == tgt_out.size(1), "Target input and output sequence sizes do not match."
+        return src, src_mask, tgt_in, tgt_out, pos, edits, orig_idx
+
+    def __iter__(self):
+        for i in range(self.__len__()):
+            yield self.__getitem__(i)
+
+    def load_doc(self, doc):
+        data = doc.get([TEXT, XPOS, LEMMA])
+        data = self.resolve_none(data)
+        return data
+
+    def resolve_none(self, data):
+        # replace None to '_'
+        for tok_idx in range(len(data)):
+            for feat_idx in range(len(data[tok_idx])):
+                if data[tok_idx][feat_idx] is None:
+                    data[tok_idx][feat_idx] = '_'
         return data
```

### Comparing `classla-2.0/classla/models/lemma/edit.py` & `classla-2.1/classla/models/lemma/edit.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,27 +1,27 @@
-"""
-Utilities for calculating edits between word and lemma forms.
-"""
-
-EDIT_TO_ID = {'none': 0, 'identity': 1, 'lower': 2}
-
-def get_edit_type(word, lemma):
-    """ Calculate edit types. """
-    if lemma == word:
-        return 'identity'
-    elif lemma == word.lower():
-        return 'lower'
-    return 'none'
-
-def edit_word(word, pred, edit_id):
-    """
-    Edit a word, given edit and seq2seq predictions.
-    """
-    if edit_id == 1:
-        return word
-    elif edit_id == 2:
-        return word.lower()
-    elif edit_id == 0:
-        return pred
-    else:
-        raise Exception("Unrecognized edit ID: {}".format(edit_id))
-
+"""
+Utilities for calculating edits between word and lemma forms.
+"""
+
+EDIT_TO_ID = {'none': 0, 'identity': 1, 'lower': 2}
+
+def get_edit_type(word, lemma):
+    """ Calculate edit types. """
+    if lemma == word:
+        return 'identity'
+    elif lemma == word.lower():
+        return 'lower'
+    return 'none'
+
+def edit_word(word, pred, edit_id):
+    """
+    Edit a word, given edit and seq2seq predictions.
+    """
+    if edit_id == 1:
+        return word
+    elif edit_id == 2:
+        return word.lower()
+    elif edit_id == 0:
+        return pred
+    else:
+        raise Exception("Unrecognized edit ID: {}".format(edit_id))
+
```

### Comparing `classla-2.0/classla/models/lemma/trainer.py` & `classla-2.1/classla/models/lemma/trainer.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,237 +1,237 @@
-"""
-A trainer class to handle training and testing of models.
-"""
-
-import sys
-import numpy as np
-from collections import Counter
-import logging
-import torch
-from torch import nn
-import torch.nn.init as init
-
-import classla.models.common.seq2seq_constant as constant
-from classla.models.pos.trainer import Trainer as PosTrainer
-from classla.models.common.seq2seq_model import Seq2SeqModel
-from classla.models.common import utils, loss
-from classla.models.lemma import edit
-from classla.models.lemma.vocab import MultiVocab
-
-logger = logging.getLogger('classla')
-
-def unpack_batch(batch, use_cuda):
-    """ Unpack a batch from the data loader. """
-    if use_cuda:
-        inputs = [b.cuda() if b is not None else None for b in batch[:6]]
-    else:
-        inputs = [b if b is not None else None for b in batch[:6]]
-    orig_idx = batch[6]
-    return inputs, orig_idx
-
-class Trainer(object):
-    """ A trainer for training models. """
-    def __init__(self, args=None, vocab=None, emb_matrix=None, model_file=None, use_cuda=False):
-        self.use_cuda = use_cuda
-        if model_file is not None:
-            # load everything from file
-            self.load(model_file, use_cuda)
-        else:
-            # build model from scratch
-            self.args = args
-            self.model = None if args['dict_only'] else Seq2SeqModel(args, emb_matrix=emb_matrix, use_cuda=use_cuda)
-            self.vocab = vocab
-            # dict-based components
-            self.word_dict = dict()
-            self.composite_dict = dict()
-        if not self.args['dict_only']:
-            if self.args.get('edit', False):
-                self.crit = loss.MixLoss(self.vocab['char'].size, self.args['alpha'])
-                logger.debug("Running seq2seq lemmatizer with edit classifier...")
-            else:
-                self.crit = loss.SequenceLoss(self.vocab['char'].size)
-            self.parameters = [p for p in self.model.parameters() if p.requires_grad]
-            if use_cuda:
-                self.model.cuda()
-                self.crit.cuda()
-            else:
-                self.model.cpu()
-                self.crit.cpu()
-            self.optimizer = utils.get_optimizer(self.args['optim'], self.parameters, self.args['lr'])
-
-            if len(self.composite_dict) == 0 and args and 'pos_model_path' in args and args['pos_model_path'] is not None:
-                self.composite_dict = PosTrainer.load_inflectional_lexicon(args['pos_model_path'])
-        self.pos_lemma_pretag = args['pos_lemma_pretag']
-
-    def update(self, batch, eval=False):
-        inputs, orig_idx = unpack_batch(batch, self.use_cuda)
-        src, src_mask, tgt_in, tgt_out, pos, edits = inputs
-
-        if eval:
-            self.model.eval()
-        else:
-            self.model.train()
-            self.optimizer.zero_grad()
-        log_probs, edit_logits = self.model(src, src_mask, tgt_in, pos)
-        if self.args.get('edit', False):
-            assert edit_logits is not None
-            loss = self.crit(log_probs.view(-1, self.vocab['char'].size), tgt_out.view(-1), \
-                    edit_logits, edits)
-        else:
-            loss = self.crit(log_probs.view(-1, self.vocab['char'].size), tgt_out.view(-1))
-        loss_val = loss.data.item()
-        if eval:
-            return loss_val
-
-        loss.backward()
-        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args['max_grad_norm'])
-        self.optimizer.step()
-        return loss_val
-
-    def predict(self, batch, beam_size=1):
-        inputs, orig_idx = unpack_batch(batch, self.use_cuda)
-        src, src_mask, tgt, tgt_mask, pos, edits = inputs
-
-        self.model.eval()
-        batch_size = src.size(0)
-        preds, edit_logits = self.model.predict(src, src_mask, pos=pos, beam_size=beam_size)
-        pred_seqs = [self.vocab['char'].unmap(ids) for ids in preds] # unmap to tokens
-        pred_seqs = utils.prune_decoded_seqs(pred_seqs)
-        pred_tokens = ["".join(seq) for seq in pred_seqs] # join chars to be tokens
-        pred_tokens = utils.unsort(pred_tokens, orig_idx)
-        if self.args.get('edit', False):
-            assert edit_logits is not None
-            edits = np.argmax(edit_logits.data.cpu().numpy(), axis=1).reshape([batch_size]).tolist()
-            edits = utils.unsort(edits, orig_idx)
-        else:
-            edits = None
-        return pred_tokens, edits
-
-    def postprocess(self, words, preds, edits=None):
-        """ Postprocess, mainly for handing edits. """
-        assert len(words) == len(preds), "Lemma predictions must have same length as words."
-        edited = []
-        if self.args.get('edit', False):
-            assert edits is not None and len(words) == len(edits)
-            for w, p, e in zip(words, preds, edits):
-                lem = edit.edit_word(w, p, e)
-                edited += [lem]
-        else:
-            edited = preds # do not edit
-        # final sanity check
-        assert len(edited) == len(words)
-        final = []
-        for lem, w in zip(edited, words):
-            if len(lem) == 0 or constant.UNK in lem:
-                final += [w] # invalid prediction, fall back to word
-            else:
-                final += [lem]
-        return final
-
-    def update_lr(self, new_lr):
-        utils.change_lr(self.optimizer, new_lr)
-
-    def train_dict(self, triples):
-        """ Train a dict lemmatizer given training (word, pos, lemma) triples. """
-        # accumulate counter
-        ctr = Counter()
-        ctr.update([(p[0], p[1], p[2]) for p in triples])
-        # find the most frequent mappings
-        for p, _ in ctr.most_common():
-            w, pos, l = p
-            if (w,pos) not in self.composite_dict:
-                self.composite_dict[(w,pos)] = l
-            if w not in self.word_dict:
-                self.word_dict[w] = l
-        return
-
-    def predict_dict(self, pairs):
-        """ Predict a list of lemmas using the dict model given (word, pos) pairs. """
-        lemmas = []
-        for p in pairs:
-            w, pos = p
-            if (w,pos) in self.composite_dict:
-                lemmas += [self.composite_dict[(w,pos)]]
-            elif w in self.word_dict:
-                lemmas += [self.word_dict[w]]
-            else:
-                lemmas += [w]
-        return lemmas
-
-    def skip_seq2seq(self, pairs):
-        """ Determine if we can skip the seq2seq module when ensembling with the frequency lexicon. """
-
-        skip = []
-        for p in pairs:
-            if len(p) == 3:
-                w, pos, lemma = p
-            else:
-                w, pos = p
-                lemma = None
-
-            if self.pos_lemma_pretag and lemma:
-                skip.append(True)
-            elif (w,pos) in self.composite_dict:
-                skip.append(True)
-            elif w in self.word_dict:
-                skip.append(True)
-            else:
-                skip.append(False)
-        return skip
-
-    def ensemble(self, pairs, other_preds):
-        """ Ensemble the dict with statistical model predictions. """
-        lemmas = []
-        assert len(pairs) == len(other_preds)
-        for p, pred in zip(pairs, other_preds):
-            if len(p) == 3:
-                w, pos, lemm = p
-            else:
-                w, pos = p
-                lemm = None
-
-            if self.pos_lemma_pretag and lemm:
-                lemma = lemm
-            elif (w,pos) in self.composite_dict:
-                lemma = self.composite_dict[(w,pos)]
-            elif w in self.word_dict:
-                lemma = self.word_dict[w]
-            elif any([c not in self.vocab['char'] for c in w]):
-                lemma = w
-            else:
-                lemma = pred
-            if lemma is None:
-                lemma = w
-            lemmas.append(lemma)
-        return lemmas
-
-    def save(self, filename):
-        params = {
-                'model': self.model.state_dict() if self.model is not None else None,
-                'vocab': self.vocab.state_dict(),
-                'config': self.args
-                }
-
-        if len(self.composite_dict) > 0 and ('pos_model_path' not in self.args or self.args['pos_model_path'] is None):
-            params['dicts'] = (self.word_dict, self.composite_dict)
-
-        try:
-            torch.save(params, filename)
-            logger.info("Model saved to {}".format(filename))
-        except BaseException:
-            logger.warning("Saving failed... continuing anyway.")
-
-    def load(self, filename, use_cuda=False):
-        try:
-            checkpoint = torch.load(filename, lambda storage, loc: storage)
-        except BaseException:
-            logger.error("Cannot load model from {}".format(filename))
-            raise
-        self.args = checkpoint['config']
-        self.word_dict, self.composite_dict = checkpoint['dicts'] if 'dicts' in checkpoint else ({}, {})
-
-        if not self.args['dict_only']:
-            self.model = Seq2SeqModel(self.args, use_cuda=use_cuda)
-            self.model.load_state_dict(checkpoint['model'])
-        else:
-            self.model = None
-        self.vocab = MultiVocab.load_state_dict(checkpoint['vocab'])
+"""
+A trainer class to handle training and testing of models.
+"""
+
+import sys
+import numpy as np
+from collections import Counter
+import logging
+import torch
+from torch import nn
+import torch.nn.init as init
+
+import classla.models.common.seq2seq_constant as constant
+from classla.models.pos.trainer import Trainer as PosTrainer
+from classla.models.common.seq2seq_model import Seq2SeqModel
+from classla.models.common import utils, loss
+from classla.models.lemma import edit
+from classla.models.lemma.vocab import MultiVocab
+
+logger = logging.getLogger('classla')
+
+def unpack_batch(batch, use_cuda):
+    """ Unpack a batch from the data loader. """
+    if use_cuda:
+        inputs = [b.cuda() if b is not None else None for b in batch[:6]]
+    else:
+        inputs = [b if b is not None else None for b in batch[:6]]
+    orig_idx = batch[6]
+    return inputs, orig_idx
+
+class Trainer(object):
+    """ A trainer for training models. """
+    def __init__(self, args=None, vocab=None, emb_matrix=None, model_file=None, use_cuda=False):
+        self.use_cuda = use_cuda
+        if model_file is not None:
+            # load everything from file
+            self.load(model_file, use_cuda)
+        else:
+            # build model from scratch
+            self.args = args
+            self.model = None if args['dict_only'] else Seq2SeqModel(args, emb_matrix=emb_matrix, use_cuda=use_cuda)
+            self.vocab = vocab
+            # dict-based components
+            self.word_dict = dict()
+            self.composite_dict = dict()
+        if not self.args['dict_only']:
+            if self.args.get('edit', False):
+                self.crit = loss.MixLoss(self.vocab['char'].size, self.args['alpha'])
+                logger.debug("Running seq2seq lemmatizer with edit classifier...")
+            else:
+                self.crit = loss.SequenceLoss(self.vocab['char'].size)
+            self.parameters = [p for p in self.model.parameters() if p.requires_grad]
+            if use_cuda:
+                self.model.cuda()
+                self.crit.cuda()
+            else:
+                self.model.cpu()
+                self.crit.cpu()
+            self.optimizer = utils.get_optimizer(self.args['optim'], self.parameters, self.args['lr'])
+
+            if len(self.composite_dict) == 0 and args and 'pos_model_path' in args and args['pos_model_path'] is not None:
+                self.composite_dict = PosTrainer.load_inflectional_lexicon(args['pos_model_path'])
+        self.pos_lemma_pretag = args['pos_lemma_pretag']
+
+    def update(self, batch, eval=False):
+        inputs, orig_idx = unpack_batch(batch, self.use_cuda)
+        src, src_mask, tgt_in, tgt_out, pos, edits = inputs
+
+        if eval:
+            self.model.eval()
+        else:
+            self.model.train()
+            self.optimizer.zero_grad()
+        log_probs, edit_logits = self.model(src, src_mask, tgt_in, pos)
+        if self.args.get('edit', False):
+            assert edit_logits is not None
+            loss = self.crit(log_probs.view(-1, self.vocab['char'].size), tgt_out.view(-1), \
+                    edit_logits, edits)
+        else:
+            loss = self.crit(log_probs.view(-1, self.vocab['char'].size), tgt_out.view(-1))
+        loss_val = loss.data.item()
+        if eval:
+            return loss_val
+
+        loss.backward()
+        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args['max_grad_norm'])
+        self.optimizer.step()
+        return loss_val
+
+    def predict(self, batch, beam_size=1):
+        inputs, orig_idx = unpack_batch(batch, self.use_cuda)
+        src, src_mask, tgt, tgt_mask, pos, edits = inputs
+
+        self.model.eval()
+        batch_size = src.size(0)
+        preds, edit_logits = self.model.predict(src, src_mask, pos=pos, beam_size=beam_size)
+        pred_seqs = [self.vocab['char'].unmap(ids) for ids in preds] # unmap to tokens
+        pred_seqs = utils.prune_decoded_seqs(pred_seqs)
+        pred_tokens = ["".join(seq) for seq in pred_seqs] # join chars to be tokens
+        pred_tokens = utils.unsort(pred_tokens, orig_idx)
+        if self.args.get('edit', False):
+            assert edit_logits is not None
+            edits = np.argmax(edit_logits.data.cpu().numpy(), axis=1).reshape([batch_size]).tolist()
+            edits = utils.unsort(edits, orig_idx)
+        else:
+            edits = None
+        return pred_tokens, edits
+
+    def postprocess(self, words, preds, edits=None):
+        """ Postprocess, mainly for handing edits. """
+        assert len(words) == len(preds), "Lemma predictions must have same length as words."
+        edited = []
+        if self.args.get('edit', False):
+            assert edits is not None and len(words) == len(edits)
+            for w, p, e in zip(words, preds, edits):
+                lem = edit.edit_word(w, p, e)
+                edited += [lem]
+        else:
+            edited = preds # do not edit
+        # final sanity check
+        assert len(edited) == len(words)
+        final = []
+        for lem, w in zip(edited, words):
+            if len(lem) == 0 or constant.UNK in lem:
+                final += [w] # invalid prediction, fall back to word
+            else:
+                final += [lem]
+        return final
+
+    def update_lr(self, new_lr):
+        utils.change_lr(self.optimizer, new_lr)
+
+    def train_dict(self, triples):
+        """ Train a dict lemmatizer given training (word, pos, lemma) triples. """
+        # accumulate counter
+        ctr = Counter()
+        ctr.update([(p[0], p[1], p[2]) for p in triples])
+        # find the most frequent mappings
+        for p, _ in ctr.most_common():
+            w, pos, l = p
+            if (w,pos) not in self.composite_dict:
+                self.composite_dict[(w,pos)] = l
+            if w not in self.word_dict:
+                self.word_dict[w] = l
+        return
+
+    def predict_dict(self, pairs):
+        """ Predict a list of lemmas using the dict model given (word, pos) pairs. """
+        lemmas = []
+        for p in pairs:
+            w, pos = p
+            if (w,pos) in self.composite_dict:
+                lemmas += [self.composite_dict[(w,pos)]]
+            elif w in self.word_dict:
+                lemmas += [self.word_dict[w]]
+            else:
+                lemmas += [w]
+        return lemmas
+
+    def skip_seq2seq(self, pairs):
+        """ Determine if we can skip the seq2seq module when ensembling with the frequency lexicon. """
+
+        skip = []
+        for p in pairs:
+            if len(p) == 3:
+                w, pos, lemma = p
+            else:
+                w, pos = p
+                lemma = None
+
+            if self.pos_lemma_pretag and lemma:
+                skip.append(True)
+            elif (w,pos) in self.composite_dict:
+                skip.append(True)
+            elif w in self.word_dict:
+                skip.append(True)
+            else:
+                skip.append(False)
+        return skip
+
+    def ensemble(self, pairs, other_preds):
+        """ Ensemble the dict with statistical model predictions. """
+        lemmas = []
+        assert len(pairs) == len(other_preds)
+        for p, pred in zip(pairs, other_preds):
+            if len(p) == 3:
+                w, pos, lemm = p
+            else:
+                w, pos = p
+                lemm = None
+
+            if self.pos_lemma_pretag and lemm:
+                lemma = lemm
+            elif (w,pos) in self.composite_dict:
+                lemma = self.composite_dict[(w,pos)]
+            elif w in self.word_dict:
+                lemma = self.word_dict[w]
+            elif any([c not in self.vocab['char'] for c in w]):
+                lemma = w
+            else:
+                lemma = pred
+            if lemma is None:
+                lemma = w
+            lemmas.append(lemma)
+        return lemmas
+
+    def save(self, filename):
+        params = {
+                'model': self.model.state_dict() if self.model is not None else None,
+                'vocab': self.vocab.state_dict(),
+                'config': self.args
+                }
+
+        if len(self.composite_dict) > 0 and ('pos_model_path' not in self.args or self.args['pos_model_path'] is None):
+            params['dicts'] = (self.word_dict, self.composite_dict)
+
+        try:
+            torch.save(params, filename)
+            logger.info("Model saved to {}".format(filename))
+        except BaseException:
+            logger.warning("Saving failed... continuing anyway.")
+
+    def load(self, filename, use_cuda=False):
+        try:
+            checkpoint = torch.load(filename, lambda storage, loc: storage)
+        except BaseException:
+            logger.error("Cannot load model from {}".format(filename))
+            raise
+        self.args = checkpoint['config']
+        self.word_dict, self.composite_dict = checkpoint['dicts'] if 'dicts' in checkpoint else ({}, {})
+
+        if not self.args['dict_only']:
+            self.model = Seq2SeqModel(self.args, use_cuda=use_cuda)
+            self.model.load_state_dict(checkpoint['model'])
+        else:
+            self.model = None
+        self.vocab = MultiVocab.load_state_dict(checkpoint['vocab'])
```

### Comparing `classla-2.0/classla/models/lemmatizer.py` & `classla-2.1/classla/models/lemmatizer.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,317 +1,317 @@
-"""
-Entry point for training and evaluating a lemmatizer.
-
-This lemmatizer combines a neural sequence-to-sequence architecture with an `edit` classifier 
-and two dictionaries to produce robust lemmas from word forms.
-For details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.
-"""
-
-import sys
-import os
-import shutil
-import time
-from datetime import datetime
-import argparse
-import numpy as np
-import random
-import torch
-from torch import nn, optim
-
-from classla.models.lemma.data import DataLoader
-from classla.models.lemma.vocab import Vocab
-from classla.models.lemma.trainer import Trainer
-from classla.models.lemma import scorer, edit
-from classla.models.common import utils
-import classla.models.common.seq2seq_constant as constant
-from classla.models.common.doc import *
-from classla.utils.conll import CoNLL
-from classla.models import _training_logging
-
-def parse_args(args=None):
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--data_dir', type=str, default='data/lemma', help='Directory for all lemma data.')
-    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')
-    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')
-    parser.add_argument('--output_file', type=str, default=None, help='Output CoNLL-U file.')
-    parser.add_argument('--gold_file', type=str, default=None, help='Output CoNLL-U file.')
-
-    parser.add_argument('--mode', default='train', choices=['train', 'predict'])
-    parser.add_argument('--lang', type=str, help='Language')
-
-    parser.add_argument('--no_dict', dest='ensemble_dict', action='store_false', help='Do not ensemble dictionary with seq2seq. By default use ensemble.')
-    parser.add_argument('--dict_only', action='store_true', help='Only train a dictionary-based lemmatizer.')
-    parser.add_argument('--external_dict', type=str, default=None, help='External dictionary in form (token, lemma, XPOS)')
-
-    parser.add_argument('--hidden_dim', type=int, default=200)
-    parser.add_argument('--emb_dim', type=int, default=50)
-    parser.add_argument('--num_layers', type=int, default=1)
-    parser.add_argument('--emb_dropout', type=float, default=0.5)
-    parser.add_argument('--dropout', type=float, default=0.5)
-    parser.add_argument('--max_dec_len', type=int, default=50)
-    parser.add_argument('--beam_size', type=int, default=1)
-
-    parser.add_argument('--attn_type', default='soft', choices=['soft', 'mlp', 'linear', 'deep'], help='Attention type')
-    parser.add_argument('--pos', action='store_true', help='Use XPOS in lemmatization.')
-    parser.add_argument('--pos_dim', type=int, default=50)
-    parser.add_argument('--pos_dropout', type=float, default=0.5)
-    parser.add_argument('--no_edit', dest='edit', action='store_false', help='Do not use edit classifier in lemmatization. By default use an edit classifier.')
-    parser.add_argument('--num_edit', type=int, default=len(edit.EDIT_TO_ID))
-    parser.add_argument('--alpha', type=float, default=1.0)
-    parser.add_argument('--no_pos', dest='pos', action='store_false', help='Do not use UPOS in lemmatization. By default UPOS is used.')
-
-    parser.add_argument('--sample_train', type=float, default=1.0, help='Subsample training data.')
-    parser.add_argument('--optim', type=str, default='adam', help='sgd, adagrad, adam or adamax.')
-    parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate')
-    parser.add_argument('--lr_decay', type=float, default=0.9)
-    parser.add_argument('--decay_epoch', type=int, default=30, help="Decay the lr starting from this epoch.")
-    parser.add_argument('--num_epoch', type=int, default=60)
-    parser.add_argument('--batch_size', type=int, default=50)
-    parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')
-    parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')
-    parser.add_argument('--model_dir', type=str, default='saved_models/lemma', help='Root dir for saving models.')
-    parser.add_argument('--model_file', type=str, default='saved_models/lemma', help='File for saving models.')
-    parser.add_argument('--pos_lemma_pretag', type=bool, default=False, help='File for saving models.')
-    parser.add_argument('--pos_model_path', type=str, default=None, help='Location of pos model with inf. lexicon.')
-    parser.add_argument('--pos_force_inf_lexicon', action='store_true', help='Boolean tag, that forces program to use inf. lexicon from POS model.')
-
-    parser.add_argument('--seed', type=int, default=1234)
-    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())
-    parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')
-    args = parser.parse_args(args=args)
-    return args
-
-def main(args=None):
-    sys.setrecursionlimit(50000)
-
-    args = parse_args(args=args)
-
-    torch.manual_seed(args.seed)
-    np.random.seed(args.seed)
-    random.seed(args.seed)
-    if args.cpu:
-        args.cuda = False
-    elif args.cuda:
-        torch.cuda.manual_seed(args.seed)
-
-    args = vars(args)
-    print("Running lemmatizer in {} mode".format(args['mode']))
-
-    # manually correct for training epochs
-    if args['lang'] in ['cs_pdt', 'ru_syntagrus']:
-        args['num_epoch'] = 30
-
-    if args['mode'] == 'train':
-        train(args)
-    else:
-        evaluate(args)
-
-def train(args):
-    # load data
-    print("[Loading data with batch size {}...]".format(args['batch_size']))
-    doc, metasentences = CoNLL.conll2dict(input_file=args['train_file'])
-    train_doc = Document(doc, metasentences=metasentences)
-    train_batch = DataLoader(train_doc, args['batch_size'], args, evaluation=False)
-    vocab = train_batch.vocab
-    args['vocab_size'] = vocab['char'].size
-    args['pos_vocab_size'] = vocab['pos'].size
-    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
-    dev_doc = Document(doc, metasentences=metasentences)
-    dev_batch = DataLoader(dev_doc, args['batch_size'], args, vocab=vocab, evaluation=True)
-
-    utils.ensure_dir(args['model_dir'])
-    model_file = '{}/{}_lemmatizer.pt'.format(args['model_dir'], args['model_file'])
-
-    # pred and gold path
-    system_pred_file = args['output_file']
-    gold_file = args['gold_file']
-
-    utils.print_config(args)
-
-    # skip training if the language does not have training or dev data
-    if len(train_batch) == 0 or len(dev_batch) == 0:
-        print("[Skip training because no data available...]")
-        sys.exit(0)
-
-    # start training
-    # train a dictionary-based lemmatizer
-    trainer = Trainer(args=args, vocab=vocab, use_cuda=args['cuda'])
-    print("[Training dictionary-based lemmatizer...]")
-    dict = train_batch.doc.get([TEXT, XPOS, LEMMA])
-    dict = [(e[0].lower(), e[1], e[2]) for e in dict]
-    if args.get('external_dict', None) is not None:
-        extra_dict = []
-        for line in open(args['external_dict']):
-            word,lemma,xpos = line.rstrip('\r\n').split('\t')
-            extra_dict.append((word.lower(),xpos,lemma))
-        dict = extra_dict + dict
-    trainer.train_dict(dict)
-    print("Evaluating on dev set...")
-    dev_preds = trainer.predict_dict([(e[0].lower(),e[1]) for e in dev_batch.doc.get([TEXT, XPOS])])
-    dev_batch.doc.set([LEMMA], dev_preds)
-    CoNLL.write_doc2conll(dev_batch.doc, system_pred_file)
-    _, _, dev_f = scorer.score(system_pred_file, gold_file)
-    print("Dev F1 = {:.2f}".format(dev_f * 100))
-
-    if args.get('dict_only', False):
-        # save dictionaries
-        trainer.save(model_file)
-    else:
-        # train a seq2seq model
-        print("[Training seq2seq-based lemmatizer...]")
-        global_step = 0
-        max_steps = len(train_batch) * args['num_epoch']
-        dev_score_history = []
-        best_dev_preds = []
-        current_lr = args['lr']
-        global_start_time = time.time()
-        format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'
-
-        # start training
-        for epoch in range(1, args['num_epoch']+1):
-            train_loss = 0
-            for i, batch in enumerate(train_batch):
-                start_time = time.time()
-                global_step += 1
-                loss = trainer.update(batch, eval=False) # update step
-                train_loss += loss
-                if global_step % args['log_step'] == 0:
-                    duration = time.time() - start_time
-                    print(format_str.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"), global_step,\
-                            max_steps, epoch, args['num_epoch'], loss, duration, current_lr))
-
-            # eval on dev
-            print("Evaluating on dev set...")
-            dev_preds = []
-            dev_edits = []
-
-            # try speeding up dev eval
-            dict_preds = trainer.predict_dict([(e[0].lower(),e[1]) for e in dev_batch.doc.get([TEXT, XPOS])])
-            if args.get('ensemble_dict', False):
-                skip = trainer.skip_seq2seq([(e[0].lower(),e[1]) for e in dev_batch.doc.get([TEXT, XPOS])])
-                doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
-                dev_doc = Document(doc, metasentences=metasentences)
-                seq2seq_batch = DataLoader(dev_doc, args['batch_size'], args, vocab=vocab, evaluation=True, skip=skip)
-            else:
-                seq2seq_batch = dev_batch
-            for i, b in enumerate(seq2seq_batch):
-                ps, es = trainer.predict(b, args['beam_size'])
-                dev_preds += ps
-                if es is not None:
-                    dev_edits += es
-            if args.get('ensemble_dict', False):
-                dev_preds = trainer.postprocess([x for x, y in zip(dev_batch.doc.get([TEXT]), skip) if not y], dev_preds, edits=dev_edits)
-                print("[Ensembling dict with seq2seq lemmatizer...]")
-                i = 0
-                preds1 = []
-                for s in skip:
-                    if s:
-                        preds1.append('')
-                    else:
-                        preds1.append(dev_preds[i])
-                        i += 1
-                dev_preds = trainer.ensemble([(e[0].lower(),e[1]) for e in dev_batch.doc.get([TEXT, XPOS])], preds1)
-            else:
-                dev_preds = trainer.postprocess(dev_batch.doc.get([TEXT]), dev_preds, edits=dev_edits)
-
-            dev_batch.doc.set([LEMMA], dev_preds)
-            CoNLL.write_doc2conll(dev_batch.doc, system_pred_file)
-            _, _, dev_score = scorer.score(system_pred_file, gold_file)
-
-            train_loss = train_loss / train_batch.num_examples * args['batch_size'] # avg loss per batch
-            print("epoch {}: train_loss = {:.6f}, dev_score = {:.4f}".format(epoch, train_loss, dev_score))
-
-            # save best model
-            if epoch == 1 or dev_score > max(dev_score_history):
-                trainer.save(model_file)
-                print("new best model saved.")
-                best_dev_preds = dev_preds
-
-            # lr schedule
-            if epoch > args['decay_epoch'] and dev_score <= dev_score_history[-1] and \
-                    args['optim'] in ['sgd', 'adagrad']:
-                current_lr *= args['lr_decay']
-                trainer.update_lr(current_lr)
-
-            dev_score_history += [dev_score]
-            print("")
-
-        print("Training ended with {} epochs.".format(epoch))
-
-        best_f, best_epoch = max(dev_score_history)*100, np.argmax(dev_score_history)+1
-        print("Best dev F1 = {:.2f}, at epoch = {}".format(best_f, best_epoch))
-
-def evaluate(args):
-    # file paths
-    system_pred_file = args['output_file']
-    gold_file = args['gold_file']
-    model_file = '{}/{}_lemmatizer.pt'.format(args['model_dir'], args['model_file'])
-
-    # load model
-    use_cuda = args['cuda'] and not args['cpu']
-    trainer = Trainer(args=args, model_file=model_file, use_cuda=use_cuda)
-    loaded_args, vocab = trainer.args, trainer.vocab
-
-    for k in args:
-        if k.endswith('_dir') or k.endswith('_file') or k in ['shorthand']:
-            loaded_args[k] = args[k]
-
-    # laod data
-    print("Loading data with batch size {}...".format(args['batch_size']))
-    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
-    eval_doc = Document(doc, metasentences=metasentences)
-    batch = DataLoader(eval_doc, args['batch_size'], loaded_args, vocab=vocab, evaluation=True)
-
-    # skip eval if dev data does not exist
-    if len(batch) == 0:
-        print("Skip evaluation because no dev data is available...")
-        print("Lemma score:")
-        print("{} ".format(args['lang']))
-        sys.exit(0)
-
-    dict_preds = trainer.predict_dict([(e[0].lower(),e[1]) for e in batch.doc.get([TEXT, XPOS])])
-
-    if loaded_args.get('dict_only', False):
-        preds = dict_preds
-    else:
-        if loaded_args.get('ensemble_dict', False):
-            skip = trainer.skip_seq2seq([(e[0].lower(),e[1]) for e in batch.doc.get([TEXT, XPOS])])
-            doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
-            dev_doc = Document(doc, metasentences=metasentences)
-            seq2seq_batch = DataLoader(dev_doc, args['batch_size'], loaded_args, vocab=vocab, evaluation=True, skip=skip)
-        else:
-            seq2seq_batch = batch
-        print("Running the seq2seq model...")
-        preds = []
-        edits = []
-        for i, b in enumerate(seq2seq_batch):
-            ps, es = trainer.predict(b, args['beam_size'])
-            preds += ps
-            if es is not None:
-                edits += es
-
-        if loaded_args.get('ensemble_dict', False):
-            preds = trainer.postprocess([x for x, y in zip(batch.doc.get([TEXT]), skip) if not y], preds, edits=edits)
-            print("[Ensembling dict with seq2seq lemmatizer...]")
-            i = 0
-            preds1 = []
-            for s in skip:
-                if s:
-                    preds1.append('')
-                else:
-                    preds1.append(preds[i])
-                    i += 1
-            preds = trainer.ensemble([(e[0].lower(),e[1]) for e in batch.doc.get([TEXT, XPOS])], preds1)
-        else:
-            preds = trainer.postprocess(batch.doc.get([TEXT]), preds, edits=edits)
-
-    # write to file and score
-    batch.doc.set([LEMMA], preds)
-    CoNLL.write_doc2conll(batch.doc, system_pred_file)
-    if gold_file is not None:
-        _, _, score = scorer.score(system_pred_file, gold_file)
-
-        print("Lemma score:")
-        print("{} {:.2f}".format(args['lang'], score*100))
-
-if __name__ == '__main__':
-    main()
+"""
+Entry point for training and evaluating a lemmatizer.
+
+This lemmatizer combines a neural sequence-to-sequence architecture with an `edit` classifier 
+and two dictionaries to produce robust lemmas from word forms.
+For details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.
+"""
+
+import sys
+import os
+import shutil
+import time
+from datetime import datetime
+import argparse
+import numpy as np
+import random
+import torch
+from torch import nn, optim
+
+from classla.models.lemma.data import DataLoader
+from classla.models.lemma.vocab import Vocab
+from classla.models.lemma.trainer import Trainer
+from classla.models.lemma import scorer, edit
+from classla.models.common import utils
+import classla.models.common.seq2seq_constant as constant
+from classla.models.common.doc import *
+from classla.utils.conll import CoNLL
+from classla.models import _training_logging
+
+def parse_args(args=None):
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--data_dir', type=str, default='data/lemma', help='Directory for all lemma data.')
+    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')
+    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')
+    parser.add_argument('--output_file', type=str, default=None, help='Output CoNLL-U file.')
+    parser.add_argument('--gold_file', type=str, default=None, help='Output CoNLL-U file.')
+
+    parser.add_argument('--mode', default='train', choices=['train', 'predict'])
+    parser.add_argument('--lang', type=str, help='Language')
+
+    parser.add_argument('--no_dict', dest='ensemble_dict', action='store_false', help='Do not ensemble dictionary with seq2seq. By default use ensemble.')
+    parser.add_argument('--dict_only', action='store_true', help='Only train a dictionary-based lemmatizer.')
+    parser.add_argument('--external_dict', type=str, default=None, help='External dictionary in form (token, lemma, XPOS)')
+
+    parser.add_argument('--hidden_dim', type=int, default=200)
+    parser.add_argument('--emb_dim', type=int, default=50)
+    parser.add_argument('--num_layers', type=int, default=1)
+    parser.add_argument('--emb_dropout', type=float, default=0.5)
+    parser.add_argument('--dropout', type=float, default=0.5)
+    parser.add_argument('--max_dec_len', type=int, default=50)
+    parser.add_argument('--beam_size', type=int, default=1)
+
+    parser.add_argument('--attn_type', default='soft', choices=['soft', 'mlp', 'linear', 'deep'], help='Attention type')
+    parser.add_argument('--pos', action='store_true', help='Use XPOS in lemmatization.')
+    parser.add_argument('--pos_dim', type=int, default=50)
+    parser.add_argument('--pos_dropout', type=float, default=0.5)
+    parser.add_argument('--no_edit', dest='edit', action='store_false', help='Do not use edit classifier in lemmatization. By default use an edit classifier.')
+    parser.add_argument('--num_edit', type=int, default=len(edit.EDIT_TO_ID))
+    parser.add_argument('--alpha', type=float, default=1.0)
+    parser.add_argument('--no_pos', dest='pos', action='store_false', help='Do not use UPOS in lemmatization. By default UPOS is used.')
+
+    parser.add_argument('--sample_train', type=float, default=1.0, help='Subsample training data.')
+    parser.add_argument('--optim', type=str, default='adam', help='sgd, adagrad, adam or adamax.')
+    parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate')
+    parser.add_argument('--lr_decay', type=float, default=0.9)
+    parser.add_argument('--decay_epoch', type=int, default=30, help="Decay the lr starting from this epoch.")
+    parser.add_argument('--num_epoch', type=int, default=60)
+    parser.add_argument('--batch_size', type=int, default=50)
+    parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')
+    parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')
+    parser.add_argument('--model_dir', type=str, default='saved_models/lemma', help='Root dir for saving models.')
+    parser.add_argument('--model_file', type=str, default='saved_models/lemma', help='File for saving models.')
+    parser.add_argument('--pos_lemma_pretag', type=bool, default=False, help='File for saving models.')
+    parser.add_argument('--pos_model_path', type=str, default=None, help='Location of pos model with inf. lexicon.')
+    parser.add_argument('--pos_force_inf_lexicon', action='store_true', help='Boolean tag, that forces program to use inf. lexicon from POS model.')
+
+    parser.add_argument('--seed', type=int, default=1234)
+    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())
+    parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')
+    args = parser.parse_args(args=args)
+    return args
+
+def main(args=None):
+    sys.setrecursionlimit(50000)
+
+    args = parse_args(args=args)
+
+    torch.manual_seed(args.seed)
+    np.random.seed(args.seed)
+    random.seed(args.seed)
+    if args.cpu:
+        args.cuda = False
+    elif args.cuda:
+        torch.cuda.manual_seed(args.seed)
+
+    args = vars(args)
+    print("Running lemmatizer in {} mode".format(args['mode']))
+
+    # manually correct for training epochs
+    if args['lang'] in ['cs_pdt', 'ru_syntagrus']:
+        args['num_epoch'] = 30
+
+    if args['mode'] == 'train':
+        train(args)
+    else:
+        evaluate(args)
+
+def train(args):
+    # load data
+    print("[Loading data with batch size {}...]".format(args['batch_size']))
+    doc, metasentences = CoNLL.conll2dict(input_file=args['train_file'])
+    train_doc = Document(doc, metasentences=metasentences)
+    train_batch = DataLoader(train_doc, args['batch_size'], args, evaluation=False)
+    vocab = train_batch.vocab
+    args['vocab_size'] = vocab['char'].size
+    args['pos_vocab_size'] = vocab['pos'].size
+    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
+    dev_doc = Document(doc, metasentences=metasentences)
+    dev_batch = DataLoader(dev_doc, args['batch_size'], args, vocab=vocab, evaluation=True)
+
+    utils.ensure_dir(args['model_dir'])
+    model_file = '{}/{}_lemmatizer.pt'.format(args['model_dir'], args['model_file'])
+
+    # pred and gold path
+    system_pred_file = args['output_file']
+    gold_file = args['gold_file']
+
+    utils.print_config(args)
+
+    # skip training if the language does not have training or dev data
+    if len(train_batch) == 0 or len(dev_batch) == 0:
+        print("[Skip training because no data available...]")
+        sys.exit(0)
+
+    # start training
+    # train a dictionary-based lemmatizer
+    trainer = Trainer(args=args, vocab=vocab, use_cuda=args['cuda'])
+    print("[Training dictionary-based lemmatizer...]")
+    dict = train_batch.doc.get([TEXT, XPOS, LEMMA])
+    dict = [(e[0].lower(), e[1], e[2]) for e in dict]
+    if args.get('external_dict', None) is not None:
+        extra_dict = []
+        for line in open(args['external_dict']):
+            word,lemma,xpos = line.rstrip('\r\n').split('\t')
+            extra_dict.append((word.lower(),xpos,lemma))
+        dict = extra_dict + dict
+    trainer.train_dict(dict)
+    print("Evaluating on dev set...")
+    dev_preds = trainer.predict_dict([(e[0].lower(),e[1]) for e in dev_batch.doc.get([TEXT, XPOS])])
+    dev_batch.doc.set([LEMMA], dev_preds)
+    CoNLL.write_doc2conll(dev_batch.doc, system_pred_file)
+    _, _, dev_f = scorer.score(system_pred_file, gold_file)
+    print("Dev F1 = {:.2f}".format(dev_f * 100))
+
+    if args.get('dict_only', False):
+        # save dictionaries
+        trainer.save(model_file)
+    else:
+        # train a seq2seq model
+        print("[Training seq2seq-based lemmatizer...]")
+        global_step = 0
+        max_steps = len(train_batch) * args['num_epoch']
+        dev_score_history = []
+        best_dev_preds = []
+        current_lr = args['lr']
+        global_start_time = time.time()
+        format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'
+
+        # start training
+        for epoch in range(1, args['num_epoch']+1):
+            train_loss = 0
+            for i, batch in enumerate(train_batch):
+                start_time = time.time()
+                global_step += 1
+                loss = trainer.update(batch, eval=False) # update step
+                train_loss += loss
+                if global_step % args['log_step'] == 0:
+                    duration = time.time() - start_time
+                    print(format_str.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"), global_step,\
+                            max_steps, epoch, args['num_epoch'], loss, duration, current_lr))
+
+            # eval on dev
+            print("Evaluating on dev set...")
+            dev_preds = []
+            dev_edits = []
+
+            # try speeding up dev eval
+            dict_preds = trainer.predict_dict([(e[0].lower(),e[1]) for e in dev_batch.doc.get([TEXT, XPOS])])
+            if args.get('ensemble_dict', False):
+                skip = trainer.skip_seq2seq([(e[0].lower(),e[1]) for e in dev_batch.doc.get([TEXT, XPOS])])
+                doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
+                dev_doc = Document(doc, metasentences=metasentences)
+                seq2seq_batch = DataLoader(dev_doc, args['batch_size'], args, vocab=vocab, evaluation=True, skip=skip)
+            else:
+                seq2seq_batch = dev_batch
+            for i, b in enumerate(seq2seq_batch):
+                ps, es = trainer.predict(b, args['beam_size'])
+                dev_preds += ps
+                if es is not None:
+                    dev_edits += es
+            if args.get('ensemble_dict', False):
+                dev_preds = trainer.postprocess([x for x, y in zip(dev_batch.doc.get([TEXT]), skip) if not y], dev_preds, edits=dev_edits)
+                print("[Ensembling dict with seq2seq lemmatizer...]")
+                i = 0
+                preds1 = []
+                for s in skip:
+                    if s:
+                        preds1.append('')
+                    else:
+                        preds1.append(dev_preds[i])
+                        i += 1
+                dev_preds = trainer.ensemble([(e[0].lower(),e[1]) for e in dev_batch.doc.get([TEXT, XPOS])], preds1)
+            else:
+                dev_preds = trainer.postprocess(dev_batch.doc.get([TEXT]), dev_preds, edits=dev_edits)
+
+            dev_batch.doc.set([LEMMA], dev_preds)
+            CoNLL.write_doc2conll(dev_batch.doc, system_pred_file)
+            _, _, dev_score = scorer.score(system_pred_file, gold_file)
+
+            train_loss = train_loss / train_batch.num_examples * args['batch_size'] # avg loss per batch
+            print("epoch {}: train_loss = {:.6f}, dev_score = {:.4f}".format(epoch, train_loss, dev_score))
+
+            # save best model
+            if epoch == 1 or dev_score > max(dev_score_history):
+                trainer.save(model_file)
+                print("new best model saved.")
+                best_dev_preds = dev_preds
+
+            # lr schedule
+            if epoch > args['decay_epoch'] and dev_score <= dev_score_history[-1] and \
+                    args['optim'] in ['sgd', 'adagrad']:
+                current_lr *= args['lr_decay']
+                trainer.update_lr(current_lr)
+
+            dev_score_history += [dev_score]
+            print("")
+
+        print("Training ended with {} epochs.".format(epoch))
+
+        best_f, best_epoch = max(dev_score_history)*100, np.argmax(dev_score_history)+1
+        print("Best dev F1 = {:.2f}, at epoch = {}".format(best_f, best_epoch))
+
+def evaluate(args):
+    # file paths
+    system_pred_file = args['output_file']
+    gold_file = args['gold_file']
+    model_file = '{}/{}_lemmatizer.pt'.format(args['model_dir'], args['model_file'])
+
+    # load model
+    use_cuda = args['cuda'] and not args['cpu']
+    trainer = Trainer(args=args, model_file=model_file, use_cuda=use_cuda)
+    loaded_args, vocab = trainer.args, trainer.vocab
+
+    for k in args:
+        if k.endswith('_dir') or k.endswith('_file') or k in ['shorthand']:
+            loaded_args[k] = args[k]
+
+    # laod data
+    print("Loading data with batch size {}...".format(args['batch_size']))
+    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
+    eval_doc = Document(doc, metasentences=metasentences)
+    batch = DataLoader(eval_doc, args['batch_size'], loaded_args, vocab=vocab, evaluation=True)
+
+    # skip eval if dev data does not exist
+    if len(batch) == 0:
+        print("Skip evaluation because no dev data is available...")
+        print("Lemma score:")
+        print("{} ".format(args['lang']))
+        sys.exit(0)
+
+    dict_preds = trainer.predict_dict([(e[0].lower(),e[1]) for e in batch.doc.get([TEXT, XPOS])])
+
+    if loaded_args.get('dict_only', False):
+        preds = dict_preds
+    else:
+        if loaded_args.get('ensemble_dict', False):
+            skip = trainer.skip_seq2seq([(e[0].lower(),e[1]) for e in batch.doc.get([TEXT, XPOS])])
+            doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
+            dev_doc = Document(doc, metasentences=metasentences)
+            seq2seq_batch = DataLoader(dev_doc, args['batch_size'], loaded_args, vocab=vocab, evaluation=True, skip=skip)
+        else:
+            seq2seq_batch = batch
+        print("Running the seq2seq model...")
+        preds = []
+        edits = []
+        for i, b in enumerate(seq2seq_batch):
+            ps, es = trainer.predict(b, args['beam_size'])
+            preds += ps
+            if es is not None:
+                edits += es
+
+        if loaded_args.get('ensemble_dict', False):
+            preds = trainer.postprocess([x for x, y in zip(batch.doc.get([TEXT]), skip) if not y], preds, edits=edits)
+            print("[Ensembling dict with seq2seq lemmatizer...]")
+            i = 0
+            preds1 = []
+            for s in skip:
+                if s:
+                    preds1.append('')
+                else:
+                    preds1.append(preds[i])
+                    i += 1
+            preds = trainer.ensemble([(e[0].lower(),e[1]) for e in batch.doc.get([TEXT, XPOS])], preds1)
+        else:
+            preds = trainer.postprocess(batch.doc.get([TEXT]), preds, edits=edits)
+
+    # write to file and score
+    batch.doc.set([LEMMA], preds)
+    CoNLL.write_doc2conll(batch.doc, system_pred_file)
+    if gold_file is not None:
+        _, _, score = scorer.score(system_pred_file, gold_file)
+
+        print("Lemma score:")
+        print("{} {:.2f}".format(args['lang'], score*100))
+
+if __name__ == '__main__':
+    main()
```

### Comparing `classla-2.0/classla/models/mwt/data.py` & `classla-2.1/classla/models/mwt/data.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,106 +1,106 @@
-import random
-import numpy as np
-import os
-from collections import Counter
-import logging
-import torch
-
-import classla.models.common.seq2seq_constant as constant
-from classla.models.common.data import map_to_ids, get_long_tensor, get_float_tensor, sort_all
-from classla.models.mwt.vocab import Vocab
-from classla.models.common.doc import Document
-
-logger = logging.getLogger('classla')
-
-class DataLoader:
-    def __init__(self, doc, batch_size, args, vocab=None, evaluation=False):
-        self.batch_size = batch_size
-        self.args = args
-        self.eval = evaluation
-        self.shuffled = not self.eval
-        self.doc = doc
-
-        data = self.load_doc(self.doc, evaluation=self.eval)
-
-        # handle vocab
-        if vocab is None:
-            self.vocab = self.init_vocab(data)
-        else:
-            self.vocab = vocab
-
-        # filter and sample data
-        if args.get('sample_train', 1.0) < 1.0 and not self.eval:
-            keep = int(args['sample_train'] * len(data))
-            data = random.sample(data, keep)
-            logger.debug("Subsample training set with rate {:g}".format(args['sample_train']))
-
-        data = self.preprocess(data, self.vocab, args)
-        # shuffle for training
-        if self.shuffled:
-            indices = list(range(len(data)))
-            random.shuffle(indices)
-            data = [data[i] for i in indices]
-        self.num_examples = len(data)
-
-        # chunk into batches
-        data = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]
-        self.data = data
-        logger.debug("{} batches created.".format(len(data)))
-
-    def init_vocab(self, data):
-        assert self.eval == False # for eval vocab must exist
-        vocab = Vocab(data, self.args['shorthand'])
-        return vocab
-
-    def preprocess(self, data, vocab, args):
-        processed = []
-        for d in data:
-            src = list(d[0])
-            src = [constant.SOS] + src + [constant.EOS]
-            src = vocab.map(src)
-            if self.eval:
-                tgt = src # as a placeholder
-            else:
-                tgt = list(d[1])
-            tgt_in = vocab.map([constant.SOS] + tgt)
-            tgt_out = vocab.map(tgt + [constant.EOS])
-            processed += [[src, tgt_in, tgt_out]]
-        return processed
-
-    def __len__(self):
-        return len(self.data)
-
-    def __getitem__(self, key):
-        """ Get a batch with index. """
-        if not isinstance(key, int):
-            raise TypeError
-        if key < 0 or key >= len(self.data):
-            raise IndexError
-        batch = self.data[key]
-        batch_size = len(batch)
-        batch = list(zip(*batch))
-        assert len(batch) == 3
-
-        # sort all fields by lens for easy RNN operations
-        lens = [len(x) for x in batch[0]]
-        batch, orig_idx = sort_all(batch, lens)
-
-        # convert to tensors
-        src = batch[0]
-        src = get_long_tensor(src, batch_size)
-        src_mask = torch.eq(src, constant.PAD_ID)
-        tgt_in = get_long_tensor(batch[1], batch_size)
-        tgt_out = get_long_tensor(batch[2], batch_size)
-        assert tgt_in.size(1) == tgt_out.size(1), \
-                "Target input and output sequence sizes do not match."
-        return (src, src_mask, tgt_in, tgt_out, orig_idx)
-
-    def __iter__(self):
-        for i in range(self.__len__()):
-            yield self.__getitem__(i)
-
-    def load_doc(self, doc, evaluation=False):
-        data = doc.get_mwt_expansions(evaluation)
-        if evaluation: data = [[e] for e in data]
-        return data
-
+import random
+import numpy as np
+import os
+from collections import Counter
+import logging
+import torch
+
+import classla.models.common.seq2seq_constant as constant
+from classla.models.common.data import map_to_ids, get_long_tensor, get_float_tensor, sort_all
+from classla.models.mwt.vocab import Vocab
+from classla.models.common.doc import Document
+
+logger = logging.getLogger('classla')
+
+class DataLoader:
+    def __init__(self, doc, batch_size, args, vocab=None, evaluation=False):
+        self.batch_size = batch_size
+        self.args = args
+        self.eval = evaluation
+        self.shuffled = not self.eval
+        self.doc = doc
+
+        data = self.load_doc(self.doc, evaluation=self.eval)
+
+        # handle vocab
+        if vocab is None:
+            self.vocab = self.init_vocab(data)
+        else:
+            self.vocab = vocab
+
+        # filter and sample data
+        if args.get('sample_train', 1.0) < 1.0 and not self.eval:
+            keep = int(args['sample_train'] * len(data))
+            data = random.sample(data, keep)
+            logger.debug("Subsample training set with rate {:g}".format(args['sample_train']))
+
+        data = self.preprocess(data, self.vocab, args)
+        # shuffle for training
+        if self.shuffled:
+            indices = list(range(len(data)))
+            random.shuffle(indices)
+            data = [data[i] for i in indices]
+        self.num_examples = len(data)
+
+        # chunk into batches
+        data = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]
+        self.data = data
+        logger.debug("{} batches created.".format(len(data)))
+
+    def init_vocab(self, data):
+        assert self.eval == False # for eval vocab must exist
+        vocab = Vocab(data, self.args['shorthand'])
+        return vocab
+
+    def preprocess(self, data, vocab, args):
+        processed = []
+        for d in data:
+            src = list(d[0])
+            src = [constant.SOS] + src + [constant.EOS]
+            src = vocab.map(src)
+            if self.eval:
+                tgt = src # as a placeholder
+            else:
+                tgt = list(d[1])
+            tgt_in = vocab.map([constant.SOS] + tgt)
+            tgt_out = vocab.map(tgt + [constant.EOS])
+            processed += [[src, tgt_in, tgt_out]]
+        return processed
+
+    def __len__(self):
+        return len(self.data)
+
+    def __getitem__(self, key):
+        """ Get a batch with index. """
+        if not isinstance(key, int):
+            raise TypeError
+        if key < 0 or key >= len(self.data):
+            raise IndexError
+        batch = self.data[key]
+        batch_size = len(batch)
+        batch = list(zip(*batch))
+        assert len(batch) == 3
+
+        # sort all fields by lens for easy RNN operations
+        lens = [len(x) for x in batch[0]]
+        batch, orig_idx = sort_all(batch, lens)
+
+        # convert to tensors
+        src = batch[0]
+        src = get_long_tensor(src, batch_size)
+        src_mask = torch.eq(src, constant.PAD_ID)
+        tgt_in = get_long_tensor(batch[1], batch_size)
+        tgt_out = get_long_tensor(batch[2], batch_size)
+        assert tgt_in.size(1) == tgt_out.size(1), \
+                "Target input and output sequence sizes do not match."
+        return (src, src_mask, tgt_in, tgt_out, orig_idx)
+
+    def __iter__(self):
+        for i in range(self.__len__()):
+            yield self.__getitem__(i)
+
+    def load_doc(self, doc, evaluation=False):
+        data = doc.get_mwt_expansions(evaluation)
+        if evaluation: data = [[e] for e in data]
+        return data
+
```

### Comparing `classla-2.0/classla/models/mwt/trainer.py` & `classla-2.1/classla/models/mwt/trainer.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,153 +1,153 @@
-"""
-A trainer class to handle training and testing of models.
-"""
-
-import sys
-import numpy as np
-from collections import Counter
-import logging
-import torch
-from torch import nn
-import torch.nn.init as init
-
-import classla.models.common.seq2seq_constant as constant
-from classla.models.common.trainer import Trainer as BaseTrainer
-from classla.models.common.seq2seq_model import Seq2SeqModel
-from classla.models.common import utils, loss
-from classla.models.mwt.vocab import Vocab
-
-logger = logging.getLogger('classla')
-
-def unpack_batch(batch, use_cuda):
-    """ Unpack a batch from the data loader. """
-    if use_cuda:
-        inputs = [b.cuda() if b is not None else None for b in batch[:4]]
-    else:
-        inputs = [b if b is not None else None for b in batch[:4]]
-    orig_idx = batch[4]
-    return inputs, orig_idx
-
-class Trainer(object):
-    """ A trainer for training models. """
-    def __init__(self, args=None, vocab=None, emb_matrix=None, model_file=None, use_cuda=False):
-        self.use_cuda = use_cuda
-        if model_file is not None:
-            # load from file
-            self.load(model_file, use_cuda)
-        else:
-            self.args = args
-            self.model = None if args['dict_only'] else Seq2SeqModel(args, emb_matrix=emb_matrix)
-            self.vocab = vocab
-            self.expansion_dict = dict()
-        if not self.args['dict_only']:
-            self.crit = loss.SequenceLoss(self.vocab.size)
-            self.parameters = [p for p in self.model.parameters() if p.requires_grad]
-            if use_cuda:
-                self.model.cuda()
-                self.crit.cuda()
-            else:
-                self.model.cpu()
-                self.crit.cpu()
-            self.optimizer = utils.get_optimizer(self.args['optim'], self.parameters, self.args['lr'])
-
-    def update(self, batch, eval=False):
-        inputs, orig_idx = unpack_batch(batch, self.use_cuda)
-        src, src_mask, tgt_in, tgt_out = inputs
-
-        if eval:
-            self.model.eval()
-        else:
-            self.model.train()
-            self.optimizer.zero_grad()
-        log_probs, _ = self.model(src, src_mask, tgt_in)
-        loss = self.crit(log_probs.view(-1, self.vocab.size), tgt_out.view(-1))
-        loss_val = loss.data.item()
-        if eval:
-            return loss_val
-
-        loss.backward()
-        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args['max_grad_norm'])
-        self.optimizer.step()
-        return loss_val
-
-    def predict(self, batch, unsort=True):
-        inputs, orig_idx = unpack_batch(batch, self.use_cuda)
-        src, src_mask, tgt, tgt_mask = inputs
-
-        self.model.eval()
-        batch_size = src.size(0)
-        preds, _ = self.model.predict(src, src_mask, self.args['beam_size'])
-        pred_seqs = [self.vocab.unmap(ids) for ids in preds] # unmap to tokens
-        pred_seqs = utils.prune_decoded_seqs(pred_seqs)
-        pred_tokens = ["".join(seq) for seq in pred_seqs] # join chars to be tokens
-        if unsort:
-            pred_tokens = utils.unsort(pred_tokens, orig_idx)
-        return pred_tokens
-
-    def train_dict(self, pairs):
-        """ Train a MWT expander given training word-expansion pairs. """
-        # accumulate counter
-        ctr = Counter()
-        ctr.update([(p[0], p[1]) for p in pairs])
-        seen = set()
-        # find the most frequent mappings
-        for p, _ in ctr.most_common():
-            w, l = p
-            if w not in seen and w != l:
-                self.expansion_dict[w] = l
-            seen.add(w)
-        return
-
-    def predict_dict(self, words):
-        """ Predict a list of expansions given words. """
-        expansions = []
-        for w in words:
-            if w in self.expansion_dict:
-                expansions += [self.expansion_dict[w]]
-            elif w.lower() in self.expansion_dict:
-                expansions += [self.expansion_dict[w.lower()]]
-            else:
-                expansions += [w]
-        return expansions
-
-    def ensemble(self, cands, other_preds):
-        """ Ensemble the dict with statistical model predictions. """
-        expansions = []
-        assert len(cands) == len(other_preds)
-        for c, pred in zip(cands, other_preds):
-            if c in self.expansion_dict:
-                expansions += [self.expansion_dict[c]]
-            elif c.lower() in self.expansion_dict:
-                expansions += [self.expansion_dict[c.lower()]]
-            else:
-                expansions += [pred]
-        return expansions
-
-    def save(self, filename):
-        params = {
-                'model': self.model.state_dict() if self.model is not None else None,
-                'dict': self.expansion_dict,
-                'vocab': self.vocab.state_dict(),
-                'config': self.args
-                }
-        try:
-            torch.save(params, filename)
-            logger.info("Model saved to {}".format(filename))
-        except BaseException:
-            logger.warning("Saving failed... continuing anyway.")
-
-    def load(self, filename, use_cuda=False):
-        try:
-            checkpoint = torch.load(filename, lambda storage, loc: storage)
-        except BaseException:
-            logger.error("Cannot load model from {}".format(filename))
-            raise
-        self.args = checkpoint['config']
-        self.expansion_dict = checkpoint['dict']
-        if not self.args['dict_only']:
-            self.model = Seq2SeqModel(self.args, use_cuda=use_cuda)
-            self.model.load_state_dict(checkpoint['model'])
-        else:
-            self.model = None
-        self.vocab = Vocab.load_state_dict(checkpoint['vocab'])
-
+"""
+A trainer class to handle training and testing of models.
+"""
+
+import sys
+import numpy as np
+from collections import Counter
+import logging
+import torch
+from torch import nn
+import torch.nn.init as init
+
+import classla.models.common.seq2seq_constant as constant
+from classla.models.common.trainer import Trainer as BaseTrainer
+from classla.models.common.seq2seq_model import Seq2SeqModel
+from classla.models.common import utils, loss
+from classla.models.mwt.vocab import Vocab
+
+logger = logging.getLogger('classla')
+
+def unpack_batch(batch, use_cuda):
+    """ Unpack a batch from the data loader. """
+    if use_cuda:
+        inputs = [b.cuda() if b is not None else None for b in batch[:4]]
+    else:
+        inputs = [b if b is not None else None for b in batch[:4]]
+    orig_idx = batch[4]
+    return inputs, orig_idx
+
+class Trainer(object):
+    """ A trainer for training models. """
+    def __init__(self, args=None, vocab=None, emb_matrix=None, model_file=None, use_cuda=False):
+        self.use_cuda = use_cuda
+        if model_file is not None:
+            # load from file
+            self.load(model_file, use_cuda)
+        else:
+            self.args = args
+            self.model = None if args['dict_only'] else Seq2SeqModel(args, emb_matrix=emb_matrix)
+            self.vocab = vocab
+            self.expansion_dict = dict()
+        if not self.args['dict_only']:
+            self.crit = loss.SequenceLoss(self.vocab.size)
+            self.parameters = [p for p in self.model.parameters() if p.requires_grad]
+            if use_cuda:
+                self.model.cuda()
+                self.crit.cuda()
+            else:
+                self.model.cpu()
+                self.crit.cpu()
+            self.optimizer = utils.get_optimizer(self.args['optim'], self.parameters, self.args['lr'])
+
+    def update(self, batch, eval=False):
+        inputs, orig_idx = unpack_batch(batch, self.use_cuda)
+        src, src_mask, tgt_in, tgt_out = inputs
+
+        if eval:
+            self.model.eval()
+        else:
+            self.model.train()
+            self.optimizer.zero_grad()
+        log_probs, _ = self.model(src, src_mask, tgt_in)
+        loss = self.crit(log_probs.view(-1, self.vocab.size), tgt_out.view(-1))
+        loss_val = loss.data.item()
+        if eval:
+            return loss_val
+
+        loss.backward()
+        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args['max_grad_norm'])
+        self.optimizer.step()
+        return loss_val
+
+    def predict(self, batch, unsort=True):
+        inputs, orig_idx = unpack_batch(batch, self.use_cuda)
+        src, src_mask, tgt, tgt_mask = inputs
+
+        self.model.eval()
+        batch_size = src.size(0)
+        preds, _ = self.model.predict(src, src_mask, self.args['beam_size'])
+        pred_seqs = [self.vocab.unmap(ids) for ids in preds] # unmap to tokens
+        pred_seqs = utils.prune_decoded_seqs(pred_seqs)
+        pred_tokens = ["".join(seq) for seq in pred_seqs] # join chars to be tokens
+        if unsort:
+            pred_tokens = utils.unsort(pred_tokens, orig_idx)
+        return pred_tokens
+
+    def train_dict(self, pairs):
+        """ Train a MWT expander given training word-expansion pairs. """
+        # accumulate counter
+        ctr = Counter()
+        ctr.update([(p[0], p[1]) for p in pairs])
+        seen = set()
+        # find the most frequent mappings
+        for p, _ in ctr.most_common():
+            w, l = p
+            if w not in seen and w != l:
+                self.expansion_dict[w] = l
+            seen.add(w)
+        return
+
+    def predict_dict(self, words):
+        """ Predict a list of expansions given words. """
+        expansions = []
+        for w in words:
+            if w in self.expansion_dict:
+                expansions += [self.expansion_dict[w]]
+            elif w.lower() in self.expansion_dict:
+                expansions += [self.expansion_dict[w.lower()]]
+            else:
+                expansions += [w]
+        return expansions
+
+    def ensemble(self, cands, other_preds):
+        """ Ensemble the dict with statistical model predictions. """
+        expansions = []
+        assert len(cands) == len(other_preds)
+        for c, pred in zip(cands, other_preds):
+            if c in self.expansion_dict:
+                expansions += [self.expansion_dict[c]]
+            elif c.lower() in self.expansion_dict:
+                expansions += [self.expansion_dict[c.lower()]]
+            else:
+                expansions += [pred]
+        return expansions
+
+    def save(self, filename):
+        params = {
+                'model': self.model.state_dict() if self.model is not None else None,
+                'dict': self.expansion_dict,
+                'vocab': self.vocab.state_dict(),
+                'config': self.args
+                }
+        try:
+            torch.save(params, filename)
+            logger.info("Model saved to {}".format(filename))
+        except BaseException:
+            logger.warning("Saving failed... continuing anyway.")
+
+    def load(self, filename, use_cuda=False):
+        try:
+            checkpoint = torch.load(filename, lambda storage, loc: storage)
+        except BaseException:
+            logger.error("Cannot load model from {}".format(filename))
+            raise
+        self.args = checkpoint['config']
+        self.expansion_dict = checkpoint['dict']
+        if not self.args['dict_only']:
+            self.model = Seq2SeqModel(self.args, use_cuda=use_cuda)
+            self.model.load_state_dict(checkpoint['model'])
+        else:
+            self.model = None
+        self.vocab = Vocab.load_state_dict(checkpoint['vocab'])
+
```

### Comparing `classla-2.0/classla/models/mwt_expander.py` & `classla-2.1/classla/models/mwt_expander.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,258 +1,258 @@
-"""
-Entry point for training and evaluating a multi-word token (MWT) expander.
-
-This MWT expander combines a neural sequence-to-sequence architecture with a dictionary
-to decode the token into multiple words.
-For details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.
-"""
-
-import sys
-import os
-import shutil
-import time
-from datetime import datetime
-import argparse
-import numpy as np
-import random
-import torch
-from torch import nn, optim
-import copy
-
-from classla.models.mwt.data import DataLoader
-from classla.models.mwt.vocab import Vocab
-from classla.models.mwt.trainer import Trainer
-from classla.models.mwt import scorer
-from classla.models.common import utils
-import classla.models.common.seq2seq_constant as constant
-from classla.models.common.doc import Document
-from classla.utils.conll import CoNLL
-from classla.models import _training_logging
-
-def parse_args():
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--data_dir', type=str, default='data/mwt', help='Root dir for saving models.')
-    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')
-    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')
-    parser.add_argument('--output_file', type=str, default=None, help='Output CoNLL-U file.')
-    parser.add_argument('--gold_file', type=str, default=None, help='Output CoNLL-U file.')
-
-    parser.add_argument('--mode', default='train', choices=['train', 'predict'])
-    parser.add_argument('--lang', type=str, help='Language')
-    parser.add_argument('--shorthand', type=str, help="Treebank shorthand")
-
-    parser.add_argument('--no_dict', dest='ensemble_dict', action='store_false', help='Do not ensemble dictionary with seq2seq. By default ensemble a dict.')
-    parser.add_argument('--ensemble_early_stop', action='store_true', help='Early stopping based on ensemble performance.')
-    parser.add_argument('--dict_only', action='store_true', help='Only train a dictionary-based MWT expander.')
-
-    parser.add_argument('--hidden_dim', type=int, default=100)
-    parser.add_argument('--emb_dim', type=int, default=50)
-    parser.add_argument('--num_layers', type=int, default=1)
-    parser.add_argument('--emb_dropout', type=float, default=0.5)
-    parser.add_argument('--dropout', type=float, default=0.5)
-    parser.add_argument('--max_dec_len', type=int, default=50)
-    parser.add_argument('--beam_size', type=int, default=1)
-    parser.add_argument('--attn_type', default='soft', choices=['soft', 'mlp', 'linear', 'deep'], help='Attention type')
-
-    parser.add_argument('--sample_train', type=float, default=1.0, help='Subsample training data.')
-    parser.add_argument('--optim', type=str, default='adam', help='sgd, adagrad, adam or adamax.')
-    parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate')
-    parser.add_argument('--lr_decay', type=float, default=0.9)
-    parser.add_argument('--decay_epoch', type=int, default=30, help="Decay the lr starting from this epoch.")
-    parser.add_argument('--num_epoch', type=int, default=30)
-    parser.add_argument('--batch_size', type=int, default=50)
-    parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')
-    parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')
-    parser.add_argument('--save_dir', type=str, default='saved_models/mwt', help='Root dir for saving models.')
-    parser.add_argument('--save_name', type=str, default=None, help="File name to save the model")
-
-    parser.add_argument('--seed', type=int, default=1234)
-    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())
-    parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')
-    args = parser.parse_args()
-    return args
-
-def main():
-    args = parse_args()
-
-    torch.manual_seed(args.seed)
-    np.random.seed(args.seed)
-    random.seed(args.seed)
-    if args.cpu:
-        args.cuda = False
-    elif args.cuda:
-        torch.cuda.manual_seed(args.seed)
-
-    args = vars(args)
-    print("Running MWT expander in {} mode".format(args['mode']))
-
-    if args['mode'] == 'train':
-        train(args)
-    else:
-        evaluate(args)
-
-def train(args):
-    # load data
-    print('max_dec_len:', args['max_dec_len'])
-    print("Loading data with batch size {}...".format(args['batch_size']))
-    doc, metasentences = CoNLL.conll2dict(input_file=args['train_file'])
-    train_doc = Document(doc, metasentences=metasentences)
-    train_batch = DataLoader(train_doc, args['batch_size'], args, evaluation=False)
-    vocab = train_batch.vocab
-    args['vocab_size'] = vocab.size
-    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
-    dev_doc = Document(doc, metasentences=metasentences)
-    dev_batch = DataLoader(dev_doc, args['batch_size'], args, vocab=vocab, evaluation=True)
-
-    utils.ensure_dir(args['save_dir'])
-    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
-            else '{}/{}_mwt_expander.pt'.format(args['save_dir'], args['shorthand'])
-
-    # pred and gold path
-    system_pred_file = args['output_file']
-    gold_file = args['gold_file']
-
-    # skip training if the language does not have training or dev data
-    if len(train_batch) == 0 or len(dev_batch) == 0:
-        print("Skip training because no data available...")
-        sys.exit(0)
-
-    # train a dictionary-based MWT expander
-    trainer = Trainer(args=args, vocab=vocab, use_cuda=args['cuda'])
-    print("Training dictionary-based MWT expander...")
-    trainer.train_dict(train_batch.doc.get_mwt_expansions(evaluation=False))
-    print("Evaluating on dev set...")
-    dev_preds = trainer.predict_dict(dev_batch.doc.get_mwt_expansions(evaluation=True))
-    doc = copy.deepcopy(dev_batch.doc)
-    doc.set_mwt_expansions(dev_preds)
-    CoNLL.dict2conll(doc.to_dict(), system_pred_file)
-    _, _, dev_f = scorer.score(system_pred_file, gold_file)
-    print("Dev F1 = {:.2f}".format(dev_f * 100))
-
-    if args.get('dict_only', False):
-        # save dictionaries
-        trainer.save(model_file)
-    else:
-        # train a seq2seq model
-        print("Training seq2seq-based MWT expander...")
-        global_step = 0
-        max_steps = len(train_batch) * args['num_epoch']
-        dev_score_history = []
-        best_dev_preds = []
-        current_lr = args['lr']
-        global_start_time = time.time()
-        format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'
-
-        # start training
-        for epoch in range(1, args['num_epoch']+1):
-            train_loss = 0
-            for i, batch in enumerate(train_batch):
-                start_time = time.time()
-                global_step += 1
-                loss = trainer.update(batch, eval=False) # update step
-                train_loss += loss
-                if global_step % args['log_step'] == 0:
-                    duration = time.time() - start_time
-                    print(format_str.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"), global_step,\
-                            max_steps, epoch, args['num_epoch'], loss, duration, current_lr))
-
-            # eval on dev
-            print("Evaluating on dev set...")
-            dev_preds = []
-            for i, batch in enumerate(dev_batch):
-                preds = trainer.predict(batch)
-                dev_preds += preds
-            if args.get('ensemble_dict', False) and args.get('ensemble_early_stop', False):
-                print("[Ensembling dict with seq2seq model...]")
-                dev_preds = trainer.ensemble(dev_batch.doc.get_mwt_expansions(evaluation=True), dev_preds)
-            doc = copy.deepcopy(dev_batch.doc)
-            doc.set_mwt_expansions(dev_preds)
-            CoNLL.dict2conll(doc.to_dict(), system_pred_file)
-            _, _, dev_score = scorer.score(system_pred_file, gold_file)
-
-            train_loss = train_loss / train_batch.num_examples * args['batch_size'] # avg loss per batch
-            print("epoch {}: train_loss = {:.6f}, dev_score = {:.4f}".format(epoch, train_loss, dev_score))
-
-            # save best model
-            if epoch == 1 or dev_score > max(dev_score_history):
-                trainer.save(model_file)
-                print("new best model saved.")
-                best_dev_preds = dev_preds
-
-            # lr schedule
-            if epoch > args['decay_epoch'] and dev_score <= dev_score_history[-1]:
-                current_lr *= args['lr_decay']
-                trainer.change_lr(current_lr)
-
-            dev_score_history += [dev_score]
-            print("")
-
-        print("Training ended with {} epochs.".format(epoch))
-
-        best_f, best_epoch = max(dev_score_history)*100, np.argmax(dev_score_history)+1
-        print("Best dev F1 = {:.2f}, at epoch = {}".format(best_f, best_epoch))
-
-        # try ensembling with dict if necessary
-        if args.get('ensemble_dict', False):
-            print("[Ensembling dict with seq2seq model...]")
-            dev_preds = trainer.ensemble(dev_batch.doc.get_mwt_expansions(evaluation=True), best_dev_preds)
-            doc = copy.deepcopy(dev_batch.doc)
-            doc.set_mwt_expansions(dev_preds)
-            CoNLL.dict2conll(doc.to_dict(), system_pred_file)
-            _, _, dev_score = scorer.score(system_pred_file, gold_file)
-            print("Ensemble dev F1 = {:.2f}".format(dev_score*100))
-            best_f = max(best_f, dev_score)
-
-def evaluate(args):
-    # file paths
-    system_pred_file = args['output_file']
-    gold_file = args['gold_file']
-    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
-            else '{}/{}_mwt_expander.pt'.format(args['save_dir'], args['shorthand'])
-
-    # load model
-    use_cuda = args['cuda'] and not args['cpu']
-    trainer = Trainer(model_file=model_file, use_cuda=use_cuda)
-    loaded_args, vocab = trainer.args, trainer.vocab
-
-    for k in args:
-        if k.endswith('_dir') or k.endswith('_file') or k in ['shorthand']:
-            loaded_args[k] = args[k]
-    print('max_dec_len:', loaded_args['max_dec_len'])
-
-    # load data
-    print("Loading data with batch size {}...".format(args['batch_size']))
-    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
-    doc = Document(doc, metasentences=metasentences)
-    batch = DataLoader(doc, args['batch_size'], loaded_args, vocab=vocab, evaluation=True)
-
-    if len(batch) > 0:
-        dict_preds = trainer.predict_dict(batch.doc.get_mwt_expansions(evaluation=True))
-        # decide trainer type and run eval
-        if loaded_args['dict_only']:
-            preds = dict_preds
-        else:
-            print("Running the seq2seq model...")
-            preds = []
-            for i, b in enumerate(batch):
-                preds += trainer.predict(b)
-
-            if loaded_args.get('ensemble_dict', False):
-                preds = trainer.ensemble(batch.doc.get_mwt_expansions(evaluation=True), preds)
-    else:
-        # skip eval if dev data does not exist
-        preds = []
-
-    # write to file and score
-    doc = copy.deepcopy(batch.doc)
-    doc.set_mwt_expansions(preds)
-    CoNLL.dict2conll(doc.to_dict(), system_pred_file)
-
-    if gold_file is not None:
-        _, _, score = scorer.score(system_pred_file, gold_file)
-
-        print("MWT expansion score:")
-        print("{} {:.2f}".format(args['shorthand'], score*100))
-
-
-if __name__ == '__main__':
-    main()
+"""
+Entry point for training and evaluating a multi-word token (MWT) expander.
+
+This MWT expander combines a neural sequence-to-sequence architecture with a dictionary
+to decode the token into multiple words.
+For details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.
+"""
+
+import sys
+import os
+import shutil
+import time
+from datetime import datetime
+import argparse
+import numpy as np
+import random
+import torch
+from torch import nn, optim
+import copy
+
+from classla.models.mwt.data import DataLoader
+from classla.models.mwt.vocab import Vocab
+from classla.models.mwt.trainer import Trainer
+from classla.models.mwt import scorer
+from classla.models.common import utils
+import classla.models.common.seq2seq_constant as constant
+from classla.models.common.doc import Document
+from classla.utils.conll import CoNLL
+from classla.models import _training_logging
+
+def parse_args():
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--data_dir', type=str, default='data/mwt', help='Root dir for saving models.')
+    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')
+    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')
+    parser.add_argument('--output_file', type=str, default=None, help='Output CoNLL-U file.')
+    parser.add_argument('--gold_file', type=str, default=None, help='Output CoNLL-U file.')
+
+    parser.add_argument('--mode', default='train', choices=['train', 'predict'])
+    parser.add_argument('--lang', type=str, help='Language')
+    parser.add_argument('--shorthand', type=str, help="Treebank shorthand")
+
+    parser.add_argument('--no_dict', dest='ensemble_dict', action='store_false', help='Do not ensemble dictionary with seq2seq. By default ensemble a dict.')
+    parser.add_argument('--ensemble_early_stop', action='store_true', help='Early stopping based on ensemble performance.')
+    parser.add_argument('--dict_only', action='store_true', help='Only train a dictionary-based MWT expander.')
+
+    parser.add_argument('--hidden_dim', type=int, default=100)
+    parser.add_argument('--emb_dim', type=int, default=50)
+    parser.add_argument('--num_layers', type=int, default=1)
+    parser.add_argument('--emb_dropout', type=float, default=0.5)
+    parser.add_argument('--dropout', type=float, default=0.5)
+    parser.add_argument('--max_dec_len', type=int, default=50)
+    parser.add_argument('--beam_size', type=int, default=1)
+    parser.add_argument('--attn_type', default='soft', choices=['soft', 'mlp', 'linear', 'deep'], help='Attention type')
+
+    parser.add_argument('--sample_train', type=float, default=1.0, help='Subsample training data.')
+    parser.add_argument('--optim', type=str, default='adam', help='sgd, adagrad, adam or adamax.')
+    parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate')
+    parser.add_argument('--lr_decay', type=float, default=0.9)
+    parser.add_argument('--decay_epoch', type=int, default=30, help="Decay the lr starting from this epoch.")
+    parser.add_argument('--num_epoch', type=int, default=30)
+    parser.add_argument('--batch_size', type=int, default=50)
+    parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')
+    parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')
+    parser.add_argument('--save_dir', type=str, default='saved_models/mwt', help='Root dir for saving models.')
+    parser.add_argument('--save_name', type=str, default=None, help="File name to save the model")
+
+    parser.add_argument('--seed', type=int, default=1234)
+    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())
+    parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')
+    args = parser.parse_args()
+    return args
+
+def main():
+    args = parse_args()
+
+    torch.manual_seed(args.seed)
+    np.random.seed(args.seed)
+    random.seed(args.seed)
+    if args.cpu:
+        args.cuda = False
+    elif args.cuda:
+        torch.cuda.manual_seed(args.seed)
+
+    args = vars(args)
+    print("Running MWT expander in {} mode".format(args['mode']))
+
+    if args['mode'] == 'train':
+        train(args)
+    else:
+        evaluate(args)
+
+def train(args):
+    # load data
+    print('max_dec_len:', args['max_dec_len'])
+    print("Loading data with batch size {}...".format(args['batch_size']))
+    doc, metasentences = CoNLL.conll2dict(input_file=args['train_file'])
+    train_doc = Document(doc, metasentences=metasentences)
+    train_batch = DataLoader(train_doc, args['batch_size'], args, evaluation=False)
+    vocab = train_batch.vocab
+    args['vocab_size'] = vocab.size
+    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
+    dev_doc = Document(doc, metasentences=metasentences)
+    dev_batch = DataLoader(dev_doc, args['batch_size'], args, vocab=vocab, evaluation=True)
+
+    utils.ensure_dir(args['save_dir'])
+    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
+            else '{}/{}_mwt_expander.pt'.format(args['save_dir'], args['shorthand'])
+
+    # pred and gold path
+    system_pred_file = args['output_file']
+    gold_file = args['gold_file']
+
+    # skip training if the language does not have training or dev data
+    if len(train_batch) == 0 or len(dev_batch) == 0:
+        print("Skip training because no data available...")
+        sys.exit(0)
+
+    # train a dictionary-based MWT expander
+    trainer = Trainer(args=args, vocab=vocab, use_cuda=args['cuda'])
+    print("Training dictionary-based MWT expander...")
+    trainer.train_dict(train_batch.doc.get_mwt_expansions(evaluation=False))
+    print("Evaluating on dev set...")
+    dev_preds = trainer.predict_dict(dev_batch.doc.get_mwt_expansions(evaluation=True))
+    doc = copy.deepcopy(dev_batch.doc)
+    doc.set_mwt_expansions(dev_preds)
+    CoNLL.dict2conll(doc.to_dict(), system_pred_file)
+    _, _, dev_f = scorer.score(system_pred_file, gold_file)
+    print("Dev F1 = {:.2f}".format(dev_f * 100))
+
+    if args.get('dict_only', False):
+        # save dictionaries
+        trainer.save(model_file)
+    else:
+        # train a seq2seq model
+        print("Training seq2seq-based MWT expander...")
+        global_step = 0
+        max_steps = len(train_batch) * args['num_epoch']
+        dev_score_history = []
+        best_dev_preds = []
+        current_lr = args['lr']
+        global_start_time = time.time()
+        format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'
+
+        # start training
+        for epoch in range(1, args['num_epoch']+1):
+            train_loss = 0
+            for i, batch in enumerate(train_batch):
+                start_time = time.time()
+                global_step += 1
+                loss = trainer.update(batch, eval=False) # update step
+                train_loss += loss
+                if global_step % args['log_step'] == 0:
+                    duration = time.time() - start_time
+                    print(format_str.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"), global_step,\
+                            max_steps, epoch, args['num_epoch'], loss, duration, current_lr))
+
+            # eval on dev
+            print("Evaluating on dev set...")
+            dev_preds = []
+            for i, batch in enumerate(dev_batch):
+                preds = trainer.predict(batch)
+                dev_preds += preds
+            if args.get('ensemble_dict', False) and args.get('ensemble_early_stop', False):
+                print("[Ensembling dict with seq2seq model...]")
+                dev_preds = trainer.ensemble(dev_batch.doc.get_mwt_expansions(evaluation=True), dev_preds)
+            doc = copy.deepcopy(dev_batch.doc)
+            doc.set_mwt_expansions(dev_preds)
+            CoNLL.dict2conll(doc.to_dict(), system_pred_file)
+            _, _, dev_score = scorer.score(system_pred_file, gold_file)
+
+            train_loss = train_loss / train_batch.num_examples * args['batch_size'] # avg loss per batch
+            print("epoch {}: train_loss = {:.6f}, dev_score = {:.4f}".format(epoch, train_loss, dev_score))
+
+            # save best model
+            if epoch == 1 or dev_score > max(dev_score_history):
+                trainer.save(model_file)
+                print("new best model saved.")
+                best_dev_preds = dev_preds
+
+            # lr schedule
+            if epoch > args['decay_epoch'] and dev_score <= dev_score_history[-1]:
+                current_lr *= args['lr_decay']
+                trainer.change_lr(current_lr)
+
+            dev_score_history += [dev_score]
+            print("")
+
+        print("Training ended with {} epochs.".format(epoch))
+
+        best_f, best_epoch = max(dev_score_history)*100, np.argmax(dev_score_history)+1
+        print("Best dev F1 = {:.2f}, at epoch = {}".format(best_f, best_epoch))
+
+        # try ensembling with dict if necessary
+        if args.get('ensemble_dict', False):
+            print("[Ensembling dict with seq2seq model...]")
+            dev_preds = trainer.ensemble(dev_batch.doc.get_mwt_expansions(evaluation=True), best_dev_preds)
+            doc = copy.deepcopy(dev_batch.doc)
+            doc.set_mwt_expansions(dev_preds)
+            CoNLL.dict2conll(doc.to_dict(), system_pred_file)
+            _, _, dev_score = scorer.score(system_pred_file, gold_file)
+            print("Ensemble dev F1 = {:.2f}".format(dev_score*100))
+            best_f = max(best_f, dev_score)
+
+def evaluate(args):
+    # file paths
+    system_pred_file = args['output_file']
+    gold_file = args['gold_file']
+    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
+            else '{}/{}_mwt_expander.pt'.format(args['save_dir'], args['shorthand'])
+
+    # load model
+    use_cuda = args['cuda'] and not args['cpu']
+    trainer = Trainer(model_file=model_file, use_cuda=use_cuda)
+    loaded_args, vocab = trainer.args, trainer.vocab
+
+    for k in args:
+        if k.endswith('_dir') or k.endswith('_file') or k in ['shorthand']:
+            loaded_args[k] = args[k]
+    print('max_dec_len:', loaded_args['max_dec_len'])
+
+    # load data
+    print("Loading data with batch size {}...".format(args['batch_size']))
+    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
+    doc = Document(doc, metasentences=metasentences)
+    batch = DataLoader(doc, args['batch_size'], loaded_args, vocab=vocab, evaluation=True)
+
+    if len(batch) > 0:
+        dict_preds = trainer.predict_dict(batch.doc.get_mwt_expansions(evaluation=True))
+        # decide trainer type and run eval
+        if loaded_args['dict_only']:
+            preds = dict_preds
+        else:
+            print("Running the seq2seq model...")
+            preds = []
+            for i, b in enumerate(batch):
+                preds += trainer.predict(b)
+
+            if loaded_args.get('ensemble_dict', False):
+                preds = trainer.ensemble(batch.doc.get_mwt_expansions(evaluation=True), preds)
+    else:
+        # skip eval if dev data does not exist
+        preds = []
+
+    # write to file and score
+    doc = copy.deepcopy(batch.doc)
+    doc.set_mwt_expansions(preds)
+    CoNLL.dict2conll(doc.to_dict(), system_pred_file)
+
+    if gold_file is not None:
+        _, _, score = scorer.score(system_pred_file, gold_file)
+
+        print("MWT expansion score:")
+        print("{} {:.2f}".format(args['shorthand'], score*100))
+
+
+if __name__ == '__main__':
+    main()
```

### Comparing `classla-2.0/classla/models/ner/data.py` & `classla-2.1/classla/models/ner/data.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,196 +1,196 @@
-import random
-import logging
-import torch
-
-from classla.models.common.data import map_to_ids, get_long_tensor, get_float_tensor, sort_all
-from classla.models.common.vocab import PAD_ID, VOCAB_PREFIX
-from classla.models.pos.vocab import CharVocab, WordVocab
-from classla.models.ner.vocab import TagVocab, MultiVocab
-from classla.models.common.doc import *
-from classla.models.ner.utils import is_bio_scheme, to_bio2, bio2_to_bioes
-
-logger = logging.getLogger('classla')
-
-class DataLoader:
-    def __init__(self, doc, batch_size, args, pretrain=None, vocab=None, evaluation=False, preprocess_tags=True):
-        self.batch_size = batch_size
-        self.args = args
-        self.eval = evaluation
-        self.shuffled = not self.eval
-        self.doc = doc
-        self.preprocess_tags = preprocess_tags
-
-        data = self.load_doc(self.doc)
-        self.tags = [[w[1] for w in sent] for sent in data]
-
-        # handle vocab
-        self.pretrain = pretrain
-        if vocab is None:
-            self.vocab = self.init_vocab(data)
-        else:
-            self.vocab = vocab
-
-        # filter and sample data
-        if args.get('sample_train', 1.0) < 1.0 and not self.eval:
-            keep = int(args['sample_train'] * len(data))
-            data = random.sample(data, keep)
-            logger.debug("Subsample training set with rate {:g}".format(args['sample_train']))
-
-        data = self.preprocess(data, self.vocab, args)
-        # shuffle for training
-        if self.shuffled:
-            random.shuffle(data)
-        self.num_examples = len(data)
-
-        # chunk into batches
-        self.data = self.chunk_batches(data)
-        logger.debug("{} batches created.".format(len(self.data)))
-
-    def init_vocab(self, data):
-        def from_model(model_filename):
-            """ Try loading vocab from charLM model file. """
-            state_dict = torch.load(model_filename, lambda storage, loc: storage)
-            assert 'vocab' in state_dict, "Cannot find vocab in charLM model file."
-            return state_dict['vocab']
-
-        if self.eval:
-            raise Exception("Vocab must exist for evaluation.")
-        if self.args['charlm']:
-            charvocab = CharVocab.load_state_dict(from_model(self.args['charlm_forward_file']))
-        else: 
-            charvocab = CharVocab(data, self.args['shorthand'])
-        wordvocab = self.pretrain.vocab
-        tagvocab = TagVocab(data, self.args['shorthand'], idx=1)
-        vocab = MultiVocab({'char': charvocab,
-                            'word': wordvocab,
-                            'tag': tagvocab})
-        return vocab
-
-    def preprocess(self, data, vocab, args):
-        processed = []
-        if args.get('lowercase', True): # handle word case
-            case = lambda x: x.lower()
-        else:
-            case = lambda x: x
-        if args.get('char_lowercase', False): # handle character case
-            char_case = lambda x: x.lower()
-        else:
-            char_case = lambda x: x
-        for sent in data:
-            processed_sent = [vocab['word'].map([case(w[0]) for w in sent])]
-            processed_sent += [[vocab['char'].map([char_case(x) for x in w[0]]) for w in sent]]
-            processed_sent += [vocab['tag'].map([w[1] for w in sent])]
-            processed.append(processed_sent)
-        return processed
-
-    def __len__(self):
-        return len(self.data)
-
-    def __getitem__(self, key):
-        """ Get a batch with index. """
-        if not isinstance(key, int):
-            raise TypeError
-        if key < 0 or key >= len(self.data):
-            raise IndexError
-        batch = self.data[key]
-        batch_size = len(batch)
-        batch = list(zip(*batch))
-        assert len(batch) == 3 # words: List[List[int]], chars: List[List[List[int]]], tags: List[List[int]]
-
-        # sort sentences by lens for easy RNN operations
-        sentlens = [len(x) for x in batch[0]]
-        batch, orig_idx = sort_all(batch, sentlens)
-        sentlens = [len(x) for x in batch[0]]
-
-        # sort chars by lens for easy char-LM operations
-        chars_forward, chars_backward, charoffsets_forward, charoffsets_backward, charlens = self.process_chars(batch[1])
-        chars_sorted, char_orig_idx = sort_all([chars_forward, chars_backward, charoffsets_forward, charoffsets_backward], charlens)
-        chars_forward, chars_backward, charoffsets_forward, charoffsets_backward = chars_sorted
-        charlens = [len(sent) for sent in chars_forward]
-
-        # sort words by lens for easy char-RNN operations
-        batch_words = [w for sent in batch[1] for w in sent]
-        wordlens = [len(x) for x in batch_words]
-        batch_words, word_orig_idx = sort_all([batch_words], wordlens)
-        batch_words = batch_words[0]
-        wordlens = [len(x) for x in batch_words]
-
-        # convert to tensors
-        words = get_long_tensor(batch[0], batch_size)
-        words_mask = torch.eq(words, PAD_ID)
-        wordchars = get_long_tensor(batch_words, len(wordlens))
-        wordchars_mask = torch.eq(wordchars, PAD_ID)
-        chars_forward = get_long_tensor(chars_forward, batch_size, pad_id=self.vocab['char'].unit2id(' '))
-        chars_backward = get_long_tensor(chars_backward, batch_size, pad_id=self.vocab['char'].unit2id(' '))
-        chars = torch.cat([chars_forward.unsqueeze(0), chars_backward.unsqueeze(0)]) # padded forward and backward char idx
-        charoffsets = [charoffsets_forward, charoffsets_backward] # idx for forward and backward lm to get word representation
-        tags = get_long_tensor(batch[2], batch_size)
-
-        return words, words_mask, wordchars, wordchars_mask, chars, tags, orig_idx, word_orig_idx, char_orig_idx, sentlens, wordlens, charlens, charoffsets
-
-    def __iter__(self):
-        for i in range(self.__len__()):
-            yield self.__getitem__(i)
-
-    def load_doc(self, doc):
-        data = doc.get([TEXT, NER], as_sentences=True, from_token=True)
-        if self.preprocess_tags: # preprocess tags
-            data = self.process_tags(data)
-        return data
-
-    def process_tags(self, sentences):
-        res = []
-        # check if tag conversion is needed
-        convert_to_bioes = False
-        is_bio = is_bio_scheme([x[1] for sent in sentences for x in sent])
-        if is_bio and self.args.get('scheme', 'bio').lower() == 'bioes':
-            convert_to_bioes = True
-            logger.debug("BIO tagging scheme found in input; converting into BIOES scheme...")
-        # process tags
-        for sent in sentences:
-            words, tags = zip(*sent)
-            # NER field sanity checking
-            if any([x is None or x == '_' for x in tags]):
-                raise Exception("NER tag not found for some input data.")
-            # first ensure BIO2 scheme
-            tags = to_bio2(tags)
-            # then convert to BIOES
-            if convert_to_bioes:
-                tags = bio2_to_bioes(tags)
-            res.append([[w,t] for w,t in zip(words, tags)])
-        return res
-
-    def process_chars(self, sents):
-        start_id, end_id = self.vocab['char'].unit2id('\n'), self.vocab['char'].unit2id(' ') # special token
-        start_offset, end_offset = 1, 1
-        chars_forward, chars_backward, charoffsets_forward, charoffsets_backward = [], [], [], []
-        # get char representation for each sentence
-        for sent in sents:
-            chars_forward_sent, chars_backward_sent, charoffsets_forward_sent, charoffsets_backward_sent = [start_id], [start_id], [], []
-            # forward lm
-            for word in sent:
-                chars_forward_sent += word
-                charoffsets_forward_sent = charoffsets_forward_sent + [len(chars_forward_sent)] # add each token offset in the last for forward lm
-                chars_forward_sent += [end_id]
-            # backward lm
-            for word in sent[::-1]:
-                chars_backward_sent += word[::-1]
-                charoffsets_backward_sent = [len(chars_backward_sent)] + charoffsets_backward_sent # add each offset in the first for backward lm
-                chars_backward_sent += [end_id]
-            # store each sentence
-            chars_forward.append(chars_forward_sent)
-            chars_backward.append(chars_backward_sent)
-            charoffsets_forward.append(charoffsets_forward_sent)
-            charoffsets_backward.append(charoffsets_backward_sent)
-        charlens = [len(sent) for sent in chars_forward] # forward lm and backward lm should have the same lengths
-        return chars_forward, chars_backward, charoffsets_forward, charoffsets_backward, charlens
-
-    def reshuffle(self):
-        data = [y for x in self.data for y in x]
-        random.shuffle(data)
-        self.data = self.chunk_batches(data)
-
-    def chunk_batches(self, data):
-        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]
-        return data
-
+import random
+import logging
+import torch
+
+from classla.models.common.data import map_to_ids, get_long_tensor, get_float_tensor, sort_all
+from classla.models.common.vocab import PAD_ID, VOCAB_PREFIX
+from classla.models.pos.vocab import CharVocab, WordVocab
+from classla.models.ner.vocab import TagVocab, MultiVocab
+from classla.models.common.doc import *
+from classla.models.ner.utils import is_bio_scheme, to_bio2, bio2_to_bioes
+
+logger = logging.getLogger('classla')
+
+class DataLoader:
+    def __init__(self, doc, batch_size, args, pretrain=None, vocab=None, evaluation=False, preprocess_tags=True):
+        self.batch_size = batch_size
+        self.args = args
+        self.eval = evaluation
+        self.shuffled = not self.eval
+        self.doc = doc
+        self.preprocess_tags = preprocess_tags
+
+        data = self.load_doc(self.doc)
+        self.tags = [[w[1] for w in sent] for sent in data]
+
+        # handle vocab
+        self.pretrain = pretrain
+        if vocab is None:
+            self.vocab = self.init_vocab(data)
+        else:
+            self.vocab = vocab
+
+        # filter and sample data
+        if args.get('sample_train', 1.0) < 1.0 and not self.eval:
+            keep = int(args['sample_train'] * len(data))
+            data = random.sample(data, keep)
+            logger.debug("Subsample training set with rate {:g}".format(args['sample_train']))
+
+        data = self.preprocess(data, self.vocab, args)
+        # shuffle for training
+        if self.shuffled:
+            random.shuffle(data)
+        self.num_examples = len(data)
+
+        # chunk into batches
+        self.data = self.chunk_batches(data)
+        logger.debug("{} batches created.".format(len(self.data)))
+
+    def init_vocab(self, data):
+        def from_model(model_filename):
+            """ Try loading vocab from charLM model file. """
+            state_dict = torch.load(model_filename, lambda storage, loc: storage)
+            assert 'vocab' in state_dict, "Cannot find vocab in charLM model file."
+            return state_dict['vocab']
+
+        if self.eval:
+            raise Exception("Vocab must exist for evaluation.")
+        if self.args['charlm']:
+            charvocab = CharVocab.load_state_dict(from_model(self.args['charlm_forward_file']))
+        else: 
+            charvocab = CharVocab(data, self.args['shorthand'])
+        wordvocab = self.pretrain.vocab
+        tagvocab = TagVocab(data, self.args['shorthand'], idx=1)
+        vocab = MultiVocab({'char': charvocab,
+                            'word': wordvocab,
+                            'tag': tagvocab})
+        return vocab
+
+    def preprocess(self, data, vocab, args):
+        processed = []
+        if args.get('lowercase', True): # handle word case
+            case = lambda x: x.lower()
+        else:
+            case = lambda x: x
+        if args.get('char_lowercase', False): # handle character case
+            char_case = lambda x: x.lower()
+        else:
+            char_case = lambda x: x
+        for sent in data:
+            processed_sent = [vocab['word'].map([case(w[0]) for w in sent])]
+            processed_sent += [[vocab['char'].map([char_case(x) for x in w[0]]) for w in sent]]
+            processed_sent += [vocab['tag'].map([w[1] for w in sent])]
+            processed.append(processed_sent)
+        return processed
+
+    def __len__(self):
+        return len(self.data)
+
+    def __getitem__(self, key):
+        """ Get a batch with index. """
+        if not isinstance(key, int):
+            raise TypeError
+        if key < 0 or key >= len(self.data):
+            raise IndexError
+        batch = self.data[key]
+        batch_size = len(batch)
+        batch = list(zip(*batch))
+        assert len(batch) == 3 # words: List[List[int]], chars: List[List[List[int]]], tags: List[List[int]]
+
+        # sort sentences by lens for easy RNN operations
+        sentlens = [len(x) for x in batch[0]]
+        batch, orig_idx = sort_all(batch, sentlens)
+        sentlens = [len(x) for x in batch[0]]
+
+        # sort chars by lens for easy char-LM operations
+        chars_forward, chars_backward, charoffsets_forward, charoffsets_backward, charlens = self.process_chars(batch[1])
+        chars_sorted, char_orig_idx = sort_all([chars_forward, chars_backward, charoffsets_forward, charoffsets_backward], charlens)
+        chars_forward, chars_backward, charoffsets_forward, charoffsets_backward = chars_sorted
+        charlens = [len(sent) for sent in chars_forward]
+
+        # sort words by lens for easy char-RNN operations
+        batch_words = [w for sent in batch[1] for w in sent]
+        wordlens = [len(x) for x in batch_words]
+        batch_words, word_orig_idx = sort_all([batch_words], wordlens)
+        batch_words = batch_words[0]
+        wordlens = [len(x) for x in batch_words]
+
+        # convert to tensors
+        words = get_long_tensor(batch[0], batch_size)
+        words_mask = torch.eq(words, PAD_ID)
+        wordchars = get_long_tensor(batch_words, len(wordlens))
+        wordchars_mask = torch.eq(wordchars, PAD_ID)
+        chars_forward = get_long_tensor(chars_forward, batch_size, pad_id=self.vocab['char'].unit2id(' '))
+        chars_backward = get_long_tensor(chars_backward, batch_size, pad_id=self.vocab['char'].unit2id(' '))
+        chars = torch.cat([chars_forward.unsqueeze(0), chars_backward.unsqueeze(0)]) # padded forward and backward char idx
+        charoffsets = [charoffsets_forward, charoffsets_backward] # idx for forward and backward lm to get word representation
+        tags = get_long_tensor(batch[2], batch_size)
+
+        return words, words_mask, wordchars, wordchars_mask, chars, tags, orig_idx, word_orig_idx, char_orig_idx, sentlens, wordlens, charlens, charoffsets
+
+    def __iter__(self):
+        for i in range(self.__len__()):
+            yield self.__getitem__(i)
+
+    def load_doc(self, doc):
+        data = doc.get([TEXT, NER], as_sentences=True, from_token=True)
+        if self.preprocess_tags: # preprocess tags
+            data = self.process_tags(data)
+        return data
+
+    def process_tags(self, sentences):
+        res = []
+        # check if tag conversion is needed
+        convert_to_bioes = False
+        is_bio = is_bio_scheme([x[1] for sent in sentences for x in sent])
+        if is_bio and self.args.get('scheme', 'bio').lower() == 'bioes':
+            convert_to_bioes = True
+            logger.debug("BIO tagging scheme found in input; converting into BIOES scheme...")
+        # process tags
+        for sent in sentences:
+            words, tags = zip(*sent)
+            # NER field sanity checking
+            if any([x is None or x == '_' for x in tags]):
+                raise Exception("NER tag not found for some input data.")
+            # first ensure BIO2 scheme
+            tags = to_bio2(tags)
+            # then convert to BIOES
+            if convert_to_bioes:
+                tags = bio2_to_bioes(tags)
+            res.append([[w,t] for w,t in zip(words, tags)])
+        return res
+
+    def process_chars(self, sents):
+        start_id, end_id = self.vocab['char'].unit2id('\n'), self.vocab['char'].unit2id(' ') # special token
+        start_offset, end_offset = 1, 1
+        chars_forward, chars_backward, charoffsets_forward, charoffsets_backward = [], [], [], []
+        # get char representation for each sentence
+        for sent in sents:
+            chars_forward_sent, chars_backward_sent, charoffsets_forward_sent, charoffsets_backward_sent = [start_id], [start_id], [], []
+            # forward lm
+            for word in sent:
+                chars_forward_sent += word
+                charoffsets_forward_sent = charoffsets_forward_sent + [len(chars_forward_sent)] # add each token offset in the last for forward lm
+                chars_forward_sent += [end_id]
+            # backward lm
+            for word in sent[::-1]:
+                chars_backward_sent += word[::-1]
+                charoffsets_backward_sent = [len(chars_backward_sent)] + charoffsets_backward_sent # add each offset in the first for backward lm
+                chars_backward_sent += [end_id]
+            # store each sentence
+            chars_forward.append(chars_forward_sent)
+            chars_backward.append(chars_backward_sent)
+            charoffsets_forward.append(charoffsets_forward_sent)
+            charoffsets_backward.append(charoffsets_backward_sent)
+        charlens = [len(sent) for sent in chars_forward] # forward lm and backward lm should have the same lengths
+        return chars_forward, chars_backward, charoffsets_forward, charoffsets_backward, charlens
+
+    def reshuffle(self):
+        data = [y for x in self.data for y in x]
+        random.shuffle(data)
+        self.data = self.chunk_batches(data)
+
+    def chunk_batches(self, data):
+        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]
+        return data
+
```

### Comparing `classla-2.0/classla/models/ner/model.py` & `classla-2.1/classla/models/ner/model.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,131 +1,131 @@
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence
-
-from classla.models.common.packed_lstm import PackedLSTM
-from classla.models.common.dropout import WordDropout, LockedDropout
-from classla.models.common.char_model import CharacterModel, CharacterLanguageModel
-from classla.models.common.crf import CRFLoss
-from classla.models.common.vocab import PAD_ID
-
-class NERTagger(nn.Module):
-    def __init__(self, args, vocab, emb_matrix=None):
-        super().__init__()
-
-        self.vocab = vocab
-        self.args = args
-        self.unsaved_modules = []
-
-        def add_unsaved_module(name, module):
-            self.unsaved_modules += [name]
-            setattr(self, name, module)
-
-        # input layers
-        input_size = 0
-        if self.args['word_emb_dim'] > 0:
-            self.word_emb = nn.Embedding(len(self.vocab['word']), self.args['word_emb_dim'], PAD_ID)
-            # load pretrained embeddings if specified
-            if emb_matrix is not None:
-                self.init_emb(emb_matrix)
-            if not self.args.get('emb_finetune', True):
-                self.word_emb.weight.detach_()
-            input_size += self.args['word_emb_dim']
-
-        if self.args['char'] and self.args['char_emb_dim'] > 0:
-            if self.args['charlm']:
-                add_unsaved_module('charmodel_forward', CharacterLanguageModel.load(args['charlm_forward_file'], finetune=False))
-                add_unsaved_module('charmodel_backward', CharacterLanguageModel.load(args['charlm_backward_file'], finetune=False))
-            else:
-                self.charmodel = CharacterModel(args, vocab, bidirectional=True, attention=False)
-            input_size += self.args['char_hidden_dim'] * 2
-        
-        # optionally add a input transformation layer
-        if self.args.get('input_transform', False):
-            self.input_transform = nn.Linear(input_size, input_size)
-        else:
-            self.input_transform = None
-       
-        # recurrent layers
-        self.taggerlstm = PackedLSTM(input_size, self.args['hidden_dim'], self.args['num_layers'], batch_first=True, \
-                bidirectional=True, dropout=0 if self.args['num_layers'] == 1 else self.args['dropout'])
-        # self.drop_replacement = nn.Parameter(torch.randn(input_size) / np.sqrt(input_size))
-        self.drop_replacement = None
-        self.taggerlstm_h_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']), requires_grad=False)
-        self.taggerlstm_c_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']), requires_grad=False)
-
-        # tag classifier
-        num_tag = len(self.vocab['tag'])
-        self.tag_clf = nn.Linear(self.args['hidden_dim']*2, num_tag)
-        self.tag_clf.bias.data.zero_()
-
-        # criterion
-        self.crit = CRFLoss(num_tag)
-
-        self.drop = nn.Dropout(args['dropout'])
-        self.worddrop = WordDropout(args['word_dropout'])
-        self.lockeddrop = LockedDropout(args['locked_dropout'])
-
-    def init_emb(self, emb_matrix):
-        if isinstance(emb_matrix, np.ndarray):
-            emb_matrix = torch.from_numpy(emb_matrix)
-        vocab_size = len(self.vocab['word'])
-        dim = self.args['word_emb_dim']
-        assert emb_matrix.size() == (vocab_size, dim), \
-            "Input embedding matrix must match size: {} x {}".format(vocab_size, dim)
-        self.word_emb.weight.data.copy_(emb_matrix)
-
-    def forward(self, word, word_mask, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx):
-        
-        def pack(x):
-            return pack_padded_sequence(x, sentlens, batch_first=True)
-        
-        inputs = []
-        if self.args['word_emb_dim'] > 0:
-            word_emb = self.word_emb(word)
-            word_emb = pack(word_emb)
-            inputs += [word_emb]
-
-        def pad(x):
-            return pad_packed_sequence(PackedSequence(x, word_emb.batch_sizes), batch_first=True)[0]
-
-        if self.args['char'] and self.args['char_emb_dim'] > 0:
-            if self.args.get('charlm', None):
-                char_reps_forward = self.charmodel_forward.get_representation(chars[0], charoffsets[0], charlens, char_orig_idx)
-                char_reps_forward = PackedSequence(char_reps_forward.data, char_reps_forward.batch_sizes)
-                char_reps_backward = self.charmodel_backward.get_representation(chars[1], charoffsets[1], charlens, char_orig_idx)
-                char_reps_backward = PackedSequence(char_reps_backward.data, char_reps_backward.batch_sizes)
-                inputs += [char_reps_forward, char_reps_backward]
-            else:
-                char_reps = self.charmodel(wordchars, wordchars_mask, word_orig_idx, sentlens, wordlens)
-                char_reps = PackedSequence(char_reps.data, char_reps.batch_sizes)
-                inputs += [char_reps]
-
-        lstm_inputs = torch.cat([x.data for x in inputs], 1)
-        if self.args['word_dropout'] > 0:
-            lstm_inputs = self.worddrop(lstm_inputs, self.drop_replacement)
-        lstm_inputs = self.drop(lstm_inputs)
-        lstm_inputs = pad(lstm_inputs)
-        lstm_inputs = self.lockeddrop(lstm_inputs)
-        lstm_inputs = pack(lstm_inputs).data
-
-        if self.input_transform:
-            lstm_inputs = self.input_transform(lstm_inputs)
-
-        lstm_inputs = PackedSequence(lstm_inputs, inputs[0].batch_sizes)
-        lstm_outputs, _ = self.taggerlstm(lstm_inputs, sentlens, hx=(\
-                self.taggerlstm_h_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous(), \
-                self.taggerlstm_c_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous()))
-        lstm_outputs = lstm_outputs.data
-
-
-        # prediction layer
-        lstm_outputs = self.drop(lstm_outputs)
-        lstm_outputs = pad(lstm_outputs)
-        lstm_outputs = self.lockeddrop(lstm_outputs)
-        lstm_outputs = pack(lstm_outputs).data
-        logits = pad(self.tag_clf(lstm_outputs)).contiguous()
-        loss, trans = self.crit(logits, word_mask, tags)
-        
-        return loss, logits, trans
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence
+
+from classla.models.common.packed_lstm import PackedLSTM
+from classla.models.common.dropout import WordDropout, LockedDropout
+from classla.models.common.char_model import CharacterModel, CharacterLanguageModel
+from classla.models.common.crf import CRFLoss
+from classla.models.common.vocab import PAD_ID
+
+class NERTagger(nn.Module):
+    def __init__(self, args, vocab, emb_matrix=None):
+        super().__init__()
+
+        self.vocab = vocab
+        self.args = args
+        self.unsaved_modules = []
+
+        def add_unsaved_module(name, module):
+            self.unsaved_modules += [name]
+            setattr(self, name, module)
+
+        # input layers
+        input_size = 0
+        if self.args['word_emb_dim'] > 0:
+            self.word_emb = nn.Embedding(len(self.vocab['word']), self.args['word_emb_dim'], PAD_ID)
+            # load pretrained embeddings if specified
+            if emb_matrix is not None:
+                self.init_emb(emb_matrix)
+            if not self.args.get('emb_finetune', True):
+                self.word_emb.weight.detach_()
+            input_size += self.args['word_emb_dim']
+
+        if self.args['char'] and self.args['char_emb_dim'] > 0:
+            if self.args['charlm']:
+                add_unsaved_module('charmodel_forward', CharacterLanguageModel.load(args['charlm_forward_file'], finetune=False))
+                add_unsaved_module('charmodel_backward', CharacterLanguageModel.load(args['charlm_backward_file'], finetune=False))
+            else:
+                self.charmodel = CharacterModel(args, vocab, bidirectional=True, attention=False)
+            input_size += self.args['char_hidden_dim'] * 2
+        
+        # optionally add a input transformation layer
+        if self.args.get('input_transform', False):
+            self.input_transform = nn.Linear(input_size, input_size)
+        else:
+            self.input_transform = None
+       
+        # recurrent layers
+        self.taggerlstm = PackedLSTM(input_size, self.args['hidden_dim'], self.args['num_layers'], batch_first=True, \
+                bidirectional=True, dropout=0 if self.args['num_layers'] == 1 else self.args['dropout'])
+        # self.drop_replacement = nn.Parameter(torch.randn(input_size) / np.sqrt(input_size))
+        self.drop_replacement = None
+        self.taggerlstm_h_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']), requires_grad=False)
+        self.taggerlstm_c_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']), requires_grad=False)
+
+        # tag classifier
+        num_tag = len(self.vocab['tag'])
+        self.tag_clf = nn.Linear(self.args['hidden_dim']*2, num_tag)
+        self.tag_clf.bias.data.zero_()
+
+        # criterion
+        self.crit = CRFLoss(num_tag)
+
+        self.drop = nn.Dropout(args['dropout'])
+        self.worddrop = WordDropout(args['word_dropout'])
+        self.lockeddrop = LockedDropout(args['locked_dropout'])
+
+    def init_emb(self, emb_matrix):
+        if isinstance(emb_matrix, np.ndarray):
+            emb_matrix = torch.from_numpy(emb_matrix)
+        vocab_size = len(self.vocab['word'])
+        dim = self.args['word_emb_dim']
+        assert emb_matrix.size() == (vocab_size, dim), \
+            "Input embedding matrix must match size: {} x {}".format(vocab_size, dim)
+        self.word_emb.weight.data.copy_(emb_matrix)
+
+    def forward(self, word, word_mask, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx):
+        
+        def pack(x):
+            return pack_padded_sequence(x, sentlens, batch_first=True)
+        
+        inputs = []
+        if self.args['word_emb_dim'] > 0:
+            word_emb = self.word_emb(word)
+            word_emb = pack(word_emb)
+            inputs += [word_emb]
+
+        def pad(x):
+            return pad_packed_sequence(PackedSequence(x, word_emb.batch_sizes), batch_first=True)[0]
+
+        if self.args['char'] and self.args['char_emb_dim'] > 0:
+            if self.args.get('charlm', None):
+                char_reps_forward = self.charmodel_forward.get_representation(chars[0], charoffsets[0], charlens, char_orig_idx)
+                char_reps_forward = PackedSequence(char_reps_forward.data, char_reps_forward.batch_sizes)
+                char_reps_backward = self.charmodel_backward.get_representation(chars[1], charoffsets[1], charlens, char_orig_idx)
+                char_reps_backward = PackedSequence(char_reps_backward.data, char_reps_backward.batch_sizes)
+                inputs += [char_reps_forward, char_reps_backward]
+            else:
+                char_reps = self.charmodel(wordchars, wordchars_mask, word_orig_idx, sentlens, wordlens)
+                char_reps = PackedSequence(char_reps.data, char_reps.batch_sizes)
+                inputs += [char_reps]
+
+        lstm_inputs = torch.cat([x.data for x in inputs], 1)
+        if self.args['word_dropout'] > 0:
+            lstm_inputs = self.worddrop(lstm_inputs, self.drop_replacement)
+        lstm_inputs = self.drop(lstm_inputs)
+        lstm_inputs = pad(lstm_inputs)
+        lstm_inputs = self.lockeddrop(lstm_inputs)
+        lstm_inputs = pack(lstm_inputs).data
+
+        if self.input_transform:
+            lstm_inputs = self.input_transform(lstm_inputs)
+
+        lstm_inputs = PackedSequence(lstm_inputs, inputs[0].batch_sizes)
+        lstm_outputs, _ = self.taggerlstm(lstm_inputs, sentlens, hx=(\
+                self.taggerlstm_h_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous(), \
+                self.taggerlstm_c_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous()))
+        lstm_outputs = lstm_outputs.data
+
+
+        # prediction layer
+        lstm_outputs = self.drop(lstm_outputs)
+        lstm_outputs = pad(lstm_outputs)
+        lstm_outputs = self.lockeddrop(lstm_outputs)
+        lstm_outputs = pack(lstm_outputs).data
+        logits = pad(self.tag_clf(lstm_outputs)).contiguous()
+        loss, trans = self.crit(logits, word_mask, tags)
+        
+        return loss, logits, trans
```

### Comparing `classla-2.0/classla/models/ner/scorer.py` & `classla-2.1/classla/models/ner/scorer.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,128 +1,128 @@
-"""
-An NER scorer that calculates F1 score given gold and predicted tags.
-"""
-import sys
-import os
-import logging
-from collections import Counter
-
-from classla.models.ner.utils import decode_from_bioes
-
-logger = logging.getLogger('classla')
-
-def score_by_entity(pred_tag_sequences, gold_tag_sequences, verbose=True):
-    """ Score predicted tags at the entity level.
-
-    Args:
-        pred_tags_sequences: a list of list of predicted tags for each word
-        gold_tags_sequences: a list of list of gold tags for each word
-        verbose: print log with results
-    
-    Returns:
-        Precision, recall and F1 scores.
-    """
-    assert(len(gold_tag_sequences) == len(pred_tag_sequences)), \
-        "Number of predicted tag sequences does not match gold sequences."
-    
-    def decode_all(tag_sequences):
-        # decode from all sequences, each sequence with a unique id
-        ents = []
-        for sent_id, tags in enumerate(tag_sequences):
-            for ent in decode_from_bioes([e.upper() for e in tags]): # dirty hack given that predictions are uppercased due to a consistent bug in training data
-                ent['sent_id'] = sent_id
-                ents += [ent]
-        return ents
-
-    gold_ents = decode_all(gold_tag_sequences)
-    pred_ents = decode_all(pred_tag_sequences)
-
-    # scoring
-    correct_by_type = Counter()
-    guessed_by_type = Counter()
-    gold_by_type = Counter()
-
-    for p in pred_ents:
-        guessed_by_type[p['type']] += 1
-        if p in gold_ents:
-            correct_by_type[p['type']] += 1
-    for g in gold_ents:
-        gold_by_type[g['type']] += 1
-    
-    prec_micro = 0.0
-    if sum(guessed_by_type.values()) > 0:
-        prec_micro = sum(correct_by_type.values()) * 1.0 / sum(guessed_by_type.values())
-    rec_micro = 0.0
-    if sum(gold_by_type.values()) > 0:
-        rec_micro = sum(correct_by_type.values()) * 1.0 / sum(gold_by_type.values())
-    f_micro = 0.0
-    if prec_micro + rec_micro > 0:
-        f_micro = 2.0 * prec_micro * rec_micro / (prec_micro + rec_micro)
-    
-    if verbose:
-        logger.info("Prec.\tRec.\tF1")
-        logger.info("{:.2f}\t{:.2f}\t{:.2f}".format( \
-            prec_micro*100, rec_micro*100, f_micro*100))
-    return prec_micro, rec_micro, f_micro
-
-
-def score_by_token(pred_tag_sequences, gold_tag_sequences, verbose=True):
-    """ Score predicted tags at the token level.
-
-    Args:
-        pred_tags_sequences: a list of list of predicted tags for each word
-        gold_tags_sequences: a list of list of gold tags for each word
-        verbose: print log with results
-    
-    Returns:
-        Precision, recall and F1 scores.
-    """
-    assert(len(gold_tag_sequences) == len(pred_tag_sequences)), \
-        "Number of predicted tag sequences does not match gold sequences."
-    
-    correct_by_tag = Counter()
-    guessed_by_tag = Counter()
-    gold_by_tag = Counter()
-
-    for gold_tags, pred_tags in zip(gold_tag_sequences, pred_tag_sequences):
-        assert(len(gold_tags) == len(pred_tags)), \
-            "Number of predicted tags does not match gold."
-        for g, p in zip(gold_tags, pred_tags):
-            if g == 'O' and p == 'O':
-                continue
-            elif g == 'O' and p != 'O':
-                guessed_by_tag[p] += 1
-            elif g != 'O' and p == 'O':
-                gold_by_tag[g] += 1
-            else:
-                guessed_by_tag[p] += 1
-                gold_by_tag[p] += 1
-                if g == p:
-                    correct_by_tag[p] += 1
-    
-    prec_micro = 0.0
-    if sum(guessed_by_tag.values()) > 0:
-        prec_micro = sum(correct_by_tag.values()) * 1.0 / sum(guessed_by_tag.values())
-    rec_micro = 0.0
-    if sum(gold_by_tag.values()) > 0:
-        rec_micro = sum(correct_by_tag.values()) * 1.0 / sum(gold_by_tag.values())
-    f_micro = 0.0
-    if prec_micro + rec_micro > 0:
-        f_micro = 2.0 * prec_micro * rec_micro / (prec_micro + rec_micro)
-    
-    if verbose:
-        logger.info("Prec.\tRec.\tF1")
-        logger.info("{:.2f}\t{:.2f}\t{:.2f}".format( \
-            prec_micro*100, rec_micro*100, f_micro*100))
-    return prec_micro, rec_micro, f_micro
-
-def test():
-    pred_sequences = [['O', 'S-LOC', 'O', 'O', 'B-PER', 'E-PER'],
-                    ['O', 'S-MISC', 'O', 'E-ORG', 'O', 'B-PER', 'I-PER', 'E-PER']]
-    gold_sequences = [['O', 'B-LOC', 'E-LOC', 'O', 'B-PER', 'E-PER'],
-                    ['O', 'S-MISC', 'B-ORG', 'E-ORG', 'O', 'B-PER', 'E-PER', 'S-LOC']]
-    print(score_by_token(pred_sequences, gold_sequences))
-    print(score_by_entity(pred_sequences, gold_sequences))
-
-if __name__ == '__main__':
-    test()
-
+"""
+An NER scorer that calculates F1 score given gold and predicted tags.
+"""
+import sys
+import os
+import logging
+from collections import Counter
+
+from classla.models.ner.utils import decode_from_bioes
+
+logger = logging.getLogger('classla')
+
+def score_by_entity(pred_tag_sequences, gold_tag_sequences, verbose=True):
+    """ Score predicted tags at the entity level.
+
+    Args:
+        pred_tags_sequences: a list of list of predicted tags for each word
+        gold_tags_sequences: a list of list of gold tags for each word
+        verbose: print log with results
+    
+    Returns:
+        Precision, recall and F1 scores.
+    """
+    assert(len(gold_tag_sequences) == len(pred_tag_sequences)), \
+        "Number of predicted tag sequences does not match gold sequences."
+    
+    def decode_all(tag_sequences):
+        # decode from all sequences, each sequence with a unique id
+        ents = []
+        for sent_id, tags in enumerate(tag_sequences):
+            for ent in decode_from_bioes([e.upper() for e in tags]): # dirty hack given that predictions are uppercased due to a consistent bug in training data
+                ent['sent_id'] = sent_id
+                ents += [ent]
+        return ents
+
+    gold_ents = decode_all(gold_tag_sequences)
+    pred_ents = decode_all(pred_tag_sequences)
+
+    # scoring
+    correct_by_type = Counter()
+    guessed_by_type = Counter()
+    gold_by_type = Counter()
+
+    for p in pred_ents:
+        guessed_by_type[p['type']] += 1
+        if p in gold_ents:
+            correct_by_type[p['type']] += 1
+    for g in gold_ents:
+        gold_by_type[g['type']] += 1
+    
+    prec_micro = 0.0
+    if sum(guessed_by_type.values()) > 0:
+        prec_micro = sum(correct_by_type.values()) * 1.0 / sum(guessed_by_type.values())
+    rec_micro = 0.0
+    if sum(gold_by_type.values()) > 0:
+        rec_micro = sum(correct_by_type.values()) * 1.0 / sum(gold_by_type.values())
+    f_micro = 0.0
+    if prec_micro + rec_micro > 0:
+        f_micro = 2.0 * prec_micro * rec_micro / (prec_micro + rec_micro)
+    
+    if verbose:
+        logger.info("Prec.\tRec.\tF1")
+        logger.info("{:.2f}\t{:.2f}\t{:.2f}".format( \
+            prec_micro*100, rec_micro*100, f_micro*100))
+    return prec_micro, rec_micro, f_micro
+
+
+def score_by_token(pred_tag_sequences, gold_tag_sequences, verbose=True):
+    """ Score predicted tags at the token level.
+
+    Args:
+        pred_tags_sequences: a list of list of predicted tags for each word
+        gold_tags_sequences: a list of list of gold tags for each word
+        verbose: print log with results
+    
+    Returns:
+        Precision, recall and F1 scores.
+    """
+    assert(len(gold_tag_sequences) == len(pred_tag_sequences)), \
+        "Number of predicted tag sequences does not match gold sequences."
+    
+    correct_by_tag = Counter()
+    guessed_by_tag = Counter()
+    gold_by_tag = Counter()
+
+    for gold_tags, pred_tags in zip(gold_tag_sequences, pred_tag_sequences):
+        assert(len(gold_tags) == len(pred_tags)), \
+            "Number of predicted tags does not match gold."
+        for g, p in zip(gold_tags, pred_tags):
+            if g == 'O' and p == 'O':
+                continue
+            elif g == 'O' and p != 'O':
+                guessed_by_tag[p] += 1
+            elif g != 'O' and p == 'O':
+                gold_by_tag[g] += 1
+            else:
+                guessed_by_tag[p] += 1
+                gold_by_tag[p] += 1
+                if g == p:
+                    correct_by_tag[p] += 1
+    
+    prec_micro = 0.0
+    if sum(guessed_by_tag.values()) > 0:
+        prec_micro = sum(correct_by_tag.values()) * 1.0 / sum(guessed_by_tag.values())
+    rec_micro = 0.0
+    if sum(gold_by_tag.values()) > 0:
+        rec_micro = sum(correct_by_tag.values()) * 1.0 / sum(gold_by_tag.values())
+    f_micro = 0.0
+    if prec_micro + rec_micro > 0:
+        f_micro = 2.0 * prec_micro * rec_micro / (prec_micro + rec_micro)
+    
+    if verbose:
+        logger.info("Prec.\tRec.\tF1")
+        logger.info("{:.2f}\t{:.2f}\t{:.2f}".format( \
+            prec_micro*100, rec_micro*100, f_micro*100))
+    return prec_micro, rec_micro, f_micro
+
+def test():
+    pred_sequences = [['O', 'S-LOC', 'O', 'O', 'B-PER', 'E-PER'],
+                    ['O', 'S-MISC', 'O', 'E-ORG', 'O', 'B-PER', 'I-PER', 'E-PER']]
+    gold_sequences = [['O', 'B-LOC', 'E-LOC', 'O', 'B-PER', 'E-PER'],
+                    ['O', 'S-MISC', 'B-ORG', 'E-ORG', 'O', 'B-PER', 'E-PER', 'S-LOC']]
+    print(score_by_token(pred_sequences, gold_sequences))
+    print(score_by_entity(pred_sequences, gold_sequences))
+
+if __name__ == '__main__':
+    test()
+
```

### Comparing `classla-2.0/classla/models/ner/trainer.py` & `classla-2.1/classla/models/ner/trainer.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,134 +1,134 @@
-"""
-A trainer class to handle training and testing of models.
-"""
-
-import sys
-import logging
-import torch
-from torch import nn
-
-from classla.models.common.trainer import Trainer as BaseTrainer
-from classla.models.common import utils, loss
-from classla.models.ner.model import NERTagger
-from classla.models.ner.vocab import MultiVocab
-from classla.models.common.crf import viterbi_decode
-
-logger = logging.getLogger('classla')
-
-def unpack_batch(batch, use_cuda):
-    """ Unpack a batch from the data loader. """
-    if use_cuda:
-        inputs = [b.cuda() if b is not None else None for b in batch[:6]]
-    else:
-        inputs = batch[:6]
-    orig_idx = batch[6]
-    word_orig_idx = batch[7]
-    char_orig_idx = batch[8]
-    sentlens = batch[9]
-    wordlens = batch[10]
-    charlens = batch[11]
-    charoffsets = batch[12]
-    return inputs, orig_idx, word_orig_idx, char_orig_idx, sentlens, wordlens, charlens, charoffsets
-
-class Trainer(BaseTrainer):
-    """ A trainer for training models. """
-    def __init__(self, args=None, vocab=None, pretrain=None, model_file=None, use_cuda=False):
-        self.use_cuda = use_cuda
-        if model_file is not None:
-            # load everything from file
-            self.load(model_file, args)
-        else:
-            assert all(var is not None for var in [args, vocab, pretrain])
-            # build model from scratch
-            self.args = args
-            self.vocab = vocab
-            self.model = NERTagger(args, vocab, emb_matrix=pretrain.emb)
-        self.parameters = [p for p in self.model.parameters() if p.requires_grad]
-        if self.use_cuda:
-            self.model.cuda()
-        else:
-            self.model.cpu()
-        self.optimizer = utils.get_optimizer(self.args['optim'], self.parameters, self.args['lr'], momentum=self.args['momentum'])
-
-    def update(self, batch, eval=False):
-        inputs, orig_idx, word_orig_idx, char_orig_idx, sentlens, wordlens, charlens, charoffsets = unpack_batch(batch, self.use_cuda)
-        word, word_mask, wordchars, wordchars_mask, chars, tags = inputs
-
-        if eval:
-            self.model.eval()
-        else:
-            self.model.train()
-            self.optimizer.zero_grad()
-        loss, _, _ = self.model(word, word_mask, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx)
-        loss_val = loss.data.item()
-        if eval:
-            return loss_val
-
-        loss.backward()
-        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args['max_grad_norm'])
-        self.optimizer.step()
-        return loss_val
-
-    def predict(self, batch, unsort=True):
-        inputs, orig_idx, word_orig_idx, char_orig_idx, sentlens, wordlens, charlens, charoffsets = unpack_batch(batch, self.use_cuda)
-        word, word_mask, wordchars, wordchars_mask, chars, tags = inputs
-
-        self.model.eval()
-        batch_size = word.size(0)
-        _, logits, trans = self.model(word, word_mask, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx)
-
-        # decode
-        trans = trans.data.cpu().numpy()
-        scores = logits.data.cpu().numpy()
-        bs = logits.size(0)
-        tag_seqs = []
-        for i in range(bs):
-            tags, _ = viterbi_decode(scores[i, :sentlens[i]], trans)
-            tag_names = []
-            used_tags = set()
-            # hand-fixed BIO tags that don't begin with B but I
-            for e in self.vocab['tag'].unmap(tags):
-                tag = e.upper()
-                entity_type = tag[2:]
-                if tag != 'O' and tag[0] == 'I' and entity_type not in used_tags:
-                    tag = 'B-' + entity_type
-                used_tags.add(entity_type)
-                tag_names.append(tag) # uppercased tags, dirty hack to have unified NER tags, to be removed once training datasets are corrected
-            tag_seqs += [tag_names]
-
-        if unsort:
-            tag_seqs = utils.unsort(tag_seqs, orig_idx)
-        return tag_seqs
-
-    def save(self, filename, skip_modules=True):
-        model_state = self.model.state_dict()
-        # skip saving modules like pretrained embeddings, because they are large and will be saved in a separate file
-        if skip_modules:
-            skipped = [k for k in model_state.keys() if k.split('.')[0] in self.model.unsaved_modules]
-            for k in skipped:
-                del model_state[k]
-        params = {
-                'model': model_state,
-                'vocab': self.vocab.state_dict(),
-                'config': self.args
-                }
-        try:
-            torch.save(params, filename)
-            logger.info("Model saved to {}".format(filename))
-        except (KeyboardInterrupt, SystemExit):
-            raise
-        except:
-            logger.warning("Saving failed... continuing anyway.")
-
-    def load(self, filename, args=None):
-        try:
-            checkpoint = torch.load(filename, lambda storage, loc: storage)
-        except BaseException:
-            logger.error("Cannot load model from {}".format(filename))
-            raise
-        self.args = checkpoint['config']
-        if args: self.args.update(args)
-        self.vocab = MultiVocab.load_state_dict(checkpoint['vocab'])
-        self.model = NERTagger(self.args, self.vocab)
-        self.model.load_state_dict(checkpoint['model'], strict=False)
-
+"""
+A trainer class to handle training and testing of models.
+"""
+
+import sys
+import logging
+import torch
+from torch import nn
+
+from classla.models.common.trainer import Trainer as BaseTrainer
+from classla.models.common import utils, loss
+from classla.models.ner.model import NERTagger
+from classla.models.ner.vocab import MultiVocab
+from classla.models.common.crf import viterbi_decode
+
+logger = logging.getLogger('classla')
+
+def unpack_batch(batch, use_cuda):
+    """ Unpack a batch from the data loader. """
+    if use_cuda:
+        inputs = [b.cuda() if b is not None else None for b in batch[:6]]
+    else:
+        inputs = batch[:6]
+    orig_idx = batch[6]
+    word_orig_idx = batch[7]
+    char_orig_idx = batch[8]
+    sentlens = batch[9]
+    wordlens = batch[10]
+    charlens = batch[11]
+    charoffsets = batch[12]
+    return inputs, orig_idx, word_orig_idx, char_orig_idx, sentlens, wordlens, charlens, charoffsets
+
+class Trainer(BaseTrainer):
+    """ A trainer for training models. """
+    def __init__(self, args=None, vocab=None, pretrain=None, model_file=None, use_cuda=False):
+        self.use_cuda = use_cuda
+        if model_file is not None:
+            # load everything from file
+            self.load(model_file, args)
+        else:
+            assert all(var is not None for var in [args, vocab, pretrain])
+            # build model from scratch
+            self.args = args
+            self.vocab = vocab
+            self.model = NERTagger(args, vocab, emb_matrix=pretrain.emb)
+        self.parameters = [p for p in self.model.parameters() if p.requires_grad]
+        if self.use_cuda:
+            self.model.cuda()
+        else:
+            self.model.cpu()
+        self.optimizer = utils.get_optimizer(self.args['optim'], self.parameters, self.args['lr'], momentum=self.args['momentum'])
+
+    def update(self, batch, eval=False):
+        inputs, orig_idx, word_orig_idx, char_orig_idx, sentlens, wordlens, charlens, charoffsets = unpack_batch(batch, self.use_cuda)
+        word, word_mask, wordchars, wordchars_mask, chars, tags = inputs
+
+        if eval:
+            self.model.eval()
+        else:
+            self.model.train()
+            self.optimizer.zero_grad()
+        loss, _, _ = self.model(word, word_mask, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx)
+        loss_val = loss.data.item()
+        if eval:
+            return loss_val
+
+        loss.backward()
+        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args['max_grad_norm'])
+        self.optimizer.step()
+        return loss_val
+
+    def predict(self, batch, unsort=True):
+        inputs, orig_idx, word_orig_idx, char_orig_idx, sentlens, wordlens, charlens, charoffsets = unpack_batch(batch, self.use_cuda)
+        word, word_mask, wordchars, wordchars_mask, chars, tags = inputs
+
+        self.model.eval()
+        batch_size = word.size(0)
+        _, logits, trans = self.model(word, word_mask, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx)
+
+        # decode
+        trans = trans.data.cpu().numpy()
+        scores = logits.data.cpu().numpy()
+        bs = logits.size(0)
+        tag_seqs = []
+        for i in range(bs):
+            tags, _ = viterbi_decode(scores[i, :sentlens[i]], trans)
+            tag_names = []
+            used_tags = set()
+            # hand-fixed BIO tags that don't begin with B but I
+            for e in self.vocab['tag'].unmap(tags):
+                tag = e.upper()
+                entity_type = tag[2:]
+                if tag != 'O' and tag[0] == 'I' and entity_type not in used_tags:
+                    tag = 'B-' + entity_type
+                used_tags.add(entity_type)
+                tag_names.append(tag) # uppercased tags, dirty hack to have unified NER tags, to be removed once training datasets are corrected
+            tag_seqs += [tag_names]
+
+        if unsort:
+            tag_seqs = utils.unsort(tag_seqs, orig_idx)
+        return tag_seqs
+
+    def save(self, filename, skip_modules=True):
+        model_state = self.model.state_dict()
+        # skip saving modules like pretrained embeddings, because they are large and will be saved in a separate file
+        if skip_modules:
+            skipped = [k for k in model_state.keys() if k.split('.')[0] in self.model.unsaved_modules]
+            for k in skipped:
+                del model_state[k]
+        params = {
+                'model': model_state,
+                'vocab': self.vocab.state_dict(),
+                'config': self.args
+                }
+        try:
+            torch.save(params, filename)
+            logger.info("Model saved to {}".format(filename))
+        except (KeyboardInterrupt, SystemExit):
+            raise
+        except:
+            logger.warning("Saving failed... continuing anyway.")
+
+    def load(self, filename, args=None):
+        try:
+            checkpoint = torch.load(filename, lambda storage, loc: storage)
+        except BaseException:
+            logger.error("Cannot load model from {}".format(filename))
+            raise
+        self.args = checkpoint['config']
+        if args: self.args.update(args)
+        self.vocab = MultiVocab.load_state_dict(checkpoint['vocab'])
+        self.model = NERTagger(self.args, self.vocab)
+        self.model.load_state_dict(checkpoint['model'], strict=False)
+
```

### Comparing `classla-2.0/classla/models/ner/vocab.py` & `classla-2.1/classla/models/srl/vocab.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,39 +1,31 @@
-from collections import Counter, OrderedDict
-
-from classla.models.common.vocab import BaseVocab, BaseMultiVocab
-from classla.models.common.vocab import VOCAB_PREFIX
-from classla.models.common.pretrain import PretrainedWordVocab
-from classla.models.pos.vocab import CharVocab, WordVocab
-
-class TagVocab(BaseVocab):
-    """ A vocab for the output tag sequence. """
-    def build_vocab(self):
-        counter = Counter([w[self.idx] for sent in self.data for w in sent])
-
-        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))
-        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}
-
-class MultiVocab(BaseMultiVocab):
-    def state_dict(self):
-        """ Also save a vocab name to class name mapping in state dict. """
-        state = OrderedDict()
-        key2class = OrderedDict()
-        for k, v in self._vocabs.items():
-            state[k] = v.state_dict()
-            key2class[k] = type(v).__name__
-        state['_key2class'] = key2class
-        return state
-
-    @classmethod
-    def load_state_dict(cls, state_dict):
-        class_dict = {'CharVocab': CharVocab,
-                'PretrainedWordVocab': PretrainedWordVocab,
-                'TagVocab': TagVocab}
-        new = cls()
-        assert '_key2class' in state_dict, "Cannot find class name mapping in state dict!"
-        key2class = state_dict.pop('_key2class')
-        for k,v in state_dict.items():
-            classname = key2class[k]
-            new[k] = class_dict[classname].load_state_dict(v)
-        return new
-
+from collections import Counter, OrderedDict
+
+from classla.models.common.vocab import BaseVocab, BaseMultiVocab
+from classla.models.common.vocab import VOCAB_PREFIX
+from classla.models.lemma.vocab import Vocab
+from classla.models.pos.vocab import WordVocab
+
+
+class MultiVocab(BaseMultiVocab):
+    def state_dict(self):
+        """ Also save a vocab name to class name mapping in state dict. """
+        state = OrderedDict()
+        key2class = OrderedDict()
+        for k, v in self._vocabs.items():
+            state[k] = v.state_dict()
+            key2class[k] = type(v).__name__
+        state['_key2class'] = key2class
+        return state
+
+    @classmethod
+    def load_state_dict(cls, state_dict):
+        class_dict = {'WordVocab': WordVocab,
+                      'Vocab': Vocab}
+        new = cls()
+        assert '_key2class' in state_dict, "Cannot find class name mapping in state dict!"
+        key2class = state_dict.pop('_key2class')
+        for k,v in state_dict.items():
+            classname = key2class[k]
+            new[k] = class_dict[classname].load_state_dict(v)
+        return new
+
```

### Comparing `classla-2.0/classla/models/ner_tagger.py` & `classla-2.1/classla/models/ner_tagger.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,250 +1,250 @@
-"""
-Entry point for training and evaluating an NER tagger.
-
-This tagger uses BiLSTM layers with character and word-level representations, and a CRF decoding layer 
-to produce NER predictions.
-For details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.
-"""
-
-import sys
-import os
-import time
-from datetime import datetime
-import argparse
-import logging
-import numpy as np
-import random
-import json
-import torch
-from torch import nn, optim
-
-from classla.models.ner.data import DataLoader
-from classla.models.ner.trainer import Trainer
-from classla.models.ner import scorer
-from classla.models.common import utils
-from classla.models.common.pretrain import Pretrain
-from classla.utils.conll import CoNLL
-from classla.models.common.doc import *
-from classla.models import _training_logging
-
-logger = logging.getLogger('classla')
-
-def parse_args(args=None):
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--data_dir', type=str, default='data/ner', help='Root dir for saving models.')
-    parser.add_argument('--wordvec_dir', type=str, default='extern_data/word2vec', help='Directory of word vectors')
-    parser.add_argument('--wordvec_file', type=str, default='', help='File that contains word vectors')
-    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')
-    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')
-
-    parser.add_argument('--mode', default='train', choices=['train', 'predict'])
-    parser.add_argument('--lang', type=str, help='Language')
-    parser.add_argument('--shorthand', type=str, help="Treebank shorthand")
-
-    parser.add_argument('--hidden_dim', type=int, default=256)
-    parser.add_argument('--char_hidden_dim', type=int, default=100)
-    parser.add_argument('--word_emb_dim', type=int, default=100)
-    parser.add_argument('--char_emb_dim', type=int, default=100)
-    parser.add_argument('--num_layers', type=int, default=1)
-    parser.add_argument('--char_num_layers', type=int, default=1)
-    parser.add_argument('--pretrain_max_vocab', type=int, default=100000)
-    parser.add_argument('--word_dropout', type=float, default=0)
-    parser.add_argument('--locked_dropout', type=float, default=0.0)
-    parser.add_argument('--dropout', type=float, default=0.5)
-    parser.add_argument('--rec_dropout', type=float, default=0, help="Word recurrent dropout")
-    parser.add_argument('--char_rec_dropout', type=float, default=0, help="Character recurrent dropout")
-    parser.add_argument('--char_dropout', type=float, default=0, help="Character-level language model dropout")
-    parser.add_argument('--no_char', dest='char', action='store_false', help="Turn off character model.")
-    parser.add_argument('--charlm', action='store_true', help="Turn on contextualized char embedding using character-level language model.")
-    parser.add_argument('--charlm_save_dir', type=str, default='saved_models/charlm', help="Root dir for pretrained character-level language model.")
-    parser.add_argument('--charlm_shorthand', type=str, default=None, help="Shorthand for character-level language model training corpus.")
-    parser.add_argument('--char_lowercase', dest='char_lowercase', action='store_true', help="Use lowercased characters in charater model.")
-    parser.add_argument('--no_lowercase', dest='lowercase', action='store_false', help="Use cased word vectors.")
-    parser.add_argument('--no_emb_finetune', dest='emb_finetune', action='store_false', help="Turn off finetuning of the embedding matrix.")
-    parser.add_argument('--no_input_transform', dest='input_transform', action='store_false', help="Do not use input transformation layer before tagger lstm.")
-    parser.add_argument('--scheme', type=str, default='bioes', help="The tagging scheme to use: bio or bioes.")
-
-    parser.add_argument('--sample_train', type=float, default=1.0, help='Subsample training data.')
-    parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')
-    parser.add_argument('--lr', type=float, default=0.1, help='Learning rate.')
-    parser.add_argument('--min_lr', type=float, default=1e-4, help='Minimum learning rate to stop training.')
-    parser.add_argument('--momentum', type=float, default=0, help='Momentum for SGD.')
-    parser.add_argument('--lr_decay', type=float, default=0.5, help="LR decay rate.")
-    parser.add_argument('--patience', type=int, default=3, help="Patience for LR decay.")
-
-    parser.add_argument('--max_steps', type=int, default=200000)
-    parser.add_argument('--eval_interval', type=int, default=500)
-    parser.add_argument('--batch_size', type=int, default=32)
-    parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')
-    parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')
-    parser.add_argument('--save_dir', type=str, default='saved_models/ner', help='Root dir for saving models.')
-    parser.add_argument('--save_name', type=str, default=None, help="File name to save the model")
-
-    parser.add_argument('--seed', type=int, default=1234)
-    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())
-    parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')
-    args = parser.parse_args(args=args)
-    return args
-
-def main(args=None):
-    args = parse_args(args=args)
-
-    torch.manual_seed(args.seed)
-    np.random.seed(args.seed)
-    random.seed(args.seed)
-    if args.cpu:
-        args.cuda = False
-    elif args.cuda:
-        torch.cuda.manual_seed(args.seed)
-
-    args = vars(args)
-    logger.info("Running tagger in {} mode".format(args['mode']))
-
-    if args['mode'] == 'train':
-        train(args)
-    else:
-        evaluate(args)
-
-def train(args):
-    utils.ensure_dir(args['save_dir'])
-    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
-            else '{}/{}_nertagger.pt'.format(args['save_dir'], args['shorthand'])
-
-    # load pretrained vectors
-    if len(args['wordvec_file']) == 0:
-        vec_file = utils.get_wordvec_file(args['wordvec_dir'], args['shorthand'])
-    else:
-        vec_file = args['wordvec_file']
-    # do not save pretrained embeddings individually
-    pretrain = Pretrain(None, vec_file, args['pretrain_max_vocab'], save_to_file=False)
-
-    if args['charlm']:
-        if args['charlm_shorthand'] is None: 
-            logger.info("CharLM Shorthand is required for loading pretrained CharLM model...")
-            sys.exit(0)
-        logger.info('Use pretrained contextualized char embedding')
-        args['charlm_forward_file'] = '{}/{}_forward_charlm.pt'.format(args['charlm_save_dir'], args['charlm_shorthand'])
-        args['charlm_backward_file'] = '{}/{}_backward_charlm.pt'.format(args['charlm_save_dir'], args['charlm_shorthand'])
-
-    # load data
-    logger.info("Loading data with batch size {}...".format(args['batch_size']))
-    train_doc = Document(json.load(open(args['train_file'])))
-    train_batch = DataLoader(train_doc, args['batch_size'], args, pretrain, evaluation=False)
-    vocab = train_batch.vocab
-    dev_doc = Document(json.load(open(args['eval_file'])))
-    dev_batch = DataLoader(dev_doc, args['batch_size'], args, pretrain, vocab=vocab, evaluation=True)
-    dev_gold_tags = dev_batch.tags
-
-    # skip training if the language does not have training or dev data
-    if len(train_batch) == 0 or len(dev_batch) == 0:
-        logger.info("Skip training because no data available...")
-        sys.exit(0)
-
-    logger.info("Training tagger...")
-    trainer = Trainer(args=args, vocab=vocab, pretrain=pretrain, use_cuda=args['cuda'])
-    logger.info(trainer.model)
-
-    global_step = 0
-    max_steps = args['max_steps']
-    dev_score_history = []
-    best_dev_preds = []
-    current_lr = trainer.optimizer.param_groups[0]['lr']
-    global_start_time = time.time()
-    format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'
-
-    # LR scheduling
-    if args['lr_decay'] > 0:
-        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(trainer.optimizer, mode='max', factor=args['lr_decay'], \
-            patience=args['patience'], verbose=True, min_lr=args['min_lr'])
-    else:
-        scheduler = None
-
-    # start training
-    train_loss = 0
-    while True:
-        should_stop = False
-        for i, batch in enumerate(train_batch):
-            start_time = time.time()
-            global_step += 1
-            loss = trainer.update(batch, eval=False) # update step
-            train_loss += loss
-            if global_step % args['log_step'] == 0:
-                duration = time.time() - start_time
-                logger.info(format_str.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"), global_step,\
-                        max_steps, loss, duration, current_lr))
-
-            if global_step % args['eval_interval'] == 0:
-                # eval on dev
-                logger.info("Evaluating on dev set...")
-                dev_preds = []
-                for batch in dev_batch:
-                    preds = trainer.predict(batch)
-                    dev_preds += preds
-                _, _, dev_score = scorer.score_by_entity(dev_preds, dev_gold_tags)
-
-                train_loss = train_loss / args['eval_interval'] # avg loss per batch
-                logger.info("step {}: train_loss = {:.6f}, dev_score = {:.4f}".format(global_step, train_loss, dev_score))
-                train_loss = 0
-
-                # save best model
-                if len(dev_score_history) == 0 or dev_score > max(dev_score_history):
-                    trainer.save(model_file)
-                    logger.info("New best model saved.")
-                    best_dev_preds = dev_preds
-
-                dev_score_history += [dev_score]
-                logger.info("")
-
-                # lr schedule
-                if scheduler is not None:
-                    scheduler.step(dev_score)
-            
-            # check stopping
-            current_lr = trainer.optimizer.param_groups[0]['lr']
-            if global_step >= args['max_steps'] or current_lr <= args['min_lr']:
-                should_stop = True
-                break
-
-        if should_stop:
-            break
-
-        train_batch.reshuffle()
-
-    logger.info("Training ended with {} steps.".format(global_step))
-
-    best_f, best_eval = max(dev_score_history)*100, np.argmax(dev_score_history)+1
-    logger.info("Best dev F1 = {:.2f}, at iteration = {}".format(best_f, best_eval * args['eval_interval']))
-
-def evaluate(args):
-    # file paths
-    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
-            else '{}/{}_nertagger.pt'.format(args['save_dir'], args['shorthand'])
-
-    # load model
-    use_cuda = args['cuda'] and not args['cpu']
-    trainer = Trainer(model_file=model_file, use_cuda=use_cuda)
-    loaded_args, vocab = trainer.args, trainer.vocab
-
-    # load config
-    for k in args:
-        if k.endswith('_dir') or k.endswith('_file') or k in ['shorthand', 'mode', 'scheme']:
-            loaded_args[k] = args[k]
-
-    # load data
-    logger.info("Loading data with batch size {}...".format(args['batch_size']))
-    doc = Document(json.load(open(args['eval_file'])))
-    batch = DataLoader(doc, args['batch_size'], loaded_args, vocab=vocab, evaluation=True)
-    
-    logger.info("Start evaluation...")
-    preds = []
-    for i, b in enumerate(batch):
-        preds += trainer.predict(b)
-
-    gold_tags = batch.tags
-    _, _, score = scorer.score_by_entity(preds, gold_tags)
-
-    logger.info("NER tagger score:")
-    logger.info("{} {:.2f}".format(args['shorthand'], score*100))
-
-if __name__ == '__main__':
-    main()
+"""
+Entry point for training and evaluating an NER tagger.
+
+This tagger uses BiLSTM layers with character and word-level representations, and a CRF decoding layer 
+to produce NER predictions.
+For details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.
+"""
+
+import sys
+import os
+import time
+from datetime import datetime
+import argparse
+import logging
+import numpy as np
+import random
+import json
+import torch
+from torch import nn, optim
+
+from classla.models.ner.data import DataLoader
+from classla.models.ner.trainer import Trainer
+from classla.models.ner import scorer
+from classla.models.common import utils
+from classla.models.common.pretrain import Pretrain
+from classla.utils.conll import CoNLL
+from classla.models.common.doc import *
+from classla.models import _training_logging
+
+logger = logging.getLogger('classla')
+
+def parse_args(args=None):
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--data_dir', type=str, default='data/ner', help='Root dir for saving models.')
+    parser.add_argument('--wordvec_dir', type=str, default='extern_data/word2vec', help='Directory of word vectors')
+    parser.add_argument('--wordvec_file', type=str, default='', help='File that contains word vectors')
+    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')
+    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')
+
+    parser.add_argument('--mode', default='train', choices=['train', 'predict'])
+    parser.add_argument('--lang', type=str, help='Language')
+    parser.add_argument('--shorthand', type=str, help="Treebank shorthand")
+
+    parser.add_argument('--hidden_dim', type=int, default=256)
+    parser.add_argument('--char_hidden_dim', type=int, default=100)
+    parser.add_argument('--word_emb_dim', type=int, default=100)
+    parser.add_argument('--char_emb_dim', type=int, default=100)
+    parser.add_argument('--num_layers', type=int, default=1)
+    parser.add_argument('--char_num_layers', type=int, default=1)
+    parser.add_argument('--pretrain_max_vocab', type=int, default=100000)
+    parser.add_argument('--word_dropout', type=float, default=0)
+    parser.add_argument('--locked_dropout', type=float, default=0.0)
+    parser.add_argument('--dropout', type=float, default=0.5)
+    parser.add_argument('--rec_dropout', type=float, default=0, help="Word recurrent dropout")
+    parser.add_argument('--char_rec_dropout', type=float, default=0, help="Character recurrent dropout")
+    parser.add_argument('--char_dropout', type=float, default=0, help="Character-level language model dropout")
+    parser.add_argument('--no_char', dest='char', action='store_false', help="Turn off character model.")
+    parser.add_argument('--charlm', action='store_true', help="Turn on contextualized char embedding using character-level language model.")
+    parser.add_argument('--charlm_save_dir', type=str, default='saved_models/charlm', help="Root dir for pretrained character-level language model.")
+    parser.add_argument('--charlm_shorthand', type=str, default=None, help="Shorthand for character-level language model training corpus.")
+    parser.add_argument('--char_lowercase', dest='char_lowercase', action='store_true', help="Use lowercased characters in charater model.")
+    parser.add_argument('--no_lowercase', dest='lowercase', action='store_false', help="Use cased word vectors.")
+    parser.add_argument('--no_emb_finetune', dest='emb_finetune', action='store_false', help="Turn off finetuning of the embedding matrix.")
+    parser.add_argument('--no_input_transform', dest='input_transform', action='store_false', help="Do not use input transformation layer before tagger lstm.")
+    parser.add_argument('--scheme', type=str, default='bioes', help="The tagging scheme to use: bio or bioes.")
+
+    parser.add_argument('--sample_train', type=float, default=1.0, help='Subsample training data.')
+    parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')
+    parser.add_argument('--lr', type=float, default=0.1, help='Learning rate.')
+    parser.add_argument('--min_lr', type=float, default=1e-4, help='Minimum learning rate to stop training.')
+    parser.add_argument('--momentum', type=float, default=0, help='Momentum for SGD.')
+    parser.add_argument('--lr_decay', type=float, default=0.5, help="LR decay rate.")
+    parser.add_argument('--patience', type=int, default=3, help="Patience for LR decay.")
+
+    parser.add_argument('--max_steps', type=int, default=200000)
+    parser.add_argument('--eval_interval', type=int, default=500)
+    parser.add_argument('--batch_size', type=int, default=32)
+    parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')
+    parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')
+    parser.add_argument('--save_dir', type=str, default='saved_models/ner', help='Root dir for saving models.')
+    parser.add_argument('--save_name', type=str, default=None, help="File name to save the model")
+
+    parser.add_argument('--seed', type=int, default=1234)
+    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())
+    parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')
+    args = parser.parse_args(args=args)
+    return args
+
+def main(args=None):
+    args = parse_args(args=args)
+
+    torch.manual_seed(args.seed)
+    np.random.seed(args.seed)
+    random.seed(args.seed)
+    if args.cpu:
+        args.cuda = False
+    elif args.cuda:
+        torch.cuda.manual_seed(args.seed)
+
+    args = vars(args)
+    logger.info("Running tagger in {} mode".format(args['mode']))
+
+    if args['mode'] == 'train':
+        train(args)
+    else:
+        evaluate(args)
+
+def train(args):
+    utils.ensure_dir(args['save_dir'])
+    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
+            else '{}/{}_nertagger.pt'.format(args['save_dir'], args['shorthand'])
+
+    # load pretrained vectors
+    if len(args['wordvec_file']) == 0:
+        vec_file = utils.get_wordvec_file(args['wordvec_dir'], args['shorthand'])
+    else:
+        vec_file = args['wordvec_file']
+    # do not save pretrained embeddings individually
+    pretrain = Pretrain(None, vec_file, args['pretrain_max_vocab'], save_to_file=False)
+
+    if args['charlm']:
+        if args['charlm_shorthand'] is None: 
+            logger.info("CharLM Shorthand is required for loading pretrained CharLM model...")
+            sys.exit(0)
+        logger.info('Use pretrained contextualized char embedding')
+        args['charlm_forward_file'] = '{}/{}_forward_charlm.pt'.format(args['charlm_save_dir'], args['charlm_shorthand'])
+        args['charlm_backward_file'] = '{}/{}_backward_charlm.pt'.format(args['charlm_save_dir'], args['charlm_shorthand'])
+
+    # load data
+    logger.info("Loading data with batch size {}...".format(args['batch_size']))
+    train_doc = Document(json.load(open(args['train_file'])))
+    train_batch = DataLoader(train_doc, args['batch_size'], args, pretrain, evaluation=False)
+    vocab = train_batch.vocab
+    dev_doc = Document(json.load(open(args['eval_file'])))
+    dev_batch = DataLoader(dev_doc, args['batch_size'], args, pretrain, vocab=vocab, evaluation=True)
+    dev_gold_tags = dev_batch.tags
+
+    # skip training if the language does not have training or dev data
+    if len(train_batch) == 0 or len(dev_batch) == 0:
+        logger.info("Skip training because no data available...")
+        sys.exit(0)
+
+    logger.info("Training tagger...")
+    trainer = Trainer(args=args, vocab=vocab, pretrain=pretrain, use_cuda=args['cuda'])
+    logger.info(trainer.model)
+
+    global_step = 0
+    max_steps = args['max_steps']
+    dev_score_history = []
+    best_dev_preds = []
+    current_lr = trainer.optimizer.param_groups[0]['lr']
+    global_start_time = time.time()
+    format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'
+
+    # LR scheduling
+    if args['lr_decay'] > 0:
+        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(trainer.optimizer, mode='max', factor=args['lr_decay'], \
+            patience=args['patience'], verbose=True, min_lr=args['min_lr'])
+    else:
+        scheduler = None
+
+    # start training
+    train_loss = 0
+    while True:
+        should_stop = False
+        for i, batch in enumerate(train_batch):
+            start_time = time.time()
+            global_step += 1
+            loss = trainer.update(batch, eval=False) # update step
+            train_loss += loss
+            if global_step % args['log_step'] == 0:
+                duration = time.time() - start_time
+                logger.info(format_str.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"), global_step,\
+                        max_steps, loss, duration, current_lr))
+
+            if global_step % args['eval_interval'] == 0:
+                # eval on dev
+                logger.info("Evaluating on dev set...")
+                dev_preds = []
+                for batch in dev_batch:
+                    preds = trainer.predict(batch)
+                    dev_preds += preds
+                _, _, dev_score = scorer.score_by_entity(dev_preds, dev_gold_tags)
+
+                train_loss = train_loss / args['eval_interval'] # avg loss per batch
+                logger.info("step {}: train_loss = {:.6f}, dev_score = {:.4f}".format(global_step, train_loss, dev_score))
+                train_loss = 0
+
+                # save best model
+                if len(dev_score_history) == 0 or dev_score > max(dev_score_history):
+                    trainer.save(model_file)
+                    logger.info("New best model saved.")
+                    best_dev_preds = dev_preds
+
+                dev_score_history += [dev_score]
+                logger.info("")
+
+                # lr schedule
+                if scheduler is not None:
+                    scheduler.step(dev_score)
+            
+            # check stopping
+            current_lr = trainer.optimizer.param_groups[0]['lr']
+            if global_step >= args['max_steps'] or current_lr <= args['min_lr']:
+                should_stop = True
+                break
+
+        if should_stop:
+            break
+
+        train_batch.reshuffle()
+
+    logger.info("Training ended with {} steps.".format(global_step))
+
+    best_f, best_eval = max(dev_score_history)*100, np.argmax(dev_score_history)+1
+    logger.info("Best dev F1 = {:.2f}, at iteration = {}".format(best_f, best_eval * args['eval_interval']))
+
+def evaluate(args):
+    # file paths
+    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
+            else '{}/{}_nertagger.pt'.format(args['save_dir'], args['shorthand'])
+
+    # load model
+    use_cuda = args['cuda'] and not args['cpu']
+    trainer = Trainer(model_file=model_file, use_cuda=use_cuda)
+    loaded_args, vocab = trainer.args, trainer.vocab
+
+    # load config
+    for k in args:
+        if k.endswith('_dir') or k.endswith('_file') or k in ['shorthand', 'mode', 'scheme']:
+            loaded_args[k] = args[k]
+
+    # load data
+    logger.info("Loading data with batch size {}...".format(args['batch_size']))
+    doc = Document(json.load(open(args['eval_file'])))
+    batch = DataLoader(doc, args['batch_size'], loaded_args, vocab=vocab, evaluation=True)
+    
+    logger.info("Start evaluation...")
+    preds = []
+    for i, b in enumerate(batch):
+        preds += trainer.predict(b)
+
+    gold_tags = batch.tags
+    _, _, score = scorer.score_by_entity(preds, gold_tags)
+
+    logger.info("NER tagger score:")
+    logger.info("{} {:.2f}".format(args['shorthand'], score*100))
+
+if __name__ == '__main__':
+    main()
```

### Comparing `classla-2.0/classla/models/parser.py` & `classla-2.1/classla/models/parser.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,268 +1,268 @@
-"""
-Entry point for training and evaluating a dependency parser.
-
-This implementation combines a deep biaffine graph-based parser with linearization and distance features.
-For details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.
-"""
-
-"""
-Training and evaluation for the parser.
-"""
-
-import sys
-import os
-import shutil
-import time
-from datetime import datetime
-import argparse
-import numpy as np
-import random
-import torch
-from torch import nn, optim
-
-from classla.models.depparse.data import DataLoader
-from classla.models.depparse.trainer import Trainer
-from classla.models.depparse import scorer
-from classla.models.common import utils
-from classla.models.common.pretrain import Pretrain
-from classla.models.common.doc import *
-from classla.utils.conll import CoNLL
-from classla.models import _training_logging
-
-def parse_args(args=None):
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--data_dir', type=str, default='data/depparse', help='Root dir for saving models.')
-    parser.add_argument('--wordvec_dir', type=str, default='extern_data/word2vec', help='Directory of word vectors.')
-    parser.add_argument('--wordvec_file', type=str, default=None, help='Word vectors filename.')
-    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')
-    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')
-    parser.add_argument('--output_file', type=str, default=None, help='Output CoNLL-U file.')
-    parser.add_argument('--gold_file', type=str, default=None, help='Output CoNLL-U file.')
-    parser.add_argument('--pretrain_file', type=str, default=None, help='Input file containing pretrained data.')
-
-    parser.add_argument('--mode', default='train', choices=['train', 'predict'])
-    parser.add_argument('--lang', type=str, help='Language')
-    parser.add_argument('--shorthand', type=str, help="Treebank shorthand")
-    parser.add_argument('--multi_root', dest='multi_root', action='store_true', help="Allow multiple roots in trees (JOS)")
-
-    parser.add_argument('--hidden_dim', type=int, default=400)
-    parser.add_argument('--char_hidden_dim', type=int, default=400)
-    parser.add_argument('--deep_biaff_hidden_dim', type=int, default=400)
-    parser.add_argument('--composite_deep_biaff_hidden_dim', type=int, default=100)
-    parser.add_argument('--word_emb_dim', type=int, default=75)
-    parser.add_argument('--char_emb_dim', type=int, default=100)
-    parser.add_argument('--tag_emb_dim', type=int, default=50)
-    parser.add_argument('--transformed_dim', type=int, default=125)
-    parser.add_argument('--num_layers', type=int, default=3)
-    parser.add_argument('--char_num_layers', type=int, default=1)
-    parser.add_argument('--pretrain_max_vocab', type=int, default=250000)
-    parser.add_argument('--word_dropout', type=float, default=0.33)
-    parser.add_argument('--dropout', type=float, default=0.5)
-    parser.add_argument('--rec_dropout', type=float, default=0, help="Recurrent dropout")
-    parser.add_argument('--char_rec_dropout', type=float, default=0, help="Recurrent dropout")
-    parser.add_argument('--no_char', dest='char', action='store_false', help="Turn off character model.")
-    parser.add_argument('--no_pretrain', dest='pretrain', action='store_false', help="Turn off pretrained embeddings.")
-    parser.add_argument('--no_linearization', dest='linearization', action='store_false', help="Turn off linearization term.")
-    parser.add_argument('--no_distance', dest='distance', action='store_false', help="Turn off distance term.")
-
-    parser.add_argument('--sample_train', type=float, default=1.0, help='Subsample training data.')
-    parser.add_argument('--optim', type=str, default='adam', help='sgd, adagrad, adam or adamax.')
-    parser.add_argument('--lr', type=float, default=3e-3, help='Learning rate')
-    parser.add_argument('--beta2', type=float, default=0.95)
-
-    parser.add_argument('--max_steps', type=int, default=50000)
-    parser.add_argument('--eval_interval', type=int, default=100)
-    parser.add_argument('--max_steps_before_stop', type=int, default=3000)
-    parser.add_argument('--batch_size', type=int, default=5000)
-    parser.add_argument('--max_grad_norm', type=float, default=1.0, help='Gradient clipping.')
-    parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')
-    parser.add_argument('--save_dir', type=str, default='saved_models/depparse', help='Root dir for saving models.')
-    parser.add_argument('--save_name', type=str, default=None, help="File name to save the model")
-
-    parser.add_argument('--seed', type=int, default=1234)
-    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())
-    parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')
-    args = parser.parse_args(args=args)
-    return args
-
-def main(args=None):
-    sys.setrecursionlimit(50000)
-
-    args = parse_args(args=args)
-
-    torch.manual_seed(args.seed)
-    np.random.seed(args.seed)
-    random.seed(args.seed)
-    if args.cpu:
-        args.cuda = False
-    elif args.cuda:
-        torch.cuda.manual_seed(args.seed)
-
-    args = vars(args)
-    print("Running parser in {} mode".format(args['mode']))
-
-    if args['mode'] == 'train':
-        train(args)
-    else:
-        evaluate(args)
-
-def train(args):
-    utils.ensure_dir(args['save_dir'])
-    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
-            else '{}/{}_parser.pt'.format(args['save_dir'], args['shorthand'])
-
-    # load pretrained vectors if needed
-    pretrain = None
-    if args['pretrain']:
-        vec_file = args['wordvec_file'] if args['wordvec_file'] else utils.get_wordvec_file(args['wordvec_dir'], args['shorthand'])
-        pretrain_file = '{}/{}.pretrain.pt'.format(args['save_dir'], args['shorthand']) if args['pretrain_file'] is None \
-            else args['pretrain_file']
-        pretrain = Pretrain(pretrain_file, vec_file, args['pretrain_max_vocab'])
-
-    # load data
-    print("Loading data with batch size {}...".format(args['batch_size']))
-    doc, metasentences = CoNLL.conll2dict(input_file=args['train_file'])
-    train_doc = Document(doc, metasentences=metasentences)
-    train_batch = DataLoader(train_doc, args['batch_size'], args, pretrain, evaluation=False)
-    vocab = train_batch.vocab
-    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
-    dev_doc = Document(doc, metasentences=metasentences)
-    dev_batch = DataLoader(dev_doc, args['batch_size'], args, pretrain, vocab=vocab, evaluation=True, sort_during_eval=True)
-
-    # pred and gold path
-    system_pred_file = args['output_file']
-    gold_file = args['gold_file']
-
-    # skip training if the language does not have training or dev data
-    if len(train_batch) == 0 or len(dev_batch) == 0:
-        print("Skip training because no data available...")
-        sys.exit(0)
-
-    print("Training parser...")
-    trainer = Trainer(args=args, vocab=vocab, pretrain=pretrain, use_cuda=args['cuda'])
-
-    global_step = 0
-    max_steps = args['max_steps']
-    dev_score_history = []
-    best_dev_preds = []
-    current_lr = args['lr']
-    global_start_time = time.time()
-    format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'
-
-    using_amsgrad = False
-    last_best_step = 0
-    # start training
-    train_loss = 0
-    while True:
-        do_break = False
-        for i, batch in enumerate(train_batch):
-            start_time = time.time()
-            global_step += 1
-            loss = trainer.update(batch, eval=False) # update step
-            train_loss += loss
-            if global_step % args['log_step'] == 0:
-                duration = time.time() - start_time
-                print(format_str.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"), global_step,\
-                        max_steps, loss, duration, current_lr))
-
-            if global_step % args['eval_interval'] == 0:
-                # eval on dev
-                print("Evaluating on dev set...")
-                dev_preds = []
-                for batch in dev_batch:
-                    preds = trainer.predict(batch)
-                    dev_preds += preds
-                dev_preds = utils.unsort(dev_preds, dev_batch.data_orig_idx)
-
-                dev_batch.doc.set([HEAD, DEPREL], [y for x in dev_preds for y in x])
-                CoNLL.dict2conll(dev_batch.doc.to_dict(), system_pred_file)
-                _, _, dev_score = scorer.score(system_pred_file, gold_file)
-
-                train_loss = train_loss / args['eval_interval'] # avg loss per batch
-                print("step {}: train_loss = {:.6f}, dev_score = {:.4f}".format(global_step, train_loss, dev_score))
-                train_loss = 0
-
-                # save best model
-                if len(dev_score_history) == 0 or dev_score > max(dev_score_history):
-                    last_best_step = global_step
-                    trainer.save(model_file)
-                    print("new best model saved.")
-                    best_dev_preds = dev_preds
-
-                dev_score_history += [dev_score]
-                print("")
-
-            if global_step - last_best_step >= args['max_steps_before_stop']:
-                if not using_amsgrad:
-                    print("Switching to AMSGrad")
-                    last_best_step = global_step
-                    using_amsgrad = True
-                    trainer.optimizer = optim.Adam(trainer.model.parameters(), amsgrad=True, lr=args['lr'], betas=(.9, args['beta2']), eps=1e-6)
-                else:
-                    do_break = True
-                    break
-
-            if global_step >= args['max_steps']:
-                do_break = True
-                break
-
-        if do_break: break
-
-        train_batch.reshuffle()
-
-    print("Training ended with {} steps.".format(global_step))
-
-    best_f, best_eval = max(dev_score_history)*100, np.argmax(dev_score_history)+1
-    print("Best dev F1 = {:.2f}, at iteration = {}".format(best_f, best_eval * args['eval_interval']))
-
-def evaluate(args):
-    # file paths
-    system_pred_file = args['output_file']
-    gold_file = args['gold_file']
-    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
-            else '{}/{}_parser.pt'.format(args['save_dir'], args['shorthand'])
-
-    # load pretrain; note that we allow the pretrain_file to be non-existent
-    pretrain_file = '{}/{}.pretrain.pt'.format(args['save_dir'], args['shorthand']) if args['pretrain_file'] is None \
-        else args['pretrain_file']
-    pretrain = Pretrain(pretrain_file)
-
-    # load model
-    print("Loading model from: {}".format(model_file))
-    use_cuda = args['cuda'] and not args['cpu']
-    trainer = Trainer(pretrain=pretrain, model_file=model_file, use_cuda=use_cuda)
-    loaded_args, vocab = trainer.args, trainer.vocab
-
-    # load config
-    for k in args:
-        if k.endswith('_dir') or k.endswith('_file') or k in ['shorthand'] or k == 'mode':
-            loaded_args[k] = args[k]
-
-    # load data
-    print("Loading data with batch size {}...".format(args['batch_size']))
-    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
-    doc = Document(doc, metasentences=metasentences)
-    batch = DataLoader(doc, args['batch_size'], loaded_args, pretrain, vocab=vocab, evaluation=True, sort_during_eval=True)
-
-    if len(batch) > 0:
-        print("Start evaluation...")
-        preds = []
-        for i, b in enumerate(batch):
-            preds += trainer.predict(b)
-    else:
-        # skip eval if dev data does not exist
-        preds = []
-    preds = utils.unsort(preds, batch.data_orig_idx)
-
-    # write to file and score
-    batch.doc.set([HEAD, DEPREL], [y for x in preds for y in x])
-    CoNLL.dict2conll(batch.doc.to_dict(), system_pred_file)
-
-    if gold_file is not None:
-        _, _, score = scorer.score(system_pred_file, gold_file)
-
-        print("Parser score:")
-        print("{} {:.2f}".format(args['shorthand'], score*100))
-
-if __name__ == '__main__':
-    main()
+"""
+Entry point for training and evaluating a dependency parser.
+
+This implementation combines a deep biaffine graph-based parser with linearization and distance features.
+For details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.
+"""
+
+"""
+Training and evaluation for the parser.
+"""
+
+import sys
+import os
+import shutil
+import time
+from datetime import datetime
+import argparse
+import numpy as np
+import random
+import torch
+from torch import nn, optim
+
+from classla.models.depparse.data import DataLoader
+from classla.models.depparse.trainer import Trainer
+from classla.models.depparse import scorer
+from classla.models.common import utils
+from classla.models.common.pretrain import Pretrain
+from classla.models.common.doc import *
+from classla.utils.conll import CoNLL
+from classla.models import _training_logging
+
+def parse_args(args=None):
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--data_dir', type=str, default='data/depparse', help='Root dir for saving models.')
+    parser.add_argument('--wordvec_dir', type=str, default='extern_data/word2vec', help='Directory of word vectors.')
+    parser.add_argument('--wordvec_file', type=str, default=None, help='Word vectors filename.')
+    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')
+    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')
+    parser.add_argument('--output_file', type=str, default=None, help='Output CoNLL-U file.')
+    parser.add_argument('--gold_file', type=str, default=None, help='Output CoNLL-U file.')
+    parser.add_argument('--pretrain_file', type=str, default=None, help='Input file containing pretrained data.')
+
+    parser.add_argument('--mode', default='train', choices=['train', 'predict'])
+    parser.add_argument('--lang', type=str, help='Language')
+    parser.add_argument('--shorthand', type=str, help="Treebank shorthand")
+    parser.add_argument('--multi_root', dest='multi_root', action='store_true', help="Allow multiple roots in trees (JOS)")
+
+    parser.add_argument('--hidden_dim', type=int, default=400)
+    parser.add_argument('--char_hidden_dim', type=int, default=400)
+    parser.add_argument('--deep_biaff_hidden_dim', type=int, default=400)
+    parser.add_argument('--composite_deep_biaff_hidden_dim', type=int, default=100)
+    parser.add_argument('--word_emb_dim', type=int, default=75)
+    parser.add_argument('--char_emb_dim', type=int, default=100)
+    parser.add_argument('--tag_emb_dim', type=int, default=50)
+    parser.add_argument('--transformed_dim', type=int, default=125)
+    parser.add_argument('--num_layers', type=int, default=3)
+    parser.add_argument('--char_num_layers', type=int, default=1)
+    parser.add_argument('--pretrain_max_vocab', type=int, default=250000)
+    parser.add_argument('--word_dropout', type=float, default=0.33)
+    parser.add_argument('--dropout', type=float, default=0.5)
+    parser.add_argument('--rec_dropout', type=float, default=0, help="Recurrent dropout")
+    parser.add_argument('--char_rec_dropout', type=float, default=0, help="Recurrent dropout")
+    parser.add_argument('--no_char', dest='char', action='store_false', help="Turn off character model.")
+    parser.add_argument('--no_pretrain', dest='pretrain', action='store_false', help="Turn off pretrained embeddings.")
+    parser.add_argument('--no_linearization', dest='linearization', action='store_false', help="Turn off linearization term.")
+    parser.add_argument('--no_distance', dest='distance', action='store_false', help="Turn off distance term.")
+
+    parser.add_argument('--sample_train', type=float, default=1.0, help='Subsample training data.')
+    parser.add_argument('--optim', type=str, default='adam', help='sgd, adagrad, adam or adamax.')
+    parser.add_argument('--lr', type=float, default=3e-3, help='Learning rate')
+    parser.add_argument('--beta2', type=float, default=0.95)
+
+    parser.add_argument('--max_steps', type=int, default=50000)
+    parser.add_argument('--eval_interval', type=int, default=100)
+    parser.add_argument('--max_steps_before_stop', type=int, default=3000)
+    parser.add_argument('--batch_size', type=int, default=5000)
+    parser.add_argument('--max_grad_norm', type=float, default=1.0, help='Gradient clipping.')
+    parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')
+    parser.add_argument('--save_dir', type=str, default='saved_models/depparse', help='Root dir for saving models.')
+    parser.add_argument('--save_name', type=str, default=None, help="File name to save the model")
+
+    parser.add_argument('--seed', type=int, default=1234)
+    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())
+    parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')
+    args = parser.parse_args(args=args)
+    return args
+
+def main(args=None):
+    sys.setrecursionlimit(50000)
+
+    args = parse_args(args=args)
+
+    torch.manual_seed(args.seed)
+    np.random.seed(args.seed)
+    random.seed(args.seed)
+    if args.cpu:
+        args.cuda = False
+    elif args.cuda:
+        torch.cuda.manual_seed(args.seed)
+
+    args = vars(args)
+    print("Running parser in {} mode".format(args['mode']))
+
+    if args['mode'] == 'train':
+        train(args)
+    else:
+        evaluate(args)
+
+def train(args):
+    utils.ensure_dir(args['save_dir'])
+    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
+            else '{}/{}_parser.pt'.format(args['save_dir'], args['shorthand'])
+
+    # load pretrained vectors if needed
+    pretrain = None
+    if args['pretrain']:
+        vec_file = args['wordvec_file'] if args['wordvec_file'] else utils.get_wordvec_file(args['wordvec_dir'], args['shorthand'])
+        pretrain_file = '{}/{}.pretrain.pt'.format(args['save_dir'], args['shorthand']) if args['pretrain_file'] is None \
+            else args['pretrain_file']
+        pretrain = Pretrain(pretrain_file, vec_file, args['pretrain_max_vocab'])
+
+    # load data
+    print("Loading data with batch size {}...".format(args['batch_size']))
+    doc, metasentences = CoNLL.conll2dict(input_file=args['train_file'])
+    train_doc = Document(doc, metasentences=metasentences)
+    train_batch = DataLoader(train_doc, args['batch_size'], args, pretrain, evaluation=False)
+    vocab = train_batch.vocab
+    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
+    dev_doc = Document(doc, metasentences=metasentences)
+    dev_batch = DataLoader(dev_doc, args['batch_size'], args, pretrain, vocab=vocab, evaluation=True, sort_during_eval=True)
+
+    # pred and gold path
+    system_pred_file = args['output_file']
+    gold_file = args['gold_file']
+
+    # skip training if the language does not have training or dev data
+    if len(train_batch) == 0 or len(dev_batch) == 0:
+        print("Skip training because no data available...")
+        sys.exit(0)
+
+    print("Training parser...")
+    trainer = Trainer(args=args, vocab=vocab, pretrain=pretrain, use_cuda=args['cuda'])
+
+    global_step = 0
+    max_steps = args['max_steps']
+    dev_score_history = []
+    best_dev_preds = []
+    current_lr = args['lr']
+    global_start_time = time.time()
+    format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'
+
+    using_amsgrad = False
+    last_best_step = 0
+    # start training
+    train_loss = 0
+    while True:
+        do_break = False
+        for i, batch in enumerate(train_batch):
+            start_time = time.time()
+            global_step += 1
+            loss = trainer.update(batch, eval=False) # update step
+            train_loss += loss
+            if global_step % args['log_step'] == 0:
+                duration = time.time() - start_time
+                print(format_str.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"), global_step,\
+                        max_steps, loss, duration, current_lr))
+
+            if global_step % args['eval_interval'] == 0:
+                # eval on dev
+                print("Evaluating on dev set...")
+                dev_preds = []
+                for batch in dev_batch:
+                    preds = trainer.predict(batch)
+                    dev_preds += preds
+                dev_preds = utils.unsort(dev_preds, dev_batch.data_orig_idx)
+
+                dev_batch.doc.set([HEAD, DEPREL], [y for x in dev_preds for y in x])
+                CoNLL.dict2conll(dev_batch.doc.to_dict(), system_pred_file)
+                _, _, dev_score = scorer.score(system_pred_file, gold_file)
+
+                train_loss = train_loss / args['eval_interval'] # avg loss per batch
+                print("step {}: train_loss = {:.6f}, dev_score = {:.4f}".format(global_step, train_loss, dev_score))
+                train_loss = 0
+
+                # save best model
+                if len(dev_score_history) == 0 or dev_score > max(dev_score_history):
+                    last_best_step = global_step
+                    trainer.save(model_file)
+                    print("new best model saved.")
+                    best_dev_preds = dev_preds
+
+                dev_score_history += [dev_score]
+                print("")
+
+            if global_step - last_best_step >= args['max_steps_before_stop']:
+                if not using_amsgrad:
+                    print("Switching to AMSGrad")
+                    last_best_step = global_step
+                    using_amsgrad = True
+                    trainer.optimizer = optim.Adam(trainer.model.parameters(), amsgrad=True, lr=args['lr'], betas=(.9, args['beta2']), eps=1e-6)
+                else:
+                    do_break = True
+                    break
+
+            if global_step >= args['max_steps']:
+                do_break = True
+                break
+
+        if do_break: break
+
+        train_batch.reshuffle()
+
+    print("Training ended with {} steps.".format(global_step))
+
+    best_f, best_eval = max(dev_score_history)*100, np.argmax(dev_score_history)+1
+    print("Best dev F1 = {:.2f}, at iteration = {}".format(best_f, best_eval * args['eval_interval']))
+
+def evaluate(args):
+    # file paths
+    system_pred_file = args['output_file']
+    gold_file = args['gold_file']
+    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
+            else '{}/{}_parser.pt'.format(args['save_dir'], args['shorthand'])
+
+    # load pretrain; note that we allow the pretrain_file to be non-existent
+    pretrain_file = '{}/{}.pretrain.pt'.format(args['save_dir'], args['shorthand']) if args['pretrain_file'] is None \
+        else args['pretrain_file']
+    pretrain = Pretrain(pretrain_file)
+
+    # load model
+    print("Loading model from: {}".format(model_file))
+    use_cuda = args['cuda'] and not args['cpu']
+    trainer = Trainer(pretrain=pretrain, model_file=model_file, use_cuda=use_cuda)
+    loaded_args, vocab = trainer.args, trainer.vocab
+
+    # load config
+    for k in args:
+        if k.endswith('_dir') or k.endswith('_file') or k in ['shorthand'] or k == 'mode':
+            loaded_args[k] = args[k]
+
+    # load data
+    print("Loading data with batch size {}...".format(args['batch_size']))
+    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
+    doc = Document(doc, metasentences=metasentences)
+    batch = DataLoader(doc, args['batch_size'], loaded_args, pretrain, vocab=vocab, evaluation=True, sort_during_eval=True)
+
+    if len(batch) > 0:
+        print("Start evaluation...")
+        preds = []
+        for i, b in enumerate(batch):
+            preds += trainer.predict(b)
+    else:
+        # skip eval if dev data does not exist
+        preds = []
+    preds = utils.unsort(preds, batch.data_orig_idx)
+
+    # write to file and score
+    batch.doc.set([HEAD, DEPREL], [y for x in preds for y in x])
+    CoNLL.dict2conll(batch.doc.to_dict(), system_pred_file)
+
+    if gold_file is not None:
+        _, _, score = scorer.score(system_pred_file, gold_file)
+
+        print("Parser score:")
+        print("{} {:.2f}".format(args['shorthand'], score*100))
+
+if __name__ == '__main__':
+    main()
```

### Comparing `classla-2.0/classla/models/pos/build_xpos_vocab_factory.py` & `classla-2.1/classla/models/pos/build_xpos_vocab_factory.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,101 +1,101 @@
-from collections import defaultdict
-import os
-import sys
-from classla.models.common.vocab import VOCAB_PREFIX
-from classla.models.common.constant import lang2lcode
-from classla.models.pos.vocab import XPOSVocab, WordVocab
-from classla.models.common.doc import *
-from classla.utils.conll import CoNLL
-
-if len(sys.argv) != 3:
-    print('Usage: {} list_of_tb_file output_factory_file'.format(sys.argv[0]))
-    sys.exit(0)
-
-# Read list of all treebanks of concern
-list_of_tb_file, output_file = sys.argv[1:]
-
-def treebank_to_short_name(treebank):
-    """ Convert treebank name to short code. """
-    if treebank.startswith('UD_'):
-        treebank = treebank[3:]
-    splits = treebank.split('-')
-    assert len(splits) == 2
-    lang, corpus = splits
-    lcode = lang2lcode[lang]
-    short = "{}_{}".format(lcode, corpus.lower())
-    return short
-
-shorthands = []
-fullnames = []
-with open(list_of_tb_file) as f:
-    for line in f:
-        treebank = line.strip()
-        fullnames.append(treebank)
-        shorthands.append(treebank_to_short_name(treebank))
-
-def filter_data(data, idx):
-    data_filtered = []
-    for sentence in data:
-        flag = True
-        for token in sentence:
-            if token[idx] is None:
-                flag = False
-        if flag: data_filtered.append(sentence)
-    return data_filtered
-
-
-# For each treebank, we would like to find the XPOS Vocab configuration that minimizes
-# the number of total classes needed to predict by all tagger classifiers. This is
-# achieved by enumerating different options of separators that different treebanks might
-# use, and comparing that to treating the XPOS tags as separate categories (using a
-# WordVocab).
-mapping = defaultdict(list)
-for sh, fn in zip(shorthands, fullnames):
-    print('Resolving vocab option for {}...'.format(sh))
-    if not os.path.exists('data/pos/{}.train.in.conllu'.format(sh)):
-        raise UserWarning('Training data for {} not found in the data directory, falling back to using WordVocab. To generate the '
-            'XPOS vocabulary for this treebank properly, please run the following command first:\n'
-            '\tbash scripts/prep_pos_data.sh {}'.format(fn, fn))
-        # without the training file, there's not much we can do
-        key = 'WordVocab(data, shorthand, idx=2)'
-        mapping[key].append(sh)
-        continue
-
-    doc, metasentences = CoNLL.conll2dict(input_file='data/pos/{}.train.in.conllu'.format(sh))
-    doc = Document(doc, metasentences=metasentences)
-    data = doc.get([TEXT, UPOS, XPOS, FEATS], as_sentences=True)
-    print(f'Original length = {len(data)}')
-    data = filter_data(data, idx=2)
-    print(f'Filtered length = {len(data)}')
-    vocab = WordVocab(data, sh, idx=2, ignore=["_"])
-    key = 'WordVocab(data, shorthand, idx=2, ignore=["_"])'
-    best_size = len(vocab) - len(VOCAB_PREFIX)
-    if best_size > 20:
-        for sep in ['', '-', '+', '|', ',', ':']: # separators
-            vocab = XPOSVocab(data, sh, idx=2, sep=sep)
-            length = sum(len(x) - len(VOCAB_PREFIX) for x in vocab._id2unit.values())
-            if length < best_size:
-                key = 'XPOSVocab(data, shorthand, idx=2, sep="{}")'.format(sep)
-                best_size = length
-    mapping[key].append(sh)
-
-# Generate code. This takes the XPOS vocabulary classes selected above, and generates the
-# actual factory class as seen in models.pos.xpos_vocab_factory.
-first = True
-with open(output_file, 'w') as f:
-    print('''# This is the XPOS factory method generated automatically from models.pos.build_xpos_factory.
-# Please don't edit it!
-
-from models.pos.vocab import WordVocab, XPOSVocab
-
-def xpos_vocab_factory(data, shorthand):''', file=f)
-
-    for key in mapping:
-        print("    {} shorthand in [{}]:".format('if' if first else 'elif', ', '.join(['"{}"'.format(x) for x in mapping[key]])), file=f)
-        print("        return {}".format(key), file=f)
-
-        first = False
-    print('''    else:
-        raise NotImplementedError('Language shorthand "{}" not found!'.format(shorthand))''', file=f)
-
-print('Done!')
+from collections import defaultdict
+import os
+import sys
+from classla.models.common.vocab import VOCAB_PREFIX
+from classla.models.common.constant import lang2lcode
+from classla.models.pos.vocab import XPOSVocab, WordVocab
+from classla.models.common.doc import *
+from classla.utils.conll import CoNLL
+
+if len(sys.argv) != 3:
+    print('Usage: {} list_of_tb_file output_factory_file'.format(sys.argv[0]))
+    sys.exit(0)
+
+# Read list of all treebanks of concern
+list_of_tb_file, output_file = sys.argv[1:]
+
+def treebank_to_short_name(treebank):
+    """ Convert treebank name to short code. """
+    if treebank.startswith('UD_'):
+        treebank = treebank[3:]
+    splits = treebank.split('-')
+    assert len(splits) == 2
+    lang, corpus = splits
+    lcode = lang2lcode[lang]
+    short = "{}_{}".format(lcode, corpus.lower())
+    return short
+
+shorthands = []
+fullnames = []
+with open(list_of_tb_file) as f:
+    for line in f:
+        treebank = line.strip()
+        fullnames.append(treebank)
+        shorthands.append(treebank_to_short_name(treebank))
+
+def filter_data(data, idx):
+    data_filtered = []
+    for sentence in data:
+        flag = True
+        for token in sentence:
+            if token[idx] is None:
+                flag = False
+        if flag: data_filtered.append(sentence)
+    return data_filtered
+
+
+# For each treebank, we would like to find the XPOS Vocab configuration that minimizes
+# the number of total classes needed to predict by all tagger classifiers. This is
+# achieved by enumerating different options of separators that different treebanks might
+# use, and comparing that to treating the XPOS tags as separate categories (using a
+# WordVocab).
+mapping = defaultdict(list)
+for sh, fn in zip(shorthands, fullnames):
+    print('Resolving vocab option for {}...'.format(sh))
+    if not os.path.exists('data/pos/{}.train.in.conllu'.format(sh)):
+        raise UserWarning('Training data for {} not found in the data directory, falling back to using WordVocab. To generate the '
+            'XPOS vocabulary for this treebank properly, please run the following command first:\n'
+            '\tbash scripts/prep_pos_data.sh {}'.format(fn, fn))
+        # without the training file, there's not much we can do
+        key = 'WordVocab(data, shorthand, idx=2)'
+        mapping[key].append(sh)
+        continue
+
+    doc, metasentences = CoNLL.conll2dict(input_file='data/pos/{}.train.in.conllu'.format(sh))
+    doc = Document(doc, metasentences=metasentences)
+    data = doc.get([TEXT, UPOS, XPOS, FEATS], as_sentences=True)
+    print(f'Original length = {len(data)}')
+    data = filter_data(data, idx=2)
+    print(f'Filtered length = {len(data)}')
+    vocab = WordVocab(data, sh, idx=2, ignore=["_"])
+    key = 'WordVocab(data, shorthand, idx=2, ignore=["_"])'
+    best_size = len(vocab) - len(VOCAB_PREFIX)
+    if best_size > 20:
+        for sep in ['', '-', '+', '|', ',', ':']: # separators
+            vocab = XPOSVocab(data, sh, idx=2, sep=sep)
+            length = sum(len(x) - len(VOCAB_PREFIX) for x in vocab._id2unit.values())
+            if length < best_size:
+                key = 'XPOSVocab(data, shorthand, idx=2, sep="{}")'.format(sep)
+                best_size = length
+    mapping[key].append(sh)
+
+# Generate code. This takes the XPOS vocabulary classes selected above, and generates the
+# actual factory class as seen in models.pos.xpos_vocab_factory.
+first = True
+with open(output_file, 'w') as f:
+    print('''# This is the XPOS factory method generated automatically from models.pos.build_xpos_factory.
+# Please don't edit it!
+
+from models.pos.vocab import WordVocab, XPOSVocab
+
+def xpos_vocab_factory(data, shorthand):''', file=f)
+
+    for key in mapping:
+        print("    {} shorthand in [{}]:".format('if' if first else 'elif', ', '.join(['"{}"'.format(x) for x in mapping[key]])), file=f)
+        print("        return {}".format(key), file=f)
+
+        first = False
+    print('''    else:
+        raise NotImplementedError('Language shorthand "{}" not found!'.format(shorthand))''', file=f)
+
+print('Done!')
```

### Comparing `classla-2.0/classla/models/pos/model.py` & `classla-2.1/classla/models/pos/model.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,187 +1,187 @@
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence
-
-from classla.models.common.biaffine import BiaffineScorer
-from classla.models.common.hlstm import HighwayLSTM
-from classla.models.common.dropout import WordDropout
-from classla.models.common.vocab import CompositeVocab
-from classla.models.common.char_model import CharacterModel
-
-class Tagger(nn.Module):
-    def __init__(self, args, vocab, emb_matrix=None, share_hid=False):
-        super().__init__()
-
-        self.vocab = vocab
-        self.args = args
-        self.share_hid = share_hid
-        self.unsaved_modules = []
-
-        def add_unsaved_module(name, module):
-            self.unsaved_modules += [name]
-            setattr(self, name, module)
-
-        # input layers
-        input_size = 0
-        if self.args['word_emb_dim'] > 0:
-            # frequent word embeddings
-            self.word_emb = nn.Embedding(len(vocab['word']), self.args['word_emb_dim'], padding_idx=0)
-            input_size += self.args['word_emb_dim']
-
-        if not share_hid:
-            # upos embeddings
-            self.upos_emb = nn.Embedding(len(vocab['upos']), self.args['tag_emb_dim'], padding_idx=0)
-
-        if self.args['char'] and self.args['char_emb_dim'] > 0:
-            self.charmodel = CharacterModel(args, vocab)
-            self.trans_char = nn.Linear(self.args['char_hidden_dim'], self.args['transformed_dim'], bias=False)
-            input_size += self.args['transformed_dim']
-
-        if self.args['pretrain']:    
-            # pretrained embeddings, by default this won't be saved into model file
-            add_unsaved_module('pretrained_emb', nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True))
-            self.trans_pretrained = nn.Linear(emb_matrix.shape[1], self.args['transformed_dim'], bias=False)
-            input_size += self.args['transformed_dim']
-        
-        # recurrent layers
-        self.taggerlstm = HighwayLSTM(input_size, self.args['hidden_dim'], self.args['num_layers'], batch_first=True, bidirectional=True, dropout=self.args['dropout'], rec_dropout=self.args['rec_dropout'], highway_func=torch.tanh)
-        self.drop_replacement = nn.Parameter(torch.randn(input_size) / np.sqrt(input_size))
-        self.taggerlstm_h_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))
-        self.taggerlstm_c_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))
-
-        # classifiers
-        self.upos_hid = nn.Linear(self.args['hidden_dim'] * 2, self.args['deep_biaff_hidden_dim'])
-        self.upos_clf = nn.Linear(self.args['deep_biaff_hidden_dim'], len(vocab['upos']))
-        self.upos_clf.weight.data.zero_()
-        self.upos_clf.bias.data.zero_()
-
-        if share_hid:
-            clf_constructor = lambda insize, outsize: nn.Linear(insize, outsize)
-        else:
-            self.xpos_hid = nn.Linear(self.args['hidden_dim'] * 2, self.args['deep_biaff_hidden_dim'] if not isinstance(vocab['xpos'], CompositeVocab) else self.args['composite_deep_biaff_hidden_dim'])
-            self.ufeats_hid = nn.Linear(self.args['hidden_dim'] * 2, self.args['composite_deep_biaff_hidden_dim'])
-            clf_constructor = lambda insize, outsize: BiaffineScorer(insize, self.args['tag_emb_dim'], outsize)
-
-        if isinstance(vocab['xpos'], CompositeVocab):
-            self.xpos_clf = nn.ModuleList()
-            for l in vocab['xpos'].lens():
-                self.xpos_clf.append(clf_constructor(self.args['composite_deep_biaff_hidden_dim'], l))
-        else:
-            self.xpos_clf = clf_constructor(self.args['deep_biaff_hidden_dim'], len(vocab['xpos']))
-            if share_hid:
-                self.xpos_clf.weight.data.zero_()
-                self.xpos_clf.bias.data.zero_()
-
-        self.ufeats_clf = nn.ModuleList()
-        for l in vocab['feats'].lens():
-            if share_hid:
-                self.ufeats_clf.append(clf_constructor(self.args['deep_biaff_hidden_dim'], l))
-                self.ufeats_clf[-1].weight.data.zero_()
-                self.ufeats_clf[-1].bias.data.zero_()
-            else:
-                self.ufeats_clf.append(clf_constructor(self.args['composite_deep_biaff_hidden_dim'], l))
-
-        # criterion
-        self.crit = nn.CrossEntropyLoss(ignore_index=0) # ignore padding
-
-        self.drop = nn.Dropout(args['dropout'])
-        self.worddrop = WordDropout(args['word_dropout'])
-
-    def forward(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, word_string, postprocessor=None):
-        
-        def pack(x):
-            return pack_padded_sequence(x, sentlens, batch_first=True)
-        
-        inputs = []
-        if self.args['word_emb_dim'] > 0:
-            word_emb = self.word_emb(word)
-            word_emb = pack(word_emb)
-            inputs += [word_emb]
-
-        if self.args['pretrain']:
-            pretrained_emb = self.pretrained_emb(pretrained)
-            pretrained_emb = self.trans_pretrained(pretrained_emb)
-            pretrained_emb = pack(pretrained_emb)
-            inputs += [pretrained_emb]
-
-        def pad(x):
-            return pad_packed_sequence(PackedSequence(x, word_emb.batch_sizes), batch_first=True)[0]
-
-        if self.args['char'] and self.args['char_emb_dim'] > 0:
-            char_reps = self.charmodel(wordchars, wordchars_mask, word_orig_idx, sentlens, wordlens)
-            char_reps = PackedSequence(self.trans_char(self.drop(char_reps.data)), char_reps.batch_sizes)
-            inputs += [char_reps]
-
-        lstm_inputs = torch.cat([x.data for x in inputs], 1)
-        lstm_inputs = self.worddrop(lstm_inputs, self.drop_replacement)
-        lstm_inputs = self.drop(lstm_inputs)
-        lstm_inputs = PackedSequence(lstm_inputs, inputs[0].batch_sizes)
-
-        lstm_outputs, _ = self.taggerlstm(lstm_inputs, sentlens, hx=(self.taggerlstm_h_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous(), self.taggerlstm_c_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous()))
-        lstm_outputs = lstm_outputs.data
-
-        upos_hid = F.relu(self.upos_hid(self.drop(lstm_outputs)))
-        upos_pred = self.upos_clf(self.drop(upos_hid))
-
-        upos = pack(upos).data
-        loss = self.crit(upos_pred.view(-1, upos_pred.size(-1)), upos.view(-1))
-
-        if self.share_hid:
-            xpos_hid = upos_hid
-            ufeats_hid = upos_hid
-
-            clffunc = lambda clf, hid: clf(self.drop(hid))
-        else:
-            xpos_hid = F.relu(self.xpos_hid(self.drop(lstm_outputs)))
-            ufeats_hid = F.relu(self.ufeats_hid(self.drop(lstm_outputs)))
-
-            if self.training:
-                upos_emb = self.upos_emb(upos)
-            else:
-                upos_emb = self.upos_emb(upos_pred.max(1)[1])
-
-            clffunc = lambda clf, hid: clf(self.drop(hid), self.drop(upos_emb))
-
-        xpos = pack(xpos).data
-        if isinstance(self.vocab['xpos'], CompositeVocab):
-            xpos_preds = []
-            for i in range(len(self.vocab['xpos'])):
-                xpos_pred = clffunc(self.xpos_clf[i], xpos_hid)
-                loss += self.crit(xpos_pred.view(-1, xpos_pred.size(-1)), xpos[:, i].view(-1))
-                xpos_preds.append(pad(xpos_pred).max(2, keepdim=True)[1])
-            max_xpos_value = torch.cat(xpos_preds, 2)
-
-        else:
-            xpos_pred = clffunc(self.xpos_clf, xpos_hid)
-            padded_xpos_pred = pad(xpos_pred)
-            if postprocessor is not None:
-                max_xpos_value = postprocessor.process_xpos(padded_xpos_pred, word_string)
-            else:
-                max_xpos_value = padded_xpos_pred.max(2)[1]
-            loss += self.crit(xpos_pred.view(-1, xpos_pred.size(-1)), xpos.view(-1))
-
-        padded_upos_pred = pad(upos_pred)
-        # if postprocessor.processor and postprocessor.processor.hypothesis_dictionary_upos:
-        if postprocessor is not None:
-            max_upos_value = postprocessor.process_upos(padded_upos_pred, word_string, max_xpos_value)
-            preds = [max_upos_value]
-        else:
-            preds = [padded_upos_pred.max(2)[1]]
-
-        preds.append(max_xpos_value)
-        ufeats_preds = []
-        ufeats = pack(ufeats).data
-        for i in range(len(self.vocab['feats'])):
-            ufeats_pred = clffunc(self.ufeats_clf[i], ufeats_hid)
-            loss += self.crit(ufeats_pred.view(-1, ufeats_pred.size(-1)), ufeats[:, i].view(-1))
-            ufeats_preds.append(pad(ufeats_pred).max(2, keepdim=True)[1])
-
-        # if postprocessor.processor and postprocessor.processor.hypothesis_dictionary_feats:
-        if postprocessor is not None:
-            preds.append(postprocessor.process_feats(ufeats_preds, word_string, max_xpos_value, max_upos_value))
-        else:
-            preds.append(torch.cat(ufeats_preds, 2))
-
-        return loss, preds
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence
+
+from classla.models.common.biaffine import BiaffineScorer
+from classla.models.common.hlstm import HighwayLSTM
+from classla.models.common.dropout import WordDropout
+from classla.models.common.vocab import CompositeVocab
+from classla.models.common.char_model import CharacterModel
+
+class Tagger(nn.Module):
+    def __init__(self, args, vocab, emb_matrix=None, share_hid=False):
+        super().__init__()
+
+        self.vocab = vocab
+        self.args = args
+        self.share_hid = share_hid
+        self.unsaved_modules = []
+
+        def add_unsaved_module(name, module):
+            self.unsaved_modules += [name]
+            setattr(self, name, module)
+
+        # input layers
+        input_size = 0
+        if self.args['word_emb_dim'] > 0:
+            # frequent word embeddings
+            self.word_emb = nn.Embedding(len(vocab['word']), self.args['word_emb_dim'], padding_idx=0)
+            input_size += self.args['word_emb_dim']
+
+        if not share_hid:
+            # upos embeddings
+            self.upos_emb = nn.Embedding(len(vocab['upos']), self.args['tag_emb_dim'], padding_idx=0)
+
+        if self.args['char'] and self.args['char_emb_dim'] > 0:
+            self.charmodel = CharacterModel(args, vocab)
+            self.trans_char = nn.Linear(self.args['char_hidden_dim'], self.args['transformed_dim'], bias=False)
+            input_size += self.args['transformed_dim']
+
+        if self.args['pretrain']:    
+            # pretrained embeddings, by default this won't be saved into model file
+            add_unsaved_module('pretrained_emb', nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True))
+            self.trans_pretrained = nn.Linear(emb_matrix.shape[1], self.args['transformed_dim'], bias=False)
+            input_size += self.args['transformed_dim']
+        
+        # recurrent layers
+        self.taggerlstm = HighwayLSTM(input_size, self.args['hidden_dim'], self.args['num_layers'], batch_first=True, bidirectional=True, dropout=self.args['dropout'], rec_dropout=self.args['rec_dropout'], highway_func=torch.tanh)
+        self.drop_replacement = nn.Parameter(torch.randn(input_size) / np.sqrt(input_size))
+        self.taggerlstm_h_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))
+        self.taggerlstm_c_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))
+
+        # classifiers
+        self.upos_hid = nn.Linear(self.args['hidden_dim'] * 2, self.args['deep_biaff_hidden_dim'])
+        self.upos_clf = nn.Linear(self.args['deep_biaff_hidden_dim'], len(vocab['upos']))
+        self.upos_clf.weight.data.zero_()
+        self.upos_clf.bias.data.zero_()
+
+        if share_hid:
+            clf_constructor = lambda insize, outsize: nn.Linear(insize, outsize)
+        else:
+            self.xpos_hid = nn.Linear(self.args['hidden_dim'] * 2, self.args['deep_biaff_hidden_dim'] if not isinstance(vocab['xpos'], CompositeVocab) else self.args['composite_deep_biaff_hidden_dim'])
+            self.ufeats_hid = nn.Linear(self.args['hidden_dim'] * 2, self.args['composite_deep_biaff_hidden_dim'])
+            clf_constructor = lambda insize, outsize: BiaffineScorer(insize, self.args['tag_emb_dim'], outsize)
+
+        if isinstance(vocab['xpos'], CompositeVocab):
+            self.xpos_clf = nn.ModuleList()
+            for l in vocab['xpos'].lens():
+                self.xpos_clf.append(clf_constructor(self.args['composite_deep_biaff_hidden_dim'], l))
+        else:
+            self.xpos_clf = clf_constructor(self.args['deep_biaff_hidden_dim'], len(vocab['xpos']))
+            if share_hid:
+                self.xpos_clf.weight.data.zero_()
+                self.xpos_clf.bias.data.zero_()
+
+        self.ufeats_clf = nn.ModuleList()
+        for l in vocab['feats'].lens():
+            if share_hid:
+                self.ufeats_clf.append(clf_constructor(self.args['deep_biaff_hidden_dim'], l))
+                self.ufeats_clf[-1].weight.data.zero_()
+                self.ufeats_clf[-1].bias.data.zero_()
+            else:
+                self.ufeats_clf.append(clf_constructor(self.args['composite_deep_biaff_hidden_dim'], l))
+
+        # criterion
+        self.crit = nn.CrossEntropyLoss(ignore_index=0) # ignore padding
+
+        self.drop = nn.Dropout(args['dropout'])
+        self.worddrop = WordDropout(args['word_dropout'])
+
+    def forward(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, word_string, postprocessor=None):
+        
+        def pack(x):
+            return pack_padded_sequence(x, sentlens, batch_first=True)
+        
+        inputs = []
+        if self.args['word_emb_dim'] > 0:
+            word_emb = self.word_emb(word)
+            word_emb = pack(word_emb)
+            inputs += [word_emb]
+
+        if self.args['pretrain']:
+            pretrained_emb = self.pretrained_emb(pretrained)
+            pretrained_emb = self.trans_pretrained(pretrained_emb)
+            pretrained_emb = pack(pretrained_emb)
+            inputs += [pretrained_emb]
+
+        def pad(x):
+            return pad_packed_sequence(PackedSequence(x, word_emb.batch_sizes), batch_first=True)[0]
+
+        if self.args['char'] and self.args['char_emb_dim'] > 0:
+            char_reps = self.charmodel(wordchars, wordchars_mask, word_orig_idx, sentlens, wordlens)
+            char_reps = PackedSequence(self.trans_char(self.drop(char_reps.data)), char_reps.batch_sizes)
+            inputs += [char_reps]
+
+        lstm_inputs = torch.cat([x.data for x in inputs], 1)
+        lstm_inputs = self.worddrop(lstm_inputs, self.drop_replacement)
+        lstm_inputs = self.drop(lstm_inputs)
+        lstm_inputs = PackedSequence(lstm_inputs, inputs[0].batch_sizes)
+
+        lstm_outputs, _ = self.taggerlstm(lstm_inputs, sentlens, hx=(self.taggerlstm_h_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous(), self.taggerlstm_c_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous()))
+        lstm_outputs = lstm_outputs.data
+
+        upos_hid = F.relu(self.upos_hid(self.drop(lstm_outputs)))
+        upos_pred = self.upos_clf(self.drop(upos_hid))
+
+        upos = pack(upos).data
+        loss = self.crit(upos_pred.view(-1, upos_pred.size(-1)), upos.view(-1))
+
+        if self.share_hid:
+            xpos_hid = upos_hid
+            ufeats_hid = upos_hid
+
+            clffunc = lambda clf, hid: clf(self.drop(hid))
+        else:
+            xpos_hid = F.relu(self.xpos_hid(self.drop(lstm_outputs)))
+            ufeats_hid = F.relu(self.ufeats_hid(self.drop(lstm_outputs)))
+
+            if self.training:
+                upos_emb = self.upos_emb(upos)
+            else:
+                upos_emb = self.upos_emb(upos_pred.max(1)[1])
+
+            clffunc = lambda clf, hid: clf(self.drop(hid), self.drop(upos_emb))
+
+        xpos = pack(xpos).data
+        if isinstance(self.vocab['xpos'], CompositeVocab):
+            xpos_preds = []
+            for i in range(len(self.vocab['xpos'])):
+                xpos_pred = clffunc(self.xpos_clf[i], xpos_hid)
+                loss += self.crit(xpos_pred.view(-1, xpos_pred.size(-1)), xpos[:, i].view(-1))
+                xpos_preds.append(pad(xpos_pred).max(2, keepdim=True)[1])
+            max_xpos_value = torch.cat(xpos_preds, 2)
+
+        else:
+            xpos_pred = clffunc(self.xpos_clf, xpos_hid)
+            padded_xpos_pred = pad(xpos_pred)
+            if postprocessor is not None:
+                max_xpos_value = postprocessor.process_xpos(padded_xpos_pred, word_string)
+            else:
+                max_xpos_value = padded_xpos_pred.max(2)[1]
+            loss += self.crit(xpos_pred.view(-1, xpos_pred.size(-1)), xpos.view(-1))
+
+        padded_upos_pred = pad(upos_pred)
+        # if postprocessor.processor and postprocessor.processor.hypothesis_dictionary_upos:
+        if postprocessor is not None:
+            max_upos_value = postprocessor.process_upos(padded_upos_pred, word_string, max_xpos_value)
+            preds = [max_upos_value]
+        else:
+            preds = [padded_upos_pred.max(2)[1]]
+
+        preds.append(max_xpos_value)
+        ufeats_preds = []
+        ufeats = pack(ufeats).data
+        for i in range(len(self.vocab['feats'])):
+            ufeats_pred = clffunc(self.ufeats_clf[i], ufeats_hid)
+            loss += self.crit(ufeats_pred.view(-1, ufeats_pred.size(-1)), ufeats[:, i].view(-1))
+            ufeats_preds.append(pad(ufeats_pred).max(2, keepdim=True)[1])
+
+        # if postprocessor.processor and postprocessor.processor.hypothesis_dictionary_feats:
+        if postprocessor is not None:
+            preds.append(postprocessor.process_feats(ufeats_preds, word_string, max_xpos_value, max_upos_value))
+        else:
+            preds.append(torch.cat(ufeats_preds, 2))
+
+        return loss, preds
```

### Comparing `classla-2.0/classla/models/pos/postprocessor.py` & `classla-2.1/classla/models/pos/postprocessor.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,245 +1,245 @@
-import sys
-import unicodedata
-
-
-class InflectionalLexiconProcessor(object):
-    def __init__(self, lexicon, vocab, pretrain, pos_lemma_pretag=False):
-        # fills hypothesis_dictionary_xpos
-        self.hypothesis_dictionary_xpos = {}
-        self.hypothesis_dictionary_upos = {}
-        self.hypothesis_dictionary_feats = {}
-        # fallback for when xpos tag in influectional lexicon is not in vocab
-        self.hypothesis_dictionary_xpos_fallback = {}
-        self.hypothesis_dictionary_upos_fallback = {}
-        self.hypothesis_dictionary_feats_fallback = {}
-
-        closed_classes_xpos_rules = ['Z', 'punct'] if pos_lemma_pretag else []
-        closed_classes_upos_rules = ['PUNCT', 'SYM'] if pos_lemma_pretag else []
-
-        self.closed_classes_xpos = set()
-        self.closed_classes_upos = set()
-
-        self.xpos_vocab = vocab['xpos']
-        self.upos_vocab = vocab['upos']
-        self.feats_vocab = vocab['feats']
-
-        self.create_closed_classes_upos(vocab['upos'], closed_classes_upos_rules)
-        self.create_closed_classes_xpos(vocab['xpos'], closed_classes_xpos_rules)
-
-        self.closed_classes_upos_inverse = [vocab['upos'][el] for el in vocab['upos'] if
-                                       vocab['upos'][el] not in self.closed_classes_upos]
-        self.closed_classes_xpos_inverse = [vocab['xpos'][el] for el in vocab['xpos'] if
-                                       vocab['xpos'][el] not in self.closed_classes_xpos]
-
-    def process_xpos(self, padded_prediction, word_strings):
-        predictions = []
-
-        max_value = padded_prediction.max(2)[1]
-
-        for sent_id, (sent_indices, sent_strings) in enumerate(zip(max_value, word_strings)):
-            sent_predictions = []
-            for word_id, (word_prediction, word_string) in enumerate(zip(sent_indices, sent_strings)):
-                if word_prediction.item() not in self.closed_classes_xpos:
-                    prediction = self.xpos_vocab[word_prediction.item()]
-                else:
-                    prediction = self.xpos_vocab[self.closed_classes_xpos_inverse[
-                        padded_prediction[sent_id, word_id, self.closed_classes_xpos_inverse].argmax().item()]]
-                sent_predictions.append(prediction)
-            predictions.append(sent_predictions)
-        return predictions
-
-    def process_upos(self, padded_prediction, word_strings, xpos_preds):
-        predictions = []
-
-        max_value = padded_prediction.max(2)[1]
-
-        for sent_id, (sent_indices, sent_strings) in enumerate(zip(max_value, word_strings)):
-            sent_predictions = []
-            for word_id, (word_prediction, word_string) in enumerate(zip(sent_indices, sent_strings)):
-                if word_prediction.item() not in self.closed_classes_upos:
-                    prediction = self.upos_vocab[word_prediction.item()]
-                else:
-                    prediction = self.upos_vocab[self.closed_classes_upos_inverse[
-                        padded_prediction[sent_id, word_id, self.closed_classes_upos_inverse].argmax().item()]]
-                sent_predictions.append(prediction)
-            predictions.append(sent_predictions)
-        return predictions
-
-    def process_feats(self, padded_prediction, word_strings, xpos_preds, upos_preds):
-        predictions = []
-
-        for sent_id, (sent_strings, sent_xpos, sent_upos) in enumerate(zip(word_strings, xpos_preds, upos_preds)):
-            sent_predictions = []
-            for word_id in range(len(sent_strings)):
-                word_feat = []
-                for feat_id in range(len(padded_prediction)):
-                    word_feat.append(padded_prediction[feat_id][sent_id, word_id].item())
-                prediction = self.feats_vocab[word_feat]
-                sent_predictions.append(prediction)
-            predictions.append(sent_predictions)
-        return predictions
-
-    def create_closed_classes_xpos(self, vocab, closed_classes_rules):
-        """ Fills a set of closed classes, that contains xpos ids that are not permitted. """
-        for key in vocab:
-            if key[0] in closed_classes_rules or key == 'punct' and key in closed_classes_rules:
-                self.closed_classes_xpos.add(vocab[key])
-
-    def create_closed_classes_upos(self, vocab, closed_classes_rules):
-        """ Fills a set of closed classes, that contains xpos ids that are not permitted. """
-        for key in vocab:
-            if key in closed_classes_rules:
-                self.closed_classes_upos.add(vocab[key])
-
-
-class SloveneInflectionalLexiconProcessor(InflectionalLexiconProcessor):
-    def __init__(self, lexicon, vocab, pretrain, pos_lemma_pretag=False):
-        super(SloveneInflectionalLexiconProcessor, self).__init__(lexicon, vocab, pretrain, pos_lemma_pretag)
-        closed_classes_xpos_rules = ['P', 'S', 'C', 'Q']
-        closed_classes_upos_rules = ['PRON', 'DET', 'ADP', 'CCONJ', 'SCONJ', 'PART']
-
-        self.xpos_vocab = vocab['xpos']
-        self.upos_vocab = vocab['upos']
-        self.feats_vocab = vocab['feats']
-
-        self.extract_lexicon_data(lexicon)
-
-        # fills closed_classes
-        # self.closed_classes = set()
-        self.create_closed_classes_xpos(vocab['xpos'], closed_classes_xpos_rules)
-        self.create_closed_classes_xpos(vocab['upos'], closed_classes_upos_rules)
-        self.closed_classes_xpos_inverse = [vocab['xpos'][el] for el in vocab['xpos'] if vocab['xpos'][el] not in self.closed_classes_xpos]
-        self.closed_classes_upos_inverse = [vocab['upos'][el] for el in vocab['upos'] if vocab['upos'][el] not in self.closed_classes_upos]
-
-    def process_xpos(self, padded_prediction, word_strings):
-        predictions = []
-
-        max_value = padded_prediction.max(2)[1]
-
-        for sent_id, (sent_indices, sent_strings) in enumerate(zip(max_value, word_strings)):
-            sent_predictions = []
-            for word_id, (word_prediction, word_string) in enumerate(zip(sent_indices, sent_strings)):
-                if word_string in self.hypothesis_dictionary_xpos:
-                    # if only one possible prediction in hypothesis dictionary, take it!
-                    if len(self.hypothesis_dictionary_xpos[word_string]) == 1:
-                        prediction = self.hypothesis_dictionary_xpos[word_string][0]
-                    elif self.xpos_vocab[word_prediction.item()] in self.hypothesis_dictionary_xpos[word_string]:
-                        prediction = self.xpos_vocab[word_prediction.item()]
-                    else:
-                        optional_indices = [self.xpos_vocab[el] for el in self.hypothesis_dictionary_xpos[word_string]]
-                        prediction = self.xpos_vocab[optional_indices[padded_prediction[sent_id, word_id, optional_indices].argmax().item()]]
-                elif word_string in self.hypothesis_dictionary_xpos_fallback:
-                    prediction = self.hypothesis_dictionary_xpos_fallback[word_string][0]
-                else:
-                    if word_prediction.item() not in self.closed_classes_xpos:
-                        prediction = self.xpos_vocab[word_prediction.item()]
-                    else:
-                        prediction = self.xpos_vocab[self.closed_classes_xpos_inverse[padded_prediction[sent_id, word_id, self.closed_classes_xpos_inverse].argmax().item()]]
-                sent_predictions.append(prediction)
-            predictions.append(sent_predictions)
-        return predictions
-
-    def process_upos(self, padded_prediction, word_strings, upos_preds):
-        predictions = []
-
-        max_value = padded_prediction.max(2)[1]
-
-        for sent_id, (sent_indices, sent_strings, sent_upos) in enumerate(zip(max_value, word_strings, upos_preds)):
-            sent_predictions = []
-            for word_id, (word_prediction, word_string, word_upos) in enumerate(zip(sent_indices, sent_strings, sent_upos)):
-                key_tuple = (word_string, word_upos)
-                if key_tuple in self.hypothesis_dictionary_upos:
-                    # if only one possible prediction in hypothesis dictionary, take it!
-                    if len(self.hypothesis_dictionary_upos[key_tuple]) == 1:
-                        prediction = self.hypothesis_dictionary_upos[key_tuple][0]
-                    elif self.upos_vocab[word_prediction.item()] in self.hypothesis_dictionary_upos[key_tuple]:
-                        prediction = self.upos_vocab[word_prediction.item()]
-                    else:
-                        optional_indices = [self.upos_vocab[el] for el in self.hypothesis_dictionary_upos[key_tuple]]
-                        prediction = self.upos_vocab[optional_indices[padded_prediction[sent_id, word_id, optional_indices].argmax().item()]]
-                elif key_tuple in self.hypothesis_dictionary_upos_fallback:
-                    prediction = self.hypothesis_dictionary_upos_fallback[key_tuple][0]
-                else:
-                    # prediction = self.upos_vocab[padded_prediction[sent_id, word_id].argmax().item()]
-                    if word_prediction.item() not in self.closed_classes_upos:
-                        prediction = self.upos_vocab[word_prediction.item()]
-                    else:
-                        prediction = self.upos_vocab[self.closed_classes_upos_inverse[
-                            padded_prediction[sent_id, word_id, self.closed_classes_upos_inverse].argmax().item()]]
-                sent_predictions.append(prediction)
-            predictions.append(sent_predictions)
-        return predictions
-
-    def process_feats(self, padded_prediction, word_strings, xpos_preds, upos_preds):
-        predictions = []
-
-        for sent_id, (sent_strings, sent_xpos, sent_upos) in enumerate(zip(word_strings, xpos_preds, upos_preds)):
-            sent_predictions = []
-            for word_id, (word_string, word_xpos, word_upos) in enumerate(zip(sent_strings, sent_xpos, sent_upos)):
-                key_tuple = (word_string, word_xpos, word_upos)
-                if key_tuple in self.hypothesis_dictionary_feats:
-                    prediction = self.hypothesis_dictionary_feats[key_tuple][0]
-                else:
-                    word_feat = []
-                    for feat_id in range(len(padded_prediction)):
-                        word_feat.append(padded_prediction[feat_id][sent_id, word_id].item())
-                    prediction = self.feats_vocab[word_feat]
-                sent_predictions.append(prediction)
-            predictions.append(sent_predictions)
-        return predictions
-
-    def convert_feats(self, feats_string):
-        return feats_string.replace(' ', '|')
-
-    def extract_lexicon_data(self, lexicon):
-        """ Creates hypothesis dictionary from lexicon. """
-        if lexicon is None:
-            raise Exception("Inflectional lexicon is only supported for Slovenian standard models. If you are using other models, set `pos_use_lexicon` to `False` (or remove it). Otherwise, you have to re-download Slovenian models. You can do this by using the following command: classla.download('sl')")
-        for key in lexicon:
-            if key[1] in self.xpos_vocab:
-                self.hypothesis_dictionary_xpos.setdefault(key[0].lower(), []).append(key[1])
-            else:
-                self.hypothesis_dictionary_xpos_fallback.setdefault(key[0].lower(), []).append(key[1])
-
-            if key[2] in self.upos_vocab:
-                self.hypothesis_dictionary_upos.setdefault((key[0].lower(), key[1]), []).append(key[2])
-            else:
-                self.hypothesis_dictionary_upos_fallback.setdefault((key[0].lower(), key[1]), []).append(key[2])
-
-            feats = self.convert_feats(key[3]) if key[3] else '_'
-            self.hypothesis_dictionary_feats.setdefault((key[0].lower(), key[1], key[2]), []).append(feats)
-
-
-processors = {"ssj": SloveneInflectionalLexiconProcessor, "sl_ssj": SloveneInflectionalLexiconProcessor}
-
-
-class InflectionalLexicon:
-    def __init__(self, lexicon, shorthand, vocab, pretrain, pos_lemma_pretag=False):
-        """Base class for data converters for sequence classification data sets."""
-        self.shorthand = shorthand
-        assert shorthand in processors, f"Tag {shorthand} is not supported by inflectional lexicon."
-        self.processor = processors[shorthand](lexicon, vocab, pretrain, pos_lemma_pretag)
-        self.default_processor = DefaultPostprocessor(lexicon, vocab, pretrain, pos_lemma_pretag)
-
-    def process_xpos(self, padded_prediction, word_strings):
-        return self.processor.process_xpos(padded_prediction, word_strings)
-
-    def process_upos(self, padded_prediction, word_strings, xpos_preds):
-        return self.processor.process_upos(padded_prediction, word_strings, xpos_preds)
-
-    def process_feats(self, padded_prediction, word_strings, xpos_preds, upos_preds):
-        return self.processor.process_feats(padded_prediction, word_strings, xpos_preds, upos_preds)
-
-
-class DefaultPostprocessor(InflectionalLexiconProcessor):
-    def __init__(self, lexicon, vocab, pretrain, pos_lemma_pretag=False):
-        super(DefaultPostprocessor, self).__init__(lexicon, vocab, pretrain, pos_lemma_pretag)
-
-    def process_xpos(self, padded_prediction, word_strings):
-        return super(DefaultPostprocessor, self).process_xpos(padded_prediction, word_strings)
-
-    def process_upos(self, padded_prediction, word_strings, xpos_preds):
-        return super(DefaultPostprocessor, self).process_upos(padded_prediction, word_strings, xpos_preds)
-
-    def process_feats(self, padded_prediction, word_strings, xpos_preds, upos_preds):
-        return super(DefaultPostprocessor, self).process_feats(padded_prediction, word_strings, xpos_preds, upos_preds)
+import sys
+import unicodedata
+
+
+class InflectionalLexiconProcessor(object):
+    def __init__(self, lexicon, vocab, pretrain, pos_lemma_pretag=False):
+        # fills hypothesis_dictionary_xpos
+        self.hypothesis_dictionary_xpos = {}
+        self.hypothesis_dictionary_upos = {}
+        self.hypothesis_dictionary_feats = {}
+        # fallback for when xpos tag in influectional lexicon is not in vocab
+        self.hypothesis_dictionary_xpos_fallback = {}
+        self.hypothesis_dictionary_upos_fallback = {}
+        self.hypothesis_dictionary_feats_fallback = {}
+
+        closed_classes_xpos_rules = ['Z', 'punct'] if pos_lemma_pretag else []
+        closed_classes_upos_rules = ['PUNCT', 'SYM'] if pos_lemma_pretag else []
+
+        self.closed_classes_xpos = set()
+        self.closed_classes_upos = set()
+
+        self.xpos_vocab = vocab['xpos']
+        self.upos_vocab = vocab['upos']
+        self.feats_vocab = vocab['feats']
+
+        self.create_closed_classes_upos(vocab['upos'], closed_classes_upos_rules)
+        self.create_closed_classes_xpos(vocab['xpos'], closed_classes_xpos_rules)
+
+        self.closed_classes_upos_inverse = [vocab['upos'][el] for el in vocab['upos'] if
+                                       vocab['upos'][el] not in self.closed_classes_upos]
+        self.closed_classes_xpos_inverse = [vocab['xpos'][el] for el in vocab['xpos'] if
+                                       vocab['xpos'][el] not in self.closed_classes_xpos]
+
+    def process_xpos(self, padded_prediction, word_strings):
+        predictions = []
+
+        max_value = padded_prediction.max(2)[1]
+
+        for sent_id, (sent_indices, sent_strings) in enumerate(zip(max_value, word_strings)):
+            sent_predictions = []
+            for word_id, (word_prediction, word_string) in enumerate(zip(sent_indices, sent_strings)):
+                if word_prediction.item() not in self.closed_classes_xpos:
+                    prediction = self.xpos_vocab[word_prediction.item()]
+                else:
+                    prediction = self.xpos_vocab[self.closed_classes_xpos_inverse[
+                        padded_prediction[sent_id, word_id, self.closed_classes_xpos_inverse].argmax().item()]]
+                sent_predictions.append(prediction)
+            predictions.append(sent_predictions)
+        return predictions
+
+    def process_upos(self, padded_prediction, word_strings, xpos_preds):
+        predictions = []
+
+        max_value = padded_prediction.max(2)[1]
+
+        for sent_id, (sent_indices, sent_strings) in enumerate(zip(max_value, word_strings)):
+            sent_predictions = []
+            for word_id, (word_prediction, word_string) in enumerate(zip(sent_indices, sent_strings)):
+                if word_prediction.item() not in self.closed_classes_upos:
+                    prediction = self.upos_vocab[word_prediction.item()]
+                else:
+                    prediction = self.upos_vocab[self.closed_classes_upos_inverse[
+                        padded_prediction[sent_id, word_id, self.closed_classes_upos_inverse].argmax().item()]]
+                sent_predictions.append(prediction)
+            predictions.append(sent_predictions)
+        return predictions
+
+    def process_feats(self, padded_prediction, word_strings, xpos_preds, upos_preds):
+        predictions = []
+
+        for sent_id, (sent_strings, sent_xpos, sent_upos) in enumerate(zip(word_strings, xpos_preds, upos_preds)):
+            sent_predictions = []
+            for word_id in range(len(sent_strings)):
+                word_feat = []
+                for feat_id in range(len(padded_prediction)):
+                    word_feat.append(padded_prediction[feat_id][sent_id, word_id].item())
+                prediction = self.feats_vocab[word_feat]
+                sent_predictions.append(prediction)
+            predictions.append(sent_predictions)
+        return predictions
+
+    def create_closed_classes_xpos(self, vocab, closed_classes_rules):
+        """ Fills a set of closed classes, that contains xpos ids that are not permitted. """
+        for key in vocab:
+            if key[0] in closed_classes_rules or key == 'punct' and key in closed_classes_rules:
+                self.closed_classes_xpos.add(vocab[key])
+
+    def create_closed_classes_upos(self, vocab, closed_classes_rules):
+        """ Fills a set of closed classes, that contains xpos ids that are not permitted. """
+        for key in vocab:
+            if key in closed_classes_rules:
+                self.closed_classes_upos.add(vocab[key])
+
+
+class SloveneInflectionalLexiconProcessor(InflectionalLexiconProcessor):
+    def __init__(self, lexicon, vocab, pretrain, pos_lemma_pretag=False):
+        super(SloveneInflectionalLexiconProcessor, self).__init__(lexicon, vocab, pretrain, pos_lemma_pretag)
+        closed_classes_xpos_rules = ['P', 'S', 'C', 'Q']
+        closed_classes_upos_rules = ['PRON', 'DET', 'ADP', 'CCONJ', 'SCONJ', 'PART']
+
+        self.xpos_vocab = vocab['xpos']
+        self.upos_vocab = vocab['upos']
+        self.feats_vocab = vocab['feats']
+
+        self.extract_lexicon_data(lexicon)
+
+        # fills closed_classes
+        # self.closed_classes = set()
+        self.create_closed_classes_xpos(vocab['xpos'], closed_classes_xpos_rules)
+        self.create_closed_classes_xpos(vocab['upos'], closed_classes_upos_rules)
+        self.closed_classes_xpos_inverse = [vocab['xpos'][el] for el in vocab['xpos'] if vocab['xpos'][el] not in self.closed_classes_xpos]
+        self.closed_classes_upos_inverse = [vocab['upos'][el] for el in vocab['upos'] if vocab['upos'][el] not in self.closed_classes_upos]
+
+    def process_xpos(self, padded_prediction, word_strings):
+        predictions = []
+
+        max_value = padded_prediction.max(2)[1]
+
+        for sent_id, (sent_indices, sent_strings) in enumerate(zip(max_value, word_strings)):
+            sent_predictions = []
+            for word_id, (word_prediction, word_string) in enumerate(zip(sent_indices, sent_strings)):
+                if word_string in self.hypothesis_dictionary_xpos:
+                    # if only one possible prediction in hypothesis dictionary, take it!
+                    if len(self.hypothesis_dictionary_xpos[word_string]) == 1:
+                        prediction = self.hypothesis_dictionary_xpos[word_string][0]
+                    elif self.xpos_vocab[word_prediction.item()] in self.hypothesis_dictionary_xpos[word_string]:
+                        prediction = self.xpos_vocab[word_prediction.item()]
+                    else:
+                        optional_indices = [self.xpos_vocab[el] for el in self.hypothesis_dictionary_xpos[word_string]]
+                        prediction = self.xpos_vocab[optional_indices[padded_prediction[sent_id, word_id, optional_indices].argmax().item()]]
+                elif word_string in self.hypothesis_dictionary_xpos_fallback:
+                    prediction = self.hypothesis_dictionary_xpos_fallback[word_string][0]
+                else:
+                    if word_prediction.item() not in self.closed_classes_xpos:
+                        prediction = self.xpos_vocab[word_prediction.item()]
+                    else:
+                        prediction = self.xpos_vocab[self.closed_classes_xpos_inverse[padded_prediction[sent_id, word_id, self.closed_classes_xpos_inverse].argmax().item()]]
+                sent_predictions.append(prediction)
+            predictions.append(sent_predictions)
+        return predictions
+
+    def process_upos(self, padded_prediction, word_strings, upos_preds):
+        predictions = []
+
+        max_value = padded_prediction.max(2)[1]
+
+        for sent_id, (sent_indices, sent_strings, sent_upos) in enumerate(zip(max_value, word_strings, upos_preds)):
+            sent_predictions = []
+            for word_id, (word_prediction, word_string, word_upos) in enumerate(zip(sent_indices, sent_strings, sent_upos)):
+                key_tuple = (word_string, word_upos)
+                if key_tuple in self.hypothesis_dictionary_upos:
+                    # if only one possible prediction in hypothesis dictionary, take it!
+                    if len(self.hypothesis_dictionary_upos[key_tuple]) == 1:
+                        prediction = self.hypothesis_dictionary_upos[key_tuple][0]
+                    elif self.upos_vocab[word_prediction.item()] in self.hypothesis_dictionary_upos[key_tuple]:
+                        prediction = self.upos_vocab[word_prediction.item()]
+                    else:
+                        optional_indices = [self.upos_vocab[el] for el in self.hypothesis_dictionary_upos[key_tuple]]
+                        prediction = self.upos_vocab[optional_indices[padded_prediction[sent_id, word_id, optional_indices].argmax().item()]]
+                elif key_tuple in self.hypothesis_dictionary_upos_fallback:
+                    prediction = self.hypothesis_dictionary_upos_fallback[key_tuple][0]
+                else:
+                    # prediction = self.upos_vocab[padded_prediction[sent_id, word_id].argmax().item()]
+                    if word_prediction.item() not in self.closed_classes_upos:
+                        prediction = self.upos_vocab[word_prediction.item()]
+                    else:
+                        prediction = self.upos_vocab[self.closed_classes_upos_inverse[
+                            padded_prediction[sent_id, word_id, self.closed_classes_upos_inverse].argmax().item()]]
+                sent_predictions.append(prediction)
+            predictions.append(sent_predictions)
+        return predictions
+
+    def process_feats(self, padded_prediction, word_strings, xpos_preds, upos_preds):
+        predictions = []
+
+        for sent_id, (sent_strings, sent_xpos, sent_upos) in enumerate(zip(word_strings, xpos_preds, upos_preds)):
+            sent_predictions = []
+            for word_id, (word_string, word_xpos, word_upos) in enumerate(zip(sent_strings, sent_xpos, sent_upos)):
+                key_tuple = (word_string, word_xpos, word_upos)
+                if key_tuple in self.hypothesis_dictionary_feats:
+                    prediction = self.hypothesis_dictionary_feats[key_tuple][0]
+                else:
+                    word_feat = []
+                    for feat_id in range(len(padded_prediction)):
+                        word_feat.append(padded_prediction[feat_id][sent_id, word_id].item())
+                    prediction = self.feats_vocab[word_feat]
+                sent_predictions.append(prediction)
+            predictions.append(sent_predictions)
+        return predictions
+
+    def convert_feats(self, feats_string):
+        return feats_string.replace(' ', '|')
+
+    def extract_lexicon_data(self, lexicon):
+        """ Creates hypothesis dictionary from lexicon. """
+        if lexicon is None:
+            raise Exception("Inflectional lexicon is only supported for Slovenian standard models. If you are using other models, set `pos_use_lexicon` to `False` (or remove it). Otherwise, you have to re-download Slovenian models. You can do this by using the following command: classla.download('sl')")
+        for key in lexicon:
+            if key[1] in self.xpos_vocab:
+                self.hypothesis_dictionary_xpos.setdefault(key[0].lower(), []).append(key[1])
+            else:
+                self.hypothesis_dictionary_xpos_fallback.setdefault(key[0].lower(), []).append(key[1])
+
+            if key[2] in self.upos_vocab:
+                self.hypothesis_dictionary_upos.setdefault((key[0].lower(), key[1]), []).append(key[2])
+            else:
+                self.hypothesis_dictionary_upos_fallback.setdefault((key[0].lower(), key[1]), []).append(key[2])
+
+            feats = self.convert_feats(key[3]) if key[3] else '_'
+            self.hypothesis_dictionary_feats.setdefault((key[0].lower(), key[1], key[2]), []).append(feats)
+
+
+processors = {"ssj": SloveneInflectionalLexiconProcessor, "sl_ssj": SloveneInflectionalLexiconProcessor}
+
+
+class InflectionalLexicon:
+    def __init__(self, lexicon, shorthand, vocab, pretrain, pos_lemma_pretag=False):
+        """Base class for data converters for sequence classification data sets."""
+        self.shorthand = shorthand
+        assert shorthand in processors, f"Tag {shorthand} is not supported by inflectional lexicon."
+        self.processor = processors[shorthand](lexicon, vocab, pretrain, pos_lemma_pretag)
+        self.default_processor = DefaultPostprocessor(lexicon, vocab, pretrain, pos_lemma_pretag)
+
+    def process_xpos(self, padded_prediction, word_strings):
+        return self.processor.process_xpos(padded_prediction, word_strings)
+
+    def process_upos(self, padded_prediction, word_strings, xpos_preds):
+        return self.processor.process_upos(padded_prediction, word_strings, xpos_preds)
+
+    def process_feats(self, padded_prediction, word_strings, xpos_preds, upos_preds):
+        return self.processor.process_feats(padded_prediction, word_strings, xpos_preds, upos_preds)
+
+
+class DefaultPostprocessor(InflectionalLexiconProcessor):
+    def __init__(self, lexicon, vocab, pretrain, pos_lemma_pretag=False):
+        super(DefaultPostprocessor, self).__init__(lexicon, vocab, pretrain, pos_lemma_pretag)
+
+    def process_xpos(self, padded_prediction, word_strings):
+        return super(DefaultPostprocessor, self).process_xpos(padded_prediction, word_strings)
+
+    def process_upos(self, padded_prediction, word_strings, xpos_preds):
+        return super(DefaultPostprocessor, self).process_upos(padded_prediction, word_strings, xpos_preds)
+
+    def process_feats(self, padded_prediction, word_strings, xpos_preds, upos_preds):
+        return super(DefaultPostprocessor, self).process_feats(padded_prediction, word_strings, xpos_preds, upos_preds)
```

### Comparing `classla-2.0/classla/models/pos/scorer.py` & `classla-2.1/classla/models/pos/scorer.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-"""
-Utils and wrappers for scoring taggers.
-"""
-import logging
-
-from classla.models.common.utils import ud_scores
-
-logger = logging.getLogger('classla')
-
-def score(system_conllu_file, gold_conllu_file, verbose=True):
-    """ Wrapper for tagger scorer. """
-    evaluation = ud_scores(gold_conllu_file, system_conllu_file)
-    el = evaluation['AllTags']
-    p = el.precision
-    r = el.recall
-    f = el.f1
-    if verbose:
-        scores = [evaluation[k].f1 * 100 for k in ['UPOS', 'XPOS', 'UFeats', 'AllTags']]
-        logger.info("UPOS\tXPOS\tUFeats\tAllTags")
-        logger.info("{:.2f}\t{:.2f}\t{:.2f}\t{:.2f}".format(*scores))
-    return p, r, f
-
+"""
+Utils and wrappers for scoring taggers.
+"""
+import logging
+
+from classla.models.common.utils import ud_scores
+
+logger = logging.getLogger('classla')
+
+def score(system_conllu_file, gold_conllu_file, verbose=True):
+    """ Wrapper for tagger scorer. """
+    evaluation = ud_scores(gold_conllu_file, system_conllu_file)
+    el = evaluation['AllTags']
+    p = el.precision
+    r = el.recall
+    f = el.f1
+    if verbose:
+        scores = [evaluation[k].f1 * 100 for k in ['UPOS', 'XPOS', 'UFeats', 'AllTags']]
+        logger.info("UPOS\tXPOS\tUFeats\tAllTags")
+        logger.info("{:.2f}\t{:.2f}\t{:.2f}\t{:.2f}".format(*scores))
+    return p, r, f
+
```

### Comparing `classla-2.0/classla/models/pos/trainer.py` & `classla-2.1/classla/models/pos/trainer.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,167 +1,167 @@
-"""
-A trainer class to handle training and testing of models.
-"""
-
-import sys
-import logging
-import torch
-from torch import nn
-
-from classla.models.common.trainer import Trainer as BaseTrainer
-from classla.models.common import utils, loss
-from classla.models.pos.model import Tagger
-from classla.models.pos.postprocessor import InflectionalLexicon, DefaultPostprocessor
-from classla.models.pos.vocab import MultiVocab
-
-logger = logging.getLogger('classla')
-
-def unpack_batch(batch, use_cuda):
-    """ Unpack a batch from the data loader. """
-    if use_cuda:
-        inputs = [b.cuda() if b is not None else None for b in batch[:8]]
-    else:
-        inputs = batch[:8]
-    orig_idx = batch[8]
-    word_orig_idx = batch[9]
-    sentlens = batch[10]
-    wordlens = batch[11]
-    word_string = batch[12]
-    return inputs, orig_idx, word_orig_idx, sentlens, wordlens, word_string
-
-
-class Trainer(BaseTrainer):
-    """ A trainer for training models. """
-    def __init__(self, args=None, vocab=None, pretrain=None, model_file=None, use_cuda=False):
-        self.use_cuda = use_cuda
-        if model_file is not None:
-            # load everything from file
-            self.load(model_file, pretrain)
-        else:
-            # build model from scratch
-            self.args = args
-            self.vocab = vocab
-            self.model = Tagger(args, vocab, emb_matrix=pretrain.emb if pretrain is not None else None, share_hid=args['share_hid'])
-            self.dict = None
-
-        self.use_lexicon = args['use_lexicon'] if 'use_lexicon' in args else None
-        self.pos_lemma_pretag = args['lemma_pretag'] if 'lemma_pretag' in args else None
-        self.postprocessor = None
-        if self.use_lexicon:
-            args['shorthand'] = args['shorthand'] if 'shorthand' in args else self.args['shorthand']
-            inflectional_lexicon = self.dict
-            self.postprocessor = InflectionalLexicon(inflectional_lexicon, args['shorthand'], self.vocab, pretrain,
-                                                     args['lemma_pretag'])
-        else:
-            if self.pos_lemma_pretag:
-                self.postprocessor = DefaultPostprocessor(None, self.vocab, None, pos_lemma_pretag=self.pos_lemma_pretag)
-            else:
-                self.postprocessor = None
-        self.parameters = [p for p in self.model.parameters() if p.requires_grad]
-        if self.use_cuda:
-            self.model.cuda()
-        else:
-            self.model.cpu()
-        self.optimizer = utils.get_optimizer(self.args['optim'], self.parameters, self.args['lr'], betas=(0.9, self.args['beta2']), eps=1e-6)
-
-    def update(self, batch, eval=False):
-        inputs, orig_idx, word_orig_idx, sentlens, wordlens, word_string = unpack_batch(batch, self.use_cuda)
-        word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained = inputs
-
-        if eval:
-            self.model.eval()
-        else:
-            self.model.train()
-            self.optimizer.zero_grad()
-        loss, _ = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, word_string)
-        loss_val = loss.data.item()
-        if eval:
-            return loss_val
-
-        loss.backward()
-        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args['max_grad_norm'])
-        self.optimizer.step()
-        return loss_val
-
-    def predict(self, batch, unsort=True):
-        inputs, orig_idx, word_orig_idx, sentlens, wordlens, word_string = unpack_batch(batch, self.use_cuda)
-        word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained = inputs
-
-        self.model.eval()
-        batch_size = word.size(0)
-        _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, word_string, postprocessor=self.postprocessor)
-
-        # upos_seqs = [self.vocab['upos'].unmap(sent) for sent in preds[0].tolist()]
-        # feats_seqs = [self.vocab['feats'].unmap(sent) for sent in preds[2].tolist()]
-
-        if self.postprocessor is None:
-            upos_seqs = [self.vocab['upos'].unmap(sent) for sent in preds[0].tolist()]
-            xpos_seqs = [self.vocab['xpos'].unmap(sent) for sent in preds[1].tolist()]
-            feats_seqs = [self.vocab['feats'].unmap(sent) for sent in preds[2].tolist()]
-        else:
-            upos_seqs = preds[0]
-            xpos_seqs = preds[1]
-            feats_seqs = preds[2]
-
-        pred_tokens = [[[upos_seqs[i][j], xpos_seqs[i][j], feats_seqs[i][j]] for j in range(sentlens[i])] for i in range(batch_size)]
-        if unsort:
-            pred_tokens = utils.unsort(pred_tokens, orig_idx)
-        return pred_tokens
-
-    def save(self, filename, skip_modules=True, inflectional_lexicon=None):
-        model_state = self.model.state_dict()
-        # skip saving modules like pretrained embeddings, because they are large and will be saved in a separate file
-        if skip_modules:
-            skipped = [k for k in model_state.keys() if k.split('.')[0] in self.model.unsaved_modules]
-            for k in skipped:
-                del model_state[k]
-        params = {
-                'model': model_state,
-                'vocab': self.vocab.state_dict(),
-                'config': self.args
-                }
-        if inflectional_lexicon:
-            params['dicts'] = inflectional_lexicon
-
-        try:
-            torch.save(params, filename)
-            logger.info("Model saved to {}".format(filename))
-        except (KeyboardInterrupt, SystemExit):
-            raise
-        except:
-            logger.warning("Saving failed... continuing anyway.")
-
-    def load(self, filename, pretrain):
-        """
-        Load a model from file, with preloaded pretrain embeddings. Here we allow the pretrain to be None or a dummy input,
-        and the actual use of pretrain embeddings will depend on the boolean config "pretrain" in the loaded args.
-        """
-        try:
-            checkpoint = torch.load(filename, lambda storage, loc: storage)
-        except BaseException:
-            logger.error("Cannot load model from {}".format(filename))
-            raise
-        self.args = checkpoint['config']
-        self.vocab = MultiVocab.load_state_dict(checkpoint['vocab'])
-        # load model
-        emb_matrix = None
-        if self.args['pretrain'] and pretrain is not None: # we use pretrain only if args['pretrain'] == True and pretrain is not None
-            emb_matrix = pretrain.emb
-        self.model = Tagger(self.args, self.vocab, emb_matrix=emb_matrix, share_hid=self.args['share_hid'])
-        self.model.load_state_dict(checkpoint['model'], strict=False)
-        self.dict = checkpoint['dicts'] if 'dicts' in checkpoint else None
-
-    @staticmethod
-    def load_inflectional_lexicon(filename):
-        try:
-            checkpoint = torch.load(filename, lambda storage, loc: storage)
-        except BaseException:
-            logger.error("Cannot load model from {}".format(filename))
-            raise
-
-        inf_lexicon = {}
-        assert 'dicts' in checkpoint, Exception('Can not load inflectional dictionary. Make sure that your tagger model has it.')
-        for entry in checkpoint['dicts']:
-            if (entry[0], entry[1]) not in inf_lexicon:
-                inf_lexicon[(entry[0], entry[1])] = entry[4]
-
-        return inf_lexicon
+"""
+A trainer class to handle training and testing of models.
+"""
+
+import sys
+import logging
+import torch
+from torch import nn
+
+from classla.models.common.trainer import Trainer as BaseTrainer
+from classla.models.common import utils, loss
+from classla.models.pos.model import Tagger
+from classla.models.pos.postprocessor import InflectionalLexicon, DefaultPostprocessor
+from classla.models.pos.vocab import MultiVocab
+
+logger = logging.getLogger('classla')
+
+def unpack_batch(batch, use_cuda):
+    """ Unpack a batch from the data loader. """
+    if use_cuda:
+        inputs = [b.cuda() if b is not None else None for b in batch[:8]]
+    else:
+        inputs = batch[:8]
+    orig_idx = batch[8]
+    word_orig_idx = batch[9]
+    sentlens = batch[10]
+    wordlens = batch[11]
+    word_string = batch[12]
+    return inputs, orig_idx, word_orig_idx, sentlens, wordlens, word_string
+
+
+class Trainer(BaseTrainer):
+    """ A trainer for training models. """
+    def __init__(self, args=None, vocab=None, pretrain=None, model_file=None, use_cuda=False):
+        self.use_cuda = use_cuda
+        if model_file is not None:
+            # load everything from file
+            self.load(model_file, pretrain)
+        else:
+            # build model from scratch
+            self.args = args
+            self.vocab = vocab
+            self.model = Tagger(args, vocab, emb_matrix=pretrain.emb if pretrain is not None else None, share_hid=args['share_hid'])
+            self.dict = None
+
+        self.use_lexicon = args['use_lexicon'] if 'use_lexicon' in args else None
+        self.pos_lemma_pretag = args['lemma_pretag'] if 'lemma_pretag' in args else None
+        self.postprocessor = None
+        if self.use_lexicon:
+            args['shorthand'] = args['shorthand'] if 'shorthand' in args else self.args['shorthand']
+            inflectional_lexicon = self.dict
+            self.postprocessor = InflectionalLexicon(inflectional_lexicon, args['shorthand'], self.vocab, pretrain,
+                                                     args['lemma_pretag'])
+        else:
+            if self.pos_lemma_pretag:
+                self.postprocessor = DefaultPostprocessor(None, self.vocab, None, pos_lemma_pretag=self.pos_lemma_pretag)
+            else:
+                self.postprocessor = None
+        self.parameters = [p for p in self.model.parameters() if p.requires_grad]
+        if self.use_cuda:
+            self.model.cuda()
+        else:
+            self.model.cpu()
+        self.optimizer = utils.get_optimizer(self.args['optim'], self.parameters, self.args['lr'], betas=(0.9, self.args['beta2']), eps=1e-6)
+
+    def update(self, batch, eval=False):
+        inputs, orig_idx, word_orig_idx, sentlens, wordlens, word_string = unpack_batch(batch, self.use_cuda)
+        word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained = inputs
+
+        if eval:
+            self.model.eval()
+        else:
+            self.model.train()
+            self.optimizer.zero_grad()
+        loss, _ = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, word_string)
+        loss_val = loss.data.item()
+        if eval:
+            return loss_val
+
+        loss.backward()
+        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args['max_grad_norm'])
+        self.optimizer.step()
+        return loss_val
+
+    def predict(self, batch, unsort=True):
+        inputs, orig_idx, word_orig_idx, sentlens, wordlens, word_string = unpack_batch(batch, self.use_cuda)
+        word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained = inputs
+
+        self.model.eval()
+        batch_size = word.size(0)
+        _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, word_string, postprocessor=self.postprocessor)
+
+        # upos_seqs = [self.vocab['upos'].unmap(sent) for sent in preds[0].tolist()]
+        # feats_seqs = [self.vocab['feats'].unmap(sent) for sent in preds[2].tolist()]
+
+        if self.postprocessor is None:
+            upos_seqs = [self.vocab['upos'].unmap(sent) for sent in preds[0].tolist()]
+            xpos_seqs = [self.vocab['xpos'].unmap(sent) for sent in preds[1].tolist()]
+            feats_seqs = [self.vocab['feats'].unmap(sent) for sent in preds[2].tolist()]
+        else:
+            upos_seqs = preds[0]
+            xpos_seqs = preds[1]
+            feats_seqs = preds[2]
+
+        pred_tokens = [[[upos_seqs[i][j], xpos_seqs[i][j], feats_seqs[i][j]] for j in range(sentlens[i])] for i in range(batch_size)]
+        if unsort:
+            pred_tokens = utils.unsort(pred_tokens, orig_idx)
+        return pred_tokens
+
+    def save(self, filename, skip_modules=True, inflectional_lexicon=None):
+        model_state = self.model.state_dict()
+        # skip saving modules like pretrained embeddings, because they are large and will be saved in a separate file
+        if skip_modules:
+            skipped = [k for k in model_state.keys() if k.split('.')[0] in self.model.unsaved_modules]
+            for k in skipped:
+                del model_state[k]
+        params = {
+                'model': model_state,
+                'vocab': self.vocab.state_dict(),
+                'config': self.args
+                }
+        if inflectional_lexicon:
+            params['dicts'] = inflectional_lexicon
+
+        try:
+            torch.save(params, filename)
+            logger.info("Model saved to {}".format(filename))
+        except (KeyboardInterrupt, SystemExit):
+            raise
+        except:
+            logger.warning("Saving failed... continuing anyway.")
+
+    def load(self, filename, pretrain):
+        """
+        Load a model from file, with preloaded pretrain embeddings. Here we allow the pretrain to be None or a dummy input,
+        and the actual use of pretrain embeddings will depend on the boolean config "pretrain" in the loaded args.
+        """
+        try:
+            checkpoint = torch.load(filename, lambda storage, loc: storage)
+        except BaseException:
+            logger.error("Cannot load model from {}".format(filename))
+            raise
+        self.args = checkpoint['config']
+        self.vocab = MultiVocab.load_state_dict(checkpoint['vocab'])
+        # load model
+        emb_matrix = None
+        if self.args['pretrain'] and pretrain is not None: # we use pretrain only if args['pretrain'] == True and pretrain is not None
+            emb_matrix = pretrain.emb
+        self.model = Tagger(self.args, self.vocab, emb_matrix=emb_matrix, share_hid=self.args['share_hid'])
+        self.model.load_state_dict(checkpoint['model'], strict=False)
+        self.dict = checkpoint['dicts'] if 'dicts' in checkpoint else None
+
+    @staticmethod
+    def load_inflectional_lexicon(filename):
+        try:
+            checkpoint = torch.load(filename, lambda storage, loc: storage)
+        except BaseException:
+            logger.error("Cannot load model from {}".format(filename))
+            raise
+
+        inf_lexicon = {}
+        assert 'dicts' in checkpoint, Exception('Can not load inflectional dictionary. Make sure that your tagger model has it.')
+        for entry in checkpoint['dicts']:
+            if (entry[0], entry[1]) not in inf_lexicon:
+                inf_lexicon[(entry[0], entry[1])] = entry[4]
+
+        return inf_lexicon
```

### Comparing `classla-2.0/classla/models/pos/xpos_vocab_factory.py` & `classla-2.1/classla/models/pos/xpos_vocab_factory.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-# This is the XPOS factory method generated automatically from models.pos.build_xpos_factory.
-# Please don't edit it!
-
-from classla.models.pos.vocab import WordVocab, XPOSVocab
-
-def xpos_vocab_factory(data, shorthand):
-    if shorthand in ["af_afribooms", "grc_perseus"]:
-        return XPOSVocab(data, shorthand, idx=2, sep="")
-    elif shorthand in ["grc_proiel", "hy_armtdp", "eu_bdt", "be_hse", "ca_ancora", "zh-hant_gsd", "zh-hans_gsdsimp", "lzh_kyoto", "cop_scriptorium", "da_ddt", "en_ewt", "en_gum", "et_edt", "fi_tdt", "fr_ftb", "fr_gsd", "fr_sequoia", "fr_spoken", "de_gsd", "de_hdt", "got_proiel", "el_gdt", "he_htb", "hi_hdtb", "hu_szeged", "ga_idt", "ja_bccwj", "la_proiel", "lt_hse", "mt_mudt", "mr_ufal", "nb_bokmaal", "nn_nynorsk", "nn_nynorsklia", "cu_proiel", "fro_srcmf", "orv_torot", "fa_seraji", "pt_bosque", "pt_gsd", "ru_gsd", "ru_syntagrus", "ru_taiga", "es_ancora", "es_gsd", "swl_sslc", "te_mtg", "tr_imst", "ug_udt", "vi_vtb", "wo_wtb", "bxr_bdt", "et_ewt", "kk_ktb", "kmr_mg", "olo_kkpp", "sme_giella", "hsb_ufal", "ar_padt", "bg_btb", "hr_set", "cs_cac", "cs_cltt", "cs_fictree", "cs_pdt", "en_partut", "fr_partut", "gl_ctg", "it_isdt", "it_partut", "it_postwita", "it_twittiro", "it_vit", "ja_gsd", "lv_lvtb", "lt_alksnis", "ro_nonstandard", "ro_rrt", "gd_arcosg", "sr_set", "sk_snk", "sl_ssj", "ta_ttb", "uk_iu", "gl_treegal", "la_perseus", "sl_sst"]:
-        return WordVocab(data, shorthand, idx=2, ignore=["_"])
-    elif shorthand in ["nl_alpino", "nl_lassysmall", "la_ittb", "sv_talbanken"]:
-        return XPOSVocab(data, shorthand, idx=2, sep="|")
-    elif shorthand in ["en_lines", "sv_lines", "ur_udtb"]:
-        return XPOSVocab(data, shorthand, idx=2, sep="-")
-    elif shorthand in ["fi_ftb"]:
-        return XPOSVocab(data, shorthand, idx=2, sep=",")
-    elif shorthand in ["id_gsd", "ko_gsd", "ko_kaist"]:
-        return XPOSVocab(data, shorthand, idx=2, sep="+")
-    elif shorthand in ["pl_lfg", "pl_pdb"]:
-        return XPOSVocab(data, shorthand, idx=2, sep=":")
-    else:
-        raise NotImplementedError('Language shorthand "{}" not found!'.format(shorthand))
+# This is the XPOS factory method generated automatically from models.pos.build_xpos_factory.
+# Please don't edit it!
+
+from classla.models.pos.vocab import WordVocab, XPOSVocab
+
+def xpos_vocab_factory(data, shorthand):
+    if shorthand in ["af_afribooms", "grc_perseus"]:
+        return XPOSVocab(data, shorthand, idx=2, sep="")
+    elif shorthand in ["grc_proiel", "hy_armtdp", "eu_bdt", "be_hse", "ca_ancora", "zh-hant_gsd", "zh-hans_gsdsimp", "lzh_kyoto", "cop_scriptorium", "da_ddt", "en_ewt", "en_gum", "et_edt", "fi_tdt", "fr_ftb", "fr_gsd", "fr_sequoia", "fr_spoken", "de_gsd", "de_hdt", "got_proiel", "el_gdt", "he_htb", "hi_hdtb", "hu_szeged", "ga_idt", "ja_bccwj", "la_proiel", "lt_hse", "mt_mudt", "mr_ufal", "nb_bokmaal", "nn_nynorsk", "nn_nynorsklia", "cu_proiel", "fro_srcmf", "orv_torot", "fa_seraji", "pt_bosque", "pt_gsd", "ru_gsd", "ru_syntagrus", "ru_taiga", "es_ancora", "es_gsd", "swl_sslc", "te_mtg", "tr_imst", "ug_udt", "vi_vtb", "wo_wtb", "bxr_bdt", "et_ewt", "kk_ktb", "kmr_mg", "olo_kkpp", "sme_giella", "hsb_ufal", "ar_padt", "bg_btb", "mk_set", "hr_set", "cs_cac", "cs_cltt", "cs_fictree", "cs_pdt", "en_partut", "fr_partut", "gl_ctg", "it_isdt", "it_partut", "it_postwita", "it_twittiro", "it_vit", "ja_gsd", "lv_lvtb", "lt_alksnis", "ro_nonstandard", "ro_rrt", "gd_arcosg", "sr_set", "sk_snk", "sl_ssj", "ta_ttb", "uk_iu", "gl_treegal", "la_perseus", "sl_sst"]:
+        return WordVocab(data, shorthand, idx=2, ignore=["_"])
+    elif shorthand in ["nl_alpino", "nl_lassysmall", "la_ittb", "sv_talbanken"]:
+        return XPOSVocab(data, shorthand, idx=2, sep="|")
+    elif shorthand in ["en_lines", "sv_lines", "ur_udtb"]:
+        return XPOSVocab(data, shorthand, idx=2, sep="-")
+    elif shorthand in ["fi_ftb"]:
+        return XPOSVocab(data, shorthand, idx=2, sep=",")
+    elif shorthand in ["id_gsd", "ko_gsd", "ko_kaist"]:
+        return XPOSVocab(data, shorthand, idx=2, sep="+")
+    elif shorthand in ["pl_lfg", "pl_pdb"]:
+        return XPOSVocab(data, shorthand, idx=2, sep=":")
+    else:
+        raise NotImplementedError('Language shorthand "{}" not found!'.format(shorthand))
```

### Comparing `classla-2.0/classla/models/srl/data.py` & `classla-2.1/classla/models/pos/data.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,156 +1,163 @@
-import random
-import logging
-import torch
-
-from classla.models.common.data import get_long_tensor, sort_all
-from classla.models.common.vocab import PAD_ID
-from classla.models.lemma.vocab import Vocab
-from classla.models.pos.vocab import WordVocab
-from classla.models.srl.vocab import MultiVocab
-from classla.models.common.doc import *
-
-logger = logging.getLogger('classla')
-
-class DataLoader:
-    def __init__(self, doc, batch_size, args, pretrain=None, vocab=None, evaluation=False, preprocess_tags=True):
-        self.batch_size = batch_size
-        self.args = args
-        self.eval = evaluation
-        self.shuffled = not self.eval
-        self.doc = doc
-        self.preprocess_tags = preprocess_tags
-
-        data = self.load_doc(self.doc)
-        self.srls = [[w[5] for w in sent] for sent in data]
-
-        # handle vocab
-        self.pretrain = pretrain
-        if vocab is None:
-            self.vocab = self.init_vocab(data)
-        else:
-            self.vocab = vocab
-
-        # filter and sample data
-        if args.get('sample_train', 1.0) < 1.0 and not self.eval:
-            keep = int(args['sample_train'] * len(data))
-            data = random.sample(data, keep)
-            logger.debug("Subsample training set with rate {:g}".format(args['sample_train']))
-
-        # data = self.preprocess(data, self.vocab, args)
-        data = self.preprocess(data, self.vocab, self.pretrain, args)
-        # shuffle for training
-        if self.shuffled:
-            random.shuffle(data)
-        self.num_examples = len(data)
-
-        # chunk into batches
-        self.data = self.chunk_batches(data)
-        logger.debug("{} batches created.".format(len(self.data)))
-
-    def init_vocab(self, data):
-        assert self.eval == False  # for eval vocab must exist
-        wordvocab = WordVocab(data, self.args['shorthand'], cutoff=5, lower=True)
-        pos_data = [w[1] for s in data for w in s]
-        xposvocab = Vocab(pos_data, self.args['lang'])
-        lemmavocab = WordVocab(data, self.args['shorthand'], cutoff=5, idx=2, lower=True)
-        deprelvocab = WordVocab(data, self.args['shorthand'], idx=4)
-        srlvocab = WordVocab(data, self.args['shorthand'], idx=5)
-        vocab = MultiVocab({'word': wordvocab,
-                            'xpos': xposvocab,
-                            'lemma': lemmavocab,
-                            'deprel': deprelvocab,
-                            'srl': srlvocab
-                            })
-        return vocab
-
-    def preprocess(self, data, vocab, pretrain, args):
-        processed = []
-        for sent in data:
-            processed_sent = [vocab['word'].map([w[0] for w in sent])]
-            processed_sent += [vocab['deprel'].map([w[4] for w in sent])]
-            # Head word
-            processed_sent += [vocab['word'].map([sent[w[3]-1][0] if w[3] > 0 else '<ROOT>' for w in sent])]
-            processed_sent += [vocab['xpos'].map([w[1] for w in sent])]
-            processed_sent += [vocab['lemma'].map([w[2] for w in sent])]
-            # head lemma
-            processed_sent += [vocab['lemma'].map([sent[w[3]-1][2] if w[3] > 0 else '<ROOT>' for w in sent])]
-            # head xpos
-            processed_sent += [vocab['xpos'].map([sent[w[3]-1][1] if w[3] > 0 else '<UNK>' for w in sent])]
-
-            # pretrain word encodings
-            processed_sent += [pretrain.vocab.map([w[0] for w in sent])]
-            processed_sent += [pretrain.vocab.map([sent[w[3]-1][0] if w[3] > 0 else '<ROOT>' for w in sent])]
-
-            processed_sent += [vocab['srl'].map([w[5] for w in sent])]
-
-            processed.append(processed_sent)
-        return processed
-
-    def __len__(self):
-        return len(self.data)
-
-    def __getitem__(self, key):
-        """ Get a batch with index. """
-        if not isinstance(key, int):
-            raise TypeError
-        if key < 0 or key >= len(self.data):
-            raise IndexError
-        batch = self.data[key]
-        batch_size = len(batch)
-        batch = list(zip(*batch))
-        assert len(batch) == 10 # words: List[List[int]], chars: List[List[List[int]]], tags: List[List[int]]
-
-        # sort sentences by lens for easy RNN operations
-        lens = [len(x) for x in batch[0]]
-        batch, orig_idx = sort_all(batch, lens)
-
-        # convert to tensors
-        words = batch[0]
-        words = get_long_tensor(words, batch_size)
-        words_mask = torch.eq(words, PAD_ID)
-
-        deprel = get_long_tensor(batch[1], batch_size)
-
-        head_words = get_long_tensor(batch[2], batch_size)
-
-        xpos = get_long_tensor(batch[3], batch_size)
-        lemma = get_long_tensor(batch[4], batch_size)
-        head_lemma = get_long_tensor(batch[5], batch_size)
-        sentlens = [len(x) for x in batch[0]]
-        head_xpos = get_long_tensor(batch[6], batch_size)
-
-        pretrained = get_long_tensor(batch[7], batch_size)
-        head_pretrained = get_long_tensor(batch[8], batch_size)
-
-        srl = get_long_tensor(batch[9], batch_size)
-
-        return words, words_mask, deprel, head_words, xpos, lemma, head_lemma, head_xpos, pretrained, head_pretrained, srl, orig_idx, sentlens
-
-    def __iter__(self):
-        for i in range(self.__len__()):
-            yield self.__getitem__(i)
-
-    def load_doc(self, doc):
-        # data = doc.get([TEXT, NER], as_sentences=True, from_token=True)
-        data = doc.get([TEXT, XPOS, LEMMA, HEAD, DEPREL, SRL], as_sentences=True)
-        data = self.resolve_none(data)
-        return data
-
-    def resolve_none(self, data):
-        # replace None to '_'
-        for sent_idx in range(len(data)):
-            for tok_idx in range(len(data[sent_idx])):
-                for feat_idx in range(len(data[sent_idx][tok_idx])):
-                    if data[sent_idx][tok_idx][feat_idx] is None:
-                        data[sent_idx][tok_idx][feat_idx] = '_'
-        return data
-
-    def reshuffle(self):
-        data = [y for x in self.data for y in x]
-        random.shuffle(data)
-        self.data = self.chunk_batches(data)
-
-    def chunk_batches(self, data):
-        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]
-        return data
-
+import random
+import logging
+import torch
+
+from classla.models.common.data import map_to_ids, get_long_tensor, get_float_tensor, sort_all
+from classla.models.common.vocab import PAD_ID, VOCAB_PREFIX
+from classla.models.pos.vocab import CharVocab, WordVocab, XPOSVocab, FeatureVocab, MultiVocab
+from classla.models.pos.xpos_vocab_factory import xpos_vocab_factory
+from classla.models.common.doc import *
+
+logger = logging.getLogger('classla')
+
+class DataLoader:
+    def __init__(self, doc, batch_size, args, pretrain, vocab=None, evaluation=False, sort_during_eval=False):
+        self.batch_size = batch_size
+        self.args = args
+        self.eval = evaluation
+        self.shuffled = not self.eval
+        self.sort_during_eval = sort_during_eval
+        self.doc = doc
+
+        data = self.load_doc(self.doc)
+
+        # handle vocab
+        if vocab is None:
+            self.vocab = self.init_vocab(data)
+        else:
+            self.vocab = vocab
+
+        self.pretrain_vocab = pretrain.vocab
+
+        # filter and sample data
+        if args.get('sample_train', 1.0) < 1.0 and not self.eval:
+            keep = int(args['sample_train'] * len(data))
+            data = random.sample(data, keep)
+            logger.debug("Subsample training set with rate {:g}".format(args['sample_train']))
+
+        data = self.preprocess(data, self.vocab, self.pretrain_vocab, args)
+        # shuffle for training
+        if self.shuffled:
+            random.shuffle(data)
+        self.num_examples = len(data)
+
+        # chunk into batches
+        self.data = self.chunk_batches(data)
+        logger.debug("{} batches created.".format(len(self.data)))
+
+    def init_vocab(self, data):
+        assert self.eval == False # for eval vocab must exist
+        charvocab = CharVocab(data, self.args['shorthand'])
+        wordvocab = WordVocab(data, self.args['shorthand'], cutoff=7, lower=True)
+        uposvocab = WordVocab(data, self.args['shorthand'], idx=1)
+        xposvocab = xpos_vocab_factory(data, self.args['shorthand'])
+        featsvocab = FeatureVocab(data, self.args['shorthand'], idx=3)
+        vocab = MultiVocab({'char': charvocab,
+                            'word': wordvocab,
+                            'upos': uposvocab,
+                            'xpos': xposvocab,
+                            'feats': featsvocab})
+        return vocab
+
+    def preprocess(self, data, vocab, pretrain_vocab, args):
+        processed = []
+        for sent in data:
+            processed_sent = [vocab['word'].map([w[0] for w in sent])]
+            processed_sent += [[vocab['char'].map([x for x in w[0]]) for w in sent]]
+            processed_sent += [vocab['upos'].map([w[1] for w in sent])]
+            processed_sent += [vocab['xpos'].map([w[2] for w in sent])]
+            processed_sent += [vocab['feats'].map([w[3] for w in sent])]
+            processed_sent += [pretrain_vocab.map([w[0] for w in sent])]
+            processed_sent += [[w[0].lower() for w in sent]]
+            processed.append(processed_sent)
+        return processed
+
+    def __len__(self):
+        return len(self.data)
+
+    def __getitem__(self, key):
+        """ Get a batch with index. """
+        if not isinstance(key, int):
+            raise TypeError
+        if key < 0 or key >= len(self.data):
+            raise IndexError
+        batch = self.data[key]
+        batch_size = len(batch)
+        batch = list(zip(*batch))
+        assert len(batch) == 7
+
+        # sort sentences by lens for easy RNN operations
+        lens = [len(x) for x in batch[0]]
+        batch, orig_idx = sort_all(batch, lens)
+
+        # sort words by lens for easy char-RNN operations
+        batch_words = [w for sent in batch[1] for w in sent]
+        word_lens = [len(x) for x in batch_words]
+        batch_words, word_orig_idx = sort_all([batch_words], word_lens)
+        batch_words = batch_words[0]
+        word_lens = [len(x) for x in batch_words]
+
+        # convert to tensors
+        words = batch[0]
+        words = get_long_tensor(words, batch_size)
+        words_mask = torch.eq(words, PAD_ID)
+        wordchars = get_long_tensor(batch_words, len(word_lens))
+        wordchars_mask = torch.eq(wordchars, PAD_ID)
+
+        upos = get_long_tensor(batch[2], batch_size)
+        xpos = get_long_tensor(batch[3], batch_size)
+        ufeats = get_long_tensor(batch[4], batch_size)
+        pretrained = get_long_tensor(batch[5], batch_size)
+        word_string = batch[6]
+        sentlens = [len(x) for x in batch[0]]
+        return words, words_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, orig_idx, word_orig_idx, sentlens, word_lens, word_string
+
+    def __iter__(self):
+        for i in range(self.__len__()):
+            yield self.__getitem__(i)
+
+    def load_doc(self, doc):
+        data = doc.get([TEXT, UPOS, XPOS, FEATS], as_sentences=True)
+        data = self.resolve_none(data)
+        return data
+
+    def resolve_none(self, data):
+        # replace None to '_'
+        for sent_idx in range(len(data)):
+            for tok_idx in range(len(data[sent_idx])):
+                for feat_idx in range(len(data[sent_idx][tok_idx])):
+                    if data[sent_idx][tok_idx][feat_idx] is None:
+                        data[sent_idx][tok_idx][feat_idx] = '_'
+        return data
+
+    def reshuffle(self):
+        data = [y for x in self.data for y in x]
+        self.data = self.chunk_batches(data)
+        random.shuffle(self.data)
+
+    def chunk_batches(self, data):
+        res = []
+
+        if not data:
+            return res
+
+        if not self.eval:
+            # sort sentences (roughly) by length for better memory utilization
+            data = sorted(data, key = lambda x: len(x[0]), reverse=random.random() > .5)
+        elif self.sort_during_eval:
+            (data, ), self.data_orig_idx = sort_all([data], [len(x[0]) for x in data])
+
+        current = []
+        currentlen = 0
+        for x in data:
+            if len(x[0]) + currentlen > self.batch_size and currentlen > 0:
+                res.append(current)
+                current = []
+                currentlen = 0
+            current.append(x)
+            currentlen += len(x[0])
+
+        if currentlen > 0:
+            res.append(current)
+
+        return res
```

### Comparing `classla-2.0/classla/models/srl/model.py` & `classla-2.1/classla/models/srl/model.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,158 +1,158 @@
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, PackedSequence
-
-from classla.models.common.hlstm import HighwayLSTM
-from classla.models.common.dropout import WordDropout
-
-
-class SRLTagger(nn.Module):
-    def __init__(self, args, vocab, emb_matrix=None, share_hid=False):
-        super().__init__()
-
-        self.vocab = vocab
-        self.args = args
-        self.share_hid = share_hid
-        self.unsaved_modules = []
-
-        def add_unsaved_module(name, module):
-            self.unsaved_modules += [name]
-            setattr(self, name, module)
-
-        # input layers
-        input_size = 0
-        if self.args['word_emb_dim'] > 0:
-            self.word_emb = nn.Embedding(len(vocab['word']), self.args['word_emb_dim'], padding_idx=0)
-            input_size += self.args['word_emb_dim']
-        if self.args['head_word_emb_dim'] > 0:
-            self.head_word_emb = nn.Embedding(len(vocab['word']), self.args['head_word_emb_dim'], padding_idx=0)
-            input_size += self.args['head_word_emb_dim']
-        if self.args['lemma_emb_dim'] > 0:
-            self.lemma_emb = nn.Embedding(len(vocab['lemma']), self.args['lemma_emb_dim'], padding_idx=0)
-            input_size += self.args['lemma_emb_dim']
-        if self.args['head_lemma_emb_dim'] > 0:
-            self.head_lemma_emb = nn.Embedding(len(vocab['lemma']), self.args['head_lemma_emb_dim'], padding_idx=0)
-            input_size += self.args['head_lemma_emb_dim']
-
-        if self.args['xpos_emb_dim'] > 0:
-            self.xpos_embedding = nn.Embedding(len(vocab['xpos']), self.args['xpos_emb_dim'], padding_idx=0)
-            input_size += self.args['xpos_emb_dim']
-
-        if self.args['head_xpos_emb_dim'] > 0:
-            self.head_xpos_embedding = nn.Embedding(len(vocab['xpos']), self.args['head_xpos_emb_dim'], padding_idx=0)
-            input_size += self.args['head_xpos_emb_dim']
-
-        if self.args['deprel_emb_dim'] > 0:
-            self.deprel_emb = nn.Embedding(len(vocab['deprel']), self.args['deprel_emb_dim'], padding_idx=0)
-            input_size += self.args['deprel_emb_dim']
-
-        if self.args['pretrain_file'] and self.args['word_emb_dim'] > 0:
-            # pretrained embeddings, by default this won't be saved into model file
-            add_unsaved_module('pretrained_emb', nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True))
-            self.trans_pretrained = nn.Linear(emb_matrix.shape[1], self.args['transformed_dim'], bias=False)
-            input_size += self.args['transformed_dim']
-
-        if self.args['pretrain_file'] and self.args['head_word_emb_dim'] > 0:
-            # pretrained embeddings, by default this won't be saved into model file
-            add_unsaved_module('head_pretrained_emb', nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True))
-            self.head_trans_pretrained = nn.Linear(emb_matrix.shape[1], self.args['transformed_dim'], bias=False)
-            input_size += self.args['transformed_dim']
-
-        # recurrent layers
-        self.taggerlstm = HighwayLSTM(input_size, self.args['hidden_dim'], self.args['num_layers'], batch_first=True, bidirectional=True, dropout=self.args['dropout'], rec_dropout=self.args['rec_dropout'], highway_func=torch.tanh)
-        self.drop_replacement = nn.Parameter(torch.randn(input_size) / np.sqrt(input_size))
-        self.taggerlstm_h_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))
-        self.taggerlstm_c_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))
-
-        # classifiers
-        self.srl_hid = nn.Linear(self.args['hidden_dim'] * 2, self.args['deep_biaff_hidden_dim'])
-        self.srl_clf = nn.Linear(self.args['deep_biaff_hidden_dim'], len(vocab['srl']))
-        self.srl_clf.weight.data.zero_()
-        self.srl_clf.bias.data.zero_()
-
-        # criterion
-        self.crit = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding
-
-        self.drop = nn.Dropout(args['dropout'])
-        self.worddrop = WordDropout(args['word_dropout'])
-
-    def forward(self, words, words_mask, deprel, head_words, xpos, lemma, head_lemma, head_xpos, pretrained, head_pretrained, srl, orig_idx, sentlens, postprocessor=None):
-
-        def pack(x):
-            return pack_padded_sequence(x, sentlens, batch_first=True)
-
-        inputs = []
-        if self.args['word_emb_dim'] > 0:
-            word_emb = self.word_emb(words)
-            word_emb = pack(word_emb)
-            inputs += [word_emb]
-
-        if self.args['head_word_emb_dim'] > 0:
-            head_word_emb = self.head_word_emb(head_words)
-            head_word_emb = pack(head_word_emb)
-            inputs += [head_word_emb]
-
-        if self.args['lemma_emb_dim'] > 0:
-            lemma_emb = self.lemma_emb(lemma)
-            lemma_emb = pack(lemma_emb)
-            inputs += [lemma_emb]
-
-        if self.args['head_lemma_emb_dim'] > 0:
-            head_lemma_emb = self.head_lemma_emb(head_lemma)
-            head_lemma_emb = pack(head_lemma_emb)
-            inputs += [head_lemma_emb]
-
-        if self.args['xpos_emb_dim'] > 0:
-            xpos_emb = self.xpos_embedding(xpos)
-            xpos_emb = pack(xpos_emb)
-            inputs += [xpos_emb]
-
-        if self.args['head_xpos_emb_dim'] > 0:
-            head_xpos_emb = self.head_xpos_embedding(head_xpos)
-            head_xpos_emb = pack(head_xpos_emb)
-            inputs += [head_xpos_emb]
-
-        if self.args['deprel_emb_dim'] > 0:
-            deprel_emb = self.deprel_emb(deprel)
-            deprel_emb = pack(deprel_emb)
-            inputs += [deprel_emb]
-
-        if self.args['pretrain_file'] and self.args['word_emb_dim'] > 0:
-            pretrained_emb = self.pretrained_emb(pretrained)
-            pretrained_emb = self.trans_pretrained(pretrained_emb)
-            pretrained_emb = pack(pretrained_emb)
-            inputs += [pretrained_emb]
-
-        if self.args['pretrain_file'] and self.args['head_word_emb_dim'] > 0:
-            head_pretrained_emb = self.head_pretrained_emb(head_pretrained)
-            head_pretrained_emb = self.head_trans_pretrained(head_pretrained_emb)
-            head_pretrained_emb = pack(head_pretrained_emb)
-            inputs += [head_pretrained_emb]
-
-        def pad(x):
-            if self.args['word_emb_dim'] > 0:
-                return pad_packed_sequence(PackedSequence(x, word_emb.batch_sizes), batch_first=True)[0]
-            else:
-                return pad_packed_sequence(PackedSequence(x, head_word_emb.batch_sizes), batch_first=True)[0]
-
-        lstm_inputs = torch.cat([x.data for x in inputs], 1)
-        lstm_inputs = self.worddrop(lstm_inputs, self.drop_replacement)
-        lstm_inputs = self.drop(lstm_inputs)
-        lstm_inputs = PackedSequence(lstm_inputs, inputs[0].batch_sizes)
-
-        lstm_outputs, _ = self.taggerlstm(lstm_inputs, sentlens, hx=(
-        self.taggerlstm_h_init.expand(2 * self.args['num_layers'], words.size(0), self.args['hidden_dim']).contiguous(),
-        self.taggerlstm_c_init.expand(2 * self.args['num_layers'], words.size(0), self.args['hidden_dim']).contiguous()))
-        lstm_outputs = lstm_outputs.data
-
-        srl_hid = F.relu(self.srl_hid(self.drop(lstm_outputs)))
-        srl_pred = self.srl_clf(self.drop(srl_hid))
-
-        preds = [pad(srl_pred).max(2)[1]]
-
-        srl = pack(srl).data
-        loss = self.crit(srl_pred.view(-1, srl_pred.size(-1)), srl.view(-1))
-
-        return loss, preds
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, PackedSequence
+
+from classla.models.common.hlstm import HighwayLSTM
+from classla.models.common.dropout import WordDropout
+
+
+class SRLTagger(nn.Module):
+    def __init__(self, args, vocab, emb_matrix=None, share_hid=False):
+        super().__init__()
+
+        self.vocab = vocab
+        self.args = args
+        self.share_hid = share_hid
+        self.unsaved_modules = []
+
+        def add_unsaved_module(name, module):
+            self.unsaved_modules += [name]
+            setattr(self, name, module)
+
+        # input layers
+        input_size = 0
+        if self.args['word_emb_dim'] > 0:
+            self.word_emb = nn.Embedding(len(vocab['word']), self.args['word_emb_dim'], padding_idx=0)
+            input_size += self.args['word_emb_dim']
+        if self.args['head_word_emb_dim'] > 0:
+            self.head_word_emb = nn.Embedding(len(vocab['word']), self.args['head_word_emb_dim'], padding_idx=0)
+            input_size += self.args['head_word_emb_dim']
+        if self.args['lemma_emb_dim'] > 0:
+            self.lemma_emb = nn.Embedding(len(vocab['lemma']), self.args['lemma_emb_dim'], padding_idx=0)
+            input_size += self.args['lemma_emb_dim']
+        if self.args['head_lemma_emb_dim'] > 0:
+            self.head_lemma_emb = nn.Embedding(len(vocab['lemma']), self.args['head_lemma_emb_dim'], padding_idx=0)
+            input_size += self.args['head_lemma_emb_dim']
+
+        if self.args['xpos_emb_dim'] > 0:
+            self.xpos_embedding = nn.Embedding(len(vocab['xpos']), self.args['xpos_emb_dim'], padding_idx=0)
+            input_size += self.args['xpos_emb_dim']
+
+        if self.args['head_xpos_emb_dim'] > 0:
+            self.head_xpos_embedding = nn.Embedding(len(vocab['xpos']), self.args['head_xpos_emb_dim'], padding_idx=0)
+            input_size += self.args['head_xpos_emb_dim']
+
+        if self.args['deprel_emb_dim'] > 0:
+            self.deprel_emb = nn.Embedding(len(vocab['deprel']), self.args['deprel_emb_dim'], padding_idx=0)
+            input_size += self.args['deprel_emb_dim']
+
+        if self.args['pretrain_file'] and self.args['word_emb_dim'] > 0:
+            # pretrained embeddings, by default this won't be saved into model file
+            add_unsaved_module('pretrained_emb', nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True))
+            self.trans_pretrained = nn.Linear(emb_matrix.shape[1], self.args['transformed_dim'], bias=False)
+            input_size += self.args['transformed_dim']
+
+        if self.args['pretrain_file'] and self.args['head_word_emb_dim'] > 0:
+            # pretrained embeddings, by default this won't be saved into model file
+            add_unsaved_module('head_pretrained_emb', nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True))
+            self.head_trans_pretrained = nn.Linear(emb_matrix.shape[1], self.args['transformed_dim'], bias=False)
+            input_size += self.args['transformed_dim']
+
+        # recurrent layers
+        self.taggerlstm = HighwayLSTM(input_size, self.args['hidden_dim'], self.args['num_layers'], batch_first=True, bidirectional=True, dropout=self.args['dropout'], rec_dropout=self.args['rec_dropout'], highway_func=torch.tanh)
+        self.drop_replacement = nn.Parameter(torch.randn(input_size) / np.sqrt(input_size))
+        self.taggerlstm_h_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))
+        self.taggerlstm_c_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))
+
+        # classifiers
+        self.srl_hid = nn.Linear(self.args['hidden_dim'] * 2, self.args['deep_biaff_hidden_dim'])
+        self.srl_clf = nn.Linear(self.args['deep_biaff_hidden_dim'], len(vocab['srl']))
+        self.srl_clf.weight.data.zero_()
+        self.srl_clf.bias.data.zero_()
+
+        # criterion
+        self.crit = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding
+
+        self.drop = nn.Dropout(args['dropout'])
+        self.worddrop = WordDropout(args['word_dropout'])
+
+    def forward(self, words, words_mask, deprel, head_words, xpos, lemma, head_lemma, head_xpos, pretrained, head_pretrained, srl, orig_idx, sentlens, postprocessor=None):
+
+        def pack(x):
+            return pack_padded_sequence(x, sentlens, batch_first=True)
+
+        inputs = []
+        if self.args['word_emb_dim'] > 0:
+            word_emb = self.word_emb(words)
+            word_emb = pack(word_emb)
+            inputs += [word_emb]
+
+        if self.args['head_word_emb_dim'] > 0:
+            head_word_emb = self.head_word_emb(head_words)
+            head_word_emb = pack(head_word_emb)
+            inputs += [head_word_emb]
+
+        if self.args['lemma_emb_dim'] > 0:
+            lemma_emb = self.lemma_emb(lemma)
+            lemma_emb = pack(lemma_emb)
+            inputs += [lemma_emb]
+
+        if self.args['head_lemma_emb_dim'] > 0:
+            head_lemma_emb = self.head_lemma_emb(head_lemma)
+            head_lemma_emb = pack(head_lemma_emb)
+            inputs += [head_lemma_emb]
+
+        if self.args['xpos_emb_dim'] > 0:
+            xpos_emb = self.xpos_embedding(xpos)
+            xpos_emb = pack(xpos_emb)
+            inputs += [xpos_emb]
+
+        if self.args['head_xpos_emb_dim'] > 0:
+            head_xpos_emb = self.head_xpos_embedding(head_xpos)
+            head_xpos_emb = pack(head_xpos_emb)
+            inputs += [head_xpos_emb]
+
+        if self.args['deprel_emb_dim'] > 0:
+            deprel_emb = self.deprel_emb(deprel)
+            deprel_emb = pack(deprel_emb)
+            inputs += [deprel_emb]
+
+        if self.args['pretrain_file'] and self.args['word_emb_dim'] > 0:
+            pretrained_emb = self.pretrained_emb(pretrained)
+            pretrained_emb = self.trans_pretrained(pretrained_emb)
+            pretrained_emb = pack(pretrained_emb)
+            inputs += [pretrained_emb]
+
+        if self.args['pretrain_file'] and self.args['head_word_emb_dim'] > 0:
+            head_pretrained_emb = self.head_pretrained_emb(head_pretrained)
+            head_pretrained_emb = self.head_trans_pretrained(head_pretrained_emb)
+            head_pretrained_emb = pack(head_pretrained_emb)
+            inputs += [head_pretrained_emb]
+
+        def pad(x):
+            if self.args['word_emb_dim'] > 0:
+                return pad_packed_sequence(PackedSequence(x, word_emb.batch_sizes), batch_first=True)[0]
+            else:
+                return pad_packed_sequence(PackedSequence(x, head_word_emb.batch_sizes), batch_first=True)[0]
+
+        lstm_inputs = torch.cat([x.data for x in inputs], 1)
+        lstm_inputs = self.worddrop(lstm_inputs, self.drop_replacement)
+        lstm_inputs = self.drop(lstm_inputs)
+        lstm_inputs = PackedSequence(lstm_inputs, inputs[0].batch_sizes)
+
+        lstm_outputs, _ = self.taggerlstm(lstm_inputs, sentlens, hx=(
+        self.taggerlstm_h_init.expand(2 * self.args['num_layers'], words.size(0), self.args['hidden_dim']).contiguous(),
+        self.taggerlstm_c_init.expand(2 * self.args['num_layers'], words.size(0), self.args['hidden_dim']).contiguous()))
+        lstm_outputs = lstm_outputs.data
+
+        srl_hid = F.relu(self.srl_hid(self.drop(lstm_outputs)))
+        srl_pred = self.srl_clf(self.drop(srl_hid))
+
+        preds = [pad(srl_pred).max(2)[1]]
+
+        srl = pack(srl).data
+        loss = self.crit(srl_pred.view(-1, srl_pred.size(-1)), srl.view(-1))
+
+        return loss, preds
```

### Comparing `classla-2.0/classla/models/srl/scorer.py` & `classla-2.1/classla/models/srl/scorer.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,54 +1,54 @@
-"""
-Utils and wrappers for scoring taggers.
-"""
-import logging
-from collections import Counter
-
-logger = logging.getLogger('classla')
-
-
-def score_by_token(pred_tag_sequences, gold_tag_sequences, verbose=True):
-    """ Score predicted tags at the token level.
-
-    Args:
-        pred_tags_sequences: a list of list of predicted tags for each word
-        gold_tags_sequences: a list of list of gold tags for each word
-        verbose: print log with results
-
-    Returns:
-        Precision, recall and F1 scores.
-    """
-    assert (len(gold_tag_sequences) == len(pred_tag_sequences)), \
-        "Number of predicted tag sequences does not match gold sequences."
-
-    correct_by_tag = Counter()
-    guessed_by_tag = Counter()
-    gold_by_tag = Counter()
-
-    for gold_tags, pred_tags in zip(gold_tag_sequences, pred_tag_sequences):
-        assert (len(gold_tags) == len(pred_tags)), \
-            "Number of predicted tags does not match gold."
-        for g, p in zip(gold_tags, pred_tags):
-            if p != '_':
-                guessed_by_tag[p] += 1
-            if g != '_':
-                gold_by_tag[g] += 1
-                if g == p:
-                    correct_by_tag[p] += 1
-
-    prec_micro = 0.0
-    if sum(guessed_by_tag.values()) > 0:
-        prec_micro = sum(correct_by_tag.values()) * 1.0 / sum(guessed_by_tag.values())
-    rec_micro = 0.0
-    if sum(gold_by_tag.values()) > 0:
-        rec_micro = sum(correct_by_tag.values()) * 1.0 / sum(gold_by_tag.values())
-    f_micro = 0.0
-    if prec_micro + rec_micro > 0:
-        f_micro = 2.0 * prec_micro * rec_micro / (prec_micro + rec_micro)
-
-    if verbose:
-        logger.info("Prec.\tRec.\tF1")
-        logger.info("{:.2f}\t{:.2f}\t{:.2f}".format( \
-            prec_micro * 100, rec_micro * 100, f_micro * 100))
-    return prec_micro, rec_micro, f_micro
-
+"""
+Utils and wrappers for scoring taggers.
+"""
+import logging
+from collections import Counter
+
+logger = logging.getLogger('classla')
+
+
+def score_by_token(pred_tag_sequences, gold_tag_sequences, verbose=True):
+    """ Score predicted tags at the token level.
+
+    Args:
+        pred_tags_sequences: a list of list of predicted tags for each word
+        gold_tags_sequences: a list of list of gold tags for each word
+        verbose: print log with results
+
+    Returns:
+        Precision, recall and F1 scores.
+    """
+    assert (len(gold_tag_sequences) == len(pred_tag_sequences)), \
+        "Number of predicted tag sequences does not match gold sequences."
+
+    correct_by_tag = Counter()
+    guessed_by_tag = Counter()
+    gold_by_tag = Counter()
+
+    for gold_tags, pred_tags in zip(gold_tag_sequences, pred_tag_sequences):
+        assert (len(gold_tags) == len(pred_tags)), \
+            "Number of predicted tags does not match gold."
+        for g, p in zip(gold_tags, pred_tags):
+            if p != '_':
+                guessed_by_tag[p] += 1
+            if g != '_':
+                gold_by_tag[g] += 1
+                if g == p:
+                    correct_by_tag[p] += 1
+
+    prec_micro = 0.0
+    if sum(guessed_by_tag.values()) > 0:
+        prec_micro = sum(correct_by_tag.values()) * 1.0 / sum(guessed_by_tag.values())
+    rec_micro = 0.0
+    if sum(gold_by_tag.values()) > 0:
+        rec_micro = sum(correct_by_tag.values()) * 1.0 / sum(gold_by_tag.values())
+    f_micro = 0.0
+    if prec_micro + rec_micro > 0:
+        f_micro = 2.0 * prec_micro * rec_micro / (prec_micro + rec_micro)
+
+    if verbose:
+        logger.info("Prec.\tRec.\tF1")
+        logger.info("{:.2f}\t{:.2f}\t{:.2f}".format( \
+            prec_micro * 100, rec_micro * 100, f_micro * 100))
+    return prec_micro, rec_micro, f_micro
+
```

### Comparing `classla-2.0/classla/models/srl/trainer.py` & `classla-2.1/classla/models/srl/trainer.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,111 +1,111 @@
-"""
-A trainer class to handle training and testing of models.
-"""
-
-import logging
-import torch
-
-from classla.models.common.trainer import Trainer as BaseTrainer
-from classla.models.common import utils
-from classla.models.srl.model import SRLTagger
-from classla.models.srl.vocab import MultiVocab
-
-logger = logging.getLogger('classla')
-
-def unpack_batch(batch, use_cuda):
-    """ Unpack a batch from the data loader. """
-    if use_cuda:
-        inputs = [b.cuda() if b is not None else None for b in batch[:11]]
-    else:
-        inputs = batch[:11]
-    orig_idx = batch[11]
-    sentlens = batch[12]
-    return inputs, orig_idx, sentlens
-
-class Trainer(BaseTrainer):
-    """ A trainer for training models. """
-    def __init__(self, args=None, vocab=None, pretrain=None, model_file=None, use_cuda=False):
-        self.use_cuda = use_cuda
-        if model_file is not None:
-            # load everything from file
-            self.load(model_file, pretrain=pretrain, args=args)
-        else:
-            assert all(var is not None for var in [args, vocab, pretrain])
-            # build model from scratch
-            self.args = args
-            self.vocab = vocab
-            self.model = SRLTagger(args, vocab, emb_matrix=pretrain.emb)
-        self.parameters = [p for p in self.model.parameters() if p.requires_grad]
-        if self.use_cuda:
-            self.model.cuda()
-        else:
-            self.model.cpu()
-        self.optimizer = utils.get_optimizer(self.args['optim'], self.parameters, self.args['lr'], betas=(0.9, self.args['beta2']), eps=1e-6)
-
-    def update(self, batch, eval=False):
-        inputs, orig_idx, sentlens = unpack_batch(batch, self.use_cuda)
-        words, words_mask, deprel, head_words, xpos, lemma, head_lemma, head_xpos, pretrained, head_pretrained, srl = inputs
-
-        if eval:
-            self.model.eval()
-        else:
-            self.model.train()
-            self.optimizer.zero_grad()
-        loss, preds = self.model(words, words_mask, deprel, head_words, xpos, lemma, head_lemma, head_xpos, pretrained, head_pretrained, srl, orig_idx, sentlens)
-        loss_val = loss.data.item()
-        if eval:
-            return loss_val
-
-        loss.backward()
-        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args['max_grad_norm'])
-        self.optimizer.step()
-        return loss_val
-
-    def predict(self, batch, unsort=True):
-        inputs, orig_idx, sentlens = unpack_batch(batch, self.use_cuda)
-        words, words_mask, deprel, head_words, xpos, lemma, head_lemma, head_xpos, pretrained, head_pretrained, srl = inputs
-
-        self.model.eval()
-        batch_size = words.size(0)
-        _, preds = self.model(words, words_mask, deprel, head_words, xpos, lemma, head_lemma, head_xpos, pretrained, head_pretrained, srl, orig_idx, sentlens)
-
-        # decode
-        srl_seqs = [self.vocab['srl'].unmap(sent) for sent in preds[0].tolist()]
-
-        pred_tokens = [[srl_seqs[i][j] for j in range(sentlens[i])] for i in range(batch_size)]
-        if unsort:
-            pred_tokens = utils.unsort(pred_tokens, orig_idx)
-        return pred_tokens
-
-    def save(self, filename, skip_modules=True):
-        model_state = self.model.state_dict()
-        # skip saving modules like pretrained embeddings, because they are large and will be saved in a separate file
-        if skip_modules:
-            skipped = [k for k in model_state.keys() if k.split('.')[0] in self.model.unsaved_modules]
-            for k in skipped:
-                del model_state[k]
-        params = {
-                'model': model_state,
-                'vocab': self.vocab.state_dict(),
-                'config': self.args
-                }
-        try:
-            torch.save(params, filename)
-            logger.info("Model saved to {}".format(filename))
-        except (KeyboardInterrupt, SystemExit):
-            raise
-        except:
-            logger.warning("Saving failed... continuing anyway.")
-
-    def load(self, filename, pretrain=None, args=None):
-        try:
-            checkpoint = torch.load(filename, lambda storage, loc: storage)
-        except BaseException:
-            logger.error("Cannot load model from {}".format(filename))
-            raise
-        self.args = checkpoint['config']
-        if args: self.args.update(args)
-        self.vocab = MultiVocab.load_state_dict(checkpoint['vocab'])
-        self.model = SRLTagger(self.args, self.vocab, emb_matrix=pretrain.emb)
-        self.model.load_state_dict(checkpoint['model'], strict=False)
-
+"""
+A trainer class to handle training and testing of models.
+"""
+
+import logging
+import torch
+
+from classla.models.common.trainer import Trainer as BaseTrainer
+from classla.models.common import utils
+from classla.models.srl.model import SRLTagger
+from classla.models.srl.vocab import MultiVocab
+
+logger = logging.getLogger('classla')
+
+def unpack_batch(batch, use_cuda):
+    """ Unpack a batch from the data loader. """
+    if use_cuda:
+        inputs = [b.cuda() if b is not None else None for b in batch[:11]]
+    else:
+        inputs = batch[:11]
+    orig_idx = batch[11]
+    sentlens = batch[12]
+    return inputs, orig_idx, sentlens
+
+class Trainer(BaseTrainer):
+    """ A trainer for training models. """
+    def __init__(self, args=None, vocab=None, pretrain=None, model_file=None, use_cuda=False):
+        self.use_cuda = use_cuda
+        if model_file is not None:
+            # load everything from file
+            self.load(model_file, pretrain=pretrain, args=args)
+        else:
+            assert all(var is not None for var in [args, vocab, pretrain])
+            # build model from scratch
+            self.args = args
+            self.vocab = vocab
+            self.model = SRLTagger(args, vocab, emb_matrix=pretrain.emb)
+        self.parameters = [p for p in self.model.parameters() if p.requires_grad]
+        if self.use_cuda:
+            self.model.cuda()
+        else:
+            self.model.cpu()
+        self.optimizer = utils.get_optimizer(self.args['optim'], self.parameters, self.args['lr'], betas=(0.9, self.args['beta2']), eps=1e-6)
+
+    def update(self, batch, eval=False):
+        inputs, orig_idx, sentlens = unpack_batch(batch, self.use_cuda)
+        words, words_mask, deprel, head_words, xpos, lemma, head_lemma, head_xpos, pretrained, head_pretrained, srl = inputs
+
+        if eval:
+            self.model.eval()
+        else:
+            self.model.train()
+            self.optimizer.zero_grad()
+        loss, preds = self.model(words, words_mask, deprel, head_words, xpos, lemma, head_lemma, head_xpos, pretrained, head_pretrained, srl, orig_idx, sentlens)
+        loss_val = loss.data.item()
+        if eval:
+            return loss_val
+
+        loss.backward()
+        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args['max_grad_norm'])
+        self.optimizer.step()
+        return loss_val
+
+    def predict(self, batch, unsort=True):
+        inputs, orig_idx, sentlens = unpack_batch(batch, self.use_cuda)
+        words, words_mask, deprel, head_words, xpos, lemma, head_lemma, head_xpos, pretrained, head_pretrained, srl = inputs
+
+        self.model.eval()
+        batch_size = words.size(0)
+        _, preds = self.model(words, words_mask, deprel, head_words, xpos, lemma, head_lemma, head_xpos, pretrained, head_pretrained, srl, orig_idx, sentlens)
+
+        # decode
+        srl_seqs = [self.vocab['srl'].unmap(sent) for sent in preds[0].tolist()]
+
+        pred_tokens = [[srl_seqs[i][j] for j in range(sentlens[i])] for i in range(batch_size)]
+        if unsort:
+            pred_tokens = utils.unsort(pred_tokens, orig_idx)
+        return pred_tokens
+
+    def save(self, filename, skip_modules=True):
+        model_state = self.model.state_dict()
+        # skip saving modules like pretrained embeddings, because they are large and will be saved in a separate file
+        if skip_modules:
+            skipped = [k for k in model_state.keys() if k.split('.')[0] in self.model.unsaved_modules]
+            for k in skipped:
+                del model_state[k]
+        params = {
+                'model': model_state,
+                'vocab': self.vocab.state_dict(),
+                'config': self.args
+                }
+        try:
+            torch.save(params, filename)
+            logger.info("Model saved to {}".format(filename))
+        except (KeyboardInterrupt, SystemExit):
+            raise
+        except:
+            logger.warning("Saving failed... continuing anyway.")
+
+    def load(self, filename, pretrain=None, args=None):
+        try:
+            checkpoint = torch.load(filename, lambda storage, loc: storage)
+        except BaseException:
+            logger.error("Cannot load model from {}".format(filename))
+            raise
+        self.args = checkpoint['config']
+        if args: self.args.update(args)
+        self.vocab = MultiVocab.load_state_dict(checkpoint['vocab'])
+        self.model = SRLTagger(self.args, self.vocab, emb_matrix=pretrain.emb)
+        self.model.load_state_dict(checkpoint['model'], strict=False)
+
```

### Comparing `classla-2.0/classla/models/srl_tagger.py` & `classla-2.1/classla/models/srl_tagger.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,248 +1,248 @@
-"""
-Entry point for training and evaluating an NER tagger.
-
-This tagger uses BiLSTM layers with character and word-level representations, and a CRF decoding layer 
-to produce NER predictions.
-For details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.
-"""
-
-import sys
-import os
-import time
-from datetime import datetime
-import argparse
-import logging
-import numpy as np
-import random
-import json
-import torch
-from torch import nn, optim
-
-from classla.models.srl.data import DataLoader
-from classla.models.srl.trainer import Trainer
-from classla.models.srl import scorer
-from classla.models.common import utils
-from classla.models.common.pretrain import Pretrain
-from classla.utils.conll import CoNLL
-from classla.models.common.doc import *
-from classla.models import _training_logging
-
-logger = logging.getLogger('classla')
-
-def parse_args(args=None):
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--data_dir', type=str, default='data/srl', help='Root dir for saving models.')
-    parser.add_argument('--wordvec_dir', type=str, default='extern_data/word2vec', help='Directory of word vectors')
-    parser.add_argument('--wordvec_file', type=str, default='', help='File that contains word vectors')
-    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')
-    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')
-    parser.add_argument('--output_file', type=str, default=None, help='Output CoNLL-U file.')
-    parser.add_argument('--gold_file', type=str, default=None, help='Output CoNLL-U file.')
-    parser.add_argument('--pretrain_file', type=str, default=None, help='Input file for data loader.')
-
-    parser.add_argument('--mode', default='train', choices=['train', 'predict'])
-    parser.add_argument('--lang', type=str, help='Language')
-    parser.add_argument('--shorthand', type=str, help="Treebank shorthand")
-    parser.add_argument('--multi_root', dest='multi_root', action='store_true',
-                        help="Allow multiple roots in trees (JOS)")
-
-    parser.add_argument('--hidden_dim', type=int, default=400)
-    parser.add_argument('--char_hidden_dim', type=int, default=400)
-    parser.add_argument('--deep_biaff_hidden_dim', type=int, default=400)
-    parser.add_argument('--composite_deep_biaff_hidden_dim', type=int, default=100)
-    parser.add_argument('--word_emb_dim', type=int, default=75)
-    parser.add_argument('--head_word_emb_dim', type=int, default=75)
-    parser.add_argument('--lemma_emb_dim', type=int, default=75)
-    parser.add_argument('--head_lemma_emb_dim', type=int, default=75)
-    parser.add_argument('--char_emb_dim', type=int, default=100)
-    parser.add_argument('--xpos_emb_dim', type=int, default=50)
-    parser.add_argument('--head_xpos_emb_dim', type=int, default=50)
-    parser.add_argument('--deprel_emb_dim', type=int, default=50)
-    parser.add_argument('--tag_emb_dim', type=int, default=50)
-    parser.add_argument('--transformed_dim', type=int, default=125)
-    parser.add_argument('--num_layers', type=int, default=3)
-    parser.add_argument('--char_num_layers', type=int, default=1)
-    parser.add_argument('--pretrain_max_vocab', type=int, default=250000)
-    parser.add_argument('--word_dropout', type=float, default=0.33)
-    parser.add_argument('--dropout', type=float, default=0.5)
-    parser.add_argument('--rec_dropout', type=float, default=0, help="Recurrent dropout")
-    parser.add_argument('--char_rec_dropout', type=float, default=0, help="Recurrent dropout")
-    parser.add_argument('--no_char', dest='char', action='store_false', help="Turn off character model.")
-    parser.add_argument('--no_pretrain', dest='pretrain', action='store_false', help="Turn off pretrained embeddings.")
-    parser.add_argument('--no_linearization', dest='linearization', action='store_false',
-                        help="Turn off linearization term.")
-    parser.add_argument('--no_distance', dest='distance', action='store_false', help="Turn off distance term.")
-
-    parser.add_argument('--sample_train', type=float, default=1.0, help='Subsample training data.')
-    parser.add_argument('--optim', type=str, default='adam', help='sgd, adagrad, adam or adamax.')
-    parser.add_argument('--lr', type=float, default=3e-3, help='Learning rate')
-    parser.add_argument('--beta2', type=float, default=0.95)
-
-    parser.add_argument('--max_steps', type=int, default=50000)
-    parser.add_argument('--eval_interval', type=int, default=100)
-    parser.add_argument('--max_steps_before_stop', type=int, default=3000)
-    parser.add_argument('--batch_size', type=int, default=256)
-    parser.add_argument('--max_grad_norm', type=float, default=1.0, help='Gradient clipping.')
-    parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')
-    parser.add_argument('--save_dir', type=str, default='saved_models/srl', help='Root dir for saving models.')
-    parser.add_argument('--save_name', type=str, default=None, help="File name to save the model")
-
-    parser.add_argument('--seed', type=int, default=1234)
-    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())
-    parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')
-
-    args = parser.parse_args(args=args)
-    return args
-
-def main(args=None):
-    args = parse_args(args)
-
-    torch.manual_seed(args.seed)
-    np.random.seed(args.seed)
-    random.seed(args.seed)
-    if args.cpu:
-        args.cuda = False
-    elif args.cuda:
-        torch.cuda.manual_seed(args.seed)
-
-    args = vars(args)
-    logger.info("Running tagger in {} mode".format(args['mode']))
-
-    if args['mode'] == 'train':
-        train(args)
-    else:
-        evaluate(args)
-
-def train(args):
-    utils.ensure_dir(args['save_dir'])
-    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
-            else '{}/{}_srltagger.pt'.format(args['save_dir'], args['shorthand'])
-
-    # do not save pretrained embeddings individually
-    pretrain = Pretrain(args['pretrain_file'], max_vocab=args['pretrain_max_vocab'])
-
-    # load data
-    logger.info("Loading data with batch size {}...".format(args['batch_size']))
-    doc, metasentences = CoNLL.conll2dict(input_file=args['train_file'])
-    train_doc = Document(doc, metasentences=metasentences)
-    train_batch = DataLoader(train_doc, args['batch_size'], args, pretrain, evaluation=False)
-    vocab = train_batch.vocab
-    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
-    dev_doc = Document(doc, metasentences=metasentences)
-    dev_batch = DataLoader(dev_doc, args['batch_size'], args, pretrain, vocab=vocab, evaluation=True)
-    dev_gold_tags = dev_batch.srls
-
-    # skip training if the language does not have training or dev data
-    if len(train_batch) == 0 or len(dev_batch) == 0:
-        logger.info("Skip training because no data available...")
-        sys.exit(0)
-
-    print("Training tagger...")
-    trainer = Trainer(args=args, vocab=vocab, pretrain=pretrain, use_cuda=args['cuda'])
-
-    global_step = 0
-    max_steps = args['max_steps']
-    dev_score_history = []
-    current_lr = args['lr']
-    global_start_time = time.time()
-    format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'
-
-    using_amsgrad = False
-    last_best_step = 0
-    # start training
-    train_loss = 0
-    while True:
-        do_break = False
-        for i, batch in enumerate(train_batch):
-            start_time = time.time()
-            global_step += 1
-            loss = trainer.update(batch, eval=False)  # update step
-            train_loss += loss
-            if global_step % args['log_step'] == 0:
-                duration = time.time() - start_time
-                print(format_str.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"), global_step, \
-                                        max_steps, loss, duration, current_lr))
-
-            if global_step % args['eval_interval'] == 0:
-                # eval on dev
-                print("Evaluating on dev set...")
-                dev_preds = []
-                for batch in dev_batch:
-                    preds = trainer.predict(batch)
-                    dev_preds += preds
-                _, _, dev_score = scorer.score_by_token(dev_preds, dev_gold_tags)
-
-                train_loss = train_loss / args['eval_interval']  # avg loss per batch
-                print("step {}: train_loss = {:.6f}, dev_score = {:.4f}".format(global_step, train_loss, dev_score))
-                train_loss = 0
-
-                # save best model
-                if len(dev_score_history) == 0 or dev_score > max(dev_score_history):
-                    last_best_step = global_step
-                    trainer.save(model_file)
-                    print("new best model saved.")
-
-                dev_score_history += [dev_score]
-                print("")
-
-            if global_step - last_best_step >= args['max_steps_before_stop']:
-                if not using_amsgrad:
-                    print("Switching to AMSGrad")
-                    last_best_step = global_step
-                    using_amsgrad = True
-                    trainer.optimizer = optim.Adam(trainer.model.parameters(), amsgrad=True, lr=args['lr'],
-                                                   betas=(.9, args['beta2']), eps=1e-6)
-                else:
-                    do_break = True
-                    break
-
-            if global_step >= args['max_steps']:
-                do_break = True
-                break
-
-        if do_break: break
-
-        train_batch.reshuffle()
-
-    print("Training ended with {} steps.".format(global_step))
-
-    best_f, best_eval = max(dev_score_history) * 100, np.argmax(dev_score_history) + 1
-    print("Best dev F1 = {:.2f}, at iteration = {}".format(best_f, best_eval * args['eval_interval']))
-
-def evaluate(args):
-    # file paths
-    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
-            else '{}/{}_srltagger.pt'.format(args['save_dir'], args['shorthand'])
-
-    # load model
-    use_cuda = args['cuda'] and not args['cpu']
-    pretrain = Pretrain(args['pretrain_file'], max_vocab=args['pretrain_max_vocab'])
-    trainer = Trainer(model_file=model_file, use_cuda=use_cuda, pretrain=pretrain, args=args)
-    loaded_args, vocab = trainer.args, trainer.vocab
-
-    # load config
-    for k in args:
-        if k.endswith('_dir') or k.endswith('_file') or k in ['shorthand', 'mode', 'scheme']:
-            loaded_args[k] = args[k]
-
-    # load data
-    logger.info("Loading data with batch size {}...".format(args['batch_size']))
-    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
-    evaluate_doc = Document(doc, metasentences=metasentences)
-    batch = DataLoader(evaluate_doc, args['batch_size'], args, pretrain, vocab=vocab, evaluation=True)
-    
-    logger.info("Start evaluation...")
-    preds = []
-    for i, b in enumerate(batch):
-        preds += trainer.predict(b)
-
-    gold_tags = batch.srls
-    batch.doc.set([SRL], [y if y != '_' else None for x in preds for y in x], to_token=True)
-    CoNLL.dict2conll(batch.doc.to_dict(), args['output_file'])
-    _, _, score = scorer.score_by_token(preds, gold_tags)
-
-    logger.info("SRL tagger score:")
-    logger.info("{} {:.2f}".format(args['shorthand'], score*100))
-
-if __name__ == '__main__':
-    main()
+"""
+Entry point for training and evaluating an NER tagger.
+
+This tagger uses BiLSTM layers with character and word-level representations, and a CRF decoding layer 
+to produce NER predictions.
+For details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.
+"""
+
+import sys
+import os
+import time
+from datetime import datetime
+import argparse
+import logging
+import numpy as np
+import random
+import json
+import torch
+from torch import nn, optim
+
+from classla.models.srl.data import DataLoader
+from classla.models.srl.trainer import Trainer
+from classla.models.srl import scorer
+from classla.models.common import utils
+from classla.models.common.pretrain import Pretrain
+from classla.utils.conll import CoNLL
+from classla.models.common.doc import *
+from classla.models import _training_logging
+
+logger = logging.getLogger('classla')
+
+def parse_args(args=None):
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--data_dir', type=str, default='data/srl', help='Root dir for saving models.')
+    parser.add_argument('--wordvec_dir', type=str, default='extern_data/word2vec', help='Directory of word vectors')
+    parser.add_argument('--wordvec_file', type=str, default='', help='File that contains word vectors')
+    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')
+    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')
+    parser.add_argument('--output_file', type=str, default=None, help='Output CoNLL-U file.')
+    parser.add_argument('--gold_file', type=str, default=None, help='Output CoNLL-U file.')
+    parser.add_argument('--pretrain_file', type=str, default=None, help='Input file for data loader.')
+
+    parser.add_argument('--mode', default='train', choices=['train', 'predict'])
+    parser.add_argument('--lang', type=str, help='Language')
+    parser.add_argument('--shorthand', type=str, help="Treebank shorthand")
+    parser.add_argument('--multi_root', dest='multi_root', action='store_true',
+                        help="Allow multiple roots in trees (JOS)")
+
+    parser.add_argument('--hidden_dim', type=int, default=400)
+    parser.add_argument('--char_hidden_dim', type=int, default=400)
+    parser.add_argument('--deep_biaff_hidden_dim', type=int, default=400)
+    parser.add_argument('--composite_deep_biaff_hidden_dim', type=int, default=100)
+    parser.add_argument('--word_emb_dim', type=int, default=75)
+    parser.add_argument('--head_word_emb_dim', type=int, default=75)
+    parser.add_argument('--lemma_emb_dim', type=int, default=75)
+    parser.add_argument('--head_lemma_emb_dim', type=int, default=75)
+    parser.add_argument('--char_emb_dim', type=int, default=100)
+    parser.add_argument('--xpos_emb_dim', type=int, default=50)
+    parser.add_argument('--head_xpos_emb_dim', type=int, default=50)
+    parser.add_argument('--deprel_emb_dim', type=int, default=50)
+    parser.add_argument('--tag_emb_dim', type=int, default=50)
+    parser.add_argument('--transformed_dim', type=int, default=125)
+    parser.add_argument('--num_layers', type=int, default=3)
+    parser.add_argument('--char_num_layers', type=int, default=1)
+    parser.add_argument('--pretrain_max_vocab', type=int, default=250000)
+    parser.add_argument('--word_dropout', type=float, default=0.33)
+    parser.add_argument('--dropout', type=float, default=0.5)
+    parser.add_argument('--rec_dropout', type=float, default=0, help="Recurrent dropout")
+    parser.add_argument('--char_rec_dropout', type=float, default=0, help="Recurrent dropout")
+    parser.add_argument('--no_char', dest='char', action='store_false', help="Turn off character model.")
+    parser.add_argument('--no_pretrain', dest='pretrain', action='store_false', help="Turn off pretrained embeddings.")
+    parser.add_argument('--no_linearization', dest='linearization', action='store_false',
+                        help="Turn off linearization term.")
+    parser.add_argument('--no_distance', dest='distance', action='store_false', help="Turn off distance term.")
+
+    parser.add_argument('--sample_train', type=float, default=1.0, help='Subsample training data.')
+    parser.add_argument('--optim', type=str, default='adam', help='sgd, adagrad, adam or adamax.')
+    parser.add_argument('--lr', type=float, default=3e-3, help='Learning rate')
+    parser.add_argument('--beta2', type=float, default=0.95)
+
+    parser.add_argument('--max_steps', type=int, default=50000)
+    parser.add_argument('--eval_interval', type=int, default=100)
+    parser.add_argument('--max_steps_before_stop', type=int, default=3000)
+    parser.add_argument('--batch_size', type=int, default=256)
+    parser.add_argument('--max_grad_norm', type=float, default=1.0, help='Gradient clipping.')
+    parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')
+    parser.add_argument('--save_dir', type=str, default='saved_models/srl', help='Root dir for saving models.')
+    parser.add_argument('--save_name', type=str, default=None, help="File name to save the model")
+
+    parser.add_argument('--seed', type=int, default=1234)
+    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())
+    parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')
+
+    args = parser.parse_args(args=args)
+    return args
+
+def main(args=None):
+    args = parse_args(args)
+
+    torch.manual_seed(args.seed)
+    np.random.seed(args.seed)
+    random.seed(args.seed)
+    if args.cpu:
+        args.cuda = False
+    elif args.cuda:
+        torch.cuda.manual_seed(args.seed)
+
+    args = vars(args)
+    logger.info("Running tagger in {} mode".format(args['mode']))
+
+    if args['mode'] == 'train':
+        train(args)
+    else:
+        evaluate(args)
+
+def train(args):
+    utils.ensure_dir(args['save_dir'])
+    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
+            else '{}/{}_srltagger.pt'.format(args['save_dir'], args['shorthand'])
+
+    # do not save pretrained embeddings individually
+    pretrain = Pretrain(args['pretrain_file'], max_vocab=args['pretrain_max_vocab'])
+
+    # load data
+    logger.info("Loading data with batch size {}...".format(args['batch_size']))
+    doc, metasentences = CoNLL.conll2dict(input_file=args['train_file'])
+    train_doc = Document(doc, metasentences=metasentences)
+    train_batch = DataLoader(train_doc, args['batch_size'], args, pretrain, evaluation=False)
+    vocab = train_batch.vocab
+    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
+    dev_doc = Document(doc, metasentences=metasentences)
+    dev_batch = DataLoader(dev_doc, args['batch_size'], args, pretrain, vocab=vocab, evaluation=True)
+    dev_gold_tags = dev_batch.srls
+
+    # skip training if the language does not have training or dev data
+    if len(train_batch) == 0 or len(dev_batch) == 0:
+        logger.info("Skip training because no data available...")
+        sys.exit(0)
+
+    print("Training tagger...")
+    trainer = Trainer(args=args, vocab=vocab, pretrain=pretrain, use_cuda=args['cuda'])
+
+    global_step = 0
+    max_steps = args['max_steps']
+    dev_score_history = []
+    current_lr = args['lr']
+    global_start_time = time.time()
+    format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'
+
+    using_amsgrad = False
+    last_best_step = 0
+    # start training
+    train_loss = 0
+    while True:
+        do_break = False
+        for i, batch in enumerate(train_batch):
+            start_time = time.time()
+            global_step += 1
+            loss = trainer.update(batch, eval=False)  # update step
+            train_loss += loss
+            if global_step % args['log_step'] == 0:
+                duration = time.time() - start_time
+                print(format_str.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"), global_step, \
+                                        max_steps, loss, duration, current_lr))
+
+            if global_step % args['eval_interval'] == 0:
+                # eval on dev
+                print("Evaluating on dev set...")
+                dev_preds = []
+                for batch in dev_batch:
+                    preds = trainer.predict(batch)
+                    dev_preds += preds
+                _, _, dev_score = scorer.score_by_token(dev_preds, dev_gold_tags)
+
+                train_loss = train_loss / args['eval_interval']  # avg loss per batch
+                print("step {}: train_loss = {:.6f}, dev_score = {:.4f}".format(global_step, train_loss, dev_score))
+                train_loss = 0
+
+                # save best model
+                if len(dev_score_history) == 0 or dev_score > max(dev_score_history):
+                    last_best_step = global_step
+                    trainer.save(model_file)
+                    print("new best model saved.")
+
+                dev_score_history += [dev_score]
+                print("")
+
+            if global_step - last_best_step >= args['max_steps_before_stop']:
+                if not using_amsgrad:
+                    print("Switching to AMSGrad")
+                    last_best_step = global_step
+                    using_amsgrad = True
+                    trainer.optimizer = optim.Adam(trainer.model.parameters(), amsgrad=True, lr=args['lr'],
+                                                   betas=(.9, args['beta2']), eps=1e-6)
+                else:
+                    do_break = True
+                    break
+
+            if global_step >= args['max_steps']:
+                do_break = True
+                break
+
+        if do_break: break
+
+        train_batch.reshuffle()
+
+    print("Training ended with {} steps.".format(global_step))
+
+    best_f, best_eval = max(dev_score_history) * 100, np.argmax(dev_score_history) + 1
+    print("Best dev F1 = {:.2f}, at iteration = {}".format(best_f, best_eval * args['eval_interval']))
+
+def evaluate(args):
+    # file paths
+    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
+            else '{}/{}_srltagger.pt'.format(args['save_dir'], args['shorthand'])
+
+    # load model
+    use_cuda = args['cuda'] and not args['cpu']
+    pretrain = Pretrain(args['pretrain_file'], max_vocab=args['pretrain_max_vocab'])
+    trainer = Trainer(model_file=model_file, use_cuda=use_cuda, pretrain=pretrain, args=args)
+    loaded_args, vocab = trainer.args, trainer.vocab
+
+    # load config
+    for k in args:
+        if k.endswith('_dir') or k.endswith('_file') or k in ['shorthand', 'mode', 'scheme']:
+            loaded_args[k] = args[k]
+
+    # load data
+    logger.info("Loading data with batch size {}...".format(args['batch_size']))
+    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
+    evaluate_doc = Document(doc, metasentences=metasentences)
+    batch = DataLoader(evaluate_doc, args['batch_size'], args, pretrain, vocab=vocab, evaluation=True)
+    
+    logger.info("Start evaluation...")
+    preds = []
+    for i, b in enumerate(batch):
+        preds += trainer.predict(b)
+
+    gold_tags = batch.srls
+    batch.doc.set([SRL], [y if y != '_' else None for x in preds for y in x], to_token=True)
+    CoNLL.dict2conll(batch.doc.to_dict(), args['output_file'])
+    _, _, score = scorer.score_by_token(preds, gold_tags)
+
+    logger.info("SRL tagger score:")
+    logger.info("{} {:.2f}".format(args['shorthand'], score*100))
+
+if __name__ == '__main__':
+    main()
```

### Comparing `classla-2.0/classla/models/tagger.py` & `classla-2.1/classla/models/tagger.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,332 +1,332 @@
-"""
-Entry point for training and evaluating a POS/morphological features tagger.
-
-This tagger uses highway BiLSTM layers with character and word-level representations, and biaffine classifiers
-to produce consistant POS and UFeats predictions.
-For details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.
-"""
-import csv
-import sys
-import os
-import shutil
-import time
-from collections import Counter
-from datetime import datetime
-import argparse
-import numpy as np
-import random
-import torch
-from torch import nn, optim
-
-from classla.models.pos.data import DataLoader
-from classla.models.pos.trainer import Trainer
-from classla.models.pos import scorer
-from classla.models.common import utils
-from classla.models.common.pretrain import Pretrain
-from classla.models.common.doc import *
-from classla.utils.conll import CoNLL
-from classla.models import _training_logging
-
-def parse_args(args=None):
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--data_dir', type=str, default='data/pos', help='Root dir for saving models.')
-    parser.add_argument('--wordvec_dir', type=str, default='extern_data/word2vec', help='Directory of word vectors.')
-    parser.add_argument('--wordvec_file', type=str, default=None, help='Word vectors filename.')
-    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')
-    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')
-    parser.add_argument('--output_file', type=str, default=None, help='Output CoNLL-U file.')
-    parser.add_argument('--gold_file', type=str, default=None, help='Output CoNLL-U file.')
-    parser.add_argument('--pretrain_file', type=str, default=None, help='Input file containing pretrained data.')
-    parser.add_argument('--use_lexicon', type=str, default=None, help="Input location of lemmatization model.")
-    parser.add_argument('--inflectional_lexicon_path', type=str, default=None, help="Input location of inflectional lexicon (i.e. sloleks).")
-    parser.add_argument('--lemma_pretag', action='store_true', help="Use pretag from tokenization processor.")
-
-    parser.add_argument('--mode', default='train', choices=['train', 'predict'])
-    parser.add_argument('--lang', type=str, help='Language')
-    parser.add_argument('--shorthand', type=str, help="Treebank shorthand")
-
-    parser.add_argument('--hidden_dim', type=int, default=200)
-    parser.add_argument('--char_hidden_dim', type=int, default=400)
-    parser.add_argument('--deep_biaff_hidden_dim', type=int, default=400)
-    parser.add_argument('--composite_deep_biaff_hidden_dim', type=int, default=100)
-    parser.add_argument('--word_emb_dim', type=int, default=75)
-    parser.add_argument('--char_emb_dim', type=int, default=100)
-    parser.add_argument('--tag_emb_dim', type=int, default=50)
-    parser.add_argument('--transformed_dim', type=int, default=125)
-    parser.add_argument('--num_layers', type=int, default=2)
-    parser.add_argument('--char_num_layers', type=int, default=1)
-    parser.add_argument('--pretrain_max_vocab', type=int, default=250000)
-    parser.add_argument('--word_dropout', type=float, default=0.33)
-    parser.add_argument('--dropout', type=float, default=0.5)
-    parser.add_argument('--rec_dropout', type=float, default=0, help="Recurrent dropout")
-    parser.add_argument('--char_rec_dropout', type=float, default=0, help="Recurrent dropout")
-    parser.add_argument('--no_char', dest='char', action='store_false', help="Turn off character model.")
-    parser.add_argument('--no_pretrain', dest='pretrain', action='store_false', help="Turn off pretrained embeddings.")
-    parser.add_argument('--share_hid', action='store_true', help="Share hidden representations for UPOS, XPOS and UFeats.")
-    parser.set_defaults(share_hid=False)
-
-    parser.add_argument('--sample_train', type=float, default=1.0, help='Subsample training data.')
-    parser.add_argument('--optim', type=str, default='adam', help='sgd, adagrad, adam or adamax.')
-    parser.add_argument('--lr', type=float, default=3e-3, help='Learning rate')
-    parser.add_argument('--beta2', type=float, default=0.95)
-
-    parser.add_argument('--max_steps', type=int, default=50000)
-    parser.add_argument('--eval_interval', type=int, default=100)
-    parser.add_argument('--fix_eval_interval', dest='adapt_eval_interval', action='store_false', \
-            help="Use fixed evaluation interval for all treebanks, otherwise by default the interval will be increased for larger treebanks.")
-    parser.add_argument('--max_steps_before_stop', type=int, default=3000)
-    parser.add_argument('--batch_size', type=int, default=5000)
-    parser.add_argument('--max_grad_norm', type=float, default=1.0, help='Gradient clipping.')
-    parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')
-    parser.add_argument('--save_dir', type=str, default='saved_models/pos', help='Root dir for saving models.')
-    parser.add_argument('--save_name', type=str, default=None, help="File name to save the model")
-
-    parser.add_argument('--seed', type=int, default=1234)
-    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())
-    parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')
-    args = parser.parse_args(args=args)
-    return args
-
-
-def main(args=None):
-    sys.setrecursionlimit(50000)
-
-    args = parse_args(args=args)
-
-    torch.manual_seed(args.seed)
-    np.random.seed(args.seed)
-    random.seed(args.seed)
-    if args.cpu:
-        args.cuda = False
-    elif args.cuda:
-        torch.cuda.manual_seed(args.seed)
-
-    args = vars(args)
-    print("Running tagger in {} mode".format(args['mode']))
-
-    if args['mode'] == 'train':
-        train(args)
-    else:
-        evaluate(args)
-
-
-def generate_new_composite_dict(inflectional_lexicon_path, train_batch):
-    # generate initial composite dictionary from all train data
-    train_dict = train_batch.doc.get([TEXT, XPOS, UPOS, FEATS, LEMMA])
-    inflectional_dict = set()
-
-    lemmas_frequencies = {}
-    # get lemma frequencies
-    with open(inflectional_lexicon_path) as csvfile:
-        csv_reader = csv.reader(csvfile, delimiter='\t')
-        for row in csv_reader:
-            if row[0] == 'FORM' and row[1] == 'LEMMA':   # ignore header line
-                continue
-
-            # if row[6] is not float
-            if re.match(r'^-?\d+(?:\.\d+)$', row[6]) is None:
-                lemmas_frequencies[row[1]] = float(row[3]) if row[1] not in lemmas_frequencies else lemmas_frequencies[
-                                                                                                        row[1]] + float(
-                    row[3])
-                upos_ufeats = row[6].split()
-                ufeats = '|'.join(sorted(upos_ufeats[1:], key=lambda x: x.lower())) if upos_ufeats[1:] else '_'
-                inflectional_dict.add((row[0].lower(), row[2], upos_ufeats[0],
-                                       ufeats, float(row[3]), row[1]))
-
-            else:
-                lemmas_frequencies[row[1]] = float(row[6]) if row[1] not in lemmas_frequencies else lemmas_frequencies[row[1]] + float(row[6])
-                # '_' fix
-                ufeats = row[5] if row[5] else '_'
-                inflectional_dict.add((row[0].lower(), row[3], row[4], ufeats, float(row[6]), row[1]))
-
-    composite_dict = sorted(inflectional_dict, key=lambda x: x[4], reverse=True)
-    all_keys = {}
-
-    cleaned_composite_dict = []
-    i = 0
-    for el in composite_dict:
-        if (el[0], el[1], el[2], el[3]) not in all_keys:
-            all_keys[(el[0], el[1], el[2], el[3])] = (el, i)
-            cleaned_composite_dict.append((el[0], el[1], el[2], el[3], el[5]))
-            i += 1
-        else:
-            if el[5] in lemmas_frequencies and all_keys[(el[0], el[1], el[2], el[3])][0][5] in lemmas_frequencies and \
-                    lemmas_frequencies[all_keys[(el[0], el[1], el[2], el[3])][0][5]] < lemmas_frequencies[el[5]]:
-                old_el, updating_index = all_keys[(el[0], el[1], el[2], el[3])]
-                all_keys[(el[0], el[1], el[2], el[3])] = (el, updating_index)
-                cleaned_composite_dict[updating_index] = (el[0], el[1], el[2], el[3], el[5])
-
-    train_dict = [(el[0], el[1], el[2], el[3], el[4]) if el[3] is not None else (el[0], el[1], el[2], '_', el[4])
-                  for el in train_dict]
-    train_dict_most_common = Counter(train_dict).most_common()
-    filtered_train_dict = [el[0] for el in train_dict_most_common
-                           if (el[0][0], el[0][1], el[0][2], el[0][3]) not in all_keys]
-    cleaned_composite_dict.extend(filtered_train_dict)
-    return cleaned_composite_dict
-
-
-def train(args):
-    utils.ensure_dir(args['save_dir'])
-    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
-            else '{}/{}_tagger.pt'.format(args['save_dir'], args['shorthand'])
-
-    # load pretrained vectors
-    vec_file = args['wordvec_file']
-
-    pretrain_file = '{}/{}.pretrain.pt'.format(args['save_dir'], args['shorthand']) if args['pretrain_file'] is None \
-        else args['pretrain_file']
-    pretrain = Pretrain(pretrain_file, vec_file, args['pretrain_max_vocab'])
-
-
-    # load data
-    print("Loading data with batch size {}...".format(args['batch_size']))
-    doc, metasentences = CoNLL.conll2dict(input_file=args['train_file'])
-    train_doc = Document(doc, metasentences=metasentences)
-    train_batch = DataLoader(train_doc, args['batch_size'], args, pretrain, evaluation=False)
-    vocab = train_batch.vocab
-
-    # create inflectional_lexicon
-    inflectional_lexicon = generate_new_composite_dict(args['inflectional_lexicon_path'], train_batch) if args['inflectional_lexicon_path'] else None
-
-    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
-    dev_doc = Document(doc, metasentences=metasentences)
-    dev_batch = DataLoader(dev_doc, args['batch_size'], args, pretrain, vocab=vocab, evaluation=True, sort_during_eval=True)
-
-    # pred and gold path
-    system_pred_file = args['output_file']
-    gold_file = args['gold_file']
-
-    # skip training if the language does not have training or dev data
-    if len(train_batch) == 0 or len(dev_batch) == 0:
-        print("Skip training because no data available...")
-        sys.exit(0)
-
-    print("Training tagger...")
-    trainer = Trainer(args=args, vocab=vocab, pretrain=pretrain, use_cuda=args['cuda'])
-
-    global_step = 0
-    max_steps = args['max_steps']
-    dev_score_history = []
-    best_dev_preds = []
-    current_lr = args['lr']
-    global_start_time = time.time()
-    format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'
-
-    if args['adapt_eval_interval']:
-        args['eval_interval'] = utils.get_adaptive_eval_interval(dev_batch.num_examples, 2000, args['eval_interval'])
-        print("Evaluating the model every {} steps...".format(args['eval_interval']))
-
-    using_amsgrad = False
-    last_best_step = 0
-    # start training
-    train_loss = 0
-    while True:
-        do_break = False
-        for i, batch in enumerate(train_batch):
-            start_time = time.time()
-            global_step += 1
-            loss = trainer.update(batch, eval=False) # update step
-            train_loss += loss
-            if global_step % args['log_step'] == 0:
-                duration = time.time() - start_time
-                print(format_str.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"), global_step,\
-                        max_steps, loss, duration, current_lr))
-
-            if global_step % args['eval_interval'] == 0:
-                # eval on dev
-                print("Evaluating on dev set...")
-                dev_preds = []
-                for batch in dev_batch:
-                    preds = trainer.predict(batch)
-                    dev_preds += preds
-                dev_preds = utils.unsort(dev_preds, dev_batch.data_orig_idx)
-                dev_batch.doc.set([UPOS, XPOS, FEATS], [y for x in dev_preds for y in x])
-                CoNLL.dict2conll(dev_batch.doc.to_dict(), system_pred_file)
-                _, _, dev_score = scorer.score(system_pred_file, gold_file)
-
-                train_loss = train_loss / args['eval_interval'] # avg loss per batch
-                print("step {}: train_loss = {:.6f}, dev_score = {:.4f}".format(global_step, train_loss, dev_score))
-                train_loss = 0
-
-                # save best model
-                if len(dev_score_history) == 0 or dev_score > max(dev_score_history):
-                    last_best_step = global_step
-                    trainer.save(model_file, inflectional_lexicon=inflectional_lexicon)
-                    print("new best model saved.")
-                    best_dev_preds = dev_preds
-
-                dev_score_history += [dev_score]
-                print("")
-
-            if global_step - last_best_step >= args['max_steps_before_stop']:
-                if not using_amsgrad:
-                    print("Switching to AMSGrad")
-                    last_best_step = global_step
-                    using_amsgrad = True
-                    trainer.optimizer = optim.Adam(trainer.model.parameters(), amsgrad=True, lr=args['lr'], betas=(.9, args['beta2']), eps=1e-6)
-                else:
-                    do_break = True
-                    break
-
-            if global_step >= args['max_steps']:
-                do_break = True
-                break
-
-        if do_break: break
-
-        train_batch.reshuffle()
-
-    print("Training ended with {} steps.".format(global_step))
-
-    best_f, best_eval = max(dev_score_history)*100, np.argmax(dev_score_history)+1
-    print("Best dev F1 = {:.2f}, at iteration = {}".format(best_f, best_eval * args['eval_interval']))
-
-def evaluate(args):
-    # file paths
-    system_pred_file = args['output_file']
-    gold_file = args['gold_file']
-    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
-            else '{}/{}_tagger.pt'.format(args['save_dir'], args['shorthand'])
-
-    pretrain_file = '{}/{}.pretrain.pt'.format(args['save_dir'], args['shorthand']) if args['pretrain_file'] is None \
-        else args['pretrain_file']
-    pretrain = Pretrain(pretrain_file)
-
-    # load model
-    print("Loading model from: {}".format(model_file))
-    use_cuda = args['cuda'] and not args['cpu']
-    trainer = Trainer(args=args, pretrain=pretrain, model_file=model_file, use_cuda=use_cuda)
-    loaded_args, vocab = trainer.args, trainer.vocab
-
-    # load config
-    for k in args:
-        if k.endswith('_dir') or k.endswith('_file') or k in ['shorthand'] or k == 'mode':
-            loaded_args[k] = args[k]
-
-    # load data
-    print("Loading data with batch size {}...".format(args['batch_size']))
-    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
-    doc = Document(doc, metasentences=metasentences)
-    batch = DataLoader(doc, args['batch_size'], loaded_args, pretrain, vocab=vocab, evaluation=True, sort_during_eval=True)
-    if len(batch) > 0:
-        print("Start evaluation...")
-        preds = []
-        for i, b in enumerate(batch):
-            preds += trainer.predict(b)
-    else:
-        # skip eval if dev data does not exist
-        preds = []
-    preds = utils.unsort(preds, batch.data_orig_idx)
-
-    # write to file and score
-    batch.doc.set([UPOS, XPOS, FEATS], [y for x in preds for y in x])
-    CoNLL.dict2conll(batch.doc.to_dict(), system_pred_file)
-
-    if gold_file is not None:
-        _, _, score = scorer.score(system_pred_file, gold_file)
-
-        print("Tagger score:")
-        print("{} {:.2f}".format(args['shorthand'], score*100))
-
-if __name__ == '__main__':
-    main()
+"""
+Entry point for training and evaluating a POS/morphological features tagger.
+
+This tagger uses highway BiLSTM layers with character and word-level representations, and biaffine classifiers
+to produce consistant POS and UFeats predictions.
+For details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.
+"""
+import csv
+import sys
+import os
+import shutil
+import time
+from collections import Counter
+from datetime import datetime
+import argparse
+import numpy as np
+import random
+import torch
+from torch import nn, optim
+
+from classla.models.pos.data import DataLoader
+from classla.models.pos.trainer import Trainer
+from classla.models.pos import scorer
+from classla.models.common import utils
+from classla.models.common.pretrain import Pretrain
+from classla.models.common.doc import *
+from classla.utils.conll import CoNLL
+from classla.models import _training_logging
+
+def parse_args(args=None):
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--data_dir', type=str, default='data/pos', help='Root dir for saving models.')
+    parser.add_argument('--wordvec_dir', type=str, default='extern_data/word2vec', help='Directory of word vectors.')
+    parser.add_argument('--wordvec_file', type=str, default=None, help='Word vectors filename.')
+    parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')
+    parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')
+    parser.add_argument('--output_file', type=str, default=None, help='Output CoNLL-U file.')
+    parser.add_argument('--gold_file', type=str, default=None, help='Output CoNLL-U file.')
+    parser.add_argument('--pretrain_file', type=str, default=None, help='Input file containing pretrained data.')
+    parser.add_argument('--use_lexicon', type=str, default=None, help="Input location of lemmatization model.")
+    parser.add_argument('--inflectional_lexicon_path', type=str, default=None, help="Input location of inflectional lexicon (i.e. sloleks).")
+    parser.add_argument('--lemma_pretag', action='store_true', help="Use pretag from tokenization processor.")
+
+    parser.add_argument('--mode', default='train', choices=['train', 'predict'])
+    parser.add_argument('--lang', type=str, help='Language')
+    parser.add_argument('--shorthand', type=str, help="Treebank shorthand")
+
+    parser.add_argument('--hidden_dim', type=int, default=200)
+    parser.add_argument('--char_hidden_dim', type=int, default=400)
+    parser.add_argument('--deep_biaff_hidden_dim', type=int, default=400)
+    parser.add_argument('--composite_deep_biaff_hidden_dim', type=int, default=100)
+    parser.add_argument('--word_emb_dim', type=int, default=75)
+    parser.add_argument('--char_emb_dim', type=int, default=100)
+    parser.add_argument('--tag_emb_dim', type=int, default=50)
+    parser.add_argument('--transformed_dim', type=int, default=125)
+    parser.add_argument('--num_layers', type=int, default=2)
+    parser.add_argument('--char_num_layers', type=int, default=1)
+    parser.add_argument('--pretrain_max_vocab', type=int, default=250000)
+    parser.add_argument('--word_dropout', type=float, default=0.33)
+    parser.add_argument('--dropout', type=float, default=0.5)
+    parser.add_argument('--rec_dropout', type=float, default=0, help="Recurrent dropout")
+    parser.add_argument('--char_rec_dropout', type=float, default=0, help="Recurrent dropout")
+    parser.add_argument('--no_char', dest='char', action='store_false', help="Turn off character model.")
+    parser.add_argument('--no_pretrain', dest='pretrain', action='store_false', help="Turn off pretrained embeddings.")
+    parser.add_argument('--share_hid', action='store_true', help="Share hidden representations for UPOS, XPOS and UFeats.")
+    parser.set_defaults(share_hid=False)
+
+    parser.add_argument('--sample_train', type=float, default=1.0, help='Subsample training data.')
+    parser.add_argument('--optim', type=str, default='adam', help='sgd, adagrad, adam or adamax.')
+    parser.add_argument('--lr', type=float, default=3e-3, help='Learning rate')
+    parser.add_argument('--beta2', type=float, default=0.95)
+
+    parser.add_argument('--max_steps', type=int, default=50000)
+    parser.add_argument('--eval_interval', type=int, default=100)
+    parser.add_argument('--fix_eval_interval', dest='adapt_eval_interval', action='store_false', \
+            help="Use fixed evaluation interval for all treebanks, otherwise by default the interval will be increased for larger treebanks.")
+    parser.add_argument('--max_steps_before_stop', type=int, default=3000)
+    parser.add_argument('--batch_size', type=int, default=5000)
+    parser.add_argument('--max_grad_norm', type=float, default=1.0, help='Gradient clipping.')
+    parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')
+    parser.add_argument('--save_dir', type=str, default='saved_models/pos', help='Root dir for saving models.')
+    parser.add_argument('--save_name', type=str, default=None, help="File name to save the model")
+
+    parser.add_argument('--seed', type=int, default=1234)
+    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())
+    parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')
+    args = parser.parse_args(args=args)
+    return args
+
+
+def main(args=None):
+    sys.setrecursionlimit(50000)
+
+    args = parse_args(args=args)
+
+    torch.manual_seed(args.seed)
+    np.random.seed(args.seed)
+    random.seed(args.seed)
+    if args.cpu:
+        args.cuda = False
+    elif args.cuda:
+        torch.cuda.manual_seed(args.seed)
+
+    args = vars(args)
+    print("Running tagger in {} mode".format(args['mode']))
+
+    if args['mode'] == 'train':
+        train(args)
+    else:
+        evaluate(args)
+
+
+def generate_new_composite_dict(inflectional_lexicon_path, train_batch):
+    # generate initial composite dictionary from all train data
+    train_dict = train_batch.doc.get([TEXT, XPOS, UPOS, FEATS, LEMMA])
+    inflectional_dict = set()
+
+    lemmas_frequencies = {}
+    # get lemma frequencies
+    with open(inflectional_lexicon_path) as csvfile:
+        csv_reader = csv.reader(csvfile, delimiter='\t')
+        for row in csv_reader:
+            if row[0] == 'FORM' and row[1] == 'LEMMA':   # ignore header line
+                continue
+
+            # if row[6] is not float
+            if re.match(r'^-?\d+(?:\.\d+)$', row[6]) is None:
+                lemmas_frequencies[row[1]] = float(row[3]) if row[1] not in lemmas_frequencies else lemmas_frequencies[
+                                                                                                        row[1]] + float(
+                    row[3])
+                upos_ufeats = row[6].split()
+                ufeats = '|'.join(sorted(upos_ufeats[1:], key=lambda x: x.lower())) if upos_ufeats[1:] else '_'
+                inflectional_dict.add((row[0].lower(), row[2], upos_ufeats[0],
+                                       ufeats, float(row[3]), row[1]))
+
+            else:
+                lemmas_frequencies[row[1]] = float(row[6]) if row[1] not in lemmas_frequencies else lemmas_frequencies[row[1]] + float(row[6])
+                # '_' fix
+                ufeats = row[5] if row[5] else '_'
+                inflectional_dict.add((row[0].lower(), row[3], row[4], ufeats, float(row[6]), row[1]))
+
+    composite_dict = sorted(inflectional_dict, key=lambda x: x[4], reverse=True)
+    all_keys = {}
+
+    cleaned_composite_dict = []
+    i = 0
+    for el in composite_dict:
+        if (el[0], el[1], el[2], el[3]) not in all_keys:
+            all_keys[(el[0], el[1], el[2], el[3])] = (el, i)
+            cleaned_composite_dict.append((el[0], el[1], el[2], el[3], el[5]))
+            i += 1
+        else:
+            if el[5] in lemmas_frequencies and all_keys[(el[0], el[1], el[2], el[3])][0][5] in lemmas_frequencies and \
+                    lemmas_frequencies[all_keys[(el[0], el[1], el[2], el[3])][0][5]] < lemmas_frequencies[el[5]]:
+                old_el, updating_index = all_keys[(el[0], el[1], el[2], el[3])]
+                all_keys[(el[0], el[1], el[2], el[3])] = (el, updating_index)
+                cleaned_composite_dict[updating_index] = (el[0], el[1], el[2], el[3], el[5])
+
+    train_dict = [(el[0], el[1], el[2], el[3], el[4]) if el[3] is not None else (el[0], el[1], el[2], '_', el[4])
+                  for el in train_dict]
+    train_dict_most_common = Counter(train_dict).most_common()
+    filtered_train_dict = [el[0] for el in train_dict_most_common
+                           if (el[0][0], el[0][1], el[0][2], el[0][3]) not in all_keys]
+    cleaned_composite_dict.extend(filtered_train_dict)
+    return cleaned_composite_dict
+
+
+def train(args):
+    utils.ensure_dir(args['save_dir'])
+    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
+            else '{}/{}_tagger.pt'.format(args['save_dir'], args['shorthand'])
+
+    # load pretrained vectors
+    vec_file = args['wordvec_file']
+
+    pretrain_file = '{}/{}.pretrain.pt'.format(args['save_dir'], args['shorthand']) if args['pretrain_file'] is None \
+        else args['pretrain_file']
+    pretrain = Pretrain(pretrain_file, vec_file, args['pretrain_max_vocab'])
+
+
+    # load data
+    print("Loading data with batch size {}...".format(args['batch_size']))
+    doc, metasentences = CoNLL.conll2dict(input_file=args['train_file'])
+    train_doc = Document(doc, metasentences=metasentences)
+    train_batch = DataLoader(train_doc, args['batch_size'], args, pretrain, evaluation=False)
+    vocab = train_batch.vocab
+
+    # create inflectional_lexicon
+    inflectional_lexicon = generate_new_composite_dict(args['inflectional_lexicon_path'], train_batch) if args['inflectional_lexicon_path'] else None
+
+    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
+    dev_doc = Document(doc, metasentences=metasentences)
+    dev_batch = DataLoader(dev_doc, args['batch_size'], args, pretrain, vocab=vocab, evaluation=True, sort_during_eval=True)
+
+    # pred and gold path
+    system_pred_file = args['output_file']
+    gold_file = args['gold_file']
+
+    # skip training if the language does not have training or dev data
+    if len(train_batch) == 0 or len(dev_batch) == 0:
+        print("Skip training because no data available...")
+        sys.exit(0)
+
+    print("Training tagger...")
+    trainer = Trainer(args=args, vocab=vocab, pretrain=pretrain, use_cuda=args['cuda'])
+
+    global_step = 0
+    max_steps = args['max_steps']
+    dev_score_history = []
+    best_dev_preds = []
+    current_lr = args['lr']
+    global_start_time = time.time()
+    format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'
+
+    if args['adapt_eval_interval']:
+        args['eval_interval'] = utils.get_adaptive_eval_interval(dev_batch.num_examples, 2000, args['eval_interval'])
+        print("Evaluating the model every {} steps...".format(args['eval_interval']))
+
+    using_amsgrad = False
+    last_best_step = 0
+    # start training
+    train_loss = 0
+    while True:
+        do_break = False
+        for i, batch in enumerate(train_batch):
+            start_time = time.time()
+            global_step += 1
+            loss = trainer.update(batch, eval=False) # update step
+            train_loss += loss
+            if global_step % args['log_step'] == 0:
+                duration = time.time() - start_time
+                print(format_str.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"), global_step,\
+                        max_steps, loss, duration, current_lr))
+
+            if global_step % args['eval_interval'] == 0:
+                # eval on dev
+                print("Evaluating on dev set...")
+                dev_preds = []
+                for batch in dev_batch:
+                    preds = trainer.predict(batch)
+                    dev_preds += preds
+                dev_preds = utils.unsort(dev_preds, dev_batch.data_orig_idx)
+                dev_batch.doc.set([UPOS, XPOS, FEATS], [y for x in dev_preds for y in x])
+                CoNLL.dict2conll(dev_batch.doc.to_dict(), system_pred_file)
+                _, _, dev_score = scorer.score(system_pred_file, gold_file)
+
+                train_loss = train_loss / args['eval_interval'] # avg loss per batch
+                print("step {}: train_loss = {:.6f}, dev_score = {:.4f}".format(global_step, train_loss, dev_score))
+                train_loss = 0
+
+                # save best model
+                if len(dev_score_history) == 0 or dev_score > max(dev_score_history):
+                    last_best_step = global_step
+                    trainer.save(model_file, inflectional_lexicon=inflectional_lexicon)
+                    print("new best model saved.")
+                    best_dev_preds = dev_preds
+
+                dev_score_history += [dev_score]
+                print("")
+
+            if global_step - last_best_step >= args['max_steps_before_stop']:
+                if not using_amsgrad:
+                    print("Switching to AMSGrad")
+                    last_best_step = global_step
+                    using_amsgrad = True
+                    trainer.optimizer = optim.Adam(trainer.model.parameters(), amsgrad=True, lr=args['lr'], betas=(.9, args['beta2']), eps=1e-6)
+                else:
+                    do_break = True
+                    break
+
+            if global_step >= args['max_steps']:
+                do_break = True
+                break
+
+        if do_break: break
+
+        train_batch.reshuffle()
+
+    print("Training ended with {} steps.".format(global_step))
+
+    best_f, best_eval = max(dev_score_history)*100, np.argmax(dev_score_history)+1
+    print("Best dev F1 = {:.2f}, at iteration = {}".format(best_f, best_eval * args['eval_interval']))
+
+def evaluate(args):
+    # file paths
+    system_pred_file = args['output_file']
+    gold_file = args['gold_file']
+    model_file = args['save_dir'] + '/' + args['save_name'] if args['save_name'] is not None \
+            else '{}/{}_tagger.pt'.format(args['save_dir'], args['shorthand'])
+
+    pretrain_file = '{}/{}.pretrain.pt'.format(args['save_dir'], args['shorthand']) if args['pretrain_file'] is None \
+        else args['pretrain_file']
+    pretrain = Pretrain(pretrain_file)
+
+    # load model
+    print("Loading model from: {}".format(model_file))
+    use_cuda = args['cuda'] and not args['cpu']
+    trainer = Trainer(args=args, pretrain=pretrain, model_file=model_file, use_cuda=use_cuda)
+    loaded_args, vocab = trainer.args, trainer.vocab
+
+    # load config
+    for k in args:
+        if k.endswith('_dir') or k.endswith('_file') or k in ['shorthand'] or k == 'mode':
+            loaded_args[k] = args[k]
+
+    # load data
+    print("Loading data with batch size {}...".format(args['batch_size']))
+    doc, metasentences = CoNLL.conll2dict(input_file=args['eval_file'])
+    doc = Document(doc, metasentences=metasentences)
+    batch = DataLoader(doc, args['batch_size'], loaded_args, pretrain, vocab=vocab, evaluation=True, sort_during_eval=True)
+    if len(batch) > 0:
+        print("Start evaluation...")
+        preds = []
+        for i, b in enumerate(batch):
+            preds += trainer.predict(b)
+    else:
+        # skip eval if dev data does not exist
+        preds = []
+    preds = utils.unsort(preds, batch.data_orig_idx)
+
+    # write to file and score
+    batch.doc.set([UPOS, XPOS, FEATS], [y for x in preds for y in x])
+    CoNLL.dict2conll(batch.doc.to_dict(), system_pred_file)
+
+    if gold_file is not None:
+        _, _, score = scorer.score(system_pred_file, gold_file)
+
+        print("Tagger score:")
+        print("{} {:.2f}".format(args['shorthand'], score*100))
+
+if __name__ == '__main__':
+    main()
```

### Comparing `classla-2.0/classla/pipeline/core.py` & `classla-2.1/classla/pipeline/core.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,170 +1,170 @@
-"""
-Pipeline that runs tokenize,mwt,pos,lemma,depparse
-"""
-
-import io
-import itertools
-import sys
-import torch
-import logging
-import json
-import os
-
-from distutils.util import strtobool
-from classla.pipeline._constants import *
-from classla.models.common.doc import Document
-from classla.pipeline.processor import Processor, ProcessorRequirementsException
-from classla.pipeline.registry import NAME_TO_PROCESSOR_CLASS, PIPELINE_NAMES
-from classla.pipeline.tokenize_processor import TokenizeProcessor
-from classla.pipeline.mwt_processor import MWTProcessor
-from classla.pipeline.pos_processor import POSProcessor
-from classla.pipeline.lemma_processor import LemmaProcessor
-from classla.pipeline.depparse_processor import DepparseProcessor
-from classla.pipeline.sentiment_processor import SentimentProcessor
-from classla.pipeline.ner_processor import NERProcessor
-from classla.pipeline.srl_processor import SRLProcessor
-from classla.resources.common import DEFAULT_MODEL_DIR, \
-    maintain_processor_list, add_dependencies, build_default_config, set_logging_level, process_pipeline_parameters, sort_processors
-from classla.utils.helper_func import make_table
-
-logger = logging.getLogger('classla')
-
-
-class PipelineRequirementsException(Exception):
-    """
-    Exception indicating one or more requirements failures while attempting to build a pipeline.
-    Contains a ProcessorRequirementsException list.
-    """
-
-    def __init__(self, processor_req_fails):
-        self._processor_req_fails = processor_req_fails
-        self.build_message()
-
-    @property
-    def processor_req_fails(self):
-        return self._processor_req_fails
-
-    def build_message(self):
-        err_msg = io.StringIO()
-        print(*[req_fail.message for req_fail in self.processor_req_fails], sep='\n', file=err_msg)
-        self.message = '\n\n' + err_msg.getvalue()
-
-    def __str__(self):
-        return self.message
-
-
-class Pipeline:
-
-    def __init__(self, lang='sl', dir=DEFAULT_MODEL_DIR, type='default', processors={}, logging_level='INFO', verbose=None, use_gpu=True, **kwargs):
-        self.lang, self.dir, self.kwargs = lang, dir, kwargs
-        package = type
-
-        # set global logging level
-        set_logging_level(logging_level, verbose)
-        self.logging_level = logging.getLevelName(logger.level)
-        # process different pipeline parameters
-        lang, dir, package, processors = process_pipeline_parameters(lang, dir, package, processors)
-
-        # Load resources.json to obtain latest packages.
-        logger.debug('Loading resource file...')
-        resources_filepath = os.path.join(dir, 'resources.json')
-        if not os.path.exists(resources_filepath):
-            raise Exception(f"Resources file not found at: {resources_filepath}. Try to download the model again.")
-        with open(resources_filepath) as infile:
-            resources = json.load(infile)
-        if lang in resources:
-            if 'alias' in resources[lang]:
-                logger.info(f'"{lang}" is an alias for "{resources[lang]["alias"]}"')
-                lang = resources[lang]['alias']
-            lang_name = resources[lang]['lang_name'] if 'lang_name' in resources[lang] else ''
-        else:
-            logger.warning(f'Unsupported language: {lang}.')
-
-        # Maintain load list
-        self.load_list = maintain_processor_list(resources, lang, package, processors) if lang in resources else []
-        self.load_list = add_dependencies(resources, lang, self.load_list) if lang in resources else []
-        self.load_list = self.update_kwargs(kwargs, self.load_list)
-        if len(self.load_list) == 0: raise Exception('No processor to load. Please check if your language or package is correctly set.')
-        load_table = make_table(['Processor', 'Package'], [row[:2] for row in self.load_list])
-        logger.info(f'Loading these models for language: {lang} ({lang_name}):\n{load_table}')
-
-        self.config = build_default_config(resources, lang, dir, self.load_list)
-        self.config.update(kwargs)
-
-        # Load processors
-        self.processors = {}
-
-        # configs that are the same for all processors
-        pipeline_level_configs = {'lang': lang, 'mode': 'predict'}
-        self.use_gpu = torch.cuda.is_available() and use_gpu
-        logger.info("Use device: {}".format("gpu" if self.use_gpu else "cpu"))
-
-        # set up processors
-        pipeline_reqs_exceptions = []
-        for item in self.load_list:
-            processor_name, _, _ = item
-            logger.info('Loading: ' + processor_name)
-            curr_processor_config = self.filter_config(processor_name, self.config)
-            curr_processor_config.update(pipeline_level_configs)
-            logger.debug('With settings: ')
-            logger.debug(curr_processor_config)
-            try:
-                # try to build processor, throw an exception if there is a requirements issue
-                self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
-                                                                                          pipeline=self,
-                                                                                          use_gpu=self.use_gpu)
-            except ProcessorRequirementsException as e:
-                # if there was a requirements issue, add it to list which will be printed at end
-                pipeline_reqs_exceptions.append(e)
-                # add the broken processor to the loaded processors for the sake of analyzing the validity of the
-                # entire proposed pipeline, but at this point the pipeline will not be built successfully
-                self.processors[processor_name] = e.err_processor
-
-        # if there are any processor exceptions, throw an exception to indicate pipeline build failure
-        if pipeline_reqs_exceptions:
-            logger.info('\n')
-            raise PipelineRequirementsException(pipeline_reqs_exceptions)
-
-        logger.info("Done loading processors!")
-
-    def update_kwargs(self, kwargs, processor_list):
-        processor_dict = {processor: {'package': package, 'dependencies': dependencies} for (processor, package, dependencies) in processor_list}
-        for key, value in kwargs.items():
-            k, v = key.split('_', 1)
-            if v == 'model_path':
-                package = value if len(value) < 25 else value[:10]+ '...' + value[-10:]
-                dependencies = processor_dict.get(k, {}).get('dependencies')
-                processor_dict[k] = {'package': package, 'dependencies': dependencies}
-        processor_list = [[processor, processor_dict[processor]['package'], processor_dict[processor]['dependencies']] for processor in processor_dict]
-        processor_list = sort_processors(processor_list)
-        return processor_list
-
-    def filter_config(self, prefix, config_dict):
-        filtered_dict = {}
-        for key in config_dict.keys():
-            k, v = key.split('_', 1) # split tokenize_pretokenize to tokenize+pretokenize
-            if k == prefix:
-                filtered_dict[v] = config_dict[key]
-        return filtered_dict
-
-    @property
-    def loaded_processors(self):
-        """
-        Return all currently loaded processors in execution order.
-        :return: list of Processor instances
-        """
-        return [self.processors[processor_name] for processor_name in PIPELINE_NAMES if self.processors.get(processor_name)]
-
-    def process(self, doc):
-        # run the pipeline
-        for processor_name in PIPELINE_NAMES:
-            if self.processors.get(processor_name):
-                doc = self.processors[processor_name].process(doc)
-        return doc
-
-    def __call__(self, doc):
-        assert any([isinstance(doc, str), isinstance(doc, list),
-                    isinstance(doc, Document)]), 'input should be either str, list or Document'
-        doc = self.process(doc)
-        return doc
-
+"""
+Pipeline that runs tokenize,mwt,pos,lemma,depparse
+"""
+
+import io
+import itertools
+import sys
+import torch
+import logging
+import json
+import os
+
+from distutils.util import strtobool
+from classla.pipeline._constants import *
+from classla.models.common.doc import Document
+from classla.pipeline.processor import Processor, ProcessorRequirementsException
+from classla.pipeline.registry import NAME_TO_PROCESSOR_CLASS, PIPELINE_NAMES
+from classla.pipeline.tokenize_processor import TokenizeProcessor
+from classla.pipeline.mwt_processor import MWTProcessor
+from classla.pipeline.pos_processor import POSProcessor
+from classla.pipeline.lemma_processor import LemmaProcessor
+from classla.pipeline.depparse_processor import DepparseProcessor
+from classla.pipeline.sentiment_processor import SentimentProcessor
+from classla.pipeline.ner_processor import NERProcessor
+from classla.pipeline.srl_processor import SRLProcessor
+from classla.resources.common import DEFAULT_MODEL_DIR, \
+    maintain_processor_list, add_dependencies, build_default_config, set_logging_level, process_pipeline_parameters, sort_processors
+from classla.utils.helper_func import make_table
+
+logger = logging.getLogger('classla')
+
+
+class PipelineRequirementsException(Exception):
+    """
+    Exception indicating one or more requirements failures while attempting to build a pipeline.
+    Contains a ProcessorRequirementsException list.
+    """
+
+    def __init__(self, processor_req_fails):
+        self._processor_req_fails = processor_req_fails
+        self.build_message()
+
+    @property
+    def processor_req_fails(self):
+        return self._processor_req_fails
+
+    def build_message(self):
+        err_msg = io.StringIO()
+        print(*[req_fail.message for req_fail in self.processor_req_fails], sep='\n', file=err_msg)
+        self.message = '\n\n' + err_msg.getvalue()
+
+    def __str__(self):
+        return self.message
+
+
+class Pipeline:
+
+    def __init__(self, lang='sl', dir=DEFAULT_MODEL_DIR, type='default', processors={}, logging_level='INFO', verbose=None, use_gpu=True, **kwargs):
+        self.lang, self.dir, self.kwargs = lang, dir, kwargs
+        package = type
+
+        # set global logging level
+        set_logging_level(logging_level, verbose)
+        self.logging_level = logging.getLevelName(logger.level)
+        # process different pipeline parameters
+        lang, dir, package, processors = process_pipeline_parameters(lang, dir, package, processors)
+
+        # Load resources.json to obtain latest packages.
+        logger.debug('Loading resource file...')
+        resources_filepath = os.path.join(dir, 'resources.json')
+        if not os.path.exists(resources_filepath):
+            raise Exception(f"Resources file not found at: {resources_filepath}. Try to download the model again.")
+        with open(resources_filepath) as infile:
+            resources = json.load(infile)
+        if lang in resources:
+            if 'alias' in resources[lang]:
+                logger.info(f'"{lang}" is an alias for "{resources[lang]["alias"]}"')
+                lang = resources[lang]['alias']
+            lang_name = resources[lang]['lang_name'] if 'lang_name' in resources[lang] else ''
+        else:
+            logger.warning(f'Unsupported language: {lang}.')
+
+        # Maintain load list
+        self.load_list = maintain_processor_list(resources, lang, package, processors) if lang in resources else []
+        self.load_list = add_dependencies(resources, lang, self.load_list) if lang in resources else []
+        self.load_list = self.update_kwargs(kwargs, self.load_list)
+        if len(self.load_list) == 0: raise Exception('No processor to load. Please check if your language or package is correctly set.')
+        load_table = make_table(['Processor', 'Package'], [row[:2] for row in self.load_list])
+        logger.info(f'Loading these models for language: {lang} ({lang_name}):\n{load_table}')
+
+        self.config = build_default_config(resources, lang, dir, self.load_list)
+        self.config.update(kwargs)
+
+        # Load processors
+        self.processors = {}
+
+        # configs that are the same for all processors
+        pipeline_level_configs = {'lang': lang, 'mode': 'predict'}
+        self.use_gpu = torch.cuda.is_available() and use_gpu
+        logger.info("Use device: {}".format("gpu" if self.use_gpu else "cpu"))
+
+        # set up processors
+        pipeline_reqs_exceptions = []
+        for item in self.load_list:
+            processor_name, _, _ = item
+            logger.info('Loading: ' + processor_name)
+            curr_processor_config = self.filter_config(processor_name, self.config)
+            curr_processor_config.update(pipeline_level_configs)
+            logger.debug('With settings: ')
+            logger.debug(curr_processor_config)
+            try:
+                # try to build processor, throw an exception if there is a requirements issue
+                self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
+                                                                                          pipeline=self,
+                                                                                          use_gpu=self.use_gpu)
+            except ProcessorRequirementsException as e:
+                # if there was a requirements issue, add it to list which will be printed at end
+                pipeline_reqs_exceptions.append(e)
+                # add the broken processor to the loaded processors for the sake of analyzing the validity of the
+                # entire proposed pipeline, but at this point the pipeline will not be built successfully
+                self.processors[processor_name] = e.err_processor
+
+        # if there are any processor exceptions, throw an exception to indicate pipeline build failure
+        if pipeline_reqs_exceptions:
+            logger.info('\n')
+            raise PipelineRequirementsException(pipeline_reqs_exceptions)
+
+        logger.info("Done loading processors!")
+
+    def update_kwargs(self, kwargs, processor_list):
+        processor_dict = {processor: {'package': package, 'dependencies': dependencies} for (processor, package, dependencies) in processor_list}
+        for key, value in kwargs.items():
+            k, v = key.split('_', 1)
+            if v == 'model_path':
+                package = value if len(value) < 25 else value[:10]+ '...' + value[-10:]
+                dependencies = processor_dict.get(k, {}).get('dependencies')
+                processor_dict[k] = {'package': package, 'dependencies': dependencies}
+        processor_list = [[processor, processor_dict[processor]['package'], processor_dict[processor]['dependencies']] for processor in processor_dict]
+        processor_list = sort_processors(processor_list)
+        return processor_list
+
+    def filter_config(self, prefix, config_dict):
+        filtered_dict = {}
+        for key in config_dict.keys():
+            k, v = key.split('_', 1) # split tokenize_pretokenize to tokenize+pretokenize
+            if k == prefix:
+                filtered_dict[v] = config_dict[key]
+        return filtered_dict
+
+    @property
+    def loaded_processors(self):
+        """
+        Return all currently loaded processors in execution order.
+        :return: list of Processor instances
+        """
+        return [self.processors[processor_name] for processor_name in PIPELINE_NAMES if self.processors.get(processor_name)]
+
+    def process(self, doc):
+        # run the pipeline
+        for processor_name in PIPELINE_NAMES:
+            if self.processors.get(processor_name):
+                doc = self.processors[processor_name].process(doc)
+        return doc
+
+    def __call__(self, doc):
+        assert any([isinstance(doc, str), isinstance(doc, list),
+                    isinstance(doc, Document)]), 'input should be either str, list or Document'
+        doc = self.process(doc)
+        return doc
+
```

### Comparing `classla-2.0/classla/pipeline/external/spacy.py` & `classla-2.1/classla/pipeline/external/spacy.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,63 +1,63 @@
-"""
-Processors related to spaCy in the pipeline.
-"""
-
-from classla.models.common import doc
-from classla.pipeline._constants import TOKENIZE
-from classla.pipeline.processor import ProcessorVariant, register_processor_variant
-
-def check_spacy():
-    """
-    Import necessary components from spaCy to perform tokenization.
-    """
-    try:
-        import spacy
-    except ImportError:
-        raise ImportError(
-            "spaCy is used but not installed on your machine. Go to https://spacy.io/usage for installation instructions."
-        )
-    return True
-
-@register_processor_variant(TOKENIZE, 'spacy')
-class SpacyTokenizer(ProcessorVariant):
-    def __init__(self, config):
-        """ Construct a spaCy-based tokenizer by loading the spaCy pipeline.
-        """
-        if config['lang'] != 'en':
-            raise Exception("spaCy tokenizer is currently only allowed in English pipeline.")
-
-        try:
-            import spacy
-            from spacy.lang.en import English
-        except ImportError:
-            raise ImportError(
-                "spaCy 2.0+ is used but not installed on your machine. Go to https://spacy.io/usage for installation instructions."
-            )
-
-        # Create a Tokenizer with the default settings for English
-        # including punctuation rules and exceptions
-        self.nlp = English()
-        # by default spacy uses dependency parser to do ssplit
-        # we need to add a sentencizer for fast rule-based ssplit
-        sentencizer = self.nlp.create_pipe('sentencizer')
-        self.nlp.add_pipe(sentencizer)
-
-    def process(self, text):
-        """ Tokenize a document with the spaCy tokenizer and wrap the results into a Doc object.
-        """
-        if not isinstance(text, str):
-            raise Exception("Must supply a string to the spaCy tokenizer.")
-        spacy_doc = self.nlp(text)
-
-        sentences = []
-        for sent in spacy_doc.sents:
-            tokens = []
-            for tok in sent:
-                token_entry = {
-                    doc.TEXT: tok.text,
-                    doc.MISC: f"{doc.START_CHAR}={tok.idx}|{doc.END_CHAR}={tok.idx+len(tok.text)}"
-                }
-                tokens.append(token_entry)
-            sentences.append(tokens)
-
-        return doc.Document(sentences, text)
+"""
+Processors related to spaCy in the pipeline.
+"""
+
+from classla.models.common import doc
+from classla.pipeline._constants import TOKENIZE
+from classla.pipeline.processor import ProcessorVariant, register_processor_variant
+
+def check_spacy():
+    """
+    Import necessary components from spaCy to perform tokenization.
+    """
+    try:
+        import spacy
+    except ImportError:
+        raise ImportError(
+            "spaCy is used but not installed on your machine. Go to https://spacy.io/usage for installation instructions."
+        )
+    return True
+
+@register_processor_variant(TOKENIZE, 'spacy')
+class SpacyTokenizer(ProcessorVariant):
+    def __init__(self, config):
+        """ Construct a spaCy-based tokenizer by loading the spaCy pipeline.
+        """
+        if config['lang'] != 'en':
+            raise Exception("spaCy tokenizer is currently only allowed in English pipeline.")
+
+        try:
+            import spacy
+            from spacy.lang.en import English
+        except ImportError:
+            raise ImportError(
+                "spaCy 2.0+ is used but not installed on your machine. Go to https://spacy.io/usage for installation instructions."
+            )
+
+        # Create a Tokenizer with the default settings for English
+        # including punctuation rules and exceptions
+        self.nlp = English()
+        # by default spacy uses dependency parser to do ssplit
+        # we need to add a sentencizer for fast rule-based ssplit
+        sentencizer = self.nlp.create_pipe('sentencizer')
+        self.nlp.add_pipe(sentencizer)
+
+    def process(self, text):
+        """ Tokenize a document with the spaCy tokenizer and wrap the results into a Doc object.
+        """
+        if not isinstance(text, str):
+            raise Exception("Must supply a string to the spaCy tokenizer.")
+        spacy_doc = self.nlp(text)
+
+        sentences = []
+        for sent in spacy_doc.sents:
+            tokens = []
+            for tok in sent:
+                token_entry = {
+                    doc.TEXT: tok.text,
+                    doc.MISC: f"{doc.START_CHAR}={tok.idx}|{doc.END_CHAR}={tok.idx+len(tok.text)}"
+                }
+                tokens.append(token_entry)
+            sentences.append(tokens)
+
+        return doc.Document(sentences, text)
```

### Comparing `classla-2.0/classla/pipeline/external/sudachipy.py` & `classla-2.1/classla/pipeline/external/sudachipy.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,83 +1,83 @@
-"""
-Processors related to SudachiPy in the pipeline.
-
-GitHub Home: https://github.com/WorksApplications/SudachiPy
-"""
-
-import re
-
-from classla.models.common import doc
-from classla.pipeline._constants import TOKENIZE
-from classla.pipeline.processor import ProcessorVariant, register_processor_variant
-
-def check_sudachipy():
-    """
-    Import necessary components from SudachiPy to perform tokenization.
-    """
-    try:
-        import sudachipy
-        import sudachidict_core
-    except ImportError:
-        raise ImportError(
-            "Both sudachipy and sudachidict_core libraries are required. "
-            "Try install them with `pip install sudachipy sudachidict_core`. "
-            "Go to https://github.com/WorksApplications/SudachiPy for more information."
-        )
-    return True
-
-@register_processor_variant(TOKENIZE, 'sudachipy')
-class SudachiPyTokenizer(ProcessorVariant):
-    def __init__(self, config):
-        """ Construct a SudachiPy-based tokenizer.
-
-        Note that this tokenizer uses regex for sentence segmentation.
-        """
-        if config['lang'] != 'ja':
-            raise Exception("SudachiPy tokenizer is only allowed in Japanese pipelines.")
-
-        check_sudachipy()
-        from sudachipy import tokenizer
-        from sudachipy import dictionary
-
-        self.tokenizer = dictionary.Dictionary().create()
-
-    def process(self, text):
-        """ Tokenize a document with the SudachiPy tokenizer and wrap the results into a Doc object.
-        """
-        if not isinstance(text, str):
-            raise Exception("Must supply a string to the SudachiPy tokenizer.")
-
-        # we use the default sudachipy tokenization mode (i.e., mode C)
-        # more config needs to be added to support other modes
-
-        tokens = self.tokenizer.tokenize(text)
-
-        sentences = []
-        current_sentence = []
-        for token in tokens:
-            token_text = token.surface()
-            # by default sudachipy will output whitespace as a token
-            # we need to skip these tokens to be consistent with other tokenizers
-            if token_text.isspace():
-                continue
-            start = token.begin()
-            end = token.end()
-
-            token_entry = {
-                doc.TEXT: token_text,
-                doc.MISC: f"{doc.START_CHAR}={start}|{doc.END_CHAR}={end}"
-            }
-            current_sentence.append(token_entry)
-
-            if token_text in ['。', '！', '？', '!', '?']:
-                sentences.append(current_sentence)
-                current_sentence = []
-
-        if len(current_sentence) > 0:
-            sentences.append(current_sentence)
-
-        return doc.Document(sentences, text)
-
-
-
-
+"""
+Processors related to SudachiPy in the pipeline.
+
+GitHub Home: https://github.com/WorksApplications/SudachiPy
+"""
+
+import re
+
+from classla.models.common import doc
+from classla.pipeline._constants import TOKENIZE
+from classla.pipeline.processor import ProcessorVariant, register_processor_variant
+
+def check_sudachipy():
+    """
+    Import necessary components from SudachiPy to perform tokenization.
+    """
+    try:
+        import sudachipy
+        import sudachidict_core
+    except ImportError:
+        raise ImportError(
+            "Both sudachipy and sudachidict_core libraries are required. "
+            "Try install them with `pip install sudachipy sudachidict_core`. "
+            "Go to https://github.com/WorksApplications/SudachiPy for more information."
+        )
+    return True
+
+@register_processor_variant(TOKENIZE, 'sudachipy')
+class SudachiPyTokenizer(ProcessorVariant):
+    def __init__(self, config):
+        """ Construct a SudachiPy-based tokenizer.
+
+        Note that this tokenizer uses regex for sentence segmentation.
+        """
+        if config['lang'] != 'ja':
+            raise Exception("SudachiPy tokenizer is only allowed in Japanese pipelines.")
+
+        check_sudachipy()
+        from sudachipy import tokenizer
+        from sudachipy import dictionary
+
+        self.tokenizer = dictionary.Dictionary().create()
+
+    def process(self, text):
+        """ Tokenize a document with the SudachiPy tokenizer and wrap the results into a Doc object.
+        """
+        if not isinstance(text, str):
+            raise Exception("Must supply a string to the SudachiPy tokenizer.")
+
+        # we use the default sudachipy tokenization mode (i.e., mode C)
+        # more config needs to be added to support other modes
+
+        tokens = self.tokenizer.tokenize(text)
+
+        sentences = []
+        current_sentence = []
+        for token in tokens:
+            token_text = token.surface()
+            # by default sudachipy will output whitespace as a token
+            # we need to skip these tokens to be consistent with other tokenizers
+            if token_text.isspace():
+                continue
+            start = token.begin()
+            end = token.end()
+
+            token_entry = {
+                doc.TEXT: token_text,
+                doc.MISC: f"{doc.START_CHAR}={start}|{doc.END_CHAR}={end}"
+            }
+            current_sentence.append(token_entry)
+
+            if token_text in ['。', '！', '？', '!', '?']:
+                sentences.append(current_sentence)
+                current_sentence = []
+
+        if len(current_sentence) > 0:
+            sentences.append(current_sentence)
+
+        return doc.Document(sentences, text)
+
+
+
+
```

### Comparing `classla-2.0/classla/pipeline/lemma_processor.py` & `classla-2.1/classla/pipeline/lemma_processor.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,91 +1,91 @@
-"""
-Processor for performing lemmatization
-"""
-
-from classla.models.common import doc
-from classla.models.lemma.data import DataLoader
-from classla.models.lemma.trainer import Trainer
-from classla.pipeline._constants import *
-from classla.pipeline.processor import UDProcessor, register_processor
-
-@register_processor(name=LEMMA)
-class LemmaProcessor(UDProcessor):
-
-    # set of processor requirements this processor fulfills
-    PROVIDES_DEFAULT = set([LEMMA])
-    # set of processor requirements for this processor
-    # pos will be added later for non-identity lemmatizerx
-    REQUIRES_DEFAULT = set([TOKENIZE])
-    # default batch size
-    DEFAULT_BATCH_SIZE = 5000
-
-    def __init__(self, config, pipeline, use_gpu):
-        # run lemmatizer in identity mode
-        self._use_identity = None
-        super().__init__(config, pipeline, use_gpu)
-
-    @property
-    def use_identity(self):
-        return self._use_identity
-
-    def _set_up_model(self, config, use_gpu):
-        if config.get('use_identity') in ['True', True]:
-            self._use_identity = True
-            self._config = config
-            self.config['batch_size'] = LemmaProcessor.DEFAULT_BATCH_SIZE
-        else:
-            self._use_identity = False
-            # add pos_model_path for inf. lexicon load and pos_lemma_pretag to see wether lemmas are transfered or not
-            self._trainer = Trainer(args={'pos_model_path': self.pipeline.config['pos_model_path'], 'pos_lemma_pretag': self.pipeline.processors['pos'].config['lemma_pretag']}, model_file=config['model_path'], use_cuda=use_gpu)
-
-    def _set_up_requires(self):
-        if self.config.get('pos') and not self.use_identity:
-            self._requires = LemmaProcessor.REQUIRES_DEFAULT.union(set([POS]))
-        else:
-            self._requires = LemmaProcessor.REQUIRES_DEFAULT
-
-    def process(self, document):
-        if not self.use_identity:
-            batch = DataLoader(document, self.config['batch_size'], self.config, vocab=self.vocab, evaluation=True)
-        else:
-            batch = DataLoader(document, self.config['batch_size'], self.config, evaluation=True, conll_only=True)
-        if self.use_identity:
-            preds = [word.text for sent in batch.doc.sentences for word in sent.words]
-        elif self.config.get('dict_only', False):
-            preds = self.trainer.predict_dict(batch.doc.get([doc.TEXT, doc.XPOS]))
-        else:
-            if self.config.get('ensemble_dict', False):
-                # skip the seq2seq model when we can
-                skip = self.trainer.skip_seq2seq([(e[0].lower(),e[1],e[2]) for e in batch.doc.get([doc.TEXT, doc.XPOS, doc.LEMMA])])
-                seq2seq_batch = DataLoader(document, self.config['batch_size'], self.config, vocab=self.vocab,
-                                           evaluation=True, skip=skip)
-            else:
-                seq2seq_batch = batch
-
-            preds = []
-            edits = []
-            for i, b in enumerate(seq2seq_batch):
-                ps, es = self.trainer.predict(b, self.config['beam_size'])
-                preds += ps
-                if es is not None:
-                    edits += es
-
-            if self.config.get('ensemble_dict', False):
-                preds = self.trainer.postprocess([x for x, y in zip(batch.doc.get([doc.TEXT]), skip) if not y], preds, edits=edits)
-                # expand seq2seq predictions to the same size as all words
-                i = 0
-                preds1 = []
-                for s in skip:
-                    if s:
-                        preds1.append('')
-                    else:
-                        preds1.append(preds[i])
-                        i += 1
-                preds = self.trainer.ensemble([(e[0].lower(),e[1],e[2]) for e in batch.doc.get([doc.TEXT, doc.XPOS, doc.LEMMA])], preds1)
-            else:
-                preds = self.trainer.postprocess(batch.doc.get([doc.TEXT]), preds, edits=edits)
-
-        # map empty string lemmas to '_'
-        preds = [max([(len(x), x), (0, '_')])[1] for x in preds]
-        batch.doc.set([doc.LEMMA], preds)
-        return batch.doc
+"""
+Processor for performing lemmatization
+"""
+
+from classla.models.common import doc
+from classla.models.lemma.data import DataLoader
+from classla.models.lemma.trainer import Trainer
+from classla.pipeline._constants import *
+from classla.pipeline.processor import UDProcessor, register_processor
+
+@register_processor(name=LEMMA)
+class LemmaProcessor(UDProcessor):
+
+    # set of processor requirements this processor fulfills
+    PROVIDES_DEFAULT = set([LEMMA])
+    # set of processor requirements for this processor
+    # pos will be added later for non-identity lemmatizerx
+    REQUIRES_DEFAULT = set([TOKENIZE])
+    # default batch size
+    DEFAULT_BATCH_SIZE = 5000
+
+    def __init__(self, config, pipeline, use_gpu):
+        # run lemmatizer in identity mode
+        self._use_identity = None
+        super().__init__(config, pipeline, use_gpu)
+
+    @property
+    def use_identity(self):
+        return self._use_identity
+
+    def _set_up_model(self, config, use_gpu):
+        if config.get('use_identity') in ['True', True]:
+            self._use_identity = True
+            self._config = config
+            self.config['batch_size'] = LemmaProcessor.DEFAULT_BATCH_SIZE
+        else:
+            self._use_identity = False
+            # add pos_model_path for inf. lexicon load and pos_lemma_pretag to see wether lemmas are transfered or not
+            self._trainer = Trainer(args={'pos_model_path': self.pipeline.config['pos_model_path'], 'pos_lemma_pretag': self.pipeline.processors['pos'].config['lemma_pretag']}, model_file=config['model_path'], use_cuda=use_gpu)
+
+    def _set_up_requires(self):
+        if self.config.get('pos') and not self.use_identity:
+            self._requires = LemmaProcessor.REQUIRES_DEFAULT.union(set([POS]))
+        else:
+            self._requires = LemmaProcessor.REQUIRES_DEFAULT
+
+    def process(self, document):
+        if not self.use_identity:
+            batch = DataLoader(document, self.config['batch_size'], self.config, vocab=self.vocab, evaluation=True)
+        else:
+            batch = DataLoader(document, self.config['batch_size'], self.config, evaluation=True, conll_only=True)
+        if self.use_identity:
+            preds = [word.text for sent in batch.doc.sentences for word in sent.words]
+        elif self.config.get('dict_only', False):
+            preds = self.trainer.predict_dict(batch.doc.get([doc.TEXT, doc.XPOS]))
+        else:
+            if self.config.get('ensemble_dict', False):
+                # skip the seq2seq model when we can
+                skip = self.trainer.skip_seq2seq([(e[0].lower(),e[1],e[2]) for e in batch.doc.get([doc.TEXT, doc.XPOS, doc.LEMMA])])
+                seq2seq_batch = DataLoader(document, self.config['batch_size'], self.config, vocab=self.vocab,
+                                           evaluation=True, skip=skip)
+            else:
+                seq2seq_batch = batch
+
+            preds = []
+            edits = []
+            for i, b in enumerate(seq2seq_batch):
+                ps, es = self.trainer.predict(b, self.config['beam_size'])
+                preds += ps
+                if es is not None:
+                    edits += es
+
+            if self.config.get('ensemble_dict', False):
+                preds = self.trainer.postprocess([x for x, y in zip(batch.doc.get([doc.TEXT]), skip) if not y], preds, edits=edits)
+                # expand seq2seq predictions to the same size as all words
+                i = 0
+                preds1 = []
+                for s in skip:
+                    if s:
+                        preds1.append('')
+                    else:
+                        preds1.append(preds[i])
+                        i += 1
+                preds = self.trainer.ensemble([(e[0].lower(),e[1],e[2]) for e in batch.doc.get([doc.TEXT, doc.XPOS, doc.LEMMA])], preds1)
+            else:
+                preds = self.trainer.postprocess(batch.doc.get([doc.TEXT]), preds, edits=edits)
+
+        # map empty string lemmas to '_'
+        preds = [max([(len(x), x), (0, '_')])[1] for x in preds]
+        batch.doc.set([doc.LEMMA], preds)
+        return batch.doc
```

### Comparing `classla-2.0/classla/pipeline/mwt_processor.py` & `classla-2.1/classla/pipeline/mwt_processor.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,42 +1,42 @@
-"""
-Processor for performing multi-word-token expansion
-"""
-
-import io
-
-from classla.models.mwt.data import DataLoader
-from classla.models.mwt.trainer import Trainer
-from classla.pipeline._constants import *
-from classla.pipeline.processor import UDProcessor, register_processor
-
-@register_processor(MWT)
-class MWTProcessor(UDProcessor):
-
-    # set of processor requirements this processor fulfills
-    PROVIDES_DEFAULT = set([MWT])
-    # set of processor requirements for this processor
-    REQUIRES_DEFAULT = set([TOKENIZE])
-
-    def _set_up_model(self, config, use_gpu):
-        self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)
-
-    def process(self, document):
-        batch = DataLoader(document, self.config['batch_size'], self.config, vocab=self.vocab, evaluation=True)
-        if len(batch) > 0:
-            dict_preds = self.trainer.predict_dict(batch.doc.get_mwt_expansions(evaluation=True))
-            # decide trainer type and run eval
-            if self.config['dict_only']:
-                preds = dict_preds
-            else:
-                preds = []
-                for i, b in enumerate(batch):
-                    preds += self.trainer.predict(b)
-
-                if self.config.get('ensemble_dict', False):
-                    preds = self.trainer.ensemble(batch.doc.get_mwt_expansions(evaluation=True), preds)
-        else:
-            # skip eval if dev data does not exist
-            preds = []
-
-        batch.doc.set_mwt_expansions(preds)
-        return batch.doc
+"""
+Processor for performing multi-word-token expansion
+"""
+
+import io
+
+from classla.models.mwt.data import DataLoader
+from classla.models.mwt.trainer import Trainer
+from classla.pipeline._constants import *
+from classla.pipeline.processor import UDProcessor, register_processor
+
+@register_processor(MWT)
+class MWTProcessor(UDProcessor):
+
+    # set of processor requirements this processor fulfills
+    PROVIDES_DEFAULT = set([MWT])
+    # set of processor requirements for this processor
+    REQUIRES_DEFAULT = set([TOKENIZE])
+
+    def _set_up_model(self, config, use_gpu):
+        self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)
+
+    def process(self, document):
+        batch = DataLoader(document, self.config['batch_size'], self.config, vocab=self.vocab, evaluation=True)
+        if len(batch) > 0:
+            dict_preds = self.trainer.predict_dict(batch.doc.get_mwt_expansions(evaluation=True))
+            # decide trainer type and run eval
+            if self.config['dict_only']:
+                preds = dict_preds
+            else:
+                preds = []
+                for i, b in enumerate(batch):
+                    preds += self.trainer.predict(b)
+
+                if self.config.get('ensemble_dict', False):
+                    preds = self.trainer.ensemble(batch.doc.get_mwt_expansions(evaluation=True), preds)
+        else:
+            # skip eval if dev data does not exist
+            preds = []
+
+        batch.doc.set_mwt_expansions(preds)
+        return batch.doc
```

### Comparing `classla-2.0/classla/pipeline/ner_processor.py` & `classla-2.1/classla/pipeline/depparse_processor.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,39 +1,49 @@
-"""
-Processor for performing named entity tagging.
-"""
-import logging
-
-from classla.models.common import doc
-from classla.models.common.utils import unsort
-from classla.models.ner.data import DataLoader
-from classla.models.ner.trainer import Trainer
-from classla.pipeline._constants import *
-from classla.pipeline.processor import UDProcessor, register_processor
-
-logger = logging.getLogger('classla')
-
-@register_processor(name=NER)
-class NERProcessor(UDProcessor):
-
-    # set of processor requirements this processor fulfills
-    PROVIDES_DEFAULT = set([NER])
-    # set of processor requirements for this processor
-    REQUIRES_DEFAULT = set([TOKENIZE])
-
-    def _set_up_model(self, config, use_gpu):
-        # set up trainer
-        args = {'charlm_forward_file': config['forward_charlm_path'], 'charlm_backward_file': config['backward_charlm_path']}
-        self._trainer = Trainer(args=args, model_file=config['model_path'], use_cuda=use_gpu)
-
-    def process(self, document):
-        # set up a eval-only data loader and skip tag preprocessing
-        batch = DataLoader(
-            document, self.config['batch_size'], self.config, vocab=self.vocab, evaluation=True, preprocess_tags=False)
-        preds = []
-        for i, b in enumerate(batch):
-            preds += self.trainer.predict(b)
-        batch.doc.set([doc.NER], [y for x in preds for y in x], to_token=True)
-        # collect entities into document attribute
-        total = len(batch.doc.build_ents())
-        logger.debug(f'{total} entities found in document.')
-        return batch.doc
+"""
+Processor for performing dependency parsing
+"""
+
+from classla.models.common import doc
+from classla.models.common.pretrain import Pretrain
+from classla.models.common.utils import unsort
+from classla.models.depparse.data import DataLoader
+from classla.models.depparse.trainer import Trainer
+from classla.pipeline._constants import *
+from classla.pipeline.processor import UDProcessor, register_processor
+
+
+@register_processor(name=DEPPARSE)
+class DepparseProcessor(UDProcessor):
+
+    # set of processor requirements this processor fulfills
+    PROVIDES_DEFAULT = set([DEPPARSE])
+    # set of processor requirements for this processor
+    REQUIRES_DEFAULT = set([TOKENIZE, POS, LEMMA])
+
+    def __init__(self, config, pipeline, use_gpu):
+        self._pretagged = None
+        super().__init__(config, pipeline, use_gpu)
+
+    def _set_up_requires(self):
+        self._pretagged = self._config.get('pretagged')
+        if self._pretagged:
+            self._requires = set()
+        else:
+            self._requires = self.__class__.REQUIRES_DEFAULT
+
+    def _set_up_model(self, config, use_gpu):
+        self._pretrain = Pretrain(config['pretrain_path']) if 'pretrain_path' in config else None
+        self._trainer = Trainer(pretrain=self.pretrain, model_file=config['model_path'], use_cuda=use_gpu)
+
+    def process(self, document):
+        batch = DataLoader(document, self.config['batch_size'], self.config, self.pretrain, vocab=self.vocab, evaluation=True,
+                           sort_during_eval=self.config.get('sort_during_eval', True), max_sentence_size=self.config.get('max_sentence_size', None))
+        preds = []
+        for i, b in enumerate(batch):
+            preds += self.trainer.predict(b)
+        if batch.data_orig_idx is not None:
+            preds = unsort(preds, batch.data_orig_idx)
+        batch.doc.set([doc.HEAD, doc.DEPREL], [y for x in preds for y in x])
+        # build dependencies based on predictions
+        for sentence in batch.doc.sentences:
+            sentence.build_dependencies()
+        return batch.doc
```

### Comparing `classla-2.0/classla/pipeline/pos_processor.py` & `classla-2.1/classla/pipeline/pos_processor.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,73 +1,73 @@
-"""
-Processor for performing part-of-speech tagging
-"""
-import os
-
-from classla.models.common import doc
-from classla.models.common.pretrain import Pretrain
-from classla.models.common.utils import unsort
-from classla.models.pos.data import DataLoader
-from classla.models.pos.trainer import Trainer
-from classla.pipeline._constants import *
-from classla.pipeline.processor import UDProcessor, register_processor
-
-@register_processor(name=POS)
-class POSProcessor(UDProcessor):
-
-    # set of processor requirements this processor fulfills
-    PROVIDES_DEFAULT = set([POS])
-    # set of processor requirements for this processor
-    REQUIRES_DEFAULT = set([TOKENIZE])
-
-    def _set_up_model(self, config, use_gpu):
-        # get pretrained word vectors
-        self._pretrain = Pretrain(config['pretrain_path']) if 'pretrain_path' in config else None
-
-        if 'lemma_pretag' in self.config:
-            pos_lemma_pretag = self.config['lemma_pretag']
-        else:
-            pos_lemma_pretag = (not 'tokenize_pretokenized' in self.pipeline.config or not self.pipeline.config[
-                'tokenize_pretokenized'])
-            self.config['lemma_pretag'] = pos_lemma_pretag
-
-        arg = {'lemma_pretag': pos_lemma_pretag}
-
-        if 'use_lexicon' in self.config and self.config['use_lexicon']:
-            arg['use_lexicon'] = True
-
-        # set up trainer
-        self._trainer = Trainer(args=arg, pretrain=self.pretrain, model_file=config['model_path'], use_cuda=use_gpu)
-
-    def predetermined_punctuations(self, seq):
-        """ Determine if punctuation is already assigned by tokenizer. """
-        return [pos if pos[0] is not None else False for pos in seq]
-
-    def process(self, document):
-        batch = DataLoader(
-            document, self.config['batch_size'], self.config, self.pretrain, vocab=self.vocab, evaluation=True,
-            sort_during_eval=True)
-        preds = []
-        for i, b in enumerate(batch):
-            preds += self.trainer.predict(b)
-
-        if not preds:
-            return batch.doc
-
-        preds = unsort(preds, batch.data_orig_idx)
-        if 'lemma_pretag' in self.config and self.config['lemma_pretag']:
-            preds_flattened = []
-            # skip pos predictions for punctuations that were predicted by tokenizer
-            skip = iter(self.predetermined_punctuations(zip(batch.doc.get([doc.UPOS]), batch.doc.get([doc.XPOS]), batch.doc.get([doc.FEATS]))))
-            for x in preds:
-                for y in x:
-                    n = next(skip, None)
-                    assert n is not None
-                    if not n:
-                        preds_flattened.append(y)
-                    else:
-                        preds_flattened.append(n)
-        else:
-            preds_flattened = [y for x in preds for y in x]
-
-        batch.doc.set([doc.UPOS, doc.XPOS, doc.FEATS], preds_flattened)
-        return batch.doc
+"""
+Processor for performing part-of-speech tagging
+"""
+import os
+
+from classla.models.common import doc
+from classla.models.common.pretrain import Pretrain
+from classla.models.common.utils import unsort
+from classla.models.pos.data import DataLoader
+from classla.models.pos.trainer import Trainer
+from classla.pipeline._constants import *
+from classla.pipeline.processor import UDProcessor, register_processor
+
+@register_processor(name=POS)
+class POSProcessor(UDProcessor):
+
+    # set of processor requirements this processor fulfills
+    PROVIDES_DEFAULT = set([POS])
+    # set of processor requirements for this processor
+    REQUIRES_DEFAULT = set([TOKENIZE])
+
+    def _set_up_model(self, config, use_gpu):
+        # get pretrained word vectors
+        self._pretrain = Pretrain(config['pretrain_path']) if 'pretrain_path' in config else None
+
+        if 'lemma_pretag' in self.config:
+            pos_lemma_pretag = self.config['lemma_pretag']
+        else:
+            pos_lemma_pretag = (not 'tokenize_pretokenized' in self.pipeline.config or not self.pipeline.config[
+                'tokenize_pretokenized'])
+            self.config['lemma_pretag'] = pos_lemma_pretag
+
+        arg = {'lemma_pretag': pos_lemma_pretag}
+
+        if 'use_lexicon' in self.config and self.config['use_lexicon']:
+            arg['use_lexicon'] = True
+
+        # set up trainer
+        self._trainer = Trainer(args=arg, pretrain=self.pretrain, model_file=config['model_path'], use_cuda=use_gpu)
+
+    def predetermined_punctuations(self, seq):
+        """ Determine if punctuation is already assigned by tokenizer. """
+        return [pos if pos[0] is not None else False for pos in seq]
+
+    def process(self, document):
+        batch = DataLoader(
+            document, self.config['batch_size'], self.config, self.pretrain, vocab=self.vocab, evaluation=True,
+            sort_during_eval=True)
+        preds = []
+        for i, b in enumerate(batch):
+            preds += self.trainer.predict(b)
+
+        if not preds:
+            return batch.doc
+
+        preds = unsort(preds, batch.data_orig_idx)
+        if 'lemma_pretag' in self.config and self.config['lemma_pretag']:
+            preds_flattened = []
+            # skip pos predictions for punctuations that were predicted by tokenizer
+            skip = iter(self.predetermined_punctuations(zip(batch.doc.get([doc.UPOS]), batch.doc.get([doc.XPOS]), batch.doc.get([doc.FEATS]))))
+            for x in preds:
+                for y in x:
+                    n = next(skip, None)
+                    assert n is not None
+                    if not n:
+                        preds_flattened.append(y)
+                    else:
+                        preds_flattened.append(n)
+        else:
+            preds_flattened = [y for x in preds for y in x]
+
+        batch.doc.set([doc.UPOS, doc.XPOS, doc.FEATS], preds_flattened)
+        return batch.doc
```

### Comparing `classla-2.0/classla/pipeline/sentiment_processor.py` & `classla-2.1/classla/pipeline/sentiment_processor.py`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,43 +1,43 @@
-"""Processor that attaches a sentiment score to a sentence
-
-The model used is a generally a model trained on the Stanford
-Sentiment Treebank or some similar dataset.  When run, this processor
-attachs a score in the form of a string to each sentence in the
-document.
-
-TODO: a possible way to generalize this would be to make it a
-ClassifierProcessor and have "sentiment" be an option.
-"""
-
-import classla.models.classifiers.cnn_classifier as cnn_classifier
-
-from classla.models.common import doc
-from classla.models.common.pretrain import Pretrain
-from classla.pipeline._constants import *
-from classla.pipeline.processor import UDProcessor, register_processor
-
-@register_processor(SENTIMENT)
-class SentimentProcessor(UDProcessor):
-    # set of processor requirements this processor fulfills
-    PROVIDES_DEFAULT = set([SENTIMENT])
-    # set of processor requirements for this processor
-    REQUIRES_DEFAULT = set([TOKENIZE])
-
-    def _set_up_model(self, config, use_gpu):
-        # get pretrained word vectors
-        self._pretrain = Pretrain(config['pretrain_path'])
-        # set up model
-        self._model = cnn_classifier.load(filename=config['model_path'], pretrain=self._pretrain)
-        self._batch_size = config.get('batch_size', None)
-
-        # TODO: move this call to load()
-        if use_gpu:
-            self._model.cuda()
-
-    def process(self, document):
-        sentences = document.sentences
-        text = [" ".join(token.text for token in sentence.tokens) for sentence in sentences]
-        labels = cnn_classifier.label_text(self._model, text, batch_size=self._batch_size)
-        # TODO: allow a classifier processor for any attribute, not just sentiment
-        document.set(SENTIMENT, labels, to_sentence=True)
-        return document
+"""Processor that attaches a sentiment score to a sentence
+
+The model used is a generally a model trained on the Stanford
+Sentiment Treebank or some similar dataset.  When run, this processor
+attachs a score in the form of a string to each sentence in the
+document.
+
+TODO: a possible way to generalize this would be to make it a
+ClassifierProcessor and have "sentiment" be an option.
+"""
+
+import classla.models.classifiers.cnn_classifier as cnn_classifier
+
+from classla.models.common import doc
+from classla.models.common.pretrain import Pretrain
+from classla.pipeline._constants import *
+from classla.pipeline.processor import UDProcessor, register_processor
+
+@register_processor(SENTIMENT)
+class SentimentProcessor(UDProcessor):
+    # set of processor requirements this processor fulfills
+    PROVIDES_DEFAULT = set([SENTIMENT])
+    # set of processor requirements for this processor
+    REQUIRES_DEFAULT = set([TOKENIZE])
+
+    def _set_up_model(self, config, use_gpu):
+        # get pretrained word vectors
+        self._pretrain = Pretrain(config['pretrain_path'])
+        # set up model
+        self._model = cnn_classifier.load(filename=config['model_path'], pretrain=self._pretrain)
+        self._batch_size = config.get('batch_size', None)
+
+        # TODO: move this call to load()
+        if use_gpu:
+            self._model.cuda()
+
+    def process(self, document):
+        sentences = document.sentences
+        text = [" ".join(token.text for token in sentence.tokens) for sentence in sentences]
+        labels = cnn_classifier.label_text(self._model, text, batch_size=self._batch_size)
+        # TODO: allow a classifier processor for any attribute, not just sentiment
+        document.set(SENTIMENT, labels, to_sentence=True)
+        return document
```

### Comparing `classla-2.0/classla/pipeline/srl_processor.py` & `classla-2.1/classla/pipeline/srl_processor.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-"""
-Processor for performing named entity tagging.
-"""
-import logging
-
-from classla.models.common import doc
-from classla.models.common.utils import unsort
-from classla.models.srl.data import DataLoader
-from classla.models.srl.trainer import Trainer
-from classla.pipeline._constants import *
-from classla.pipeline.processor import UDProcessor, register_processor
-
-from classla.models.common.pretrain import Pretrain
-
-logger = logging.getLogger('classla')
-
-
-@register_processor(name=SRL)
-class SRLProcessor(UDProcessor):
-
-    # set of processor requirements this processor fulfills
-    PROVIDES_DEFAULT = set([SRL])
-    # set of processor requirements for this processor
-    REQUIRES_DEFAULT = set([TOKENIZE, POS, LEMMA, DEPPARSE])
-
-    def _set_up_model(self, config, use_gpu):
-        # get pretrained word vectors
-        self._pretrain = Pretrain(config['pretrain_path']) if 'pretrain_path' in config else None
-
-        arg = {}
-
-        # set up trainer
-        self._trainer = Trainer(args=arg, pretrain=self.pretrain, model_file=config['model_path'], use_cuda=use_gpu)
-
-    def predetermined_punctuations(self, seq):
-        """ Determine if punctuation is already assigned by tokenizer. """
-        return [pos if pos[0] is not None else False for pos in seq]
-
-    def process(self, document):
-        batch = DataLoader(document, self.config['batch_size'], self.config, self.pretrain, vocab=self.vocab, evaluation=True)
-        preds = []
-        for i, b in enumerate(batch):
-            preds += self.trainer.predict(b)
-        batch.doc.set([doc.SRL], [y for x in preds for y in x], to_token=True)
-        return batch.doc
+"""
+Processor for performing named entity tagging.
+"""
+import logging
+
+from classla.models.common import doc
+from classla.models.common.utils import unsort
+from classla.models.srl.data import DataLoader
+from classla.models.srl.trainer import Trainer
+from classla.pipeline._constants import *
+from classla.pipeline.processor import UDProcessor, register_processor
+
+from classla.models.common.pretrain import Pretrain
+
+logger = logging.getLogger('classla')
+
+
+@register_processor(name=SRL)
+class SRLProcessor(UDProcessor):
+
+    # set of processor requirements this processor fulfills
+    PROVIDES_DEFAULT = set([SRL])
+    # set of processor requirements for this processor
+    REQUIRES_DEFAULT = set([TOKENIZE, POS, LEMMA, DEPPARSE])
+
+    def _set_up_model(self, config, use_gpu):
+        # get pretrained word vectors
+        self._pretrain = Pretrain(config['pretrain_path']) if 'pretrain_path' in config else None
+
+        arg = {}
+
+        # set up trainer
+        self._trainer = Trainer(args=arg, pretrain=self.pretrain, model_file=config['model_path'], use_cuda=use_gpu)
+
+    def predetermined_punctuations(self, seq):
+        """ Determine if punctuation is already assigned by tokenizer. """
+        return [pos if pos[0] is not None else False for pos in seq]
+
+    def process(self, document):
+        batch = DataLoader(document, self.config['batch_size'], self.config, self.pretrain, vocab=self.vocab, evaluation=True)
+        preds = []
+        for i, b in enumerate(batch):
+            preds += self.trainer.predict(b)
+        batch.doc.set([doc.SRL], [y for x in preds for y in x], to_token=True)
+        return batch.doc
```

### Comparing `classla-2.0/classla/pipeline/tokenize_processor.py` & `classla-2.1/classla/pipeline/tokenize_processor.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,102 +1,102 @@
-"""
-Processor for performing tokenization
-"""
-
-import io
-import logging
-import os
-
-from classla.pipeline._constants import *
-from classla.pipeline.processor import UDProcessor, register_processor
-from classla.utils.conll import CoNLL
-from classla.utils.obeliks import ObeliksTrainer
-from classla.models.common import doc
-from classla.utils.reldi import ReldiTrainer
-
-logger = logging.getLogger('classla')
-
-# class for running the tokenizer
-@register_processor(name=TOKENIZE)
-class TokenizeProcessor(UDProcessor):
-
-    # set of processor requirements this processor fulfills
-    PROVIDES_DEFAULT = set([TOKENIZE])
-    # set of processor requirements for this processor
-    REQUIRES_DEFAULT = set([])
-    # default max sequence length
-    MAX_SEQ_LENGTH_DEFAULT = 1000
-
-    def _set_up_model(self, config, use_gpu):
-        # set up trainer
-        if os.path.basename(config['library']) == 'obeliks':
-            self._tokenizer = ObeliksTrainer(config.get('lang'), config.get('type'), 'pos' in [el[0] for el in self.pipeline.load_list])
-        elif os.path.basename(config['library']) == 'reldi':
-            self._tokenizer = ReldiTrainer(config.get('lang'), config.get('type'))
-        else:
-            raise Exception(f'Tokenizer {config["library"]} not available.')
-
-        if config.get('pretokenized'):
-            self._trainer = None
-
-    def process_pre_tokenized_text(self, input_src):
-        """
-        Pretokenized text can be provided in 2 manners:
-
-        1.) str, tokenized by whitespace, sentence split by newline
-        2.) list of token lists, each token list represents a sentence
-
-        generate dictionary data structure
-        """
-
-        document = []
-        if isinstance(input_src, str):
-            sentences = [sent.strip().split() for sent in input_src.strip().split('\n') if len(sent.strip()) > 0]
-        elif isinstance(input_src, list):
-            sentences = input_src
-        idx = 0
-        for sentence in sentences:
-            sent = []
-            for token_id, token in enumerate(sentence):
-                sent.append({doc.ID: (token_id + 1, ), doc.TEXT: token, doc.MISC: f'start_char={idx}|end_char={idx + len(token)}'})
-                idx += len(token) + 1
-            document.append(sent)
-        raw_text = ' '.join([' '.join(sentence) for sentence in sentences])
-        return raw_text, document
-
-    def process_pre_tokenized_conllu_text(self, input_src):
-        """
-        Pretokenized text in this case is provided in conllu format.
-        """
-
-        return CoNLL.conll2dict(input_str=input_src, generate_raw_text=True)
-
-    def process(self, document):
-        assert isinstance(document, str) or (self.config.get('pretokenized') or self.config.get('no_ssplit', False)), \
-            "If neither 'pretokenized' or 'no_ssplit' option is enabled, the input to the TokenizerProcessor must be a string."
-
-        if self.config.get('pretokenized'):
-            if self.config.get('pretokenized') == 'conllu':
-                document, metadocument, raw_text = self.process_pre_tokenized_conllu_text(document)
-            else:
-                raw_text, document = self.process_pre_tokenized_text(document)
-                metadocument = None
-        elif hasattr(self, '_variant'):
-            return self._variant.process(document)
-        else:
-            raw_text, document, metadocument = self._tokenizer.tokenize(document)
-        # clean unnecessary attributes
-
-        pos_lemma_pretag = (not ('pos' in self.pipeline.processors and self.pipeline.processors['pos'].config['lemma_pretag']))
-        if pos_lemma_pretag or 'pos' not in self.pipeline.processors or 'lemma' not in self.pipeline.processors:
-            for sent in document:
-                for token in sent:
-                    if 'xpos' in token and ('pos' not in self.pipeline.processors or pos_lemma_pretag):
-                        del token['xpos']
-                        if 'upos' in token:
-                            del token['upos']
-                        if 'feats' in token:
-                            del token['feats']
-                    if 'lemma' in token and ('lemma' not in self.pipeline.processors or pos_lemma_pretag):
-                        del token['lemma']
-
-        return doc.Document(document, raw_text, metasentences=metadocument)
+"""
+Processor for performing tokenization
+"""
+
+import io
+import logging
+import os
+
+from classla.pipeline._constants import *
+from classla.pipeline.processor import UDProcessor, register_processor
+from classla.utils.conll import CoNLL
+from classla.utils.obeliks import ObeliksTrainer
+from classla.models.common import doc
+from classla.utils.reldi import ReldiTrainer
+
+logger = logging.getLogger('classla')
+
+# class for running the tokenizer
+@register_processor(name=TOKENIZE)
+class TokenizeProcessor(UDProcessor):
+
+    # set of processor requirements this processor fulfills
+    PROVIDES_DEFAULT = set([TOKENIZE])
+    # set of processor requirements for this processor
+    REQUIRES_DEFAULT = set([])
+    # default max sequence length
+    MAX_SEQ_LENGTH_DEFAULT = 1000
+
+    def _set_up_model(self, config, use_gpu):
+        # set up trainer
+        if os.path.basename(config['library']) == 'obeliks':
+            self._tokenizer = ObeliksTrainer(config.get('lang'), config.get('type'), 'pos' in [el[0] for el in self.pipeline.load_list])
+        elif os.path.basename(config['library']) == 'reldi':
+            self._tokenizer = ReldiTrainer(config.get('lang'), config.get('type'))
+        else:
+            raise Exception(f'Tokenizer {config["library"]} not available.')
+
+        if config.get('pretokenized'):
+            self._trainer = None
+
+    def process_pre_tokenized_text(self, input_src):
+        """
+        Pretokenized text can be provided in 2 manners:
+
+        1.) str, tokenized by whitespace, sentence split by newline
+        2.) list of token lists, each token list represents a sentence
+
+        generate dictionary data structure
+        """
+
+        document = []
+        if isinstance(input_src, str):
+            sentences = [sent.strip().split() for sent in input_src.strip().split('\n') if len(sent.strip()) > 0]
+        elif isinstance(input_src, list):
+            sentences = input_src
+        idx = 0
+        for sentence in sentences:
+            sent = []
+            for token_id, token in enumerate(sentence):
+                sent.append({doc.ID: (token_id + 1, ), doc.TEXT: token, doc.MISC: f'start_char={idx}|end_char={idx + len(token)}'})
+                idx += len(token) + 1
+            document.append(sent)
+        raw_text = ' '.join([' '.join(sentence) for sentence in sentences])
+        return raw_text, document
+
+    def process_pre_tokenized_conllu_text(self, input_src):
+        """
+        Pretokenized text in this case is provided in conllu format.
+        """
+
+        return CoNLL.conll2dict(input_str=input_src, generate_raw_text=True)
+
+    def process(self, document):
+        assert isinstance(document, str) or (self.config.get('pretokenized') or self.config.get('no_ssplit', False)), \
+            "If neither 'pretokenized' or 'no_ssplit' option is enabled, the input to the TokenizerProcessor must be a string."
+
+        if self.config.get('pretokenized'):
+            if self.config.get('pretokenized') == 'conllu':
+                document, metadocument, raw_text = self.process_pre_tokenized_conllu_text(document)
+            else:
+                raw_text, document = self.process_pre_tokenized_text(document)
+                metadocument = None
+        elif hasattr(self, '_variant'):
+            return self._variant.process(document)
+        else:
+            raw_text, document, metadocument = self._tokenizer.tokenize(document)
+        # clean unnecessary attributes
+
+        pos_lemma_pretag = (not ('pos' in self.pipeline.processors and self.pipeline.processors['pos'].config['lemma_pretag']))
+        if pos_lemma_pretag or 'pos' not in self.pipeline.processors or 'lemma' not in self.pipeline.processors:
+            for sent in document:
+                for token in sent:
+                    if 'xpos' in token and ('pos' not in self.pipeline.processors or pos_lemma_pretag):
+                        del token['xpos']
+                        if 'upos' in token:
+                            del token['upos']
+                        if 'feats' in token:
+                            del token['feats']
+                    if 'lemma' in token and ('lemma' not in self.pipeline.processors or pos_lemma_pretag):
+                        del token['lemma']
+
+        return doc.Document(document, raw_text, metasentences=metadocument)
```

### Comparing `classla-2.0/classla/protobuf/CoreNLP_pb2.py` & `classla-2.1/classla/protobuf/CoreNLP_pb2.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,3630 +1,3630 @@
-# -*- coding: utf-8 -*-
-# Generated by the protocol buffer compiler.  DO NOT EDIT!
-# source: CoreNLP.proto
-
-from google.protobuf.internal import enum_type_wrapper
-from google.protobuf import descriptor as _descriptor
-from google.protobuf import message as _message
-from google.protobuf import reflection as _reflection
-from google.protobuf import symbol_database as _symbol_database
-# @@protoc_insertion_point(imports)
-
-_sym_db = _symbol_database.Default()
-
-
-
-
-DESCRIPTOR = _descriptor.FileDescriptor(
-  name='CoreNLP.proto',
-  package='edu.stanford.nlp.pipeline',
-  syntax='proto2',
-  serialized_options=b'\n\031edu.stanford.nlp.pipelineB\rCoreNLPProtos',
-  serialized_pb=b'\n\rCoreNLP.proto\x12\x19\x65\x64u.stanford.nlp.pipeline\"\xe1\x05\n\x08\x44ocument\x12\x0c\n\x04text\x18\x01 \x02(\t\x12\x35\n\x08sentence\x18\x02 \x03(\x0b\x32#.edu.stanford.nlp.pipeline.Sentence\x12\x39\n\ncorefChain\x18\x03 \x03(\x0b\x32%.edu.stanford.nlp.pipeline.CorefChain\x12\r\n\x05\x64ocID\x18\x04 \x01(\t\x12\x0f\n\x07\x64ocDate\x18\x07 \x01(\t\x12\x10\n\x08\x63\x61lendar\x18\x08 \x01(\x04\x12;\n\x11sentencelessToken\x18\x05 \x03(\x0b\x32 .edu.stanford.nlp.pipeline.Token\x12\x33\n\tcharacter\x18\n \x03(\x0b\x32 .edu.stanford.nlp.pipeline.Token\x12/\n\x05quote\x18\x06 \x03(\x0b\x32 .edu.stanford.nlp.pipeline.Quote\x12\x37\n\x08mentions\x18\t \x03(\x0b\x32%.edu.stanford.nlp.pipeline.NERMention\x12#\n\x1bhasEntityMentionsAnnotation\x18\r \x01(\x08\x12\x0e\n\x06xmlDoc\x18\x0b \x01(\x08\x12\x34\n\x08sections\x18\x0c \x03(\x0b\x32\".edu.stanford.nlp.pipeline.Section\x12<\n\x10mentionsForCoref\x18\x0e \x03(\x0b\x32\".edu.stanford.nlp.pipeline.Mention\x12!\n\x19hasCorefMentionAnnotation\x18\x0f \x01(\x08\x12\x1a\n\x12hasCorefAnnotation\x18\x10 \x01(\x08\x12+\n#corefMentionToEntityMentionMappings\x18\x11 \x03(\x05\x12+\n#entityMentionToCorefMentionMappings\x18\x12 \x03(\x05*\x05\x08\x64\x10\x80\x02\"\xcd\x0f\n\x08Sentence\x12/\n\x05token\x18\x01 \x03(\x0b\x32 .edu.stanford.nlp.pipeline.Token\x12\x18\n\x10tokenOffsetBegin\x18\x02 \x02(\r\x12\x16\n\x0etokenOffsetEnd\x18\x03 \x02(\r\x12\x15\n\rsentenceIndex\x18\x04 \x01(\r\x12\x1c\n\x14\x63haracterOffsetBegin\x18\x05 \x01(\r\x12\x1a\n\x12\x63haracterOffsetEnd\x18\x06 \x01(\r\x12\x37\n\tparseTree\x18\x07 \x01(\x0b\x32$.edu.stanford.nlp.pipeline.ParseTree\x12@\n\x12\x62inarizedParseTree\x18\x1f \x01(\x0b\x32$.edu.stanford.nlp.pipeline.ParseTree\x12@\n\x12\x61nnotatedParseTree\x18  \x01(\x0b\x32$.edu.stanford.nlp.pipeline.ParseTree\x12\x11\n\tsentiment\x18! \x01(\t\x12=\n\x0fkBestParseTrees\x18\" \x03(\x0b\x32$.edu.stanford.nlp.pipeline.ParseTree\x12\x45\n\x11\x62\x61sicDependencies\x18\x08 \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\x12I\n\x15\x63ollapsedDependencies\x18\t \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\x12T\n collapsedCCProcessedDependencies\x18\n \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\x12K\n\x17\x61lternativeDependencies\x18\r \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\x12?\n\x0copenieTriple\x18\x0e \x03(\x0b\x32).edu.stanford.nlp.pipeline.RelationTriple\x12<\n\tkbpTriple\x18\x10 \x03(\x0b\x32).edu.stanford.nlp.pipeline.RelationTriple\x12\x45\n\x10\x65ntailedSentence\x18\x0f \x03(\x0b\x32+.edu.stanford.nlp.pipeline.SentenceFragment\x12\x43\n\x0e\x65ntailedClause\x18# \x03(\x0b\x32+.edu.stanford.nlp.pipeline.SentenceFragment\x12H\n\x14\x65nhancedDependencies\x18\x11 \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\x12P\n\x1c\x65nhancedPlusPlusDependencies\x18\x12 \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\x12\x33\n\tcharacter\x18\x13 \x03(\x0b\x32 .edu.stanford.nlp.pipeline.Token\x12\x11\n\tparagraph\x18\x0b \x01(\r\x12\x0c\n\x04text\x18\x0c \x01(\t\x12\x12\n\nlineNumber\x18\x14 \x01(\r\x12\x1e\n\x16hasRelationAnnotations\x18\x33 \x01(\x08\x12\x31\n\x06\x65ntity\x18\x34 \x03(\x0b\x32!.edu.stanford.nlp.pipeline.Entity\x12\x35\n\x08relation\x18\x35 \x03(\x0b\x32#.edu.stanford.nlp.pipeline.Relation\x12$\n\x1chasNumerizedTokensAnnotation\x18\x36 \x01(\x08\x12\x37\n\x08mentions\x18\x37 \x03(\x0b\x32%.edu.stanford.nlp.pipeline.NERMention\x12<\n\x10mentionsForCoref\x18\x38 \x03(\x0b\x32\".edu.stanford.nlp.pipeline.Mention\x12\"\n\x1ahasCorefMentionsAnnotation\x18\x39 \x01(\x08\x12\x12\n\nsentenceID\x18: \x01(\t\x12\x13\n\x0bsectionDate\x18; \x01(\t\x12\x14\n\x0csectionIndex\x18< \x01(\r\x12\x13\n\x0bsectionName\x18= \x01(\t\x12\x15\n\rsectionAuthor\x18> \x01(\t\x12\r\n\x05\x64ocID\x18? \x01(\t\x12\x15\n\rsectionQuoted\x18@ \x01(\x08\x12#\n\x1bhasEntityMentionsAnnotation\x18\x41 \x01(\x08\x12\x1f\n\x17hasKBPTriplesAnnotation\x18\x44 \x01(\x08\x12\"\n\x1ahasOpenieTriplesAnnotation\x18\x45 \x01(\x08\x12\x14\n\x0c\x63hapterIndex\x18\x42 \x01(\r\x12\x16\n\x0eparagraphIndex\x18\x43 \x01(\r\x12=\n\x10\x65nhancedSentence\x18\x46 \x01(\x0b\x32#.edu.stanford.nlp.pipeline.Sentence*\x05\x08\x64\x10\x80\x02\"\xad\x0c\n\x05Token\x12\x0c\n\x04word\x18\x01 \x01(\t\x12\x0b\n\x03pos\x18\x02 \x01(\t\x12\r\n\x05value\x18\x03 \x01(\t\x12\x10\n\x08\x63\x61tegory\x18\x04 \x01(\t\x12\x0e\n\x06\x62\x65\x66ore\x18\x05 \x01(\t\x12\r\n\x05\x61\x66ter\x18\x06 \x01(\t\x12\x14\n\x0coriginalText\x18\x07 \x01(\t\x12\x0b\n\x03ner\x18\x08 \x01(\t\x12\x11\n\tcoarseNER\x18> \x01(\t\x12\x16\n\x0e\x66ineGrainedNER\x18? \x01(\t\x12\x15\n\rnerLabelProbs\x18\x42 \x03(\t\x12\x15\n\rnormalizedNER\x18\t \x01(\t\x12\r\n\x05lemma\x18\n \x01(\t\x12\x11\n\tbeginChar\x18\x0b \x01(\r\x12\x0f\n\x07\x65ndChar\x18\x0c \x01(\r\x12\x11\n\tutterance\x18\r \x01(\r\x12\x0f\n\x07speaker\x18\x0e \x01(\t\x12\x12\n\nbeginIndex\x18\x0f \x01(\r\x12\x10\n\x08\x65ndIndex\x18\x10 \x01(\r\x12\x17\n\x0ftokenBeginIndex\x18\x11 \x01(\r\x12\x15\n\rtokenEndIndex\x18\x12 \x01(\r\x12\x34\n\ntimexValue\x18\x13 \x01(\x0b\x32 .edu.stanford.nlp.pipeline.Timex\x12\x15\n\rhasXmlContext\x18\x15 \x01(\x08\x12\x12\n\nxmlContext\x18\x16 \x03(\t\x12\x16\n\x0e\x63orefClusterID\x18\x17 \x01(\r\x12\x0e\n\x06\x61nswer\x18\x18 \x01(\t\x12\x15\n\rheadWordIndex\x18\x1a \x01(\r\x12\x35\n\x08operator\x18\x1b \x01(\x0b\x32#.edu.stanford.nlp.pipeline.Operator\x12\x35\n\x08polarity\x18\x1c \x01(\x0b\x32#.edu.stanford.nlp.pipeline.Polarity\x12\x14\n\x0cpolarity_dir\x18\' \x01(\t\x12-\n\x04span\x18\x1d \x01(\x0b\x32\x1f.edu.stanford.nlp.pipeline.Span\x12\x11\n\tsentiment\x18\x1e \x01(\t\x12\x16\n\x0equotationIndex\x18\x1f \x01(\x05\x12\x42\n\x0e\x63onllUFeatures\x18  \x01(\x0b\x32*.edu.stanford.nlp.pipeline.MapStringString\x12\x11\n\tcoarseTag\x18! \x01(\t\x12\x38\n\x0f\x63onllUTokenSpan\x18\" \x01(\x0b\x32\x1f.edu.stanford.nlp.pipeline.Span\x12\x12\n\nconllUMisc\x18# \x01(\t\x12G\n\x13\x63onllUSecondaryDeps\x18$ \x01(\x0b\x32*.edu.stanford.nlp.pipeline.MapStringString\x12\x17\n\x0fwikipediaEntity\x18% \x01(\t\x12\x11\n\tisNewline\x18& \x01(\x08\x12\x0e\n\x06gender\x18\x33 \x01(\t\x12\x10\n\x08trueCase\x18\x34 \x01(\t\x12\x14\n\x0ctrueCaseText\x18\x35 \x01(\t\x12\x13\n\x0b\x63hineseChar\x18\x36 \x01(\t\x12\x12\n\nchineseSeg\x18\x37 \x01(\t\x12\x16\n\x0e\x63hineseXMLChar\x18< \x01(\t\x12\x11\n\tarabicSeg\x18L \x01(\t\x12\x13\n\x0bsectionName\x18\x38 \x01(\t\x12\x15\n\rsectionAuthor\x18\x39 \x01(\t\x12\x13\n\x0bsectionDate\x18: \x01(\t\x12\x17\n\x0fsectionEndLabel\x18; \x01(\t\x12\x0e\n\x06parent\x18= \x01(\t\x12\x19\n\x11\x63orefMentionIndex\x18@ \x03(\r\x12\x1a\n\x12\x65ntityMentionIndex\x18\x41 \x01(\r\x12\r\n\x05isMWT\x18\x43 \x01(\x08\x12\x12\n\nisFirstMWT\x18\x44 \x01(\x08\x12\x0f\n\x07mwtText\x18\x45 \x01(\t\x12\x14\n\x0cnumericValue\x18\x46 \x01(\x04\x12\x13\n\x0bnumericType\x18G \x01(\t\x12\x1d\n\x15numericCompositeValue\x18H \x01(\x04\x12\x1c\n\x14numericCompositeType\x18I \x01(\t\x12\x1c\n\x14\x63odepointOffsetBegin\x18J \x01(\r\x12\x1a\n\x12\x63odepointOffsetEnd\x18K \x01(\r*\x05\x08\x64\x10\x80\x02\"\xe4\x03\n\x05Quote\x12\x0c\n\x04text\x18\x01 \x01(\t\x12\r\n\x05\x62\x65gin\x18\x02 \x01(\r\x12\x0b\n\x03\x65nd\x18\x03 \x01(\r\x12\x15\n\rsentenceBegin\x18\x05 \x01(\r\x12\x13\n\x0bsentenceEnd\x18\x06 \x01(\r\x12\x12\n\ntokenBegin\x18\x07 \x01(\r\x12\x10\n\x08tokenEnd\x18\x08 \x01(\r\x12\r\n\x05\x64ocid\x18\t \x01(\t\x12\r\n\x05index\x18\n \x01(\r\x12\x0e\n\x06\x61uthor\x18\x0b \x01(\t\x12\x0f\n\x07mention\x18\x0c \x01(\t\x12\x14\n\x0cmentionBegin\x18\r \x01(\r\x12\x12\n\nmentionEnd\x18\x0e \x01(\r\x12\x13\n\x0bmentionType\x18\x0f \x01(\t\x12\x14\n\x0cmentionSieve\x18\x10 \x01(\t\x12\x0f\n\x07speaker\x18\x11 \x01(\t\x12\x14\n\x0cspeakerSieve\x18\x12 \x01(\t\x12\x18\n\x10\x63\x61nonicalMention\x18\x13 \x01(\t\x12\x1d\n\x15\x63\x61nonicalMentionBegin\x18\x14 \x01(\r\x12\x1b\n\x13\x63\x61nonicalMentionEnd\x18\x15 \x01(\r\x12N\n\x1a\x61ttributionDependencyGraph\x18\x16 \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\"\xc7\x01\n\tParseTree\x12\x33\n\x05\x63hild\x18\x01 \x03(\x0b\x32$.edu.stanford.nlp.pipeline.ParseTree\x12\r\n\x05value\x18\x02 \x01(\t\x12\x17\n\x0fyieldBeginIndex\x18\x03 \x01(\r\x12\x15\n\ryieldEndIndex\x18\x04 \x01(\r\x12\r\n\x05score\x18\x05 \x01(\x01\x12\x37\n\tsentiment\x18\x06 \x01(\x0e\x32$.edu.stanford.nlp.pipeline.Sentiment\"\x96\x03\n\x0f\x44\x65pendencyGraph\x12=\n\x04node\x18\x01 \x03(\x0b\x32/.edu.stanford.nlp.pipeline.DependencyGraph.Node\x12=\n\x04\x65\x64ge\x18\x02 \x03(\x0b\x32/.edu.stanford.nlp.pipeline.DependencyGraph.Edge\x12\x10\n\x04root\x18\x03 \x03(\rB\x02\x10\x01\x1a\x44\n\x04Node\x12\x15\n\rsentenceIndex\x18\x01 \x02(\r\x12\r\n\x05index\x18\x02 \x02(\r\x12\x16\n\x0e\x63opyAnnotation\x18\x03 \x01(\r\x1a\xac\x01\n\x04\x45\x64ge\x12\x0e\n\x06source\x18\x01 \x02(\r\x12\x0e\n\x06target\x18\x02 \x02(\r\x12\x0b\n\x03\x64\x65p\x18\x03 \x01(\t\x12\x0f\n\x07isExtra\x18\x04 \x01(\x08\x12\x12\n\nsourceCopy\x18\x05 \x01(\r\x12\x12\n\ntargetCopy\x18\x06 \x01(\r\x12>\n\x08language\x18\x07 \x01(\x0e\x32#.edu.stanford.nlp.pipeline.Language:\x07Unknown\"\xc6\x02\n\nCorefChain\x12\x0f\n\x07\x63hainID\x18\x01 \x02(\x05\x12\x43\n\x07mention\x18\x02 \x03(\x0b\x32\x32.edu.stanford.nlp.pipeline.CorefChain.CorefMention\x12\x16\n\x0erepresentative\x18\x03 \x02(\r\x1a\xc9\x01\n\x0c\x43orefMention\x12\x11\n\tmentionID\x18\x01 \x01(\x05\x12\x13\n\x0bmentionType\x18\x02 \x01(\t\x12\x0e\n\x06number\x18\x03 \x01(\t\x12\x0e\n\x06gender\x18\x04 \x01(\t\x12\x0f\n\x07\x61nimacy\x18\x05 \x01(\t\x12\x12\n\nbeginIndex\x18\x06 \x01(\r\x12\x10\n\x08\x65ndIndex\x18\x07 \x01(\r\x12\x11\n\theadIndex\x18\t \x01(\r\x12\x15\n\rsentenceIndex\x18\n \x01(\r\x12\x10\n\x08position\x18\x0b \x01(\r\"\xef\x08\n\x07Mention\x12\x11\n\tmentionID\x18\x01 \x01(\x05\x12\x13\n\x0bmentionType\x18\x02 \x01(\t\x12\x0e\n\x06number\x18\x03 \x01(\t\x12\x0e\n\x06gender\x18\x04 \x01(\t\x12\x0f\n\x07\x61nimacy\x18\x05 \x01(\t\x12\x0e\n\x06person\x18\x06 \x01(\t\x12\x12\n\nstartIndex\x18\x07 \x01(\r\x12\x10\n\x08\x65ndIndex\x18\t \x01(\r\x12\x11\n\theadIndex\x18\n \x01(\x05\x12\x12\n\nheadString\x18\x0b \x01(\t\x12\x11\n\tnerString\x18\x0c \x01(\t\x12\x13\n\x0boriginalRef\x18\r \x01(\x05\x12\x1a\n\x12goldCorefClusterID\x18\x0e \x01(\x05\x12\x16\n\x0e\x63orefClusterID\x18\x0f \x01(\x05\x12\x12\n\nmentionNum\x18\x10 \x01(\x05\x12\x0f\n\x07sentNum\x18\x11 \x01(\x05\x12\r\n\x05utter\x18\x12 \x01(\x05\x12\x11\n\tparagraph\x18\x13 \x01(\x05\x12\x11\n\tisSubject\x18\x14 \x01(\x08\x12\x16\n\x0eisDirectObject\x18\x15 \x01(\x08\x12\x18\n\x10isIndirectObject\x18\x16 \x01(\x08\x12\x1b\n\x13isPrepositionObject\x18\x17 \x01(\x08\x12\x0f\n\x07hasTwin\x18\x18 \x01(\x08\x12\x0f\n\x07generic\x18\x19 \x01(\x08\x12\x13\n\x0bisSingleton\x18\x1a \x01(\x08\x12\x1a\n\x12hasBasicDependency\x18\x1b \x01(\x08\x12\x1d\n\x15hasEnhancedDepenedncy\x18\x1c \x01(\x08\x12\x1b\n\x13hasContextParseTree\x18\x1d \x01(\x08\x12?\n\x0fheadIndexedWord\x18\x1e \x01(\x0b\x32&.edu.stanford.nlp.pipeline.IndexedWord\x12=\n\rdependingVerb\x18\x1f \x01(\x0b\x32&.edu.stanford.nlp.pipeline.IndexedWord\x12\x38\n\x08headWord\x18  \x01(\x0b\x32&.edu.stanford.nlp.pipeline.IndexedWord\x12;\n\x0bspeakerInfo\x18! \x01(\x0b\x32&.edu.stanford.nlp.pipeline.SpeakerInfo\x12=\n\rsentenceWords\x18\x32 \x03(\x0b\x32&.edu.stanford.nlp.pipeline.IndexedWord\x12<\n\x0coriginalSpan\x18\x33 \x03(\x0b\x32&.edu.stanford.nlp.pipeline.IndexedWord\x12\x12\n\ndependents\x18\x34 \x03(\t\x12\x19\n\x11preprocessedTerms\x18\x35 \x03(\t\x12\x13\n\x0b\x61ppositions\x18\x36 \x03(\x05\x12\x1c\n\x14predicateNominatives\x18\x37 \x03(\x05\x12\x18\n\x10relativePronouns\x18\x38 \x03(\x05\x12\x13\n\x0blistMembers\x18\x39 \x03(\x05\x12\x15\n\rbelongToLists\x18: \x03(\x05\"X\n\x0bIndexedWord\x12\x13\n\x0bsentenceNum\x18\x01 \x01(\x05\x12\x12\n\ntokenIndex\x18\x02 \x01(\x05\x12\r\n\x05\x64ocID\x18\x03 \x01(\x05\x12\x11\n\tcopyCount\x18\x04 \x01(\r\"4\n\x0bSpeakerInfo\x12\x13\n\x0bspeakerName\x18\x01 \x01(\t\x12\x10\n\x08mentions\x18\x02 \x03(\x05\"\"\n\x04Span\x12\r\n\x05\x62\x65gin\x18\x01 \x02(\r\x12\x0b\n\x03\x65nd\x18\x02 \x02(\r\"w\n\x05Timex\x12\r\n\x05value\x18\x01 \x01(\t\x12\x10\n\x08\x61ltValue\x18\x02 \x01(\t\x12\x0c\n\x04text\x18\x03 \x01(\t\x12\x0c\n\x04type\x18\x04 \x01(\t\x12\x0b\n\x03tid\x18\x05 \x01(\t\x12\x12\n\nbeginPoint\x18\x06 \x01(\r\x12\x10\n\x08\x65ndPoint\x18\x07 \x01(\r\"\xdb\x01\n\x06\x45ntity\x12\x11\n\theadStart\x18\x06 \x01(\r\x12\x0f\n\x07headEnd\x18\x07 \x01(\r\x12\x13\n\x0bmentionType\x18\x08 \x01(\t\x12\x16\n\x0enormalizedName\x18\t \x01(\t\x12\x16\n\x0eheadTokenIndex\x18\n \x01(\r\x12\x0f\n\x07\x63orefID\x18\x0b \x01(\t\x12\x10\n\x08objectID\x18\x01 \x01(\t\x12\x13\n\x0b\x65xtentStart\x18\x02 \x01(\r\x12\x11\n\textentEnd\x18\x03 \x01(\r\x12\x0c\n\x04type\x18\x04 \x01(\t\x12\x0f\n\x07subtype\x18\x05 \x01(\t\"\xb7\x01\n\x08Relation\x12\x0f\n\x07\x61rgName\x18\x06 \x03(\t\x12.\n\x03\x61rg\x18\x07 \x03(\x0b\x32!.edu.stanford.nlp.pipeline.Entity\x12\x11\n\tsignature\x18\x08 \x01(\t\x12\x10\n\x08objectID\x18\x01 \x01(\t\x12\x13\n\x0b\x65xtentStart\x18\x02 \x01(\r\x12\x11\n\textentEnd\x18\x03 \x01(\r\x12\x0c\n\x04type\x18\x04 \x01(\t\x12\x0f\n\x07subtype\x18\x05 \x01(\t\"\xb2\x01\n\x08Operator\x12\x0c\n\x04name\x18\x01 \x02(\t\x12\x1b\n\x13quantifierSpanBegin\x18\x02 \x02(\x05\x12\x19\n\x11quantifierSpanEnd\x18\x03 \x02(\x05\x12\x18\n\x10subjectSpanBegin\x18\x04 \x02(\x05\x12\x16\n\x0esubjectSpanEnd\x18\x05 \x02(\x05\x12\x17\n\x0fobjectSpanBegin\x18\x06 \x02(\x05\x12\x15\n\robjectSpanEnd\x18\x07 \x02(\x05\"\xa9\x04\n\x08Polarity\x12K\n\x12projectEquivalence\x18\x01 \x02(\x0e\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\x12Q\n\x18projectForwardEntailment\x18\x02 \x02(\x0e\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\x12Q\n\x18projectReverseEntailment\x18\x03 \x02(\x0e\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\x12H\n\x0fprojectNegation\x18\x04 \x02(\x0e\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\x12K\n\x12projectAlternation\x18\x05 \x02(\x0e\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\x12\x45\n\x0cprojectCover\x18\x06 \x02(\x0e\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\x12L\n\x13projectIndependence\x18\x07 \x02(\x0e\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\"\xdd\x02\n\nNERMention\x12\x15\n\rsentenceIndex\x18\x01 \x01(\r\x12%\n\x1dtokenStartInSentenceInclusive\x18\x02 \x02(\r\x12#\n\x1btokenEndInSentenceExclusive\x18\x03 \x02(\r\x12\x0b\n\x03ner\x18\x04 \x02(\t\x12\x15\n\rnormalizedNER\x18\x05 \x01(\t\x12\x12\n\nentityType\x18\x06 \x01(\t\x12/\n\x05timex\x18\x07 \x01(\x0b\x32 .edu.stanford.nlp.pipeline.Timex\x12\x17\n\x0fwikipediaEntity\x18\x08 \x01(\t\x12\x0e\n\x06gender\x18\t \x01(\t\x12\x1a\n\x12\x65ntityMentionIndex\x18\n \x01(\r\x12#\n\x1b\x63\x61nonicalEntityMentionIndex\x18\x0b \x01(\r\x12\x19\n\x11\x65ntityMentionText\x18\x0c \x01(\t\"Y\n\x10SentenceFragment\x12\x12\n\ntokenIndex\x18\x01 \x03(\r\x12\x0c\n\x04root\x18\x02 \x01(\r\x12\x14\n\x0c\x61ssumedTruth\x18\x03 \x01(\x08\x12\r\n\x05score\x18\x04 \x01(\x01\":\n\rTokenLocation\x12\x15\n\rsentenceIndex\x18\x01 \x01(\r\x12\x12\n\ntokenIndex\x18\x02 \x01(\r\"\x9a\x03\n\x0eRelationTriple\x12\x0f\n\x07subject\x18\x01 \x01(\t\x12\x10\n\x08relation\x18\x02 \x01(\t\x12\x0e\n\x06object\x18\x03 \x01(\t\x12\x12\n\nconfidence\x18\x04 \x01(\x01\x12?\n\rsubjectTokens\x18\r \x03(\x0b\x32(.edu.stanford.nlp.pipeline.TokenLocation\x12@\n\x0erelationTokens\x18\x0e \x03(\x0b\x32(.edu.stanford.nlp.pipeline.TokenLocation\x12>\n\x0cobjectTokens\x18\x0f \x03(\x0b\x32(.edu.stanford.nlp.pipeline.TokenLocation\x12\x38\n\x04tree\x18\x08 \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\x12\x0e\n\x06istmod\x18\t \x01(\x08\x12\x10\n\x08prefixBe\x18\n \x01(\x08\x12\x10\n\x08suffixBe\x18\x0b \x01(\x08\x12\x10\n\x08suffixOf\x18\x0c \x01(\x08\"-\n\x0fMapStringString\x12\x0b\n\x03key\x18\x01 \x03(\t\x12\r\n\x05value\x18\x02 \x03(\t\"*\n\x0cMapIntString\x12\x0b\n\x03key\x18\x01 \x03(\r\x12\r\n\x05value\x18\x02 \x03(\t\"\xfc\x01\n\x07Section\x12\x11\n\tcharBegin\x18\x01 \x02(\r\x12\x0f\n\x07\x63harEnd\x18\x02 \x02(\r\x12\x0e\n\x06\x61uthor\x18\x03 \x01(\t\x12\x17\n\x0fsentenceIndexes\x18\x04 \x03(\r\x12\x10\n\x08\x64\x61tetime\x18\x05 \x01(\t\x12\x30\n\x06quotes\x18\x06 \x03(\x0b\x32 .edu.stanford.nlp.pipeline.Quote\x12\x17\n\x0f\x61uthorCharBegin\x18\x07 \x01(\r\x12\x15\n\rauthorCharEnd\x18\x08 \x01(\r\x12\x30\n\x06xmlTag\x18\t \x02(\x0b\x32 .edu.stanford.nlp.pipeline.Token\"\xe4\x01\n\x0eSemgrexRequest\x12\x0f\n\x07semgrex\x18\x01 \x03(\t\x12\x45\n\x05query\x18\x02 \x03(\x0b\x32\x36.edu.stanford.nlp.pipeline.SemgrexRequest.Dependencies\x1az\n\x0c\x44\x65pendencies\x12/\n\x05token\x18\x01 \x03(\x0b\x32 .edu.stanford.nlp.pipeline.Token\x12\x39\n\x05graph\x18\x02 \x02(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\"\x80\x04\n\x0fSemgrexResponse\x12\x46\n\x06result\x18\x01 \x03(\x0b\x32\x36.edu.stanford.nlp.pipeline.SemgrexResponse.GraphResult\x1a(\n\tNamedNode\x12\x0c\n\x04name\x18\x01 \x02(\t\x12\r\n\x05index\x18\x02 \x02(\x05\x1a+\n\rNamedRelation\x12\x0c\n\x04name\x18\x01 \x02(\t\x12\x0c\n\x04reln\x18\x02 \x02(\t\x1a\xa2\x01\n\x05Match\x12\r\n\x05index\x18\x01 \x02(\x05\x12\x42\n\x04node\x18\x02 \x03(\x0b\x32\x34.edu.stanford.nlp.pipeline.SemgrexResponse.NamedNode\x12\x46\n\x04reln\x18\x03 \x03(\x0b\x32\x38.edu.stanford.nlp.pipeline.SemgrexResponse.NamedRelation\x1aP\n\rSemgrexResult\x12?\n\x05match\x18\x01 \x03(\x0b\x32\x30.edu.stanford.nlp.pipeline.SemgrexResponse.Match\x1aW\n\x0bGraphResult\x12H\n\x06result\x18\x01 \x03(\x0b\x32\x38.edu.stanford.nlp.pipeline.SemgrexResponse.SemgrexResult*\xa3\x01\n\x08Language\x12\x0b\n\x07Unknown\x10\x00\x12\x07\n\x03\x41ny\x10\x01\x12\n\n\x06\x41rabic\x10\x02\x12\x0b\n\x07\x43hinese\x10\x03\x12\x0b\n\x07\x45nglish\x10\x04\x12\n\n\x06German\x10\x05\x12\n\n\x06\x46rench\x10\x06\x12\n\n\x06Hebrew\x10\x07\x12\x0b\n\x07Spanish\x10\x08\x12\x14\n\x10UniversalEnglish\x10\t\x12\x14\n\x10UniversalChinese\x10\n*h\n\tSentiment\x12\x13\n\x0fSTRONG_NEGATIVE\x10\x00\x12\x11\n\rWEAK_NEGATIVE\x10\x01\x12\x0b\n\x07NEUTRAL\x10\x02\x12\x11\n\rWEAK_POSITIVE\x10\x03\x12\x13\n\x0fSTRONG_POSITIVE\x10\x04*\x93\x01\n\x14NaturalLogicRelation\x12\x0f\n\x0b\x45QUIVALENCE\x10\x00\x12\x16\n\x12\x46ORWARD_ENTAILMENT\x10\x01\x12\x16\n\x12REVERSE_ENTAILMENT\x10\x02\x12\x0c\n\x08NEGATION\x10\x03\x12\x0f\n\x0b\x41LTERNATION\x10\x04\x12\t\n\x05\x43OVER\x10\x05\x12\x10\n\x0cINDEPENDENCE\x10\x06\x42*\n\x19\x65\x64u.stanford.nlp.pipelineB\rCoreNLPProtos'
-)
-
-_LANGUAGE = _descriptor.EnumDescriptor(
-  name='Language',
-  full_name='edu.stanford.nlp.pipeline.Language',
-  filename=None,
-  file=DESCRIPTOR,
-  values=[
-    _descriptor.EnumValueDescriptor(
-      name='Unknown', index=0, number=0,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='Any', index=1, number=1,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='Arabic', index=2, number=2,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='Chinese', index=3, number=3,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='English', index=4, number=4,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='German', index=5, number=5,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='French', index=6, number=6,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='Hebrew', index=7, number=7,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='Spanish', index=8, number=8,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='UniversalEnglish', index=9, number=9,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='UniversalChinese', index=10, number=10,
-      serialized_options=None,
-      type=None),
-  ],
-  containing_type=None,
-  serialized_options=None,
-  serialized_start=10388,
-  serialized_end=10551,
-)
-_sym_db.RegisterEnumDescriptor(_LANGUAGE)
-
-Language = enum_type_wrapper.EnumTypeWrapper(_LANGUAGE)
-_SENTIMENT = _descriptor.EnumDescriptor(
-  name='Sentiment',
-  full_name='edu.stanford.nlp.pipeline.Sentiment',
-  filename=None,
-  file=DESCRIPTOR,
-  values=[
-    _descriptor.EnumValueDescriptor(
-      name='STRONG_NEGATIVE', index=0, number=0,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='WEAK_NEGATIVE', index=1, number=1,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='NEUTRAL', index=2, number=2,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='WEAK_POSITIVE', index=3, number=3,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='STRONG_POSITIVE', index=4, number=4,
-      serialized_options=None,
-      type=None),
-  ],
-  containing_type=None,
-  serialized_options=None,
-  serialized_start=10553,
-  serialized_end=10657,
-)
-_sym_db.RegisterEnumDescriptor(_SENTIMENT)
-
-Sentiment = enum_type_wrapper.EnumTypeWrapper(_SENTIMENT)
-_NATURALLOGICRELATION = _descriptor.EnumDescriptor(
-  name='NaturalLogicRelation',
-  full_name='edu.stanford.nlp.pipeline.NaturalLogicRelation',
-  filename=None,
-  file=DESCRIPTOR,
-  values=[
-    _descriptor.EnumValueDescriptor(
-      name='EQUIVALENCE', index=0, number=0,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='FORWARD_ENTAILMENT', index=1, number=1,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='REVERSE_ENTAILMENT', index=2, number=2,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='NEGATION', index=3, number=3,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='ALTERNATION', index=4, number=4,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='COVER', index=5, number=5,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='INDEPENDENCE', index=6, number=6,
-      serialized_options=None,
-      type=None),
-  ],
-  containing_type=None,
-  serialized_options=None,
-  serialized_start=10660,
-  serialized_end=10807,
-)
-_sym_db.RegisterEnumDescriptor(_NATURALLOGICRELATION)
-
-NaturalLogicRelation = enum_type_wrapper.EnumTypeWrapper(_NATURALLOGICRELATION)
-Unknown = 0
-Any = 1
-Arabic = 2
-Chinese = 3
-English = 4
-German = 5
-French = 6
-Hebrew = 7
-Spanish = 8
-UniversalEnglish = 9
-UniversalChinese = 10
-STRONG_NEGATIVE = 0
-WEAK_NEGATIVE = 1
-NEUTRAL = 2
-WEAK_POSITIVE = 3
-STRONG_POSITIVE = 4
-EQUIVALENCE = 0
-FORWARD_ENTAILMENT = 1
-REVERSE_ENTAILMENT = 2
-NEGATION = 3
-ALTERNATION = 4
-COVER = 5
-INDEPENDENCE = 6
-
-
-
-_DOCUMENT = _descriptor.Descriptor(
-  name='Document',
-  full_name='edu.stanford.nlp.pipeline.Document',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='text', full_name='edu.stanford.nlp.pipeline.Document.text', index=0,
-      number=1, type=9, cpp_type=9, label=2,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sentence', full_name='edu.stanford.nlp.pipeline.Document.sentence', index=1,
-      number=2, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='corefChain', full_name='edu.stanford.nlp.pipeline.Document.corefChain', index=2,
-      number=3, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='docID', full_name='edu.stanford.nlp.pipeline.Document.docID', index=3,
-      number=4, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='docDate', full_name='edu.stanford.nlp.pipeline.Document.docDate', index=4,
-      number=7, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='calendar', full_name='edu.stanford.nlp.pipeline.Document.calendar', index=5,
-      number=8, type=4, cpp_type=4, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sentencelessToken', full_name='edu.stanford.nlp.pipeline.Document.sentencelessToken', index=6,
-      number=5, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='character', full_name='edu.stanford.nlp.pipeline.Document.character', index=7,
-      number=10, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='quote', full_name='edu.stanford.nlp.pipeline.Document.quote', index=8,
-      number=6, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mentions', full_name='edu.stanford.nlp.pipeline.Document.mentions', index=9,
-      number=9, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='hasEntityMentionsAnnotation', full_name='edu.stanford.nlp.pipeline.Document.hasEntityMentionsAnnotation', index=10,
-      number=13, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='xmlDoc', full_name='edu.stanford.nlp.pipeline.Document.xmlDoc', index=11,
-      number=11, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sections', full_name='edu.stanford.nlp.pipeline.Document.sections', index=12,
-      number=12, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mentionsForCoref', full_name='edu.stanford.nlp.pipeline.Document.mentionsForCoref', index=13,
-      number=14, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='hasCorefMentionAnnotation', full_name='edu.stanford.nlp.pipeline.Document.hasCorefMentionAnnotation', index=14,
-      number=15, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='hasCorefAnnotation', full_name='edu.stanford.nlp.pipeline.Document.hasCorefAnnotation', index=15,
-      number=16, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='corefMentionToEntityMentionMappings', full_name='edu.stanford.nlp.pipeline.Document.corefMentionToEntityMentionMappings', index=16,
-      number=17, type=5, cpp_type=1, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='entityMentionToCorefMentionMappings', full_name='edu.stanford.nlp.pipeline.Document.entityMentionToCorefMentionMappings', index=17,
-      number=18, type=5, cpp_type=1, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=True,
-  syntax='proto2',
-  extension_ranges=[(100, 256), ],
-  oneofs=[
-  ],
-  serialized_start=45,
-  serialized_end=782,
-)
-
-
-_SENTENCE = _descriptor.Descriptor(
-  name='Sentence',
-  full_name='edu.stanford.nlp.pipeline.Sentence',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='token', full_name='edu.stanford.nlp.pipeline.Sentence.token', index=0,
-      number=1, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='tokenOffsetBegin', full_name='edu.stanford.nlp.pipeline.Sentence.tokenOffsetBegin', index=1,
-      number=2, type=13, cpp_type=3, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='tokenOffsetEnd', full_name='edu.stanford.nlp.pipeline.Sentence.tokenOffsetEnd', index=2,
-      number=3, type=13, cpp_type=3, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sentenceIndex', full_name='edu.stanford.nlp.pipeline.Sentence.sentenceIndex', index=3,
-      number=4, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='characterOffsetBegin', full_name='edu.stanford.nlp.pipeline.Sentence.characterOffsetBegin', index=4,
-      number=5, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='characterOffsetEnd', full_name='edu.stanford.nlp.pipeline.Sentence.characterOffsetEnd', index=5,
-      number=6, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='parseTree', full_name='edu.stanford.nlp.pipeline.Sentence.parseTree', index=6,
-      number=7, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='binarizedParseTree', full_name='edu.stanford.nlp.pipeline.Sentence.binarizedParseTree', index=7,
-      number=31, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='annotatedParseTree', full_name='edu.stanford.nlp.pipeline.Sentence.annotatedParseTree', index=8,
-      number=32, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sentiment', full_name='edu.stanford.nlp.pipeline.Sentence.sentiment', index=9,
-      number=33, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='kBestParseTrees', full_name='edu.stanford.nlp.pipeline.Sentence.kBestParseTrees', index=10,
-      number=34, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='basicDependencies', full_name='edu.stanford.nlp.pipeline.Sentence.basicDependencies', index=11,
-      number=8, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='collapsedDependencies', full_name='edu.stanford.nlp.pipeline.Sentence.collapsedDependencies', index=12,
-      number=9, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='collapsedCCProcessedDependencies', full_name='edu.stanford.nlp.pipeline.Sentence.collapsedCCProcessedDependencies', index=13,
-      number=10, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='alternativeDependencies', full_name='edu.stanford.nlp.pipeline.Sentence.alternativeDependencies', index=14,
-      number=13, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='openieTriple', full_name='edu.stanford.nlp.pipeline.Sentence.openieTriple', index=15,
-      number=14, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='kbpTriple', full_name='edu.stanford.nlp.pipeline.Sentence.kbpTriple', index=16,
-      number=16, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='entailedSentence', full_name='edu.stanford.nlp.pipeline.Sentence.entailedSentence', index=17,
-      number=15, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='entailedClause', full_name='edu.stanford.nlp.pipeline.Sentence.entailedClause', index=18,
-      number=35, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='enhancedDependencies', full_name='edu.stanford.nlp.pipeline.Sentence.enhancedDependencies', index=19,
-      number=17, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='enhancedPlusPlusDependencies', full_name='edu.stanford.nlp.pipeline.Sentence.enhancedPlusPlusDependencies', index=20,
-      number=18, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='character', full_name='edu.stanford.nlp.pipeline.Sentence.character', index=21,
-      number=19, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='paragraph', full_name='edu.stanford.nlp.pipeline.Sentence.paragraph', index=22,
-      number=11, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='text', full_name='edu.stanford.nlp.pipeline.Sentence.text', index=23,
-      number=12, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='lineNumber', full_name='edu.stanford.nlp.pipeline.Sentence.lineNumber', index=24,
-      number=20, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='hasRelationAnnotations', full_name='edu.stanford.nlp.pipeline.Sentence.hasRelationAnnotations', index=25,
-      number=51, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='entity', full_name='edu.stanford.nlp.pipeline.Sentence.entity', index=26,
-      number=52, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='relation', full_name='edu.stanford.nlp.pipeline.Sentence.relation', index=27,
-      number=53, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='hasNumerizedTokensAnnotation', full_name='edu.stanford.nlp.pipeline.Sentence.hasNumerizedTokensAnnotation', index=28,
-      number=54, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mentions', full_name='edu.stanford.nlp.pipeline.Sentence.mentions', index=29,
-      number=55, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mentionsForCoref', full_name='edu.stanford.nlp.pipeline.Sentence.mentionsForCoref', index=30,
-      number=56, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='hasCorefMentionsAnnotation', full_name='edu.stanford.nlp.pipeline.Sentence.hasCorefMentionsAnnotation', index=31,
-      number=57, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sentenceID', full_name='edu.stanford.nlp.pipeline.Sentence.sentenceID', index=32,
-      number=58, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sectionDate', full_name='edu.stanford.nlp.pipeline.Sentence.sectionDate', index=33,
-      number=59, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sectionIndex', full_name='edu.stanford.nlp.pipeline.Sentence.sectionIndex', index=34,
-      number=60, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sectionName', full_name='edu.stanford.nlp.pipeline.Sentence.sectionName', index=35,
-      number=61, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sectionAuthor', full_name='edu.stanford.nlp.pipeline.Sentence.sectionAuthor', index=36,
-      number=62, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='docID', full_name='edu.stanford.nlp.pipeline.Sentence.docID', index=37,
-      number=63, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sectionQuoted', full_name='edu.stanford.nlp.pipeline.Sentence.sectionQuoted', index=38,
-      number=64, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='hasEntityMentionsAnnotation', full_name='edu.stanford.nlp.pipeline.Sentence.hasEntityMentionsAnnotation', index=39,
-      number=65, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='hasKBPTriplesAnnotation', full_name='edu.stanford.nlp.pipeline.Sentence.hasKBPTriplesAnnotation', index=40,
-      number=68, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='hasOpenieTriplesAnnotation', full_name='edu.stanford.nlp.pipeline.Sentence.hasOpenieTriplesAnnotation', index=41,
-      number=69, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='chapterIndex', full_name='edu.stanford.nlp.pipeline.Sentence.chapterIndex', index=42,
-      number=66, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='paragraphIndex', full_name='edu.stanford.nlp.pipeline.Sentence.paragraphIndex', index=43,
-      number=67, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='enhancedSentence', full_name='edu.stanford.nlp.pipeline.Sentence.enhancedSentence', index=44,
-      number=70, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=True,
-  syntax='proto2',
-  extension_ranges=[(100, 256), ],
-  oneofs=[
-  ],
-  serialized_start=785,
-  serialized_end=2782,
-)
-
-
-_TOKEN = _descriptor.Descriptor(
-  name='Token',
-  full_name='edu.stanford.nlp.pipeline.Token',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='word', full_name='edu.stanford.nlp.pipeline.Token.word', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='pos', full_name='edu.stanford.nlp.pipeline.Token.pos', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='value', full_name='edu.stanford.nlp.pipeline.Token.value', index=2,
-      number=3, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='category', full_name='edu.stanford.nlp.pipeline.Token.category', index=3,
-      number=4, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='before', full_name='edu.stanford.nlp.pipeline.Token.before', index=4,
-      number=5, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='after', full_name='edu.stanford.nlp.pipeline.Token.after', index=5,
-      number=6, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='originalText', full_name='edu.stanford.nlp.pipeline.Token.originalText', index=6,
-      number=7, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='ner', full_name='edu.stanford.nlp.pipeline.Token.ner', index=7,
-      number=8, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='coarseNER', full_name='edu.stanford.nlp.pipeline.Token.coarseNER', index=8,
-      number=62, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='fineGrainedNER', full_name='edu.stanford.nlp.pipeline.Token.fineGrainedNER', index=9,
-      number=63, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='nerLabelProbs', full_name='edu.stanford.nlp.pipeline.Token.nerLabelProbs', index=10,
-      number=66, type=9, cpp_type=9, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='normalizedNER', full_name='edu.stanford.nlp.pipeline.Token.normalizedNER', index=11,
-      number=9, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='lemma', full_name='edu.stanford.nlp.pipeline.Token.lemma', index=12,
-      number=10, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='beginChar', full_name='edu.stanford.nlp.pipeline.Token.beginChar', index=13,
-      number=11, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='endChar', full_name='edu.stanford.nlp.pipeline.Token.endChar', index=14,
-      number=12, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='utterance', full_name='edu.stanford.nlp.pipeline.Token.utterance', index=15,
-      number=13, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='speaker', full_name='edu.stanford.nlp.pipeline.Token.speaker', index=16,
-      number=14, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='beginIndex', full_name='edu.stanford.nlp.pipeline.Token.beginIndex', index=17,
-      number=15, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='endIndex', full_name='edu.stanford.nlp.pipeline.Token.endIndex', index=18,
-      number=16, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='tokenBeginIndex', full_name='edu.stanford.nlp.pipeline.Token.tokenBeginIndex', index=19,
-      number=17, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='tokenEndIndex', full_name='edu.stanford.nlp.pipeline.Token.tokenEndIndex', index=20,
-      number=18, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='timexValue', full_name='edu.stanford.nlp.pipeline.Token.timexValue', index=21,
-      number=19, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='hasXmlContext', full_name='edu.stanford.nlp.pipeline.Token.hasXmlContext', index=22,
-      number=21, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='xmlContext', full_name='edu.stanford.nlp.pipeline.Token.xmlContext', index=23,
-      number=22, type=9, cpp_type=9, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='corefClusterID', full_name='edu.stanford.nlp.pipeline.Token.corefClusterID', index=24,
-      number=23, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='answer', full_name='edu.stanford.nlp.pipeline.Token.answer', index=25,
-      number=24, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='headWordIndex', full_name='edu.stanford.nlp.pipeline.Token.headWordIndex', index=26,
-      number=26, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='operator', full_name='edu.stanford.nlp.pipeline.Token.operator', index=27,
-      number=27, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='polarity', full_name='edu.stanford.nlp.pipeline.Token.polarity', index=28,
-      number=28, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='polarity_dir', full_name='edu.stanford.nlp.pipeline.Token.polarity_dir', index=29,
-      number=39, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='span', full_name='edu.stanford.nlp.pipeline.Token.span', index=30,
-      number=29, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sentiment', full_name='edu.stanford.nlp.pipeline.Token.sentiment', index=31,
-      number=30, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='quotationIndex', full_name='edu.stanford.nlp.pipeline.Token.quotationIndex', index=32,
-      number=31, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='conllUFeatures', full_name='edu.stanford.nlp.pipeline.Token.conllUFeatures', index=33,
-      number=32, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='coarseTag', full_name='edu.stanford.nlp.pipeline.Token.coarseTag', index=34,
-      number=33, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='conllUTokenSpan', full_name='edu.stanford.nlp.pipeline.Token.conllUTokenSpan', index=35,
-      number=34, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='conllUMisc', full_name='edu.stanford.nlp.pipeline.Token.conllUMisc', index=36,
-      number=35, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='conllUSecondaryDeps', full_name='edu.stanford.nlp.pipeline.Token.conllUSecondaryDeps', index=37,
-      number=36, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='wikipediaEntity', full_name='edu.stanford.nlp.pipeline.Token.wikipediaEntity', index=38,
-      number=37, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='isNewline', full_name='edu.stanford.nlp.pipeline.Token.isNewline', index=39,
-      number=38, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='gender', full_name='edu.stanford.nlp.pipeline.Token.gender', index=40,
-      number=51, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='trueCase', full_name='edu.stanford.nlp.pipeline.Token.trueCase', index=41,
-      number=52, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='trueCaseText', full_name='edu.stanford.nlp.pipeline.Token.trueCaseText', index=42,
-      number=53, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='chineseChar', full_name='edu.stanford.nlp.pipeline.Token.chineseChar', index=43,
-      number=54, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='chineseSeg', full_name='edu.stanford.nlp.pipeline.Token.chineseSeg', index=44,
-      number=55, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='chineseXMLChar', full_name='edu.stanford.nlp.pipeline.Token.chineseXMLChar', index=45,
-      number=60, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='arabicSeg', full_name='edu.stanford.nlp.pipeline.Token.arabicSeg', index=46,
-      number=76, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sectionName', full_name='edu.stanford.nlp.pipeline.Token.sectionName', index=47,
-      number=56, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sectionAuthor', full_name='edu.stanford.nlp.pipeline.Token.sectionAuthor', index=48,
-      number=57, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sectionDate', full_name='edu.stanford.nlp.pipeline.Token.sectionDate', index=49,
-      number=58, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sectionEndLabel', full_name='edu.stanford.nlp.pipeline.Token.sectionEndLabel', index=50,
-      number=59, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='parent', full_name='edu.stanford.nlp.pipeline.Token.parent', index=51,
-      number=61, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='corefMentionIndex', full_name='edu.stanford.nlp.pipeline.Token.corefMentionIndex', index=52,
-      number=64, type=13, cpp_type=3, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='entityMentionIndex', full_name='edu.stanford.nlp.pipeline.Token.entityMentionIndex', index=53,
-      number=65, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='isMWT', full_name='edu.stanford.nlp.pipeline.Token.isMWT', index=54,
-      number=67, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='isFirstMWT', full_name='edu.stanford.nlp.pipeline.Token.isFirstMWT', index=55,
-      number=68, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mwtText', full_name='edu.stanford.nlp.pipeline.Token.mwtText', index=56,
-      number=69, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='numericValue', full_name='edu.stanford.nlp.pipeline.Token.numericValue', index=57,
-      number=70, type=4, cpp_type=4, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='numericType', full_name='edu.stanford.nlp.pipeline.Token.numericType', index=58,
-      number=71, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='numericCompositeValue', full_name='edu.stanford.nlp.pipeline.Token.numericCompositeValue', index=59,
-      number=72, type=4, cpp_type=4, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='numericCompositeType', full_name='edu.stanford.nlp.pipeline.Token.numericCompositeType', index=60,
-      number=73, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='codepointOffsetBegin', full_name='edu.stanford.nlp.pipeline.Token.codepointOffsetBegin', index=61,
-      number=74, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='codepointOffsetEnd', full_name='edu.stanford.nlp.pipeline.Token.codepointOffsetEnd', index=62,
-      number=75, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=True,
-  syntax='proto2',
-  extension_ranges=[(100, 256), ],
-  oneofs=[
-  ],
-  serialized_start=2785,
-  serialized_end=4366,
-)
-
-
-_QUOTE = _descriptor.Descriptor(
-  name='Quote',
-  full_name='edu.stanford.nlp.pipeline.Quote',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='text', full_name='edu.stanford.nlp.pipeline.Quote.text', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='begin', full_name='edu.stanford.nlp.pipeline.Quote.begin', index=1,
-      number=2, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='end', full_name='edu.stanford.nlp.pipeline.Quote.end', index=2,
-      number=3, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sentenceBegin', full_name='edu.stanford.nlp.pipeline.Quote.sentenceBegin', index=3,
-      number=5, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sentenceEnd', full_name='edu.stanford.nlp.pipeline.Quote.sentenceEnd', index=4,
-      number=6, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='tokenBegin', full_name='edu.stanford.nlp.pipeline.Quote.tokenBegin', index=5,
-      number=7, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='tokenEnd', full_name='edu.stanford.nlp.pipeline.Quote.tokenEnd', index=6,
-      number=8, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='docid', full_name='edu.stanford.nlp.pipeline.Quote.docid', index=7,
-      number=9, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='index', full_name='edu.stanford.nlp.pipeline.Quote.index', index=8,
-      number=10, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='author', full_name='edu.stanford.nlp.pipeline.Quote.author', index=9,
-      number=11, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mention', full_name='edu.stanford.nlp.pipeline.Quote.mention', index=10,
-      number=12, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mentionBegin', full_name='edu.stanford.nlp.pipeline.Quote.mentionBegin', index=11,
-      number=13, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mentionEnd', full_name='edu.stanford.nlp.pipeline.Quote.mentionEnd', index=12,
-      number=14, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mentionType', full_name='edu.stanford.nlp.pipeline.Quote.mentionType', index=13,
-      number=15, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mentionSieve', full_name='edu.stanford.nlp.pipeline.Quote.mentionSieve', index=14,
-      number=16, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='speaker', full_name='edu.stanford.nlp.pipeline.Quote.speaker', index=15,
-      number=17, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='speakerSieve', full_name='edu.stanford.nlp.pipeline.Quote.speakerSieve', index=16,
-      number=18, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='canonicalMention', full_name='edu.stanford.nlp.pipeline.Quote.canonicalMention', index=17,
-      number=19, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='canonicalMentionBegin', full_name='edu.stanford.nlp.pipeline.Quote.canonicalMentionBegin', index=18,
-      number=20, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='canonicalMentionEnd', full_name='edu.stanford.nlp.pipeline.Quote.canonicalMentionEnd', index=19,
-      number=21, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='attributionDependencyGraph', full_name='edu.stanford.nlp.pipeline.Quote.attributionDependencyGraph', index=20,
-      number=22, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=4369,
-  serialized_end=4853,
-)
-
-
-_PARSETREE = _descriptor.Descriptor(
-  name='ParseTree',
-  full_name='edu.stanford.nlp.pipeline.ParseTree',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='child', full_name='edu.stanford.nlp.pipeline.ParseTree.child', index=0,
-      number=1, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='value', full_name='edu.stanford.nlp.pipeline.ParseTree.value', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='yieldBeginIndex', full_name='edu.stanford.nlp.pipeline.ParseTree.yieldBeginIndex', index=2,
-      number=3, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='yieldEndIndex', full_name='edu.stanford.nlp.pipeline.ParseTree.yieldEndIndex', index=3,
-      number=4, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='score', full_name='edu.stanford.nlp.pipeline.ParseTree.score', index=4,
-      number=5, type=1, cpp_type=5, label=1,
-      has_default_value=False, default_value=float(0),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sentiment', full_name='edu.stanford.nlp.pipeline.ParseTree.sentiment', index=5,
-      number=6, type=14, cpp_type=8, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=4856,
-  serialized_end=5055,
-)
-
-
-_DEPENDENCYGRAPH_NODE = _descriptor.Descriptor(
-  name='Node',
-  full_name='edu.stanford.nlp.pipeline.DependencyGraph.Node',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='sentenceIndex', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Node.sentenceIndex', index=0,
-      number=1, type=13, cpp_type=3, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='index', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Node.index', index=1,
-      number=2, type=13, cpp_type=3, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='copyAnnotation', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Node.copyAnnotation', index=2,
-      number=3, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=5221,
-  serialized_end=5289,
-)
-
-_DEPENDENCYGRAPH_EDGE = _descriptor.Descriptor(
-  name='Edge',
-  full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='source', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge.source', index=0,
-      number=1, type=13, cpp_type=3, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='target', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge.target', index=1,
-      number=2, type=13, cpp_type=3, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='dep', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge.dep', index=2,
-      number=3, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='isExtra', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge.isExtra', index=3,
-      number=4, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sourceCopy', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge.sourceCopy', index=4,
-      number=5, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='targetCopy', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge.targetCopy', index=5,
-      number=6, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='language', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge.language', index=6,
-      number=7, type=14, cpp_type=8, label=1,
-      has_default_value=True, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=5292,
-  serialized_end=5464,
-)
-
-_DEPENDENCYGRAPH = _descriptor.Descriptor(
-  name='DependencyGraph',
-  full_name='edu.stanford.nlp.pipeline.DependencyGraph',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='node', full_name='edu.stanford.nlp.pipeline.DependencyGraph.node', index=0,
-      number=1, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='edge', full_name='edu.stanford.nlp.pipeline.DependencyGraph.edge', index=1,
-      number=2, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='root', full_name='edu.stanford.nlp.pipeline.DependencyGraph.root', index=2,
-      number=3, type=13, cpp_type=3, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=b'\020\001', file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[_DEPENDENCYGRAPH_NODE, _DEPENDENCYGRAPH_EDGE, ],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=5058,
-  serialized_end=5464,
-)
-
-
-_COREFCHAIN_COREFMENTION = _descriptor.Descriptor(
-  name='CorefMention',
-  full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='mentionID', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.mentionID', index=0,
-      number=1, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mentionType', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.mentionType', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='number', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.number', index=2,
-      number=3, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='gender', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.gender', index=3,
-      number=4, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='animacy', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.animacy', index=4,
-      number=5, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='beginIndex', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.beginIndex', index=5,
-      number=6, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='endIndex', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.endIndex', index=6,
-      number=7, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='headIndex', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.headIndex', index=7,
-      number=9, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sentenceIndex', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.sentenceIndex', index=8,
-      number=10, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='position', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.position', index=9,
-      number=11, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=5592,
-  serialized_end=5793,
-)
-
-_COREFCHAIN = _descriptor.Descriptor(
-  name='CorefChain',
-  full_name='edu.stanford.nlp.pipeline.CorefChain',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='chainID', full_name='edu.stanford.nlp.pipeline.CorefChain.chainID', index=0,
-      number=1, type=5, cpp_type=1, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mention', full_name='edu.stanford.nlp.pipeline.CorefChain.mention', index=1,
-      number=2, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='representative', full_name='edu.stanford.nlp.pipeline.CorefChain.representative', index=2,
-      number=3, type=13, cpp_type=3, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[_COREFCHAIN_COREFMENTION, ],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=5467,
-  serialized_end=5793,
-)
-
-
-_MENTION = _descriptor.Descriptor(
-  name='Mention',
-  full_name='edu.stanford.nlp.pipeline.Mention',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='mentionID', full_name='edu.stanford.nlp.pipeline.Mention.mentionID', index=0,
-      number=1, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mentionType', full_name='edu.stanford.nlp.pipeline.Mention.mentionType', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='number', full_name='edu.stanford.nlp.pipeline.Mention.number', index=2,
-      number=3, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='gender', full_name='edu.stanford.nlp.pipeline.Mention.gender', index=3,
-      number=4, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='animacy', full_name='edu.stanford.nlp.pipeline.Mention.animacy', index=4,
-      number=5, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='person', full_name='edu.stanford.nlp.pipeline.Mention.person', index=5,
-      number=6, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='startIndex', full_name='edu.stanford.nlp.pipeline.Mention.startIndex', index=6,
-      number=7, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='endIndex', full_name='edu.stanford.nlp.pipeline.Mention.endIndex', index=7,
-      number=9, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='headIndex', full_name='edu.stanford.nlp.pipeline.Mention.headIndex', index=8,
-      number=10, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='headString', full_name='edu.stanford.nlp.pipeline.Mention.headString', index=9,
-      number=11, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='nerString', full_name='edu.stanford.nlp.pipeline.Mention.nerString', index=10,
-      number=12, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='originalRef', full_name='edu.stanford.nlp.pipeline.Mention.originalRef', index=11,
-      number=13, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='goldCorefClusterID', full_name='edu.stanford.nlp.pipeline.Mention.goldCorefClusterID', index=12,
-      number=14, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='corefClusterID', full_name='edu.stanford.nlp.pipeline.Mention.corefClusterID', index=13,
-      number=15, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mentionNum', full_name='edu.stanford.nlp.pipeline.Mention.mentionNum', index=14,
-      number=16, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sentNum', full_name='edu.stanford.nlp.pipeline.Mention.sentNum', index=15,
-      number=17, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='utter', full_name='edu.stanford.nlp.pipeline.Mention.utter', index=16,
-      number=18, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='paragraph', full_name='edu.stanford.nlp.pipeline.Mention.paragraph', index=17,
-      number=19, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='isSubject', full_name='edu.stanford.nlp.pipeline.Mention.isSubject', index=18,
-      number=20, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='isDirectObject', full_name='edu.stanford.nlp.pipeline.Mention.isDirectObject', index=19,
-      number=21, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='isIndirectObject', full_name='edu.stanford.nlp.pipeline.Mention.isIndirectObject', index=20,
-      number=22, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='isPrepositionObject', full_name='edu.stanford.nlp.pipeline.Mention.isPrepositionObject', index=21,
-      number=23, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='hasTwin', full_name='edu.stanford.nlp.pipeline.Mention.hasTwin', index=22,
-      number=24, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='generic', full_name='edu.stanford.nlp.pipeline.Mention.generic', index=23,
-      number=25, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='isSingleton', full_name='edu.stanford.nlp.pipeline.Mention.isSingleton', index=24,
-      number=26, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='hasBasicDependency', full_name='edu.stanford.nlp.pipeline.Mention.hasBasicDependency', index=25,
-      number=27, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='hasEnhancedDepenedncy', full_name='edu.stanford.nlp.pipeline.Mention.hasEnhancedDepenedncy', index=26,
-      number=28, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='hasContextParseTree', full_name='edu.stanford.nlp.pipeline.Mention.hasContextParseTree', index=27,
-      number=29, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='headIndexedWord', full_name='edu.stanford.nlp.pipeline.Mention.headIndexedWord', index=28,
-      number=30, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='dependingVerb', full_name='edu.stanford.nlp.pipeline.Mention.dependingVerb', index=29,
-      number=31, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='headWord', full_name='edu.stanford.nlp.pipeline.Mention.headWord', index=30,
-      number=32, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='speakerInfo', full_name='edu.stanford.nlp.pipeline.Mention.speakerInfo', index=31,
-      number=33, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sentenceWords', full_name='edu.stanford.nlp.pipeline.Mention.sentenceWords', index=32,
-      number=50, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='originalSpan', full_name='edu.stanford.nlp.pipeline.Mention.originalSpan', index=33,
-      number=51, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='dependents', full_name='edu.stanford.nlp.pipeline.Mention.dependents', index=34,
-      number=52, type=9, cpp_type=9, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='preprocessedTerms', full_name='edu.stanford.nlp.pipeline.Mention.preprocessedTerms', index=35,
-      number=53, type=9, cpp_type=9, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='appositions', full_name='edu.stanford.nlp.pipeline.Mention.appositions', index=36,
-      number=54, type=5, cpp_type=1, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='predicateNominatives', full_name='edu.stanford.nlp.pipeline.Mention.predicateNominatives', index=37,
-      number=55, type=5, cpp_type=1, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='relativePronouns', full_name='edu.stanford.nlp.pipeline.Mention.relativePronouns', index=38,
-      number=56, type=5, cpp_type=1, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='listMembers', full_name='edu.stanford.nlp.pipeline.Mention.listMembers', index=39,
-      number=57, type=5, cpp_type=1, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='belongToLists', full_name='edu.stanford.nlp.pipeline.Mention.belongToLists', index=40,
-      number=58, type=5, cpp_type=1, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=5796,
-  serialized_end=6931,
-)
-
-
-_INDEXEDWORD = _descriptor.Descriptor(
-  name='IndexedWord',
-  full_name='edu.stanford.nlp.pipeline.IndexedWord',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='sentenceNum', full_name='edu.stanford.nlp.pipeline.IndexedWord.sentenceNum', index=0,
-      number=1, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='tokenIndex', full_name='edu.stanford.nlp.pipeline.IndexedWord.tokenIndex', index=1,
-      number=2, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='docID', full_name='edu.stanford.nlp.pipeline.IndexedWord.docID', index=2,
-      number=3, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='copyCount', full_name='edu.stanford.nlp.pipeline.IndexedWord.copyCount', index=3,
-      number=4, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=6933,
-  serialized_end=7021,
-)
-
-
-_SPEAKERINFO = _descriptor.Descriptor(
-  name='SpeakerInfo',
-  full_name='edu.stanford.nlp.pipeline.SpeakerInfo',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='speakerName', full_name='edu.stanford.nlp.pipeline.SpeakerInfo.speakerName', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mentions', full_name='edu.stanford.nlp.pipeline.SpeakerInfo.mentions', index=1,
-      number=2, type=5, cpp_type=1, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=7023,
-  serialized_end=7075,
-)
-
-
-_SPAN = _descriptor.Descriptor(
-  name='Span',
-  full_name='edu.stanford.nlp.pipeline.Span',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='begin', full_name='edu.stanford.nlp.pipeline.Span.begin', index=0,
-      number=1, type=13, cpp_type=3, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='end', full_name='edu.stanford.nlp.pipeline.Span.end', index=1,
-      number=2, type=13, cpp_type=3, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=7077,
-  serialized_end=7111,
-)
-
-
-_TIMEX = _descriptor.Descriptor(
-  name='Timex',
-  full_name='edu.stanford.nlp.pipeline.Timex',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='value', full_name='edu.stanford.nlp.pipeline.Timex.value', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='altValue', full_name='edu.stanford.nlp.pipeline.Timex.altValue', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='text', full_name='edu.stanford.nlp.pipeline.Timex.text', index=2,
-      number=3, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='type', full_name='edu.stanford.nlp.pipeline.Timex.type', index=3,
-      number=4, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='tid', full_name='edu.stanford.nlp.pipeline.Timex.tid', index=4,
-      number=5, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='beginPoint', full_name='edu.stanford.nlp.pipeline.Timex.beginPoint', index=5,
-      number=6, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='endPoint', full_name='edu.stanford.nlp.pipeline.Timex.endPoint', index=6,
-      number=7, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=7113,
-  serialized_end=7232,
-)
-
-
-_ENTITY = _descriptor.Descriptor(
-  name='Entity',
-  full_name='edu.stanford.nlp.pipeline.Entity',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='headStart', full_name='edu.stanford.nlp.pipeline.Entity.headStart', index=0,
-      number=6, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='headEnd', full_name='edu.stanford.nlp.pipeline.Entity.headEnd', index=1,
-      number=7, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='mentionType', full_name='edu.stanford.nlp.pipeline.Entity.mentionType', index=2,
-      number=8, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='normalizedName', full_name='edu.stanford.nlp.pipeline.Entity.normalizedName', index=3,
-      number=9, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='headTokenIndex', full_name='edu.stanford.nlp.pipeline.Entity.headTokenIndex', index=4,
-      number=10, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='corefID', full_name='edu.stanford.nlp.pipeline.Entity.corefID', index=5,
-      number=11, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='objectID', full_name='edu.stanford.nlp.pipeline.Entity.objectID', index=6,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='extentStart', full_name='edu.stanford.nlp.pipeline.Entity.extentStart', index=7,
-      number=2, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='extentEnd', full_name='edu.stanford.nlp.pipeline.Entity.extentEnd', index=8,
-      number=3, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='type', full_name='edu.stanford.nlp.pipeline.Entity.type', index=9,
-      number=4, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='subtype', full_name='edu.stanford.nlp.pipeline.Entity.subtype', index=10,
-      number=5, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=7235,
-  serialized_end=7454,
-)
-
-
-_RELATION = _descriptor.Descriptor(
-  name='Relation',
-  full_name='edu.stanford.nlp.pipeline.Relation',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='argName', full_name='edu.stanford.nlp.pipeline.Relation.argName', index=0,
-      number=6, type=9, cpp_type=9, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='arg', full_name='edu.stanford.nlp.pipeline.Relation.arg', index=1,
-      number=7, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='signature', full_name='edu.stanford.nlp.pipeline.Relation.signature', index=2,
-      number=8, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='objectID', full_name='edu.stanford.nlp.pipeline.Relation.objectID', index=3,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='extentStart', full_name='edu.stanford.nlp.pipeline.Relation.extentStart', index=4,
-      number=2, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='extentEnd', full_name='edu.stanford.nlp.pipeline.Relation.extentEnd', index=5,
-      number=3, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='type', full_name='edu.stanford.nlp.pipeline.Relation.type', index=6,
-      number=4, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='subtype', full_name='edu.stanford.nlp.pipeline.Relation.subtype', index=7,
-      number=5, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=7457,
-  serialized_end=7640,
-)
-
-
-_OPERATOR = _descriptor.Descriptor(
-  name='Operator',
-  full_name='edu.stanford.nlp.pipeline.Operator',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='name', full_name='edu.stanford.nlp.pipeline.Operator.name', index=0,
-      number=1, type=9, cpp_type=9, label=2,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='quantifierSpanBegin', full_name='edu.stanford.nlp.pipeline.Operator.quantifierSpanBegin', index=1,
-      number=2, type=5, cpp_type=1, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='quantifierSpanEnd', full_name='edu.stanford.nlp.pipeline.Operator.quantifierSpanEnd', index=2,
-      number=3, type=5, cpp_type=1, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='subjectSpanBegin', full_name='edu.stanford.nlp.pipeline.Operator.subjectSpanBegin', index=3,
-      number=4, type=5, cpp_type=1, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='subjectSpanEnd', full_name='edu.stanford.nlp.pipeline.Operator.subjectSpanEnd', index=4,
-      number=5, type=5, cpp_type=1, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='objectSpanBegin', full_name='edu.stanford.nlp.pipeline.Operator.objectSpanBegin', index=5,
-      number=6, type=5, cpp_type=1, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='objectSpanEnd', full_name='edu.stanford.nlp.pipeline.Operator.objectSpanEnd', index=6,
-      number=7, type=5, cpp_type=1, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=7643,
-  serialized_end=7821,
-)
-
-
-_POLARITY = _descriptor.Descriptor(
-  name='Polarity',
-  full_name='edu.stanford.nlp.pipeline.Polarity',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='projectEquivalence', full_name='edu.stanford.nlp.pipeline.Polarity.projectEquivalence', index=0,
-      number=1, type=14, cpp_type=8, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='projectForwardEntailment', full_name='edu.stanford.nlp.pipeline.Polarity.projectForwardEntailment', index=1,
-      number=2, type=14, cpp_type=8, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='projectReverseEntailment', full_name='edu.stanford.nlp.pipeline.Polarity.projectReverseEntailment', index=2,
-      number=3, type=14, cpp_type=8, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='projectNegation', full_name='edu.stanford.nlp.pipeline.Polarity.projectNegation', index=3,
-      number=4, type=14, cpp_type=8, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='projectAlternation', full_name='edu.stanford.nlp.pipeline.Polarity.projectAlternation', index=4,
-      number=5, type=14, cpp_type=8, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='projectCover', full_name='edu.stanford.nlp.pipeline.Polarity.projectCover', index=5,
-      number=6, type=14, cpp_type=8, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='projectIndependence', full_name='edu.stanford.nlp.pipeline.Polarity.projectIndependence', index=6,
-      number=7, type=14, cpp_type=8, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=7824,
-  serialized_end=8377,
-)
-
-
-_NERMENTION = _descriptor.Descriptor(
-  name='NERMention',
-  full_name='edu.stanford.nlp.pipeline.NERMention',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='sentenceIndex', full_name='edu.stanford.nlp.pipeline.NERMention.sentenceIndex', index=0,
-      number=1, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='tokenStartInSentenceInclusive', full_name='edu.stanford.nlp.pipeline.NERMention.tokenStartInSentenceInclusive', index=1,
-      number=2, type=13, cpp_type=3, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='tokenEndInSentenceExclusive', full_name='edu.stanford.nlp.pipeline.NERMention.tokenEndInSentenceExclusive', index=2,
-      number=3, type=13, cpp_type=3, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='ner', full_name='edu.stanford.nlp.pipeline.NERMention.ner', index=3,
-      number=4, type=9, cpp_type=9, label=2,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='normalizedNER', full_name='edu.stanford.nlp.pipeline.NERMention.normalizedNER', index=4,
-      number=5, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='entityType', full_name='edu.stanford.nlp.pipeline.NERMention.entityType', index=5,
-      number=6, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='timex', full_name='edu.stanford.nlp.pipeline.NERMention.timex', index=6,
-      number=7, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='wikipediaEntity', full_name='edu.stanford.nlp.pipeline.NERMention.wikipediaEntity', index=7,
-      number=8, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='gender', full_name='edu.stanford.nlp.pipeline.NERMention.gender', index=8,
-      number=9, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='entityMentionIndex', full_name='edu.stanford.nlp.pipeline.NERMention.entityMentionIndex', index=9,
-      number=10, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='canonicalEntityMentionIndex', full_name='edu.stanford.nlp.pipeline.NERMention.canonicalEntityMentionIndex', index=10,
-      number=11, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='entityMentionText', full_name='edu.stanford.nlp.pipeline.NERMention.entityMentionText', index=11,
-      number=12, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=8380,
-  serialized_end=8729,
-)
-
-
-_SENTENCEFRAGMENT = _descriptor.Descriptor(
-  name='SentenceFragment',
-  full_name='edu.stanford.nlp.pipeline.SentenceFragment',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='tokenIndex', full_name='edu.stanford.nlp.pipeline.SentenceFragment.tokenIndex', index=0,
-      number=1, type=13, cpp_type=3, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='root', full_name='edu.stanford.nlp.pipeline.SentenceFragment.root', index=1,
-      number=2, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='assumedTruth', full_name='edu.stanford.nlp.pipeline.SentenceFragment.assumedTruth', index=2,
-      number=3, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='score', full_name='edu.stanford.nlp.pipeline.SentenceFragment.score', index=3,
-      number=4, type=1, cpp_type=5, label=1,
-      has_default_value=False, default_value=float(0),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=8731,
-  serialized_end=8820,
-)
-
-
-_TOKENLOCATION = _descriptor.Descriptor(
-  name='TokenLocation',
-  full_name='edu.stanford.nlp.pipeline.TokenLocation',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='sentenceIndex', full_name='edu.stanford.nlp.pipeline.TokenLocation.sentenceIndex', index=0,
-      number=1, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='tokenIndex', full_name='edu.stanford.nlp.pipeline.TokenLocation.tokenIndex', index=1,
-      number=2, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=8822,
-  serialized_end=8880,
-)
-
-
-_RELATIONTRIPLE = _descriptor.Descriptor(
-  name='RelationTriple',
-  full_name='edu.stanford.nlp.pipeline.RelationTriple',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='subject', full_name='edu.stanford.nlp.pipeline.RelationTriple.subject', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='relation', full_name='edu.stanford.nlp.pipeline.RelationTriple.relation', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='object', full_name='edu.stanford.nlp.pipeline.RelationTriple.object', index=2,
-      number=3, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='confidence', full_name='edu.stanford.nlp.pipeline.RelationTriple.confidence', index=3,
-      number=4, type=1, cpp_type=5, label=1,
-      has_default_value=False, default_value=float(0),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='subjectTokens', full_name='edu.stanford.nlp.pipeline.RelationTriple.subjectTokens', index=4,
-      number=13, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='relationTokens', full_name='edu.stanford.nlp.pipeline.RelationTriple.relationTokens', index=5,
-      number=14, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='objectTokens', full_name='edu.stanford.nlp.pipeline.RelationTriple.objectTokens', index=6,
-      number=15, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='tree', full_name='edu.stanford.nlp.pipeline.RelationTriple.tree', index=7,
-      number=8, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='istmod', full_name='edu.stanford.nlp.pipeline.RelationTriple.istmod', index=8,
-      number=9, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='prefixBe', full_name='edu.stanford.nlp.pipeline.RelationTriple.prefixBe', index=9,
-      number=10, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='suffixBe', full_name='edu.stanford.nlp.pipeline.RelationTriple.suffixBe', index=10,
-      number=11, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='suffixOf', full_name='edu.stanford.nlp.pipeline.RelationTriple.suffixOf', index=11,
-      number=12, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=8883,
-  serialized_end=9293,
-)
-
-
-_MAPSTRINGSTRING = _descriptor.Descriptor(
-  name='MapStringString',
-  full_name='edu.stanford.nlp.pipeline.MapStringString',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='key', full_name='edu.stanford.nlp.pipeline.MapStringString.key', index=0,
-      number=1, type=9, cpp_type=9, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='value', full_name='edu.stanford.nlp.pipeline.MapStringString.value', index=1,
-      number=2, type=9, cpp_type=9, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=9295,
-  serialized_end=9340,
-)
-
-
-_MAPINTSTRING = _descriptor.Descriptor(
-  name='MapIntString',
-  full_name='edu.stanford.nlp.pipeline.MapIntString',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='key', full_name='edu.stanford.nlp.pipeline.MapIntString.key', index=0,
-      number=1, type=13, cpp_type=3, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='value', full_name='edu.stanford.nlp.pipeline.MapIntString.value', index=1,
-      number=2, type=9, cpp_type=9, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=9342,
-  serialized_end=9384,
-)
-
-
-_SECTION = _descriptor.Descriptor(
-  name='Section',
-  full_name='edu.stanford.nlp.pipeline.Section',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='charBegin', full_name='edu.stanford.nlp.pipeline.Section.charBegin', index=0,
-      number=1, type=13, cpp_type=3, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='charEnd', full_name='edu.stanford.nlp.pipeline.Section.charEnd', index=1,
-      number=2, type=13, cpp_type=3, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='author', full_name='edu.stanford.nlp.pipeline.Section.author', index=2,
-      number=3, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sentenceIndexes', full_name='edu.stanford.nlp.pipeline.Section.sentenceIndexes', index=3,
-      number=4, type=13, cpp_type=3, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='datetime', full_name='edu.stanford.nlp.pipeline.Section.datetime', index=4,
-      number=5, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='quotes', full_name='edu.stanford.nlp.pipeline.Section.quotes', index=5,
-      number=6, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='authorCharBegin', full_name='edu.stanford.nlp.pipeline.Section.authorCharBegin', index=6,
-      number=7, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='authorCharEnd', full_name='edu.stanford.nlp.pipeline.Section.authorCharEnd', index=7,
-      number=8, type=13, cpp_type=3, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='xmlTag', full_name='edu.stanford.nlp.pipeline.Section.xmlTag', index=8,
-      number=9, type=11, cpp_type=10, label=2,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=9387,
-  serialized_end=9639,
-)
-
-
-_SEMGREXREQUEST_DEPENDENCIES = _descriptor.Descriptor(
-  name='Dependencies',
-  full_name='edu.stanford.nlp.pipeline.SemgrexRequest.Dependencies',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='token', full_name='edu.stanford.nlp.pipeline.SemgrexRequest.Dependencies.token', index=0,
-      number=1, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='graph', full_name='edu.stanford.nlp.pipeline.SemgrexRequest.Dependencies.graph', index=1,
-      number=2, type=11, cpp_type=10, label=2,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=9748,
-  serialized_end=9870,
-)
-
-_SEMGREXREQUEST = _descriptor.Descriptor(
-  name='SemgrexRequest',
-  full_name='edu.stanford.nlp.pipeline.SemgrexRequest',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='semgrex', full_name='edu.stanford.nlp.pipeline.SemgrexRequest.semgrex', index=0,
-      number=1, type=9, cpp_type=9, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='query', full_name='edu.stanford.nlp.pipeline.SemgrexRequest.query', index=1,
-      number=2, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[_SEMGREXREQUEST_DEPENDENCIES, ],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=9642,
-  serialized_end=9870,
-)
-
-
-_SEMGREXRESPONSE_NAMEDNODE = _descriptor.Descriptor(
-  name='NamedNode',
-  full_name='edu.stanford.nlp.pipeline.SemgrexResponse.NamedNode',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='name', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.NamedNode.name', index=0,
-      number=1, type=9, cpp_type=9, label=2,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='index', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.NamedNode.index', index=1,
-      number=2, type=5, cpp_type=1, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=9964,
-  serialized_end=10004,
-)
-
-_SEMGREXRESPONSE_NAMEDRELATION = _descriptor.Descriptor(
-  name='NamedRelation',
-  full_name='edu.stanford.nlp.pipeline.SemgrexResponse.NamedRelation',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='name', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.NamedRelation.name', index=0,
-      number=1, type=9, cpp_type=9, label=2,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='reln', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.NamedRelation.reln', index=1,
-      number=2, type=9, cpp_type=9, label=2,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=10006,
-  serialized_end=10049,
-)
-
-_SEMGREXRESPONSE_MATCH = _descriptor.Descriptor(
-  name='Match',
-  full_name='edu.stanford.nlp.pipeline.SemgrexResponse.Match',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='index', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.Match.index', index=0,
-      number=1, type=5, cpp_type=1, label=2,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='node', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.Match.node', index=1,
-      number=2, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='reln', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.Match.reln', index=2,
-      number=3, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=10052,
-  serialized_end=10214,
-)
-
-_SEMGREXRESPONSE_SEMGREXRESULT = _descriptor.Descriptor(
-  name='SemgrexResult',
-  full_name='edu.stanford.nlp.pipeline.SemgrexResponse.SemgrexResult',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='match', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.SemgrexResult.match', index=0,
-      number=1, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=10216,
-  serialized_end=10296,
-)
-
-_SEMGREXRESPONSE_GRAPHRESULT = _descriptor.Descriptor(
-  name='GraphResult',
-  full_name='edu.stanford.nlp.pipeline.SemgrexResponse.GraphResult',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='result', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.GraphResult.result', index=0,
-      number=1, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=10298,
-  serialized_end=10385,
-)
-
-_SEMGREXRESPONSE = _descriptor.Descriptor(
-  name='SemgrexResponse',
-  full_name='edu.stanford.nlp.pipeline.SemgrexResponse',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='result', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.result', index=0,
-      number=1, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[_SEMGREXRESPONSE_NAMEDNODE, _SEMGREXRESPONSE_NAMEDRELATION, _SEMGREXRESPONSE_MATCH, _SEMGREXRESPONSE_SEMGREXRESULT, _SEMGREXRESPONSE_GRAPHRESULT, ],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto2',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=9873,
-  serialized_end=10385,
-)
-
-_DOCUMENT.fields_by_name['sentence'].message_type = _SENTENCE
-_DOCUMENT.fields_by_name['corefChain'].message_type = _COREFCHAIN
-_DOCUMENT.fields_by_name['sentencelessToken'].message_type = _TOKEN
-_DOCUMENT.fields_by_name['character'].message_type = _TOKEN
-_DOCUMENT.fields_by_name['quote'].message_type = _QUOTE
-_DOCUMENT.fields_by_name['mentions'].message_type = _NERMENTION
-_DOCUMENT.fields_by_name['sections'].message_type = _SECTION
-_DOCUMENT.fields_by_name['mentionsForCoref'].message_type = _MENTION
-_SENTENCE.fields_by_name['token'].message_type = _TOKEN
-_SENTENCE.fields_by_name['parseTree'].message_type = _PARSETREE
-_SENTENCE.fields_by_name['binarizedParseTree'].message_type = _PARSETREE
-_SENTENCE.fields_by_name['annotatedParseTree'].message_type = _PARSETREE
-_SENTENCE.fields_by_name['kBestParseTrees'].message_type = _PARSETREE
-_SENTENCE.fields_by_name['basicDependencies'].message_type = _DEPENDENCYGRAPH
-_SENTENCE.fields_by_name['collapsedDependencies'].message_type = _DEPENDENCYGRAPH
-_SENTENCE.fields_by_name['collapsedCCProcessedDependencies'].message_type = _DEPENDENCYGRAPH
-_SENTENCE.fields_by_name['alternativeDependencies'].message_type = _DEPENDENCYGRAPH
-_SENTENCE.fields_by_name['openieTriple'].message_type = _RELATIONTRIPLE
-_SENTENCE.fields_by_name['kbpTriple'].message_type = _RELATIONTRIPLE
-_SENTENCE.fields_by_name['entailedSentence'].message_type = _SENTENCEFRAGMENT
-_SENTENCE.fields_by_name['entailedClause'].message_type = _SENTENCEFRAGMENT
-_SENTENCE.fields_by_name['enhancedDependencies'].message_type = _DEPENDENCYGRAPH
-_SENTENCE.fields_by_name['enhancedPlusPlusDependencies'].message_type = _DEPENDENCYGRAPH
-_SENTENCE.fields_by_name['character'].message_type = _TOKEN
-_SENTENCE.fields_by_name['entity'].message_type = _ENTITY
-_SENTENCE.fields_by_name['relation'].message_type = _RELATION
-_SENTENCE.fields_by_name['mentions'].message_type = _NERMENTION
-_SENTENCE.fields_by_name['mentionsForCoref'].message_type = _MENTION
-_SENTENCE.fields_by_name['enhancedSentence'].message_type = _SENTENCE
-_TOKEN.fields_by_name['timexValue'].message_type = _TIMEX
-_TOKEN.fields_by_name['operator'].message_type = _OPERATOR
-_TOKEN.fields_by_name['polarity'].message_type = _POLARITY
-_TOKEN.fields_by_name['span'].message_type = _SPAN
-_TOKEN.fields_by_name['conllUFeatures'].message_type = _MAPSTRINGSTRING
-_TOKEN.fields_by_name['conllUTokenSpan'].message_type = _SPAN
-_TOKEN.fields_by_name['conllUSecondaryDeps'].message_type = _MAPSTRINGSTRING
-_QUOTE.fields_by_name['attributionDependencyGraph'].message_type = _DEPENDENCYGRAPH
-_PARSETREE.fields_by_name['child'].message_type = _PARSETREE
-_PARSETREE.fields_by_name['sentiment'].enum_type = _SENTIMENT
-_DEPENDENCYGRAPH_NODE.containing_type = _DEPENDENCYGRAPH
-_DEPENDENCYGRAPH_EDGE.fields_by_name['language'].enum_type = _LANGUAGE
-_DEPENDENCYGRAPH_EDGE.containing_type = _DEPENDENCYGRAPH
-_DEPENDENCYGRAPH.fields_by_name['node'].message_type = _DEPENDENCYGRAPH_NODE
-_DEPENDENCYGRAPH.fields_by_name['edge'].message_type = _DEPENDENCYGRAPH_EDGE
-_COREFCHAIN_COREFMENTION.containing_type = _COREFCHAIN
-_COREFCHAIN.fields_by_name['mention'].message_type = _COREFCHAIN_COREFMENTION
-_MENTION.fields_by_name['headIndexedWord'].message_type = _INDEXEDWORD
-_MENTION.fields_by_name['dependingVerb'].message_type = _INDEXEDWORD
-_MENTION.fields_by_name['headWord'].message_type = _INDEXEDWORD
-_MENTION.fields_by_name['speakerInfo'].message_type = _SPEAKERINFO
-_MENTION.fields_by_name['sentenceWords'].message_type = _INDEXEDWORD
-_MENTION.fields_by_name['originalSpan'].message_type = _INDEXEDWORD
-_RELATION.fields_by_name['arg'].message_type = _ENTITY
-_POLARITY.fields_by_name['projectEquivalence'].enum_type = _NATURALLOGICRELATION
-_POLARITY.fields_by_name['projectForwardEntailment'].enum_type = _NATURALLOGICRELATION
-_POLARITY.fields_by_name['projectReverseEntailment'].enum_type = _NATURALLOGICRELATION
-_POLARITY.fields_by_name['projectNegation'].enum_type = _NATURALLOGICRELATION
-_POLARITY.fields_by_name['projectAlternation'].enum_type = _NATURALLOGICRELATION
-_POLARITY.fields_by_name['projectCover'].enum_type = _NATURALLOGICRELATION
-_POLARITY.fields_by_name['projectIndependence'].enum_type = _NATURALLOGICRELATION
-_NERMENTION.fields_by_name['timex'].message_type = _TIMEX
-_RELATIONTRIPLE.fields_by_name['subjectTokens'].message_type = _TOKENLOCATION
-_RELATIONTRIPLE.fields_by_name['relationTokens'].message_type = _TOKENLOCATION
-_RELATIONTRIPLE.fields_by_name['objectTokens'].message_type = _TOKENLOCATION
-_RELATIONTRIPLE.fields_by_name['tree'].message_type = _DEPENDENCYGRAPH
-_SECTION.fields_by_name['quotes'].message_type = _QUOTE
-_SECTION.fields_by_name['xmlTag'].message_type = _TOKEN
-_SEMGREXREQUEST_DEPENDENCIES.fields_by_name['token'].message_type = _TOKEN
-_SEMGREXREQUEST_DEPENDENCIES.fields_by_name['graph'].message_type = _DEPENDENCYGRAPH
-_SEMGREXREQUEST_DEPENDENCIES.containing_type = _SEMGREXREQUEST
-_SEMGREXREQUEST.fields_by_name['query'].message_type = _SEMGREXREQUEST_DEPENDENCIES
-_SEMGREXRESPONSE_NAMEDNODE.containing_type = _SEMGREXRESPONSE
-_SEMGREXRESPONSE_NAMEDRELATION.containing_type = _SEMGREXRESPONSE
-_SEMGREXRESPONSE_MATCH.fields_by_name['node'].message_type = _SEMGREXRESPONSE_NAMEDNODE
-_SEMGREXRESPONSE_MATCH.fields_by_name['reln'].message_type = _SEMGREXRESPONSE_NAMEDRELATION
-_SEMGREXRESPONSE_MATCH.containing_type = _SEMGREXRESPONSE
-_SEMGREXRESPONSE_SEMGREXRESULT.fields_by_name['match'].message_type = _SEMGREXRESPONSE_MATCH
-_SEMGREXRESPONSE_SEMGREXRESULT.containing_type = _SEMGREXRESPONSE
-_SEMGREXRESPONSE_GRAPHRESULT.fields_by_name['result'].message_type = _SEMGREXRESPONSE_SEMGREXRESULT
-_SEMGREXRESPONSE_GRAPHRESULT.containing_type = _SEMGREXRESPONSE
-_SEMGREXRESPONSE.fields_by_name['result'].message_type = _SEMGREXRESPONSE_GRAPHRESULT
-DESCRIPTOR.message_types_by_name['Document'] = _DOCUMENT
-DESCRIPTOR.message_types_by_name['Sentence'] = _SENTENCE
-DESCRIPTOR.message_types_by_name['Token'] = _TOKEN
-DESCRIPTOR.message_types_by_name['Quote'] = _QUOTE
-DESCRIPTOR.message_types_by_name['ParseTree'] = _PARSETREE
-DESCRIPTOR.message_types_by_name['DependencyGraph'] = _DEPENDENCYGRAPH
-DESCRIPTOR.message_types_by_name['CorefChain'] = _COREFCHAIN
-DESCRIPTOR.message_types_by_name['Mention'] = _MENTION
-DESCRIPTOR.message_types_by_name['IndexedWord'] = _INDEXEDWORD
-DESCRIPTOR.message_types_by_name['SpeakerInfo'] = _SPEAKERINFO
-DESCRIPTOR.message_types_by_name['Span'] = _SPAN
-DESCRIPTOR.message_types_by_name['Timex'] = _TIMEX
-DESCRIPTOR.message_types_by_name['Entity'] = _ENTITY
-DESCRIPTOR.message_types_by_name['Relation'] = _RELATION
-DESCRIPTOR.message_types_by_name['Operator'] = _OPERATOR
-DESCRIPTOR.message_types_by_name['Polarity'] = _POLARITY
-DESCRIPTOR.message_types_by_name['NERMention'] = _NERMENTION
-DESCRIPTOR.message_types_by_name['SentenceFragment'] = _SENTENCEFRAGMENT
-DESCRIPTOR.message_types_by_name['TokenLocation'] = _TOKENLOCATION
-DESCRIPTOR.message_types_by_name['RelationTriple'] = _RELATIONTRIPLE
-DESCRIPTOR.message_types_by_name['MapStringString'] = _MAPSTRINGSTRING
-DESCRIPTOR.message_types_by_name['MapIntString'] = _MAPINTSTRING
-DESCRIPTOR.message_types_by_name['Section'] = _SECTION
-DESCRIPTOR.message_types_by_name['SemgrexRequest'] = _SEMGREXREQUEST
-DESCRIPTOR.message_types_by_name['SemgrexResponse'] = _SEMGREXRESPONSE
-DESCRIPTOR.enum_types_by_name['Language'] = _LANGUAGE
-DESCRIPTOR.enum_types_by_name['Sentiment'] = _SENTIMENT
-DESCRIPTOR.enum_types_by_name['NaturalLogicRelation'] = _NATURALLOGICRELATION
-_sym_db.RegisterFileDescriptor(DESCRIPTOR)
-
-Document = _reflection.GeneratedProtocolMessageType('Document', (_message.Message,), {
-  'DESCRIPTOR' : _DOCUMENT,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Document)
-  })
-_sym_db.RegisterMessage(Document)
-
-Sentence = _reflection.GeneratedProtocolMessageType('Sentence', (_message.Message,), {
-  'DESCRIPTOR' : _SENTENCE,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Sentence)
-  })
-_sym_db.RegisterMessage(Sentence)
-
-Token = _reflection.GeneratedProtocolMessageType('Token', (_message.Message,), {
-  'DESCRIPTOR' : _TOKEN,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Token)
-  })
-_sym_db.RegisterMessage(Token)
-
-Quote = _reflection.GeneratedProtocolMessageType('Quote', (_message.Message,), {
-  'DESCRIPTOR' : _QUOTE,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Quote)
-  })
-_sym_db.RegisterMessage(Quote)
-
-ParseTree = _reflection.GeneratedProtocolMessageType('ParseTree', (_message.Message,), {
-  'DESCRIPTOR' : _PARSETREE,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.ParseTree)
-  })
-_sym_db.RegisterMessage(ParseTree)
-
-DependencyGraph = _reflection.GeneratedProtocolMessageType('DependencyGraph', (_message.Message,), {
-
-  'Node' : _reflection.GeneratedProtocolMessageType('Node', (_message.Message,), {
-    'DESCRIPTOR' : _DEPENDENCYGRAPH_NODE,
-    '__module__' : 'CoreNLP_pb2'
-    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.DependencyGraph.Node)
-    })
-  ,
-
-  'Edge' : _reflection.GeneratedProtocolMessageType('Edge', (_message.Message,), {
-    'DESCRIPTOR' : _DEPENDENCYGRAPH_EDGE,
-    '__module__' : 'CoreNLP_pb2'
-    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.DependencyGraph.Edge)
-    })
-  ,
-  'DESCRIPTOR' : _DEPENDENCYGRAPH,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.DependencyGraph)
-  })
-_sym_db.RegisterMessage(DependencyGraph)
-_sym_db.RegisterMessage(DependencyGraph.Node)
-_sym_db.RegisterMessage(DependencyGraph.Edge)
-
-CorefChain = _reflection.GeneratedProtocolMessageType('CorefChain', (_message.Message,), {
-
-  'CorefMention' : _reflection.GeneratedProtocolMessageType('CorefMention', (_message.Message,), {
-    'DESCRIPTOR' : _COREFCHAIN_COREFMENTION,
-    '__module__' : 'CoreNLP_pb2'
-    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.CorefChain.CorefMention)
-    })
-  ,
-  'DESCRIPTOR' : _COREFCHAIN,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.CorefChain)
-  })
-_sym_db.RegisterMessage(CorefChain)
-_sym_db.RegisterMessage(CorefChain.CorefMention)
-
-Mention = _reflection.GeneratedProtocolMessageType('Mention', (_message.Message,), {
-  'DESCRIPTOR' : _MENTION,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Mention)
-  })
-_sym_db.RegisterMessage(Mention)
-
-IndexedWord = _reflection.GeneratedProtocolMessageType('IndexedWord', (_message.Message,), {
-  'DESCRIPTOR' : _INDEXEDWORD,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.IndexedWord)
-  })
-_sym_db.RegisterMessage(IndexedWord)
-
-SpeakerInfo = _reflection.GeneratedProtocolMessageType('SpeakerInfo', (_message.Message,), {
-  'DESCRIPTOR' : _SPEAKERINFO,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SpeakerInfo)
-  })
-_sym_db.RegisterMessage(SpeakerInfo)
-
-Span = _reflection.GeneratedProtocolMessageType('Span', (_message.Message,), {
-  'DESCRIPTOR' : _SPAN,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Span)
-  })
-_sym_db.RegisterMessage(Span)
-
-Timex = _reflection.GeneratedProtocolMessageType('Timex', (_message.Message,), {
-  'DESCRIPTOR' : _TIMEX,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Timex)
-  })
-_sym_db.RegisterMessage(Timex)
-
-Entity = _reflection.GeneratedProtocolMessageType('Entity', (_message.Message,), {
-  'DESCRIPTOR' : _ENTITY,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Entity)
-  })
-_sym_db.RegisterMessage(Entity)
-
-Relation = _reflection.GeneratedProtocolMessageType('Relation', (_message.Message,), {
-  'DESCRIPTOR' : _RELATION,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Relation)
-  })
-_sym_db.RegisterMessage(Relation)
-
-Operator = _reflection.GeneratedProtocolMessageType('Operator', (_message.Message,), {
-  'DESCRIPTOR' : _OPERATOR,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Operator)
-  })
-_sym_db.RegisterMessage(Operator)
-
-Polarity = _reflection.GeneratedProtocolMessageType('Polarity', (_message.Message,), {
-  'DESCRIPTOR' : _POLARITY,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Polarity)
-  })
-_sym_db.RegisterMessage(Polarity)
-
-NERMention = _reflection.GeneratedProtocolMessageType('NERMention', (_message.Message,), {
-  'DESCRIPTOR' : _NERMENTION,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.NERMention)
-  })
-_sym_db.RegisterMessage(NERMention)
-
-SentenceFragment = _reflection.GeneratedProtocolMessageType('SentenceFragment', (_message.Message,), {
-  'DESCRIPTOR' : _SENTENCEFRAGMENT,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SentenceFragment)
-  })
-_sym_db.RegisterMessage(SentenceFragment)
-
-TokenLocation = _reflection.GeneratedProtocolMessageType('TokenLocation', (_message.Message,), {
-  'DESCRIPTOR' : _TOKENLOCATION,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.TokenLocation)
-  })
-_sym_db.RegisterMessage(TokenLocation)
-
-RelationTriple = _reflection.GeneratedProtocolMessageType('RelationTriple', (_message.Message,), {
-  'DESCRIPTOR' : _RELATIONTRIPLE,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.RelationTriple)
-  })
-_sym_db.RegisterMessage(RelationTriple)
-
-MapStringString = _reflection.GeneratedProtocolMessageType('MapStringString', (_message.Message,), {
-  'DESCRIPTOR' : _MAPSTRINGSTRING,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.MapStringString)
-  })
-_sym_db.RegisterMessage(MapStringString)
-
-MapIntString = _reflection.GeneratedProtocolMessageType('MapIntString', (_message.Message,), {
-  'DESCRIPTOR' : _MAPINTSTRING,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.MapIntString)
-  })
-_sym_db.RegisterMessage(MapIntString)
-
-Section = _reflection.GeneratedProtocolMessageType('Section', (_message.Message,), {
-  'DESCRIPTOR' : _SECTION,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Section)
-  })
-_sym_db.RegisterMessage(Section)
-
-SemgrexRequest = _reflection.GeneratedProtocolMessageType('SemgrexRequest', (_message.Message,), {
-
-  'Dependencies' : _reflection.GeneratedProtocolMessageType('Dependencies', (_message.Message,), {
-    'DESCRIPTOR' : _SEMGREXREQUEST_DEPENDENCIES,
-    '__module__' : 'CoreNLP_pb2'
-    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexRequest.Dependencies)
-    })
-  ,
-  'DESCRIPTOR' : _SEMGREXREQUEST,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexRequest)
-  })
-_sym_db.RegisterMessage(SemgrexRequest)
-_sym_db.RegisterMessage(SemgrexRequest.Dependencies)
-
-SemgrexResponse = _reflection.GeneratedProtocolMessageType('SemgrexResponse', (_message.Message,), {
-
-  'NamedNode' : _reflection.GeneratedProtocolMessageType('NamedNode', (_message.Message,), {
-    'DESCRIPTOR' : _SEMGREXRESPONSE_NAMEDNODE,
-    '__module__' : 'CoreNLP_pb2'
-    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexResponse.NamedNode)
-    })
-  ,
-
-  'NamedRelation' : _reflection.GeneratedProtocolMessageType('NamedRelation', (_message.Message,), {
-    'DESCRIPTOR' : _SEMGREXRESPONSE_NAMEDRELATION,
-    '__module__' : 'CoreNLP_pb2'
-    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexResponse.NamedRelation)
-    })
-  ,
-
-  'Match' : _reflection.GeneratedProtocolMessageType('Match', (_message.Message,), {
-    'DESCRIPTOR' : _SEMGREXRESPONSE_MATCH,
-    '__module__' : 'CoreNLP_pb2'
-    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexResponse.Match)
-    })
-  ,
-
-  'SemgrexResult' : _reflection.GeneratedProtocolMessageType('SemgrexResult', (_message.Message,), {
-    'DESCRIPTOR' : _SEMGREXRESPONSE_SEMGREXRESULT,
-    '__module__' : 'CoreNLP_pb2'
-    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexResponse.SemgrexResult)
-    })
-  ,
-
-  'GraphResult' : _reflection.GeneratedProtocolMessageType('GraphResult', (_message.Message,), {
-    'DESCRIPTOR' : _SEMGREXRESPONSE_GRAPHRESULT,
-    '__module__' : 'CoreNLP_pb2'
-    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexResponse.GraphResult)
-    })
-  ,
-  'DESCRIPTOR' : _SEMGREXRESPONSE,
-  '__module__' : 'CoreNLP_pb2'
-  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexResponse)
-  })
-_sym_db.RegisterMessage(SemgrexResponse)
-_sym_db.RegisterMessage(SemgrexResponse.NamedNode)
-_sym_db.RegisterMessage(SemgrexResponse.NamedRelation)
-_sym_db.RegisterMessage(SemgrexResponse.Match)
-_sym_db.RegisterMessage(SemgrexResponse.SemgrexResult)
-_sym_db.RegisterMessage(SemgrexResponse.GraphResult)
-
-
-DESCRIPTOR._options = None
-_DEPENDENCYGRAPH.fields_by_name['root']._options = None
-# @@protoc_insertion_point(module_scope)
+# -*- coding: utf-8 -*-
+# Generated by the protocol buffer compiler.  DO NOT EDIT!
+# source: CoreNLP.proto
+
+from google.protobuf.internal import enum_type_wrapper
+from google.protobuf import descriptor as _descriptor
+from google.protobuf import message as _message
+from google.protobuf import reflection as _reflection
+from google.protobuf import symbol_database as _symbol_database
+# @@protoc_insertion_point(imports)
+
+_sym_db = _symbol_database.Default()
+
+
+
+
+DESCRIPTOR = _descriptor.FileDescriptor(
+  name='CoreNLP.proto',
+  package='edu.stanford.nlp.pipeline',
+  syntax='proto2',
+  serialized_options=b'\n\031edu.stanford.nlp.pipelineB\rCoreNLPProtos',
+  serialized_pb=b'\n\rCoreNLP.proto\x12\x19\x65\x64u.stanford.nlp.pipeline\"\xe1\x05\n\x08\x44ocument\x12\x0c\n\x04text\x18\x01 \x02(\t\x12\x35\n\x08sentence\x18\x02 \x03(\x0b\x32#.edu.stanford.nlp.pipeline.Sentence\x12\x39\n\ncorefChain\x18\x03 \x03(\x0b\x32%.edu.stanford.nlp.pipeline.CorefChain\x12\r\n\x05\x64ocID\x18\x04 \x01(\t\x12\x0f\n\x07\x64ocDate\x18\x07 \x01(\t\x12\x10\n\x08\x63\x61lendar\x18\x08 \x01(\x04\x12;\n\x11sentencelessToken\x18\x05 \x03(\x0b\x32 .edu.stanford.nlp.pipeline.Token\x12\x33\n\tcharacter\x18\n \x03(\x0b\x32 .edu.stanford.nlp.pipeline.Token\x12/\n\x05quote\x18\x06 \x03(\x0b\x32 .edu.stanford.nlp.pipeline.Quote\x12\x37\n\x08mentions\x18\t \x03(\x0b\x32%.edu.stanford.nlp.pipeline.NERMention\x12#\n\x1bhasEntityMentionsAnnotation\x18\r \x01(\x08\x12\x0e\n\x06xmlDoc\x18\x0b \x01(\x08\x12\x34\n\x08sections\x18\x0c \x03(\x0b\x32\".edu.stanford.nlp.pipeline.Section\x12<\n\x10mentionsForCoref\x18\x0e \x03(\x0b\x32\".edu.stanford.nlp.pipeline.Mention\x12!\n\x19hasCorefMentionAnnotation\x18\x0f \x01(\x08\x12\x1a\n\x12hasCorefAnnotation\x18\x10 \x01(\x08\x12+\n#corefMentionToEntityMentionMappings\x18\x11 \x03(\x05\x12+\n#entityMentionToCorefMentionMappings\x18\x12 \x03(\x05*\x05\x08\x64\x10\x80\x02\"\xcd\x0f\n\x08Sentence\x12/\n\x05token\x18\x01 \x03(\x0b\x32 .edu.stanford.nlp.pipeline.Token\x12\x18\n\x10tokenOffsetBegin\x18\x02 \x02(\r\x12\x16\n\x0etokenOffsetEnd\x18\x03 \x02(\r\x12\x15\n\rsentenceIndex\x18\x04 \x01(\r\x12\x1c\n\x14\x63haracterOffsetBegin\x18\x05 \x01(\r\x12\x1a\n\x12\x63haracterOffsetEnd\x18\x06 \x01(\r\x12\x37\n\tparseTree\x18\x07 \x01(\x0b\x32$.edu.stanford.nlp.pipeline.ParseTree\x12@\n\x12\x62inarizedParseTree\x18\x1f \x01(\x0b\x32$.edu.stanford.nlp.pipeline.ParseTree\x12@\n\x12\x61nnotatedParseTree\x18  \x01(\x0b\x32$.edu.stanford.nlp.pipeline.ParseTree\x12\x11\n\tsentiment\x18! \x01(\t\x12=\n\x0fkBestParseTrees\x18\" \x03(\x0b\x32$.edu.stanford.nlp.pipeline.ParseTree\x12\x45\n\x11\x62\x61sicDependencies\x18\x08 \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\x12I\n\x15\x63ollapsedDependencies\x18\t \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\x12T\n collapsedCCProcessedDependencies\x18\n \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\x12K\n\x17\x61lternativeDependencies\x18\r \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\x12?\n\x0copenieTriple\x18\x0e \x03(\x0b\x32).edu.stanford.nlp.pipeline.RelationTriple\x12<\n\tkbpTriple\x18\x10 \x03(\x0b\x32).edu.stanford.nlp.pipeline.RelationTriple\x12\x45\n\x10\x65ntailedSentence\x18\x0f \x03(\x0b\x32+.edu.stanford.nlp.pipeline.SentenceFragment\x12\x43\n\x0e\x65ntailedClause\x18# \x03(\x0b\x32+.edu.stanford.nlp.pipeline.SentenceFragment\x12H\n\x14\x65nhancedDependencies\x18\x11 \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\x12P\n\x1c\x65nhancedPlusPlusDependencies\x18\x12 \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\x12\x33\n\tcharacter\x18\x13 \x03(\x0b\x32 .edu.stanford.nlp.pipeline.Token\x12\x11\n\tparagraph\x18\x0b \x01(\r\x12\x0c\n\x04text\x18\x0c \x01(\t\x12\x12\n\nlineNumber\x18\x14 \x01(\r\x12\x1e\n\x16hasRelationAnnotations\x18\x33 \x01(\x08\x12\x31\n\x06\x65ntity\x18\x34 \x03(\x0b\x32!.edu.stanford.nlp.pipeline.Entity\x12\x35\n\x08relation\x18\x35 \x03(\x0b\x32#.edu.stanford.nlp.pipeline.Relation\x12$\n\x1chasNumerizedTokensAnnotation\x18\x36 \x01(\x08\x12\x37\n\x08mentions\x18\x37 \x03(\x0b\x32%.edu.stanford.nlp.pipeline.NERMention\x12<\n\x10mentionsForCoref\x18\x38 \x03(\x0b\x32\".edu.stanford.nlp.pipeline.Mention\x12\"\n\x1ahasCorefMentionsAnnotation\x18\x39 \x01(\x08\x12\x12\n\nsentenceID\x18: \x01(\t\x12\x13\n\x0bsectionDate\x18; \x01(\t\x12\x14\n\x0csectionIndex\x18< \x01(\r\x12\x13\n\x0bsectionName\x18= \x01(\t\x12\x15\n\rsectionAuthor\x18> \x01(\t\x12\r\n\x05\x64ocID\x18? \x01(\t\x12\x15\n\rsectionQuoted\x18@ \x01(\x08\x12#\n\x1bhasEntityMentionsAnnotation\x18\x41 \x01(\x08\x12\x1f\n\x17hasKBPTriplesAnnotation\x18\x44 \x01(\x08\x12\"\n\x1ahasOpenieTriplesAnnotation\x18\x45 \x01(\x08\x12\x14\n\x0c\x63hapterIndex\x18\x42 \x01(\r\x12\x16\n\x0eparagraphIndex\x18\x43 \x01(\r\x12=\n\x10\x65nhancedSentence\x18\x46 \x01(\x0b\x32#.edu.stanford.nlp.pipeline.Sentence*\x05\x08\x64\x10\x80\x02\"\xad\x0c\n\x05Token\x12\x0c\n\x04word\x18\x01 \x01(\t\x12\x0b\n\x03pos\x18\x02 \x01(\t\x12\r\n\x05value\x18\x03 \x01(\t\x12\x10\n\x08\x63\x61tegory\x18\x04 \x01(\t\x12\x0e\n\x06\x62\x65\x66ore\x18\x05 \x01(\t\x12\r\n\x05\x61\x66ter\x18\x06 \x01(\t\x12\x14\n\x0coriginalText\x18\x07 \x01(\t\x12\x0b\n\x03ner\x18\x08 \x01(\t\x12\x11\n\tcoarseNER\x18> \x01(\t\x12\x16\n\x0e\x66ineGrainedNER\x18? \x01(\t\x12\x15\n\rnerLabelProbs\x18\x42 \x03(\t\x12\x15\n\rnormalizedNER\x18\t \x01(\t\x12\r\n\x05lemma\x18\n \x01(\t\x12\x11\n\tbeginChar\x18\x0b \x01(\r\x12\x0f\n\x07\x65ndChar\x18\x0c \x01(\r\x12\x11\n\tutterance\x18\r \x01(\r\x12\x0f\n\x07speaker\x18\x0e \x01(\t\x12\x12\n\nbeginIndex\x18\x0f \x01(\r\x12\x10\n\x08\x65ndIndex\x18\x10 \x01(\r\x12\x17\n\x0ftokenBeginIndex\x18\x11 \x01(\r\x12\x15\n\rtokenEndIndex\x18\x12 \x01(\r\x12\x34\n\ntimexValue\x18\x13 \x01(\x0b\x32 .edu.stanford.nlp.pipeline.Timex\x12\x15\n\rhasXmlContext\x18\x15 \x01(\x08\x12\x12\n\nxmlContext\x18\x16 \x03(\t\x12\x16\n\x0e\x63orefClusterID\x18\x17 \x01(\r\x12\x0e\n\x06\x61nswer\x18\x18 \x01(\t\x12\x15\n\rheadWordIndex\x18\x1a \x01(\r\x12\x35\n\x08operator\x18\x1b \x01(\x0b\x32#.edu.stanford.nlp.pipeline.Operator\x12\x35\n\x08polarity\x18\x1c \x01(\x0b\x32#.edu.stanford.nlp.pipeline.Polarity\x12\x14\n\x0cpolarity_dir\x18\' \x01(\t\x12-\n\x04span\x18\x1d \x01(\x0b\x32\x1f.edu.stanford.nlp.pipeline.Span\x12\x11\n\tsentiment\x18\x1e \x01(\t\x12\x16\n\x0equotationIndex\x18\x1f \x01(\x05\x12\x42\n\x0e\x63onllUFeatures\x18  \x01(\x0b\x32*.edu.stanford.nlp.pipeline.MapStringString\x12\x11\n\tcoarseTag\x18! \x01(\t\x12\x38\n\x0f\x63onllUTokenSpan\x18\" \x01(\x0b\x32\x1f.edu.stanford.nlp.pipeline.Span\x12\x12\n\nconllUMisc\x18# \x01(\t\x12G\n\x13\x63onllUSecondaryDeps\x18$ \x01(\x0b\x32*.edu.stanford.nlp.pipeline.MapStringString\x12\x17\n\x0fwikipediaEntity\x18% \x01(\t\x12\x11\n\tisNewline\x18& \x01(\x08\x12\x0e\n\x06gender\x18\x33 \x01(\t\x12\x10\n\x08trueCase\x18\x34 \x01(\t\x12\x14\n\x0ctrueCaseText\x18\x35 \x01(\t\x12\x13\n\x0b\x63hineseChar\x18\x36 \x01(\t\x12\x12\n\nchineseSeg\x18\x37 \x01(\t\x12\x16\n\x0e\x63hineseXMLChar\x18< \x01(\t\x12\x11\n\tarabicSeg\x18L \x01(\t\x12\x13\n\x0bsectionName\x18\x38 \x01(\t\x12\x15\n\rsectionAuthor\x18\x39 \x01(\t\x12\x13\n\x0bsectionDate\x18: \x01(\t\x12\x17\n\x0fsectionEndLabel\x18; \x01(\t\x12\x0e\n\x06parent\x18= \x01(\t\x12\x19\n\x11\x63orefMentionIndex\x18@ \x03(\r\x12\x1a\n\x12\x65ntityMentionIndex\x18\x41 \x01(\r\x12\r\n\x05isMWT\x18\x43 \x01(\x08\x12\x12\n\nisFirstMWT\x18\x44 \x01(\x08\x12\x0f\n\x07mwtText\x18\x45 \x01(\t\x12\x14\n\x0cnumericValue\x18\x46 \x01(\x04\x12\x13\n\x0bnumericType\x18G \x01(\t\x12\x1d\n\x15numericCompositeValue\x18H \x01(\x04\x12\x1c\n\x14numericCompositeType\x18I \x01(\t\x12\x1c\n\x14\x63odepointOffsetBegin\x18J \x01(\r\x12\x1a\n\x12\x63odepointOffsetEnd\x18K \x01(\r*\x05\x08\x64\x10\x80\x02\"\xe4\x03\n\x05Quote\x12\x0c\n\x04text\x18\x01 \x01(\t\x12\r\n\x05\x62\x65gin\x18\x02 \x01(\r\x12\x0b\n\x03\x65nd\x18\x03 \x01(\r\x12\x15\n\rsentenceBegin\x18\x05 \x01(\r\x12\x13\n\x0bsentenceEnd\x18\x06 \x01(\r\x12\x12\n\ntokenBegin\x18\x07 \x01(\r\x12\x10\n\x08tokenEnd\x18\x08 \x01(\r\x12\r\n\x05\x64ocid\x18\t \x01(\t\x12\r\n\x05index\x18\n \x01(\r\x12\x0e\n\x06\x61uthor\x18\x0b \x01(\t\x12\x0f\n\x07mention\x18\x0c \x01(\t\x12\x14\n\x0cmentionBegin\x18\r \x01(\r\x12\x12\n\nmentionEnd\x18\x0e \x01(\r\x12\x13\n\x0bmentionType\x18\x0f \x01(\t\x12\x14\n\x0cmentionSieve\x18\x10 \x01(\t\x12\x0f\n\x07speaker\x18\x11 \x01(\t\x12\x14\n\x0cspeakerSieve\x18\x12 \x01(\t\x12\x18\n\x10\x63\x61nonicalMention\x18\x13 \x01(\t\x12\x1d\n\x15\x63\x61nonicalMentionBegin\x18\x14 \x01(\r\x12\x1b\n\x13\x63\x61nonicalMentionEnd\x18\x15 \x01(\r\x12N\n\x1a\x61ttributionDependencyGraph\x18\x16 \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\"\xc7\x01\n\tParseTree\x12\x33\n\x05\x63hild\x18\x01 \x03(\x0b\x32$.edu.stanford.nlp.pipeline.ParseTree\x12\r\n\x05value\x18\x02 \x01(\t\x12\x17\n\x0fyieldBeginIndex\x18\x03 \x01(\r\x12\x15\n\ryieldEndIndex\x18\x04 \x01(\r\x12\r\n\x05score\x18\x05 \x01(\x01\x12\x37\n\tsentiment\x18\x06 \x01(\x0e\x32$.edu.stanford.nlp.pipeline.Sentiment\"\x96\x03\n\x0f\x44\x65pendencyGraph\x12=\n\x04node\x18\x01 \x03(\x0b\x32/.edu.stanford.nlp.pipeline.DependencyGraph.Node\x12=\n\x04\x65\x64ge\x18\x02 \x03(\x0b\x32/.edu.stanford.nlp.pipeline.DependencyGraph.Edge\x12\x10\n\x04root\x18\x03 \x03(\rB\x02\x10\x01\x1a\x44\n\x04Node\x12\x15\n\rsentenceIndex\x18\x01 \x02(\r\x12\r\n\x05index\x18\x02 \x02(\r\x12\x16\n\x0e\x63opyAnnotation\x18\x03 \x01(\r\x1a\xac\x01\n\x04\x45\x64ge\x12\x0e\n\x06source\x18\x01 \x02(\r\x12\x0e\n\x06target\x18\x02 \x02(\r\x12\x0b\n\x03\x64\x65p\x18\x03 \x01(\t\x12\x0f\n\x07isExtra\x18\x04 \x01(\x08\x12\x12\n\nsourceCopy\x18\x05 \x01(\r\x12\x12\n\ntargetCopy\x18\x06 \x01(\r\x12>\n\x08language\x18\x07 \x01(\x0e\x32#.edu.stanford.nlp.pipeline.Language:\x07Unknown\"\xc6\x02\n\nCorefChain\x12\x0f\n\x07\x63hainID\x18\x01 \x02(\x05\x12\x43\n\x07mention\x18\x02 \x03(\x0b\x32\x32.edu.stanford.nlp.pipeline.CorefChain.CorefMention\x12\x16\n\x0erepresentative\x18\x03 \x02(\r\x1a\xc9\x01\n\x0c\x43orefMention\x12\x11\n\tmentionID\x18\x01 \x01(\x05\x12\x13\n\x0bmentionType\x18\x02 \x01(\t\x12\x0e\n\x06number\x18\x03 \x01(\t\x12\x0e\n\x06gender\x18\x04 \x01(\t\x12\x0f\n\x07\x61nimacy\x18\x05 \x01(\t\x12\x12\n\nbeginIndex\x18\x06 \x01(\r\x12\x10\n\x08\x65ndIndex\x18\x07 \x01(\r\x12\x11\n\theadIndex\x18\t \x01(\r\x12\x15\n\rsentenceIndex\x18\n \x01(\r\x12\x10\n\x08position\x18\x0b \x01(\r\"\xef\x08\n\x07Mention\x12\x11\n\tmentionID\x18\x01 \x01(\x05\x12\x13\n\x0bmentionType\x18\x02 \x01(\t\x12\x0e\n\x06number\x18\x03 \x01(\t\x12\x0e\n\x06gender\x18\x04 \x01(\t\x12\x0f\n\x07\x61nimacy\x18\x05 \x01(\t\x12\x0e\n\x06person\x18\x06 \x01(\t\x12\x12\n\nstartIndex\x18\x07 \x01(\r\x12\x10\n\x08\x65ndIndex\x18\t \x01(\r\x12\x11\n\theadIndex\x18\n \x01(\x05\x12\x12\n\nheadString\x18\x0b \x01(\t\x12\x11\n\tnerString\x18\x0c \x01(\t\x12\x13\n\x0boriginalRef\x18\r \x01(\x05\x12\x1a\n\x12goldCorefClusterID\x18\x0e \x01(\x05\x12\x16\n\x0e\x63orefClusterID\x18\x0f \x01(\x05\x12\x12\n\nmentionNum\x18\x10 \x01(\x05\x12\x0f\n\x07sentNum\x18\x11 \x01(\x05\x12\r\n\x05utter\x18\x12 \x01(\x05\x12\x11\n\tparagraph\x18\x13 \x01(\x05\x12\x11\n\tisSubject\x18\x14 \x01(\x08\x12\x16\n\x0eisDirectObject\x18\x15 \x01(\x08\x12\x18\n\x10isIndirectObject\x18\x16 \x01(\x08\x12\x1b\n\x13isPrepositionObject\x18\x17 \x01(\x08\x12\x0f\n\x07hasTwin\x18\x18 \x01(\x08\x12\x0f\n\x07generic\x18\x19 \x01(\x08\x12\x13\n\x0bisSingleton\x18\x1a \x01(\x08\x12\x1a\n\x12hasBasicDependency\x18\x1b \x01(\x08\x12\x1d\n\x15hasEnhancedDepenedncy\x18\x1c \x01(\x08\x12\x1b\n\x13hasContextParseTree\x18\x1d \x01(\x08\x12?\n\x0fheadIndexedWord\x18\x1e \x01(\x0b\x32&.edu.stanford.nlp.pipeline.IndexedWord\x12=\n\rdependingVerb\x18\x1f \x01(\x0b\x32&.edu.stanford.nlp.pipeline.IndexedWord\x12\x38\n\x08headWord\x18  \x01(\x0b\x32&.edu.stanford.nlp.pipeline.IndexedWord\x12;\n\x0bspeakerInfo\x18! \x01(\x0b\x32&.edu.stanford.nlp.pipeline.SpeakerInfo\x12=\n\rsentenceWords\x18\x32 \x03(\x0b\x32&.edu.stanford.nlp.pipeline.IndexedWord\x12<\n\x0coriginalSpan\x18\x33 \x03(\x0b\x32&.edu.stanford.nlp.pipeline.IndexedWord\x12\x12\n\ndependents\x18\x34 \x03(\t\x12\x19\n\x11preprocessedTerms\x18\x35 \x03(\t\x12\x13\n\x0b\x61ppositions\x18\x36 \x03(\x05\x12\x1c\n\x14predicateNominatives\x18\x37 \x03(\x05\x12\x18\n\x10relativePronouns\x18\x38 \x03(\x05\x12\x13\n\x0blistMembers\x18\x39 \x03(\x05\x12\x15\n\rbelongToLists\x18: \x03(\x05\"X\n\x0bIndexedWord\x12\x13\n\x0bsentenceNum\x18\x01 \x01(\x05\x12\x12\n\ntokenIndex\x18\x02 \x01(\x05\x12\r\n\x05\x64ocID\x18\x03 \x01(\x05\x12\x11\n\tcopyCount\x18\x04 \x01(\r\"4\n\x0bSpeakerInfo\x12\x13\n\x0bspeakerName\x18\x01 \x01(\t\x12\x10\n\x08mentions\x18\x02 \x03(\x05\"\"\n\x04Span\x12\r\n\x05\x62\x65gin\x18\x01 \x02(\r\x12\x0b\n\x03\x65nd\x18\x02 \x02(\r\"w\n\x05Timex\x12\r\n\x05value\x18\x01 \x01(\t\x12\x10\n\x08\x61ltValue\x18\x02 \x01(\t\x12\x0c\n\x04text\x18\x03 \x01(\t\x12\x0c\n\x04type\x18\x04 \x01(\t\x12\x0b\n\x03tid\x18\x05 \x01(\t\x12\x12\n\nbeginPoint\x18\x06 \x01(\r\x12\x10\n\x08\x65ndPoint\x18\x07 \x01(\r\"\xdb\x01\n\x06\x45ntity\x12\x11\n\theadStart\x18\x06 \x01(\r\x12\x0f\n\x07headEnd\x18\x07 \x01(\r\x12\x13\n\x0bmentionType\x18\x08 \x01(\t\x12\x16\n\x0enormalizedName\x18\t \x01(\t\x12\x16\n\x0eheadTokenIndex\x18\n \x01(\r\x12\x0f\n\x07\x63orefID\x18\x0b \x01(\t\x12\x10\n\x08objectID\x18\x01 \x01(\t\x12\x13\n\x0b\x65xtentStart\x18\x02 \x01(\r\x12\x11\n\textentEnd\x18\x03 \x01(\r\x12\x0c\n\x04type\x18\x04 \x01(\t\x12\x0f\n\x07subtype\x18\x05 \x01(\t\"\xb7\x01\n\x08Relation\x12\x0f\n\x07\x61rgName\x18\x06 \x03(\t\x12.\n\x03\x61rg\x18\x07 \x03(\x0b\x32!.edu.stanford.nlp.pipeline.Entity\x12\x11\n\tsignature\x18\x08 \x01(\t\x12\x10\n\x08objectID\x18\x01 \x01(\t\x12\x13\n\x0b\x65xtentStart\x18\x02 \x01(\r\x12\x11\n\textentEnd\x18\x03 \x01(\r\x12\x0c\n\x04type\x18\x04 \x01(\t\x12\x0f\n\x07subtype\x18\x05 \x01(\t\"\xb2\x01\n\x08Operator\x12\x0c\n\x04name\x18\x01 \x02(\t\x12\x1b\n\x13quantifierSpanBegin\x18\x02 \x02(\x05\x12\x19\n\x11quantifierSpanEnd\x18\x03 \x02(\x05\x12\x18\n\x10subjectSpanBegin\x18\x04 \x02(\x05\x12\x16\n\x0esubjectSpanEnd\x18\x05 \x02(\x05\x12\x17\n\x0fobjectSpanBegin\x18\x06 \x02(\x05\x12\x15\n\robjectSpanEnd\x18\x07 \x02(\x05\"\xa9\x04\n\x08Polarity\x12K\n\x12projectEquivalence\x18\x01 \x02(\x0e\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\x12Q\n\x18projectForwardEntailment\x18\x02 \x02(\x0e\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\x12Q\n\x18projectReverseEntailment\x18\x03 \x02(\x0e\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\x12H\n\x0fprojectNegation\x18\x04 \x02(\x0e\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\x12K\n\x12projectAlternation\x18\x05 \x02(\x0e\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\x12\x45\n\x0cprojectCover\x18\x06 \x02(\x0e\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\x12L\n\x13projectIndependence\x18\x07 \x02(\x0e\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\"\xdd\x02\n\nNERMention\x12\x15\n\rsentenceIndex\x18\x01 \x01(\r\x12%\n\x1dtokenStartInSentenceInclusive\x18\x02 \x02(\r\x12#\n\x1btokenEndInSentenceExclusive\x18\x03 \x02(\r\x12\x0b\n\x03ner\x18\x04 \x02(\t\x12\x15\n\rnormalizedNER\x18\x05 \x01(\t\x12\x12\n\nentityType\x18\x06 \x01(\t\x12/\n\x05timex\x18\x07 \x01(\x0b\x32 .edu.stanford.nlp.pipeline.Timex\x12\x17\n\x0fwikipediaEntity\x18\x08 \x01(\t\x12\x0e\n\x06gender\x18\t \x01(\t\x12\x1a\n\x12\x65ntityMentionIndex\x18\n \x01(\r\x12#\n\x1b\x63\x61nonicalEntityMentionIndex\x18\x0b \x01(\r\x12\x19\n\x11\x65ntityMentionText\x18\x0c \x01(\t\"Y\n\x10SentenceFragment\x12\x12\n\ntokenIndex\x18\x01 \x03(\r\x12\x0c\n\x04root\x18\x02 \x01(\r\x12\x14\n\x0c\x61ssumedTruth\x18\x03 \x01(\x08\x12\r\n\x05score\x18\x04 \x01(\x01\":\n\rTokenLocation\x12\x15\n\rsentenceIndex\x18\x01 \x01(\r\x12\x12\n\ntokenIndex\x18\x02 \x01(\r\"\x9a\x03\n\x0eRelationTriple\x12\x0f\n\x07subject\x18\x01 \x01(\t\x12\x10\n\x08relation\x18\x02 \x01(\t\x12\x0e\n\x06object\x18\x03 \x01(\t\x12\x12\n\nconfidence\x18\x04 \x01(\x01\x12?\n\rsubjectTokens\x18\r \x03(\x0b\x32(.edu.stanford.nlp.pipeline.TokenLocation\x12@\n\x0erelationTokens\x18\x0e \x03(\x0b\x32(.edu.stanford.nlp.pipeline.TokenLocation\x12>\n\x0cobjectTokens\x18\x0f \x03(\x0b\x32(.edu.stanford.nlp.pipeline.TokenLocation\x12\x38\n\x04tree\x18\x08 \x01(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\x12\x0e\n\x06istmod\x18\t \x01(\x08\x12\x10\n\x08prefixBe\x18\n \x01(\x08\x12\x10\n\x08suffixBe\x18\x0b \x01(\x08\x12\x10\n\x08suffixOf\x18\x0c \x01(\x08\"-\n\x0fMapStringString\x12\x0b\n\x03key\x18\x01 \x03(\t\x12\r\n\x05value\x18\x02 \x03(\t\"*\n\x0cMapIntString\x12\x0b\n\x03key\x18\x01 \x03(\r\x12\r\n\x05value\x18\x02 \x03(\t\"\xfc\x01\n\x07Section\x12\x11\n\tcharBegin\x18\x01 \x02(\r\x12\x0f\n\x07\x63harEnd\x18\x02 \x02(\r\x12\x0e\n\x06\x61uthor\x18\x03 \x01(\t\x12\x17\n\x0fsentenceIndexes\x18\x04 \x03(\r\x12\x10\n\x08\x64\x61tetime\x18\x05 \x01(\t\x12\x30\n\x06quotes\x18\x06 \x03(\x0b\x32 .edu.stanford.nlp.pipeline.Quote\x12\x17\n\x0f\x61uthorCharBegin\x18\x07 \x01(\r\x12\x15\n\rauthorCharEnd\x18\x08 \x01(\r\x12\x30\n\x06xmlTag\x18\t \x02(\x0b\x32 .edu.stanford.nlp.pipeline.Token\"\xe4\x01\n\x0eSemgrexRequest\x12\x0f\n\x07semgrex\x18\x01 \x03(\t\x12\x45\n\x05query\x18\x02 \x03(\x0b\x32\x36.edu.stanford.nlp.pipeline.SemgrexRequest.Dependencies\x1az\n\x0c\x44\x65pendencies\x12/\n\x05token\x18\x01 \x03(\x0b\x32 .edu.stanford.nlp.pipeline.Token\x12\x39\n\x05graph\x18\x02 \x02(\x0b\x32*.edu.stanford.nlp.pipeline.DependencyGraph\"\x80\x04\n\x0fSemgrexResponse\x12\x46\n\x06result\x18\x01 \x03(\x0b\x32\x36.edu.stanford.nlp.pipeline.SemgrexResponse.GraphResult\x1a(\n\tNamedNode\x12\x0c\n\x04name\x18\x01 \x02(\t\x12\r\n\x05index\x18\x02 \x02(\x05\x1a+\n\rNamedRelation\x12\x0c\n\x04name\x18\x01 \x02(\t\x12\x0c\n\x04reln\x18\x02 \x02(\t\x1a\xa2\x01\n\x05Match\x12\r\n\x05index\x18\x01 \x02(\x05\x12\x42\n\x04node\x18\x02 \x03(\x0b\x32\x34.edu.stanford.nlp.pipeline.SemgrexResponse.NamedNode\x12\x46\n\x04reln\x18\x03 \x03(\x0b\x32\x38.edu.stanford.nlp.pipeline.SemgrexResponse.NamedRelation\x1aP\n\rSemgrexResult\x12?\n\x05match\x18\x01 \x03(\x0b\x32\x30.edu.stanford.nlp.pipeline.SemgrexResponse.Match\x1aW\n\x0bGraphResult\x12H\n\x06result\x18\x01 \x03(\x0b\x32\x38.edu.stanford.nlp.pipeline.SemgrexResponse.SemgrexResult*\xa3\x01\n\x08Language\x12\x0b\n\x07Unknown\x10\x00\x12\x07\n\x03\x41ny\x10\x01\x12\n\n\x06\x41rabic\x10\x02\x12\x0b\n\x07\x43hinese\x10\x03\x12\x0b\n\x07\x45nglish\x10\x04\x12\n\n\x06German\x10\x05\x12\n\n\x06\x46rench\x10\x06\x12\n\n\x06Hebrew\x10\x07\x12\x0b\n\x07Spanish\x10\x08\x12\x14\n\x10UniversalEnglish\x10\t\x12\x14\n\x10UniversalChinese\x10\n*h\n\tSentiment\x12\x13\n\x0fSTRONG_NEGATIVE\x10\x00\x12\x11\n\rWEAK_NEGATIVE\x10\x01\x12\x0b\n\x07NEUTRAL\x10\x02\x12\x11\n\rWEAK_POSITIVE\x10\x03\x12\x13\n\x0fSTRONG_POSITIVE\x10\x04*\x93\x01\n\x14NaturalLogicRelation\x12\x0f\n\x0b\x45QUIVALENCE\x10\x00\x12\x16\n\x12\x46ORWARD_ENTAILMENT\x10\x01\x12\x16\n\x12REVERSE_ENTAILMENT\x10\x02\x12\x0c\n\x08NEGATION\x10\x03\x12\x0f\n\x0b\x41LTERNATION\x10\x04\x12\t\n\x05\x43OVER\x10\x05\x12\x10\n\x0cINDEPENDENCE\x10\x06\x42*\n\x19\x65\x64u.stanford.nlp.pipelineB\rCoreNLPProtos'
+)
+
+_LANGUAGE = _descriptor.EnumDescriptor(
+  name='Language',
+  full_name='edu.stanford.nlp.pipeline.Language',
+  filename=None,
+  file=DESCRIPTOR,
+  values=[
+    _descriptor.EnumValueDescriptor(
+      name='Unknown', index=0, number=0,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='Any', index=1, number=1,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='Arabic', index=2, number=2,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='Chinese', index=3, number=3,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='English', index=4, number=4,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='German', index=5, number=5,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='French', index=6, number=6,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='Hebrew', index=7, number=7,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='Spanish', index=8, number=8,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='UniversalEnglish', index=9, number=9,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='UniversalChinese', index=10, number=10,
+      serialized_options=None,
+      type=None),
+  ],
+  containing_type=None,
+  serialized_options=None,
+  serialized_start=10388,
+  serialized_end=10551,
+)
+_sym_db.RegisterEnumDescriptor(_LANGUAGE)
+
+Language = enum_type_wrapper.EnumTypeWrapper(_LANGUAGE)
+_SENTIMENT = _descriptor.EnumDescriptor(
+  name='Sentiment',
+  full_name='edu.stanford.nlp.pipeline.Sentiment',
+  filename=None,
+  file=DESCRIPTOR,
+  values=[
+    _descriptor.EnumValueDescriptor(
+      name='STRONG_NEGATIVE', index=0, number=0,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='WEAK_NEGATIVE', index=1, number=1,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='NEUTRAL', index=2, number=2,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='WEAK_POSITIVE', index=3, number=3,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='STRONG_POSITIVE', index=4, number=4,
+      serialized_options=None,
+      type=None),
+  ],
+  containing_type=None,
+  serialized_options=None,
+  serialized_start=10553,
+  serialized_end=10657,
+)
+_sym_db.RegisterEnumDescriptor(_SENTIMENT)
+
+Sentiment = enum_type_wrapper.EnumTypeWrapper(_SENTIMENT)
+_NATURALLOGICRELATION = _descriptor.EnumDescriptor(
+  name='NaturalLogicRelation',
+  full_name='edu.stanford.nlp.pipeline.NaturalLogicRelation',
+  filename=None,
+  file=DESCRIPTOR,
+  values=[
+    _descriptor.EnumValueDescriptor(
+      name='EQUIVALENCE', index=0, number=0,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='FORWARD_ENTAILMENT', index=1, number=1,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='REVERSE_ENTAILMENT', index=2, number=2,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='NEGATION', index=3, number=3,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='ALTERNATION', index=4, number=4,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='COVER', index=5, number=5,
+      serialized_options=None,
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='INDEPENDENCE', index=6, number=6,
+      serialized_options=None,
+      type=None),
+  ],
+  containing_type=None,
+  serialized_options=None,
+  serialized_start=10660,
+  serialized_end=10807,
+)
+_sym_db.RegisterEnumDescriptor(_NATURALLOGICRELATION)
+
+NaturalLogicRelation = enum_type_wrapper.EnumTypeWrapper(_NATURALLOGICRELATION)
+Unknown = 0
+Any = 1
+Arabic = 2
+Chinese = 3
+English = 4
+German = 5
+French = 6
+Hebrew = 7
+Spanish = 8
+UniversalEnglish = 9
+UniversalChinese = 10
+STRONG_NEGATIVE = 0
+WEAK_NEGATIVE = 1
+NEUTRAL = 2
+WEAK_POSITIVE = 3
+STRONG_POSITIVE = 4
+EQUIVALENCE = 0
+FORWARD_ENTAILMENT = 1
+REVERSE_ENTAILMENT = 2
+NEGATION = 3
+ALTERNATION = 4
+COVER = 5
+INDEPENDENCE = 6
+
+
+
+_DOCUMENT = _descriptor.Descriptor(
+  name='Document',
+  full_name='edu.stanford.nlp.pipeline.Document',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='text', full_name='edu.stanford.nlp.pipeline.Document.text', index=0,
+      number=1, type=9, cpp_type=9, label=2,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sentence', full_name='edu.stanford.nlp.pipeline.Document.sentence', index=1,
+      number=2, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='corefChain', full_name='edu.stanford.nlp.pipeline.Document.corefChain', index=2,
+      number=3, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='docID', full_name='edu.stanford.nlp.pipeline.Document.docID', index=3,
+      number=4, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='docDate', full_name='edu.stanford.nlp.pipeline.Document.docDate', index=4,
+      number=7, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='calendar', full_name='edu.stanford.nlp.pipeline.Document.calendar', index=5,
+      number=8, type=4, cpp_type=4, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sentencelessToken', full_name='edu.stanford.nlp.pipeline.Document.sentencelessToken', index=6,
+      number=5, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='character', full_name='edu.stanford.nlp.pipeline.Document.character', index=7,
+      number=10, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='quote', full_name='edu.stanford.nlp.pipeline.Document.quote', index=8,
+      number=6, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mentions', full_name='edu.stanford.nlp.pipeline.Document.mentions', index=9,
+      number=9, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='hasEntityMentionsAnnotation', full_name='edu.stanford.nlp.pipeline.Document.hasEntityMentionsAnnotation', index=10,
+      number=13, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='xmlDoc', full_name='edu.stanford.nlp.pipeline.Document.xmlDoc', index=11,
+      number=11, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sections', full_name='edu.stanford.nlp.pipeline.Document.sections', index=12,
+      number=12, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mentionsForCoref', full_name='edu.stanford.nlp.pipeline.Document.mentionsForCoref', index=13,
+      number=14, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='hasCorefMentionAnnotation', full_name='edu.stanford.nlp.pipeline.Document.hasCorefMentionAnnotation', index=14,
+      number=15, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='hasCorefAnnotation', full_name='edu.stanford.nlp.pipeline.Document.hasCorefAnnotation', index=15,
+      number=16, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='corefMentionToEntityMentionMappings', full_name='edu.stanford.nlp.pipeline.Document.corefMentionToEntityMentionMappings', index=16,
+      number=17, type=5, cpp_type=1, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='entityMentionToCorefMentionMappings', full_name='edu.stanford.nlp.pipeline.Document.entityMentionToCorefMentionMappings', index=17,
+      number=18, type=5, cpp_type=1, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=True,
+  syntax='proto2',
+  extension_ranges=[(100, 256), ],
+  oneofs=[
+  ],
+  serialized_start=45,
+  serialized_end=782,
+)
+
+
+_SENTENCE = _descriptor.Descriptor(
+  name='Sentence',
+  full_name='edu.stanford.nlp.pipeline.Sentence',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='token', full_name='edu.stanford.nlp.pipeline.Sentence.token', index=0,
+      number=1, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='tokenOffsetBegin', full_name='edu.stanford.nlp.pipeline.Sentence.tokenOffsetBegin', index=1,
+      number=2, type=13, cpp_type=3, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='tokenOffsetEnd', full_name='edu.stanford.nlp.pipeline.Sentence.tokenOffsetEnd', index=2,
+      number=3, type=13, cpp_type=3, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sentenceIndex', full_name='edu.stanford.nlp.pipeline.Sentence.sentenceIndex', index=3,
+      number=4, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='characterOffsetBegin', full_name='edu.stanford.nlp.pipeline.Sentence.characterOffsetBegin', index=4,
+      number=5, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='characterOffsetEnd', full_name='edu.stanford.nlp.pipeline.Sentence.characterOffsetEnd', index=5,
+      number=6, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='parseTree', full_name='edu.stanford.nlp.pipeline.Sentence.parseTree', index=6,
+      number=7, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='binarizedParseTree', full_name='edu.stanford.nlp.pipeline.Sentence.binarizedParseTree', index=7,
+      number=31, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='annotatedParseTree', full_name='edu.stanford.nlp.pipeline.Sentence.annotatedParseTree', index=8,
+      number=32, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sentiment', full_name='edu.stanford.nlp.pipeline.Sentence.sentiment', index=9,
+      number=33, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='kBestParseTrees', full_name='edu.stanford.nlp.pipeline.Sentence.kBestParseTrees', index=10,
+      number=34, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='basicDependencies', full_name='edu.stanford.nlp.pipeline.Sentence.basicDependencies', index=11,
+      number=8, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='collapsedDependencies', full_name='edu.stanford.nlp.pipeline.Sentence.collapsedDependencies', index=12,
+      number=9, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='collapsedCCProcessedDependencies', full_name='edu.stanford.nlp.pipeline.Sentence.collapsedCCProcessedDependencies', index=13,
+      number=10, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='alternativeDependencies', full_name='edu.stanford.nlp.pipeline.Sentence.alternativeDependencies', index=14,
+      number=13, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='openieTriple', full_name='edu.stanford.nlp.pipeline.Sentence.openieTriple', index=15,
+      number=14, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='kbpTriple', full_name='edu.stanford.nlp.pipeline.Sentence.kbpTriple', index=16,
+      number=16, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='entailedSentence', full_name='edu.stanford.nlp.pipeline.Sentence.entailedSentence', index=17,
+      number=15, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='entailedClause', full_name='edu.stanford.nlp.pipeline.Sentence.entailedClause', index=18,
+      number=35, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='enhancedDependencies', full_name='edu.stanford.nlp.pipeline.Sentence.enhancedDependencies', index=19,
+      number=17, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='enhancedPlusPlusDependencies', full_name='edu.stanford.nlp.pipeline.Sentence.enhancedPlusPlusDependencies', index=20,
+      number=18, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='character', full_name='edu.stanford.nlp.pipeline.Sentence.character', index=21,
+      number=19, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='paragraph', full_name='edu.stanford.nlp.pipeline.Sentence.paragraph', index=22,
+      number=11, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='text', full_name='edu.stanford.nlp.pipeline.Sentence.text', index=23,
+      number=12, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='lineNumber', full_name='edu.stanford.nlp.pipeline.Sentence.lineNumber', index=24,
+      number=20, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='hasRelationAnnotations', full_name='edu.stanford.nlp.pipeline.Sentence.hasRelationAnnotations', index=25,
+      number=51, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='entity', full_name='edu.stanford.nlp.pipeline.Sentence.entity', index=26,
+      number=52, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='relation', full_name='edu.stanford.nlp.pipeline.Sentence.relation', index=27,
+      number=53, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='hasNumerizedTokensAnnotation', full_name='edu.stanford.nlp.pipeline.Sentence.hasNumerizedTokensAnnotation', index=28,
+      number=54, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mentions', full_name='edu.stanford.nlp.pipeline.Sentence.mentions', index=29,
+      number=55, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mentionsForCoref', full_name='edu.stanford.nlp.pipeline.Sentence.mentionsForCoref', index=30,
+      number=56, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='hasCorefMentionsAnnotation', full_name='edu.stanford.nlp.pipeline.Sentence.hasCorefMentionsAnnotation', index=31,
+      number=57, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sentenceID', full_name='edu.stanford.nlp.pipeline.Sentence.sentenceID', index=32,
+      number=58, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sectionDate', full_name='edu.stanford.nlp.pipeline.Sentence.sectionDate', index=33,
+      number=59, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sectionIndex', full_name='edu.stanford.nlp.pipeline.Sentence.sectionIndex', index=34,
+      number=60, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sectionName', full_name='edu.stanford.nlp.pipeline.Sentence.sectionName', index=35,
+      number=61, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sectionAuthor', full_name='edu.stanford.nlp.pipeline.Sentence.sectionAuthor', index=36,
+      number=62, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='docID', full_name='edu.stanford.nlp.pipeline.Sentence.docID', index=37,
+      number=63, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sectionQuoted', full_name='edu.stanford.nlp.pipeline.Sentence.sectionQuoted', index=38,
+      number=64, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='hasEntityMentionsAnnotation', full_name='edu.stanford.nlp.pipeline.Sentence.hasEntityMentionsAnnotation', index=39,
+      number=65, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='hasKBPTriplesAnnotation', full_name='edu.stanford.nlp.pipeline.Sentence.hasKBPTriplesAnnotation', index=40,
+      number=68, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='hasOpenieTriplesAnnotation', full_name='edu.stanford.nlp.pipeline.Sentence.hasOpenieTriplesAnnotation', index=41,
+      number=69, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='chapterIndex', full_name='edu.stanford.nlp.pipeline.Sentence.chapterIndex', index=42,
+      number=66, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='paragraphIndex', full_name='edu.stanford.nlp.pipeline.Sentence.paragraphIndex', index=43,
+      number=67, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='enhancedSentence', full_name='edu.stanford.nlp.pipeline.Sentence.enhancedSentence', index=44,
+      number=70, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=True,
+  syntax='proto2',
+  extension_ranges=[(100, 256), ],
+  oneofs=[
+  ],
+  serialized_start=785,
+  serialized_end=2782,
+)
+
+
+_TOKEN = _descriptor.Descriptor(
+  name='Token',
+  full_name='edu.stanford.nlp.pipeline.Token',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='word', full_name='edu.stanford.nlp.pipeline.Token.word', index=0,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='pos', full_name='edu.stanford.nlp.pipeline.Token.pos', index=1,
+      number=2, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='value', full_name='edu.stanford.nlp.pipeline.Token.value', index=2,
+      number=3, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='category', full_name='edu.stanford.nlp.pipeline.Token.category', index=3,
+      number=4, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='before', full_name='edu.stanford.nlp.pipeline.Token.before', index=4,
+      number=5, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='after', full_name='edu.stanford.nlp.pipeline.Token.after', index=5,
+      number=6, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='originalText', full_name='edu.stanford.nlp.pipeline.Token.originalText', index=6,
+      number=7, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='ner', full_name='edu.stanford.nlp.pipeline.Token.ner', index=7,
+      number=8, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='coarseNER', full_name='edu.stanford.nlp.pipeline.Token.coarseNER', index=8,
+      number=62, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='fineGrainedNER', full_name='edu.stanford.nlp.pipeline.Token.fineGrainedNER', index=9,
+      number=63, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='nerLabelProbs', full_name='edu.stanford.nlp.pipeline.Token.nerLabelProbs', index=10,
+      number=66, type=9, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='normalizedNER', full_name='edu.stanford.nlp.pipeline.Token.normalizedNER', index=11,
+      number=9, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='lemma', full_name='edu.stanford.nlp.pipeline.Token.lemma', index=12,
+      number=10, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='beginChar', full_name='edu.stanford.nlp.pipeline.Token.beginChar', index=13,
+      number=11, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='endChar', full_name='edu.stanford.nlp.pipeline.Token.endChar', index=14,
+      number=12, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='utterance', full_name='edu.stanford.nlp.pipeline.Token.utterance', index=15,
+      number=13, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='speaker', full_name='edu.stanford.nlp.pipeline.Token.speaker', index=16,
+      number=14, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='beginIndex', full_name='edu.stanford.nlp.pipeline.Token.beginIndex', index=17,
+      number=15, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='endIndex', full_name='edu.stanford.nlp.pipeline.Token.endIndex', index=18,
+      number=16, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='tokenBeginIndex', full_name='edu.stanford.nlp.pipeline.Token.tokenBeginIndex', index=19,
+      number=17, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='tokenEndIndex', full_name='edu.stanford.nlp.pipeline.Token.tokenEndIndex', index=20,
+      number=18, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='timexValue', full_name='edu.stanford.nlp.pipeline.Token.timexValue', index=21,
+      number=19, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='hasXmlContext', full_name='edu.stanford.nlp.pipeline.Token.hasXmlContext', index=22,
+      number=21, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='xmlContext', full_name='edu.stanford.nlp.pipeline.Token.xmlContext', index=23,
+      number=22, type=9, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='corefClusterID', full_name='edu.stanford.nlp.pipeline.Token.corefClusterID', index=24,
+      number=23, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='answer', full_name='edu.stanford.nlp.pipeline.Token.answer', index=25,
+      number=24, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='headWordIndex', full_name='edu.stanford.nlp.pipeline.Token.headWordIndex', index=26,
+      number=26, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='operator', full_name='edu.stanford.nlp.pipeline.Token.operator', index=27,
+      number=27, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='polarity', full_name='edu.stanford.nlp.pipeline.Token.polarity', index=28,
+      number=28, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='polarity_dir', full_name='edu.stanford.nlp.pipeline.Token.polarity_dir', index=29,
+      number=39, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='span', full_name='edu.stanford.nlp.pipeline.Token.span', index=30,
+      number=29, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sentiment', full_name='edu.stanford.nlp.pipeline.Token.sentiment', index=31,
+      number=30, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='quotationIndex', full_name='edu.stanford.nlp.pipeline.Token.quotationIndex', index=32,
+      number=31, type=5, cpp_type=1, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='conllUFeatures', full_name='edu.stanford.nlp.pipeline.Token.conllUFeatures', index=33,
+      number=32, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='coarseTag', full_name='edu.stanford.nlp.pipeline.Token.coarseTag', index=34,
+      number=33, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='conllUTokenSpan', full_name='edu.stanford.nlp.pipeline.Token.conllUTokenSpan', index=35,
+      number=34, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='conllUMisc', full_name='edu.stanford.nlp.pipeline.Token.conllUMisc', index=36,
+      number=35, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='conllUSecondaryDeps', full_name='edu.stanford.nlp.pipeline.Token.conllUSecondaryDeps', index=37,
+      number=36, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='wikipediaEntity', full_name='edu.stanford.nlp.pipeline.Token.wikipediaEntity', index=38,
+      number=37, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='isNewline', full_name='edu.stanford.nlp.pipeline.Token.isNewline', index=39,
+      number=38, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='gender', full_name='edu.stanford.nlp.pipeline.Token.gender', index=40,
+      number=51, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='trueCase', full_name='edu.stanford.nlp.pipeline.Token.trueCase', index=41,
+      number=52, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='trueCaseText', full_name='edu.stanford.nlp.pipeline.Token.trueCaseText', index=42,
+      number=53, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='chineseChar', full_name='edu.stanford.nlp.pipeline.Token.chineseChar', index=43,
+      number=54, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='chineseSeg', full_name='edu.stanford.nlp.pipeline.Token.chineseSeg', index=44,
+      number=55, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='chineseXMLChar', full_name='edu.stanford.nlp.pipeline.Token.chineseXMLChar', index=45,
+      number=60, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='arabicSeg', full_name='edu.stanford.nlp.pipeline.Token.arabicSeg', index=46,
+      number=76, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sectionName', full_name='edu.stanford.nlp.pipeline.Token.sectionName', index=47,
+      number=56, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sectionAuthor', full_name='edu.stanford.nlp.pipeline.Token.sectionAuthor', index=48,
+      number=57, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sectionDate', full_name='edu.stanford.nlp.pipeline.Token.sectionDate', index=49,
+      number=58, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sectionEndLabel', full_name='edu.stanford.nlp.pipeline.Token.sectionEndLabel', index=50,
+      number=59, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='parent', full_name='edu.stanford.nlp.pipeline.Token.parent', index=51,
+      number=61, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='corefMentionIndex', full_name='edu.stanford.nlp.pipeline.Token.corefMentionIndex', index=52,
+      number=64, type=13, cpp_type=3, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='entityMentionIndex', full_name='edu.stanford.nlp.pipeline.Token.entityMentionIndex', index=53,
+      number=65, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='isMWT', full_name='edu.stanford.nlp.pipeline.Token.isMWT', index=54,
+      number=67, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='isFirstMWT', full_name='edu.stanford.nlp.pipeline.Token.isFirstMWT', index=55,
+      number=68, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mwtText', full_name='edu.stanford.nlp.pipeline.Token.mwtText', index=56,
+      number=69, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='numericValue', full_name='edu.stanford.nlp.pipeline.Token.numericValue', index=57,
+      number=70, type=4, cpp_type=4, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='numericType', full_name='edu.stanford.nlp.pipeline.Token.numericType', index=58,
+      number=71, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='numericCompositeValue', full_name='edu.stanford.nlp.pipeline.Token.numericCompositeValue', index=59,
+      number=72, type=4, cpp_type=4, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='numericCompositeType', full_name='edu.stanford.nlp.pipeline.Token.numericCompositeType', index=60,
+      number=73, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='codepointOffsetBegin', full_name='edu.stanford.nlp.pipeline.Token.codepointOffsetBegin', index=61,
+      number=74, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='codepointOffsetEnd', full_name='edu.stanford.nlp.pipeline.Token.codepointOffsetEnd', index=62,
+      number=75, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=True,
+  syntax='proto2',
+  extension_ranges=[(100, 256), ],
+  oneofs=[
+  ],
+  serialized_start=2785,
+  serialized_end=4366,
+)
+
+
+_QUOTE = _descriptor.Descriptor(
+  name='Quote',
+  full_name='edu.stanford.nlp.pipeline.Quote',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='text', full_name='edu.stanford.nlp.pipeline.Quote.text', index=0,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='begin', full_name='edu.stanford.nlp.pipeline.Quote.begin', index=1,
+      number=2, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='end', full_name='edu.stanford.nlp.pipeline.Quote.end', index=2,
+      number=3, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sentenceBegin', full_name='edu.stanford.nlp.pipeline.Quote.sentenceBegin', index=3,
+      number=5, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sentenceEnd', full_name='edu.stanford.nlp.pipeline.Quote.sentenceEnd', index=4,
+      number=6, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='tokenBegin', full_name='edu.stanford.nlp.pipeline.Quote.tokenBegin', index=5,
+      number=7, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='tokenEnd', full_name='edu.stanford.nlp.pipeline.Quote.tokenEnd', index=6,
+      number=8, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='docid', full_name='edu.stanford.nlp.pipeline.Quote.docid', index=7,
+      number=9, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='index', full_name='edu.stanford.nlp.pipeline.Quote.index', index=8,
+      number=10, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='author', full_name='edu.stanford.nlp.pipeline.Quote.author', index=9,
+      number=11, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mention', full_name='edu.stanford.nlp.pipeline.Quote.mention', index=10,
+      number=12, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mentionBegin', full_name='edu.stanford.nlp.pipeline.Quote.mentionBegin', index=11,
+      number=13, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mentionEnd', full_name='edu.stanford.nlp.pipeline.Quote.mentionEnd', index=12,
+      number=14, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mentionType', full_name='edu.stanford.nlp.pipeline.Quote.mentionType', index=13,
+      number=15, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mentionSieve', full_name='edu.stanford.nlp.pipeline.Quote.mentionSieve', index=14,
+      number=16, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='speaker', full_name='edu.stanford.nlp.pipeline.Quote.speaker', index=15,
+      number=17, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='speakerSieve', full_name='edu.stanford.nlp.pipeline.Quote.speakerSieve', index=16,
+      number=18, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='canonicalMention', full_name='edu.stanford.nlp.pipeline.Quote.canonicalMention', index=17,
+      number=19, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='canonicalMentionBegin', full_name='edu.stanford.nlp.pipeline.Quote.canonicalMentionBegin', index=18,
+      number=20, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='canonicalMentionEnd', full_name='edu.stanford.nlp.pipeline.Quote.canonicalMentionEnd', index=19,
+      number=21, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='attributionDependencyGraph', full_name='edu.stanford.nlp.pipeline.Quote.attributionDependencyGraph', index=20,
+      number=22, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=4369,
+  serialized_end=4853,
+)
+
+
+_PARSETREE = _descriptor.Descriptor(
+  name='ParseTree',
+  full_name='edu.stanford.nlp.pipeline.ParseTree',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='child', full_name='edu.stanford.nlp.pipeline.ParseTree.child', index=0,
+      number=1, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='value', full_name='edu.stanford.nlp.pipeline.ParseTree.value', index=1,
+      number=2, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='yieldBeginIndex', full_name='edu.stanford.nlp.pipeline.ParseTree.yieldBeginIndex', index=2,
+      number=3, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='yieldEndIndex', full_name='edu.stanford.nlp.pipeline.ParseTree.yieldEndIndex', index=3,
+      number=4, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='score', full_name='edu.stanford.nlp.pipeline.ParseTree.score', index=4,
+      number=5, type=1, cpp_type=5, label=1,
+      has_default_value=False, default_value=float(0),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sentiment', full_name='edu.stanford.nlp.pipeline.ParseTree.sentiment', index=5,
+      number=6, type=14, cpp_type=8, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=4856,
+  serialized_end=5055,
+)
+
+
+_DEPENDENCYGRAPH_NODE = _descriptor.Descriptor(
+  name='Node',
+  full_name='edu.stanford.nlp.pipeline.DependencyGraph.Node',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='sentenceIndex', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Node.sentenceIndex', index=0,
+      number=1, type=13, cpp_type=3, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='index', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Node.index', index=1,
+      number=2, type=13, cpp_type=3, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='copyAnnotation', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Node.copyAnnotation', index=2,
+      number=3, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=5221,
+  serialized_end=5289,
+)
+
+_DEPENDENCYGRAPH_EDGE = _descriptor.Descriptor(
+  name='Edge',
+  full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='source', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge.source', index=0,
+      number=1, type=13, cpp_type=3, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='target', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge.target', index=1,
+      number=2, type=13, cpp_type=3, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='dep', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge.dep', index=2,
+      number=3, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='isExtra', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge.isExtra', index=3,
+      number=4, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sourceCopy', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge.sourceCopy', index=4,
+      number=5, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='targetCopy', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge.targetCopy', index=5,
+      number=6, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='language', full_name='edu.stanford.nlp.pipeline.DependencyGraph.Edge.language', index=6,
+      number=7, type=14, cpp_type=8, label=1,
+      has_default_value=True, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=5292,
+  serialized_end=5464,
+)
+
+_DEPENDENCYGRAPH = _descriptor.Descriptor(
+  name='DependencyGraph',
+  full_name='edu.stanford.nlp.pipeline.DependencyGraph',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='node', full_name='edu.stanford.nlp.pipeline.DependencyGraph.node', index=0,
+      number=1, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='edge', full_name='edu.stanford.nlp.pipeline.DependencyGraph.edge', index=1,
+      number=2, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='root', full_name='edu.stanford.nlp.pipeline.DependencyGraph.root', index=2,
+      number=3, type=13, cpp_type=3, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=b'\020\001', file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[_DEPENDENCYGRAPH_NODE, _DEPENDENCYGRAPH_EDGE, ],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=5058,
+  serialized_end=5464,
+)
+
+
+_COREFCHAIN_COREFMENTION = _descriptor.Descriptor(
+  name='CorefMention',
+  full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='mentionID', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.mentionID', index=0,
+      number=1, type=5, cpp_type=1, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mentionType', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.mentionType', index=1,
+      number=2, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='number', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.number', index=2,
+      number=3, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='gender', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.gender', index=3,
+      number=4, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='animacy', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.animacy', index=4,
+      number=5, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='beginIndex', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.beginIndex', index=5,
+      number=6, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='endIndex', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.endIndex', index=6,
+      number=7, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='headIndex', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.headIndex', index=7,
+      number=9, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sentenceIndex', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.sentenceIndex', index=8,
+      number=10, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='position', full_name='edu.stanford.nlp.pipeline.CorefChain.CorefMention.position', index=9,
+      number=11, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=5592,
+  serialized_end=5793,
+)
+
+_COREFCHAIN = _descriptor.Descriptor(
+  name='CorefChain',
+  full_name='edu.stanford.nlp.pipeline.CorefChain',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='chainID', full_name='edu.stanford.nlp.pipeline.CorefChain.chainID', index=0,
+      number=1, type=5, cpp_type=1, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mention', full_name='edu.stanford.nlp.pipeline.CorefChain.mention', index=1,
+      number=2, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='representative', full_name='edu.stanford.nlp.pipeline.CorefChain.representative', index=2,
+      number=3, type=13, cpp_type=3, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[_COREFCHAIN_COREFMENTION, ],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=5467,
+  serialized_end=5793,
+)
+
+
+_MENTION = _descriptor.Descriptor(
+  name='Mention',
+  full_name='edu.stanford.nlp.pipeline.Mention',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='mentionID', full_name='edu.stanford.nlp.pipeline.Mention.mentionID', index=0,
+      number=1, type=5, cpp_type=1, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mentionType', full_name='edu.stanford.nlp.pipeline.Mention.mentionType', index=1,
+      number=2, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='number', full_name='edu.stanford.nlp.pipeline.Mention.number', index=2,
+      number=3, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='gender', full_name='edu.stanford.nlp.pipeline.Mention.gender', index=3,
+      number=4, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='animacy', full_name='edu.stanford.nlp.pipeline.Mention.animacy', index=4,
+      number=5, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='person', full_name='edu.stanford.nlp.pipeline.Mention.person', index=5,
+      number=6, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='startIndex', full_name='edu.stanford.nlp.pipeline.Mention.startIndex', index=6,
+      number=7, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='endIndex', full_name='edu.stanford.nlp.pipeline.Mention.endIndex', index=7,
+      number=9, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='headIndex', full_name='edu.stanford.nlp.pipeline.Mention.headIndex', index=8,
+      number=10, type=5, cpp_type=1, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='headString', full_name='edu.stanford.nlp.pipeline.Mention.headString', index=9,
+      number=11, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='nerString', full_name='edu.stanford.nlp.pipeline.Mention.nerString', index=10,
+      number=12, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='originalRef', full_name='edu.stanford.nlp.pipeline.Mention.originalRef', index=11,
+      number=13, type=5, cpp_type=1, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='goldCorefClusterID', full_name='edu.stanford.nlp.pipeline.Mention.goldCorefClusterID', index=12,
+      number=14, type=5, cpp_type=1, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='corefClusterID', full_name='edu.stanford.nlp.pipeline.Mention.corefClusterID', index=13,
+      number=15, type=5, cpp_type=1, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mentionNum', full_name='edu.stanford.nlp.pipeline.Mention.mentionNum', index=14,
+      number=16, type=5, cpp_type=1, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sentNum', full_name='edu.stanford.nlp.pipeline.Mention.sentNum', index=15,
+      number=17, type=5, cpp_type=1, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='utter', full_name='edu.stanford.nlp.pipeline.Mention.utter', index=16,
+      number=18, type=5, cpp_type=1, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='paragraph', full_name='edu.stanford.nlp.pipeline.Mention.paragraph', index=17,
+      number=19, type=5, cpp_type=1, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='isSubject', full_name='edu.stanford.nlp.pipeline.Mention.isSubject', index=18,
+      number=20, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='isDirectObject', full_name='edu.stanford.nlp.pipeline.Mention.isDirectObject', index=19,
+      number=21, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='isIndirectObject', full_name='edu.stanford.nlp.pipeline.Mention.isIndirectObject', index=20,
+      number=22, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='isPrepositionObject', full_name='edu.stanford.nlp.pipeline.Mention.isPrepositionObject', index=21,
+      number=23, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='hasTwin', full_name='edu.stanford.nlp.pipeline.Mention.hasTwin', index=22,
+      number=24, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='generic', full_name='edu.stanford.nlp.pipeline.Mention.generic', index=23,
+      number=25, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='isSingleton', full_name='edu.stanford.nlp.pipeline.Mention.isSingleton', index=24,
+      number=26, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='hasBasicDependency', full_name='edu.stanford.nlp.pipeline.Mention.hasBasicDependency', index=25,
+      number=27, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='hasEnhancedDepenedncy', full_name='edu.stanford.nlp.pipeline.Mention.hasEnhancedDepenedncy', index=26,
+      number=28, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='hasContextParseTree', full_name='edu.stanford.nlp.pipeline.Mention.hasContextParseTree', index=27,
+      number=29, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='headIndexedWord', full_name='edu.stanford.nlp.pipeline.Mention.headIndexedWord', index=28,
+      number=30, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='dependingVerb', full_name='edu.stanford.nlp.pipeline.Mention.dependingVerb', index=29,
+      number=31, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='headWord', full_name='edu.stanford.nlp.pipeline.Mention.headWord', index=30,
+      number=32, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='speakerInfo', full_name='edu.stanford.nlp.pipeline.Mention.speakerInfo', index=31,
+      number=33, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sentenceWords', full_name='edu.stanford.nlp.pipeline.Mention.sentenceWords', index=32,
+      number=50, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='originalSpan', full_name='edu.stanford.nlp.pipeline.Mention.originalSpan', index=33,
+      number=51, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='dependents', full_name='edu.stanford.nlp.pipeline.Mention.dependents', index=34,
+      number=52, type=9, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='preprocessedTerms', full_name='edu.stanford.nlp.pipeline.Mention.preprocessedTerms', index=35,
+      number=53, type=9, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='appositions', full_name='edu.stanford.nlp.pipeline.Mention.appositions', index=36,
+      number=54, type=5, cpp_type=1, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='predicateNominatives', full_name='edu.stanford.nlp.pipeline.Mention.predicateNominatives', index=37,
+      number=55, type=5, cpp_type=1, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='relativePronouns', full_name='edu.stanford.nlp.pipeline.Mention.relativePronouns', index=38,
+      number=56, type=5, cpp_type=1, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='listMembers', full_name='edu.stanford.nlp.pipeline.Mention.listMembers', index=39,
+      number=57, type=5, cpp_type=1, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='belongToLists', full_name='edu.stanford.nlp.pipeline.Mention.belongToLists', index=40,
+      number=58, type=5, cpp_type=1, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=5796,
+  serialized_end=6931,
+)
+
+
+_INDEXEDWORD = _descriptor.Descriptor(
+  name='IndexedWord',
+  full_name='edu.stanford.nlp.pipeline.IndexedWord',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='sentenceNum', full_name='edu.stanford.nlp.pipeline.IndexedWord.sentenceNum', index=0,
+      number=1, type=5, cpp_type=1, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='tokenIndex', full_name='edu.stanford.nlp.pipeline.IndexedWord.tokenIndex', index=1,
+      number=2, type=5, cpp_type=1, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='docID', full_name='edu.stanford.nlp.pipeline.IndexedWord.docID', index=2,
+      number=3, type=5, cpp_type=1, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='copyCount', full_name='edu.stanford.nlp.pipeline.IndexedWord.copyCount', index=3,
+      number=4, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=6933,
+  serialized_end=7021,
+)
+
+
+_SPEAKERINFO = _descriptor.Descriptor(
+  name='SpeakerInfo',
+  full_name='edu.stanford.nlp.pipeline.SpeakerInfo',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='speakerName', full_name='edu.stanford.nlp.pipeline.SpeakerInfo.speakerName', index=0,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mentions', full_name='edu.stanford.nlp.pipeline.SpeakerInfo.mentions', index=1,
+      number=2, type=5, cpp_type=1, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=7023,
+  serialized_end=7075,
+)
+
+
+_SPAN = _descriptor.Descriptor(
+  name='Span',
+  full_name='edu.stanford.nlp.pipeline.Span',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='begin', full_name='edu.stanford.nlp.pipeline.Span.begin', index=0,
+      number=1, type=13, cpp_type=3, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='end', full_name='edu.stanford.nlp.pipeline.Span.end', index=1,
+      number=2, type=13, cpp_type=3, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=7077,
+  serialized_end=7111,
+)
+
+
+_TIMEX = _descriptor.Descriptor(
+  name='Timex',
+  full_name='edu.stanford.nlp.pipeline.Timex',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='value', full_name='edu.stanford.nlp.pipeline.Timex.value', index=0,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='altValue', full_name='edu.stanford.nlp.pipeline.Timex.altValue', index=1,
+      number=2, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='text', full_name='edu.stanford.nlp.pipeline.Timex.text', index=2,
+      number=3, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='type', full_name='edu.stanford.nlp.pipeline.Timex.type', index=3,
+      number=4, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='tid', full_name='edu.stanford.nlp.pipeline.Timex.tid', index=4,
+      number=5, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='beginPoint', full_name='edu.stanford.nlp.pipeline.Timex.beginPoint', index=5,
+      number=6, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='endPoint', full_name='edu.stanford.nlp.pipeline.Timex.endPoint', index=6,
+      number=7, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=7113,
+  serialized_end=7232,
+)
+
+
+_ENTITY = _descriptor.Descriptor(
+  name='Entity',
+  full_name='edu.stanford.nlp.pipeline.Entity',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='headStart', full_name='edu.stanford.nlp.pipeline.Entity.headStart', index=0,
+      number=6, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='headEnd', full_name='edu.stanford.nlp.pipeline.Entity.headEnd', index=1,
+      number=7, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='mentionType', full_name='edu.stanford.nlp.pipeline.Entity.mentionType', index=2,
+      number=8, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='normalizedName', full_name='edu.stanford.nlp.pipeline.Entity.normalizedName', index=3,
+      number=9, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='headTokenIndex', full_name='edu.stanford.nlp.pipeline.Entity.headTokenIndex', index=4,
+      number=10, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='corefID', full_name='edu.stanford.nlp.pipeline.Entity.corefID', index=5,
+      number=11, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='objectID', full_name='edu.stanford.nlp.pipeline.Entity.objectID', index=6,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='extentStart', full_name='edu.stanford.nlp.pipeline.Entity.extentStart', index=7,
+      number=2, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='extentEnd', full_name='edu.stanford.nlp.pipeline.Entity.extentEnd', index=8,
+      number=3, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='type', full_name='edu.stanford.nlp.pipeline.Entity.type', index=9,
+      number=4, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='subtype', full_name='edu.stanford.nlp.pipeline.Entity.subtype', index=10,
+      number=5, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=7235,
+  serialized_end=7454,
+)
+
+
+_RELATION = _descriptor.Descriptor(
+  name='Relation',
+  full_name='edu.stanford.nlp.pipeline.Relation',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='argName', full_name='edu.stanford.nlp.pipeline.Relation.argName', index=0,
+      number=6, type=9, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='arg', full_name='edu.stanford.nlp.pipeline.Relation.arg', index=1,
+      number=7, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='signature', full_name='edu.stanford.nlp.pipeline.Relation.signature', index=2,
+      number=8, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='objectID', full_name='edu.stanford.nlp.pipeline.Relation.objectID', index=3,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='extentStart', full_name='edu.stanford.nlp.pipeline.Relation.extentStart', index=4,
+      number=2, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='extentEnd', full_name='edu.stanford.nlp.pipeline.Relation.extentEnd', index=5,
+      number=3, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='type', full_name='edu.stanford.nlp.pipeline.Relation.type', index=6,
+      number=4, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='subtype', full_name='edu.stanford.nlp.pipeline.Relation.subtype', index=7,
+      number=5, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=7457,
+  serialized_end=7640,
+)
+
+
+_OPERATOR = _descriptor.Descriptor(
+  name='Operator',
+  full_name='edu.stanford.nlp.pipeline.Operator',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='name', full_name='edu.stanford.nlp.pipeline.Operator.name', index=0,
+      number=1, type=9, cpp_type=9, label=2,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='quantifierSpanBegin', full_name='edu.stanford.nlp.pipeline.Operator.quantifierSpanBegin', index=1,
+      number=2, type=5, cpp_type=1, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='quantifierSpanEnd', full_name='edu.stanford.nlp.pipeline.Operator.quantifierSpanEnd', index=2,
+      number=3, type=5, cpp_type=1, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='subjectSpanBegin', full_name='edu.stanford.nlp.pipeline.Operator.subjectSpanBegin', index=3,
+      number=4, type=5, cpp_type=1, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='subjectSpanEnd', full_name='edu.stanford.nlp.pipeline.Operator.subjectSpanEnd', index=4,
+      number=5, type=5, cpp_type=1, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='objectSpanBegin', full_name='edu.stanford.nlp.pipeline.Operator.objectSpanBegin', index=5,
+      number=6, type=5, cpp_type=1, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='objectSpanEnd', full_name='edu.stanford.nlp.pipeline.Operator.objectSpanEnd', index=6,
+      number=7, type=5, cpp_type=1, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=7643,
+  serialized_end=7821,
+)
+
+
+_POLARITY = _descriptor.Descriptor(
+  name='Polarity',
+  full_name='edu.stanford.nlp.pipeline.Polarity',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='projectEquivalence', full_name='edu.stanford.nlp.pipeline.Polarity.projectEquivalence', index=0,
+      number=1, type=14, cpp_type=8, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='projectForwardEntailment', full_name='edu.stanford.nlp.pipeline.Polarity.projectForwardEntailment', index=1,
+      number=2, type=14, cpp_type=8, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='projectReverseEntailment', full_name='edu.stanford.nlp.pipeline.Polarity.projectReverseEntailment', index=2,
+      number=3, type=14, cpp_type=8, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='projectNegation', full_name='edu.stanford.nlp.pipeline.Polarity.projectNegation', index=3,
+      number=4, type=14, cpp_type=8, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='projectAlternation', full_name='edu.stanford.nlp.pipeline.Polarity.projectAlternation', index=4,
+      number=5, type=14, cpp_type=8, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='projectCover', full_name='edu.stanford.nlp.pipeline.Polarity.projectCover', index=5,
+      number=6, type=14, cpp_type=8, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='projectIndependence', full_name='edu.stanford.nlp.pipeline.Polarity.projectIndependence', index=6,
+      number=7, type=14, cpp_type=8, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=7824,
+  serialized_end=8377,
+)
+
+
+_NERMENTION = _descriptor.Descriptor(
+  name='NERMention',
+  full_name='edu.stanford.nlp.pipeline.NERMention',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='sentenceIndex', full_name='edu.stanford.nlp.pipeline.NERMention.sentenceIndex', index=0,
+      number=1, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='tokenStartInSentenceInclusive', full_name='edu.stanford.nlp.pipeline.NERMention.tokenStartInSentenceInclusive', index=1,
+      number=2, type=13, cpp_type=3, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='tokenEndInSentenceExclusive', full_name='edu.stanford.nlp.pipeline.NERMention.tokenEndInSentenceExclusive', index=2,
+      number=3, type=13, cpp_type=3, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='ner', full_name='edu.stanford.nlp.pipeline.NERMention.ner', index=3,
+      number=4, type=9, cpp_type=9, label=2,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='normalizedNER', full_name='edu.stanford.nlp.pipeline.NERMention.normalizedNER', index=4,
+      number=5, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='entityType', full_name='edu.stanford.nlp.pipeline.NERMention.entityType', index=5,
+      number=6, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='timex', full_name='edu.stanford.nlp.pipeline.NERMention.timex', index=6,
+      number=7, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='wikipediaEntity', full_name='edu.stanford.nlp.pipeline.NERMention.wikipediaEntity', index=7,
+      number=8, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='gender', full_name='edu.stanford.nlp.pipeline.NERMention.gender', index=8,
+      number=9, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='entityMentionIndex', full_name='edu.stanford.nlp.pipeline.NERMention.entityMentionIndex', index=9,
+      number=10, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='canonicalEntityMentionIndex', full_name='edu.stanford.nlp.pipeline.NERMention.canonicalEntityMentionIndex', index=10,
+      number=11, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='entityMentionText', full_name='edu.stanford.nlp.pipeline.NERMention.entityMentionText', index=11,
+      number=12, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=8380,
+  serialized_end=8729,
+)
+
+
+_SENTENCEFRAGMENT = _descriptor.Descriptor(
+  name='SentenceFragment',
+  full_name='edu.stanford.nlp.pipeline.SentenceFragment',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='tokenIndex', full_name='edu.stanford.nlp.pipeline.SentenceFragment.tokenIndex', index=0,
+      number=1, type=13, cpp_type=3, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='root', full_name='edu.stanford.nlp.pipeline.SentenceFragment.root', index=1,
+      number=2, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='assumedTruth', full_name='edu.stanford.nlp.pipeline.SentenceFragment.assumedTruth', index=2,
+      number=3, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='score', full_name='edu.stanford.nlp.pipeline.SentenceFragment.score', index=3,
+      number=4, type=1, cpp_type=5, label=1,
+      has_default_value=False, default_value=float(0),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=8731,
+  serialized_end=8820,
+)
+
+
+_TOKENLOCATION = _descriptor.Descriptor(
+  name='TokenLocation',
+  full_name='edu.stanford.nlp.pipeline.TokenLocation',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='sentenceIndex', full_name='edu.stanford.nlp.pipeline.TokenLocation.sentenceIndex', index=0,
+      number=1, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='tokenIndex', full_name='edu.stanford.nlp.pipeline.TokenLocation.tokenIndex', index=1,
+      number=2, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=8822,
+  serialized_end=8880,
+)
+
+
+_RELATIONTRIPLE = _descriptor.Descriptor(
+  name='RelationTriple',
+  full_name='edu.stanford.nlp.pipeline.RelationTriple',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='subject', full_name='edu.stanford.nlp.pipeline.RelationTriple.subject', index=0,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='relation', full_name='edu.stanford.nlp.pipeline.RelationTriple.relation', index=1,
+      number=2, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='object', full_name='edu.stanford.nlp.pipeline.RelationTriple.object', index=2,
+      number=3, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='confidence', full_name='edu.stanford.nlp.pipeline.RelationTriple.confidence', index=3,
+      number=4, type=1, cpp_type=5, label=1,
+      has_default_value=False, default_value=float(0),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='subjectTokens', full_name='edu.stanford.nlp.pipeline.RelationTriple.subjectTokens', index=4,
+      number=13, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='relationTokens', full_name='edu.stanford.nlp.pipeline.RelationTriple.relationTokens', index=5,
+      number=14, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='objectTokens', full_name='edu.stanford.nlp.pipeline.RelationTriple.objectTokens', index=6,
+      number=15, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='tree', full_name='edu.stanford.nlp.pipeline.RelationTriple.tree', index=7,
+      number=8, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='istmod', full_name='edu.stanford.nlp.pipeline.RelationTriple.istmod', index=8,
+      number=9, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='prefixBe', full_name='edu.stanford.nlp.pipeline.RelationTriple.prefixBe', index=9,
+      number=10, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='suffixBe', full_name='edu.stanford.nlp.pipeline.RelationTriple.suffixBe', index=10,
+      number=11, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='suffixOf', full_name='edu.stanford.nlp.pipeline.RelationTriple.suffixOf', index=11,
+      number=12, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=8883,
+  serialized_end=9293,
+)
+
+
+_MAPSTRINGSTRING = _descriptor.Descriptor(
+  name='MapStringString',
+  full_name='edu.stanford.nlp.pipeline.MapStringString',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='key', full_name='edu.stanford.nlp.pipeline.MapStringString.key', index=0,
+      number=1, type=9, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='value', full_name='edu.stanford.nlp.pipeline.MapStringString.value', index=1,
+      number=2, type=9, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=9295,
+  serialized_end=9340,
+)
+
+
+_MAPINTSTRING = _descriptor.Descriptor(
+  name='MapIntString',
+  full_name='edu.stanford.nlp.pipeline.MapIntString',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='key', full_name='edu.stanford.nlp.pipeline.MapIntString.key', index=0,
+      number=1, type=13, cpp_type=3, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='value', full_name='edu.stanford.nlp.pipeline.MapIntString.value', index=1,
+      number=2, type=9, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=9342,
+  serialized_end=9384,
+)
+
+
+_SECTION = _descriptor.Descriptor(
+  name='Section',
+  full_name='edu.stanford.nlp.pipeline.Section',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='charBegin', full_name='edu.stanford.nlp.pipeline.Section.charBegin', index=0,
+      number=1, type=13, cpp_type=3, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='charEnd', full_name='edu.stanford.nlp.pipeline.Section.charEnd', index=1,
+      number=2, type=13, cpp_type=3, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='author', full_name='edu.stanford.nlp.pipeline.Section.author', index=2,
+      number=3, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='sentenceIndexes', full_name='edu.stanford.nlp.pipeline.Section.sentenceIndexes', index=3,
+      number=4, type=13, cpp_type=3, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='datetime', full_name='edu.stanford.nlp.pipeline.Section.datetime', index=4,
+      number=5, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='quotes', full_name='edu.stanford.nlp.pipeline.Section.quotes', index=5,
+      number=6, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='authorCharBegin', full_name='edu.stanford.nlp.pipeline.Section.authorCharBegin', index=6,
+      number=7, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='authorCharEnd', full_name='edu.stanford.nlp.pipeline.Section.authorCharEnd', index=7,
+      number=8, type=13, cpp_type=3, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='xmlTag', full_name='edu.stanford.nlp.pipeline.Section.xmlTag', index=8,
+      number=9, type=11, cpp_type=10, label=2,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=9387,
+  serialized_end=9639,
+)
+
+
+_SEMGREXREQUEST_DEPENDENCIES = _descriptor.Descriptor(
+  name='Dependencies',
+  full_name='edu.stanford.nlp.pipeline.SemgrexRequest.Dependencies',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='token', full_name='edu.stanford.nlp.pipeline.SemgrexRequest.Dependencies.token', index=0,
+      number=1, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='graph', full_name='edu.stanford.nlp.pipeline.SemgrexRequest.Dependencies.graph', index=1,
+      number=2, type=11, cpp_type=10, label=2,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=9748,
+  serialized_end=9870,
+)
+
+_SEMGREXREQUEST = _descriptor.Descriptor(
+  name='SemgrexRequest',
+  full_name='edu.stanford.nlp.pipeline.SemgrexRequest',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='semgrex', full_name='edu.stanford.nlp.pipeline.SemgrexRequest.semgrex', index=0,
+      number=1, type=9, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='query', full_name='edu.stanford.nlp.pipeline.SemgrexRequest.query', index=1,
+      number=2, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[_SEMGREXREQUEST_DEPENDENCIES, ],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=9642,
+  serialized_end=9870,
+)
+
+
+_SEMGREXRESPONSE_NAMEDNODE = _descriptor.Descriptor(
+  name='NamedNode',
+  full_name='edu.stanford.nlp.pipeline.SemgrexResponse.NamedNode',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='name', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.NamedNode.name', index=0,
+      number=1, type=9, cpp_type=9, label=2,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='index', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.NamedNode.index', index=1,
+      number=2, type=5, cpp_type=1, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=9964,
+  serialized_end=10004,
+)
+
+_SEMGREXRESPONSE_NAMEDRELATION = _descriptor.Descriptor(
+  name='NamedRelation',
+  full_name='edu.stanford.nlp.pipeline.SemgrexResponse.NamedRelation',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='name', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.NamedRelation.name', index=0,
+      number=1, type=9, cpp_type=9, label=2,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='reln', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.NamedRelation.reln', index=1,
+      number=2, type=9, cpp_type=9, label=2,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=10006,
+  serialized_end=10049,
+)
+
+_SEMGREXRESPONSE_MATCH = _descriptor.Descriptor(
+  name='Match',
+  full_name='edu.stanford.nlp.pipeline.SemgrexResponse.Match',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='index', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.Match.index', index=0,
+      number=1, type=5, cpp_type=1, label=2,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='node', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.Match.node', index=1,
+      number=2, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+    _descriptor.FieldDescriptor(
+      name='reln', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.Match.reln', index=2,
+      number=3, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=10052,
+  serialized_end=10214,
+)
+
+_SEMGREXRESPONSE_SEMGREXRESULT = _descriptor.Descriptor(
+  name='SemgrexResult',
+  full_name='edu.stanford.nlp.pipeline.SemgrexResponse.SemgrexResult',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='match', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.SemgrexResult.match', index=0,
+      number=1, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=10216,
+  serialized_end=10296,
+)
+
+_SEMGREXRESPONSE_GRAPHRESULT = _descriptor.Descriptor(
+  name='GraphResult',
+  full_name='edu.stanford.nlp.pipeline.SemgrexResponse.GraphResult',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='result', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.GraphResult.result', index=0,
+      number=1, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=10298,
+  serialized_end=10385,
+)
+
+_SEMGREXRESPONSE = _descriptor.Descriptor(
+  name='SemgrexResponse',
+  full_name='edu.stanford.nlp.pipeline.SemgrexResponse',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='result', full_name='edu.stanford.nlp.pipeline.SemgrexResponse.result', index=0,
+      number=1, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR),
+  ],
+  extensions=[
+  ],
+  nested_types=[_SEMGREXRESPONSE_NAMEDNODE, _SEMGREXRESPONSE_NAMEDRELATION, _SEMGREXRESPONSE_MATCH, _SEMGREXRESPONSE_SEMGREXRESULT, _SEMGREXRESPONSE_GRAPHRESULT, ],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto2',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=9873,
+  serialized_end=10385,
+)
+
+_DOCUMENT.fields_by_name['sentence'].message_type = _SENTENCE
+_DOCUMENT.fields_by_name['corefChain'].message_type = _COREFCHAIN
+_DOCUMENT.fields_by_name['sentencelessToken'].message_type = _TOKEN
+_DOCUMENT.fields_by_name['character'].message_type = _TOKEN
+_DOCUMENT.fields_by_name['quote'].message_type = _QUOTE
+_DOCUMENT.fields_by_name['mentions'].message_type = _NERMENTION
+_DOCUMENT.fields_by_name['sections'].message_type = _SECTION
+_DOCUMENT.fields_by_name['mentionsForCoref'].message_type = _MENTION
+_SENTENCE.fields_by_name['token'].message_type = _TOKEN
+_SENTENCE.fields_by_name['parseTree'].message_type = _PARSETREE
+_SENTENCE.fields_by_name['binarizedParseTree'].message_type = _PARSETREE
+_SENTENCE.fields_by_name['annotatedParseTree'].message_type = _PARSETREE
+_SENTENCE.fields_by_name['kBestParseTrees'].message_type = _PARSETREE
+_SENTENCE.fields_by_name['basicDependencies'].message_type = _DEPENDENCYGRAPH
+_SENTENCE.fields_by_name['collapsedDependencies'].message_type = _DEPENDENCYGRAPH
+_SENTENCE.fields_by_name['collapsedCCProcessedDependencies'].message_type = _DEPENDENCYGRAPH
+_SENTENCE.fields_by_name['alternativeDependencies'].message_type = _DEPENDENCYGRAPH
+_SENTENCE.fields_by_name['openieTriple'].message_type = _RELATIONTRIPLE
+_SENTENCE.fields_by_name['kbpTriple'].message_type = _RELATIONTRIPLE
+_SENTENCE.fields_by_name['entailedSentence'].message_type = _SENTENCEFRAGMENT
+_SENTENCE.fields_by_name['entailedClause'].message_type = _SENTENCEFRAGMENT
+_SENTENCE.fields_by_name['enhancedDependencies'].message_type = _DEPENDENCYGRAPH
+_SENTENCE.fields_by_name['enhancedPlusPlusDependencies'].message_type = _DEPENDENCYGRAPH
+_SENTENCE.fields_by_name['character'].message_type = _TOKEN
+_SENTENCE.fields_by_name['entity'].message_type = _ENTITY
+_SENTENCE.fields_by_name['relation'].message_type = _RELATION
+_SENTENCE.fields_by_name['mentions'].message_type = _NERMENTION
+_SENTENCE.fields_by_name['mentionsForCoref'].message_type = _MENTION
+_SENTENCE.fields_by_name['enhancedSentence'].message_type = _SENTENCE
+_TOKEN.fields_by_name['timexValue'].message_type = _TIMEX
+_TOKEN.fields_by_name['operator'].message_type = _OPERATOR
+_TOKEN.fields_by_name['polarity'].message_type = _POLARITY
+_TOKEN.fields_by_name['span'].message_type = _SPAN
+_TOKEN.fields_by_name['conllUFeatures'].message_type = _MAPSTRINGSTRING
+_TOKEN.fields_by_name['conllUTokenSpan'].message_type = _SPAN
+_TOKEN.fields_by_name['conllUSecondaryDeps'].message_type = _MAPSTRINGSTRING
+_QUOTE.fields_by_name['attributionDependencyGraph'].message_type = _DEPENDENCYGRAPH
+_PARSETREE.fields_by_name['child'].message_type = _PARSETREE
+_PARSETREE.fields_by_name['sentiment'].enum_type = _SENTIMENT
+_DEPENDENCYGRAPH_NODE.containing_type = _DEPENDENCYGRAPH
+_DEPENDENCYGRAPH_EDGE.fields_by_name['language'].enum_type = _LANGUAGE
+_DEPENDENCYGRAPH_EDGE.containing_type = _DEPENDENCYGRAPH
+_DEPENDENCYGRAPH.fields_by_name['node'].message_type = _DEPENDENCYGRAPH_NODE
+_DEPENDENCYGRAPH.fields_by_name['edge'].message_type = _DEPENDENCYGRAPH_EDGE
+_COREFCHAIN_COREFMENTION.containing_type = _COREFCHAIN
+_COREFCHAIN.fields_by_name['mention'].message_type = _COREFCHAIN_COREFMENTION
+_MENTION.fields_by_name['headIndexedWord'].message_type = _INDEXEDWORD
+_MENTION.fields_by_name['dependingVerb'].message_type = _INDEXEDWORD
+_MENTION.fields_by_name['headWord'].message_type = _INDEXEDWORD
+_MENTION.fields_by_name['speakerInfo'].message_type = _SPEAKERINFO
+_MENTION.fields_by_name['sentenceWords'].message_type = _INDEXEDWORD
+_MENTION.fields_by_name['originalSpan'].message_type = _INDEXEDWORD
+_RELATION.fields_by_name['arg'].message_type = _ENTITY
+_POLARITY.fields_by_name['projectEquivalence'].enum_type = _NATURALLOGICRELATION
+_POLARITY.fields_by_name['projectForwardEntailment'].enum_type = _NATURALLOGICRELATION
+_POLARITY.fields_by_name['projectReverseEntailment'].enum_type = _NATURALLOGICRELATION
+_POLARITY.fields_by_name['projectNegation'].enum_type = _NATURALLOGICRELATION
+_POLARITY.fields_by_name['projectAlternation'].enum_type = _NATURALLOGICRELATION
+_POLARITY.fields_by_name['projectCover'].enum_type = _NATURALLOGICRELATION
+_POLARITY.fields_by_name['projectIndependence'].enum_type = _NATURALLOGICRELATION
+_NERMENTION.fields_by_name['timex'].message_type = _TIMEX
+_RELATIONTRIPLE.fields_by_name['subjectTokens'].message_type = _TOKENLOCATION
+_RELATIONTRIPLE.fields_by_name['relationTokens'].message_type = _TOKENLOCATION
+_RELATIONTRIPLE.fields_by_name['objectTokens'].message_type = _TOKENLOCATION
+_RELATIONTRIPLE.fields_by_name['tree'].message_type = _DEPENDENCYGRAPH
+_SECTION.fields_by_name['quotes'].message_type = _QUOTE
+_SECTION.fields_by_name['xmlTag'].message_type = _TOKEN
+_SEMGREXREQUEST_DEPENDENCIES.fields_by_name['token'].message_type = _TOKEN
+_SEMGREXREQUEST_DEPENDENCIES.fields_by_name['graph'].message_type = _DEPENDENCYGRAPH
+_SEMGREXREQUEST_DEPENDENCIES.containing_type = _SEMGREXREQUEST
+_SEMGREXREQUEST.fields_by_name['query'].message_type = _SEMGREXREQUEST_DEPENDENCIES
+_SEMGREXRESPONSE_NAMEDNODE.containing_type = _SEMGREXRESPONSE
+_SEMGREXRESPONSE_NAMEDRELATION.containing_type = _SEMGREXRESPONSE
+_SEMGREXRESPONSE_MATCH.fields_by_name['node'].message_type = _SEMGREXRESPONSE_NAMEDNODE
+_SEMGREXRESPONSE_MATCH.fields_by_name['reln'].message_type = _SEMGREXRESPONSE_NAMEDRELATION
+_SEMGREXRESPONSE_MATCH.containing_type = _SEMGREXRESPONSE
+_SEMGREXRESPONSE_SEMGREXRESULT.fields_by_name['match'].message_type = _SEMGREXRESPONSE_MATCH
+_SEMGREXRESPONSE_SEMGREXRESULT.containing_type = _SEMGREXRESPONSE
+_SEMGREXRESPONSE_GRAPHRESULT.fields_by_name['result'].message_type = _SEMGREXRESPONSE_SEMGREXRESULT
+_SEMGREXRESPONSE_GRAPHRESULT.containing_type = _SEMGREXRESPONSE
+_SEMGREXRESPONSE.fields_by_name['result'].message_type = _SEMGREXRESPONSE_GRAPHRESULT
+DESCRIPTOR.message_types_by_name['Document'] = _DOCUMENT
+DESCRIPTOR.message_types_by_name['Sentence'] = _SENTENCE
+DESCRIPTOR.message_types_by_name['Token'] = _TOKEN
+DESCRIPTOR.message_types_by_name['Quote'] = _QUOTE
+DESCRIPTOR.message_types_by_name['ParseTree'] = _PARSETREE
+DESCRIPTOR.message_types_by_name['DependencyGraph'] = _DEPENDENCYGRAPH
+DESCRIPTOR.message_types_by_name['CorefChain'] = _COREFCHAIN
+DESCRIPTOR.message_types_by_name['Mention'] = _MENTION
+DESCRIPTOR.message_types_by_name['IndexedWord'] = _INDEXEDWORD
+DESCRIPTOR.message_types_by_name['SpeakerInfo'] = _SPEAKERINFO
+DESCRIPTOR.message_types_by_name['Span'] = _SPAN
+DESCRIPTOR.message_types_by_name['Timex'] = _TIMEX
+DESCRIPTOR.message_types_by_name['Entity'] = _ENTITY
+DESCRIPTOR.message_types_by_name['Relation'] = _RELATION
+DESCRIPTOR.message_types_by_name['Operator'] = _OPERATOR
+DESCRIPTOR.message_types_by_name['Polarity'] = _POLARITY
+DESCRIPTOR.message_types_by_name['NERMention'] = _NERMENTION
+DESCRIPTOR.message_types_by_name['SentenceFragment'] = _SENTENCEFRAGMENT
+DESCRIPTOR.message_types_by_name['TokenLocation'] = _TOKENLOCATION
+DESCRIPTOR.message_types_by_name['RelationTriple'] = _RELATIONTRIPLE
+DESCRIPTOR.message_types_by_name['MapStringString'] = _MAPSTRINGSTRING
+DESCRIPTOR.message_types_by_name['MapIntString'] = _MAPINTSTRING
+DESCRIPTOR.message_types_by_name['Section'] = _SECTION
+DESCRIPTOR.message_types_by_name['SemgrexRequest'] = _SEMGREXREQUEST
+DESCRIPTOR.message_types_by_name['SemgrexResponse'] = _SEMGREXRESPONSE
+DESCRIPTOR.enum_types_by_name['Language'] = _LANGUAGE
+DESCRIPTOR.enum_types_by_name['Sentiment'] = _SENTIMENT
+DESCRIPTOR.enum_types_by_name['NaturalLogicRelation'] = _NATURALLOGICRELATION
+_sym_db.RegisterFileDescriptor(DESCRIPTOR)
+
+Document = _reflection.GeneratedProtocolMessageType('Document', (_message.Message,), {
+  'DESCRIPTOR' : _DOCUMENT,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Document)
+  })
+_sym_db.RegisterMessage(Document)
+
+Sentence = _reflection.GeneratedProtocolMessageType('Sentence', (_message.Message,), {
+  'DESCRIPTOR' : _SENTENCE,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Sentence)
+  })
+_sym_db.RegisterMessage(Sentence)
+
+Token = _reflection.GeneratedProtocolMessageType('Token', (_message.Message,), {
+  'DESCRIPTOR' : _TOKEN,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Token)
+  })
+_sym_db.RegisterMessage(Token)
+
+Quote = _reflection.GeneratedProtocolMessageType('Quote', (_message.Message,), {
+  'DESCRIPTOR' : _QUOTE,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Quote)
+  })
+_sym_db.RegisterMessage(Quote)
+
+ParseTree = _reflection.GeneratedProtocolMessageType('ParseTree', (_message.Message,), {
+  'DESCRIPTOR' : _PARSETREE,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.ParseTree)
+  })
+_sym_db.RegisterMessage(ParseTree)
+
+DependencyGraph = _reflection.GeneratedProtocolMessageType('DependencyGraph', (_message.Message,), {
+
+  'Node' : _reflection.GeneratedProtocolMessageType('Node', (_message.Message,), {
+    'DESCRIPTOR' : _DEPENDENCYGRAPH_NODE,
+    '__module__' : 'CoreNLP_pb2'
+    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.DependencyGraph.Node)
+    })
+  ,
+
+  'Edge' : _reflection.GeneratedProtocolMessageType('Edge', (_message.Message,), {
+    'DESCRIPTOR' : _DEPENDENCYGRAPH_EDGE,
+    '__module__' : 'CoreNLP_pb2'
+    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.DependencyGraph.Edge)
+    })
+  ,
+  'DESCRIPTOR' : _DEPENDENCYGRAPH,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.DependencyGraph)
+  })
+_sym_db.RegisterMessage(DependencyGraph)
+_sym_db.RegisterMessage(DependencyGraph.Node)
+_sym_db.RegisterMessage(DependencyGraph.Edge)
+
+CorefChain = _reflection.GeneratedProtocolMessageType('CorefChain', (_message.Message,), {
+
+  'CorefMention' : _reflection.GeneratedProtocolMessageType('CorefMention', (_message.Message,), {
+    'DESCRIPTOR' : _COREFCHAIN_COREFMENTION,
+    '__module__' : 'CoreNLP_pb2'
+    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.CorefChain.CorefMention)
+    })
+  ,
+  'DESCRIPTOR' : _COREFCHAIN,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.CorefChain)
+  })
+_sym_db.RegisterMessage(CorefChain)
+_sym_db.RegisterMessage(CorefChain.CorefMention)
+
+Mention = _reflection.GeneratedProtocolMessageType('Mention', (_message.Message,), {
+  'DESCRIPTOR' : _MENTION,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Mention)
+  })
+_sym_db.RegisterMessage(Mention)
+
+IndexedWord = _reflection.GeneratedProtocolMessageType('IndexedWord', (_message.Message,), {
+  'DESCRIPTOR' : _INDEXEDWORD,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.IndexedWord)
+  })
+_sym_db.RegisterMessage(IndexedWord)
+
+SpeakerInfo = _reflection.GeneratedProtocolMessageType('SpeakerInfo', (_message.Message,), {
+  'DESCRIPTOR' : _SPEAKERINFO,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SpeakerInfo)
+  })
+_sym_db.RegisterMessage(SpeakerInfo)
+
+Span = _reflection.GeneratedProtocolMessageType('Span', (_message.Message,), {
+  'DESCRIPTOR' : _SPAN,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Span)
+  })
+_sym_db.RegisterMessage(Span)
+
+Timex = _reflection.GeneratedProtocolMessageType('Timex', (_message.Message,), {
+  'DESCRIPTOR' : _TIMEX,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Timex)
+  })
+_sym_db.RegisterMessage(Timex)
+
+Entity = _reflection.GeneratedProtocolMessageType('Entity', (_message.Message,), {
+  'DESCRIPTOR' : _ENTITY,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Entity)
+  })
+_sym_db.RegisterMessage(Entity)
+
+Relation = _reflection.GeneratedProtocolMessageType('Relation', (_message.Message,), {
+  'DESCRIPTOR' : _RELATION,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Relation)
+  })
+_sym_db.RegisterMessage(Relation)
+
+Operator = _reflection.GeneratedProtocolMessageType('Operator', (_message.Message,), {
+  'DESCRIPTOR' : _OPERATOR,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Operator)
+  })
+_sym_db.RegisterMessage(Operator)
+
+Polarity = _reflection.GeneratedProtocolMessageType('Polarity', (_message.Message,), {
+  'DESCRIPTOR' : _POLARITY,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Polarity)
+  })
+_sym_db.RegisterMessage(Polarity)
+
+NERMention = _reflection.GeneratedProtocolMessageType('NERMention', (_message.Message,), {
+  'DESCRIPTOR' : _NERMENTION,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.NERMention)
+  })
+_sym_db.RegisterMessage(NERMention)
+
+SentenceFragment = _reflection.GeneratedProtocolMessageType('SentenceFragment', (_message.Message,), {
+  'DESCRIPTOR' : _SENTENCEFRAGMENT,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SentenceFragment)
+  })
+_sym_db.RegisterMessage(SentenceFragment)
+
+TokenLocation = _reflection.GeneratedProtocolMessageType('TokenLocation', (_message.Message,), {
+  'DESCRIPTOR' : _TOKENLOCATION,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.TokenLocation)
+  })
+_sym_db.RegisterMessage(TokenLocation)
+
+RelationTriple = _reflection.GeneratedProtocolMessageType('RelationTriple', (_message.Message,), {
+  'DESCRIPTOR' : _RELATIONTRIPLE,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.RelationTriple)
+  })
+_sym_db.RegisterMessage(RelationTriple)
+
+MapStringString = _reflection.GeneratedProtocolMessageType('MapStringString', (_message.Message,), {
+  'DESCRIPTOR' : _MAPSTRINGSTRING,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.MapStringString)
+  })
+_sym_db.RegisterMessage(MapStringString)
+
+MapIntString = _reflection.GeneratedProtocolMessageType('MapIntString', (_message.Message,), {
+  'DESCRIPTOR' : _MAPINTSTRING,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.MapIntString)
+  })
+_sym_db.RegisterMessage(MapIntString)
+
+Section = _reflection.GeneratedProtocolMessageType('Section', (_message.Message,), {
+  'DESCRIPTOR' : _SECTION,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Section)
+  })
+_sym_db.RegisterMessage(Section)
+
+SemgrexRequest = _reflection.GeneratedProtocolMessageType('SemgrexRequest', (_message.Message,), {
+
+  'Dependencies' : _reflection.GeneratedProtocolMessageType('Dependencies', (_message.Message,), {
+    'DESCRIPTOR' : _SEMGREXREQUEST_DEPENDENCIES,
+    '__module__' : 'CoreNLP_pb2'
+    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexRequest.Dependencies)
+    })
+  ,
+  'DESCRIPTOR' : _SEMGREXREQUEST,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexRequest)
+  })
+_sym_db.RegisterMessage(SemgrexRequest)
+_sym_db.RegisterMessage(SemgrexRequest.Dependencies)
+
+SemgrexResponse = _reflection.GeneratedProtocolMessageType('SemgrexResponse', (_message.Message,), {
+
+  'NamedNode' : _reflection.GeneratedProtocolMessageType('NamedNode', (_message.Message,), {
+    'DESCRIPTOR' : _SEMGREXRESPONSE_NAMEDNODE,
+    '__module__' : 'CoreNLP_pb2'
+    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexResponse.NamedNode)
+    })
+  ,
+
+  'NamedRelation' : _reflection.GeneratedProtocolMessageType('NamedRelation', (_message.Message,), {
+    'DESCRIPTOR' : _SEMGREXRESPONSE_NAMEDRELATION,
+    '__module__' : 'CoreNLP_pb2'
+    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexResponse.NamedRelation)
+    })
+  ,
+
+  'Match' : _reflection.GeneratedProtocolMessageType('Match', (_message.Message,), {
+    'DESCRIPTOR' : _SEMGREXRESPONSE_MATCH,
+    '__module__' : 'CoreNLP_pb2'
+    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexResponse.Match)
+    })
+  ,
+
+  'SemgrexResult' : _reflection.GeneratedProtocolMessageType('SemgrexResult', (_message.Message,), {
+    'DESCRIPTOR' : _SEMGREXRESPONSE_SEMGREXRESULT,
+    '__module__' : 'CoreNLP_pb2'
+    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexResponse.SemgrexResult)
+    })
+  ,
+
+  'GraphResult' : _reflection.GeneratedProtocolMessageType('GraphResult', (_message.Message,), {
+    'DESCRIPTOR' : _SEMGREXRESPONSE_GRAPHRESULT,
+    '__module__' : 'CoreNLP_pb2'
+    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexResponse.GraphResult)
+    })
+  ,
+  'DESCRIPTOR' : _SEMGREXRESPONSE,
+  '__module__' : 'CoreNLP_pb2'
+  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SemgrexResponse)
+  })
+_sym_db.RegisterMessage(SemgrexResponse)
+_sym_db.RegisterMessage(SemgrexResponse.NamedNode)
+_sym_db.RegisterMessage(SemgrexResponse.NamedRelation)
+_sym_db.RegisterMessage(SemgrexResponse.Match)
+_sym_db.RegisterMessage(SemgrexResponse.SemgrexResult)
+_sym_db.RegisterMessage(SemgrexResponse.GraphResult)
+
+
+DESCRIPTOR._options = None
+_DEPENDENCYGRAPH.fields_by_name['root']._options = None
+# @@protoc_insertion_point(module_scope)
```

### Comparing `classla-2.0/classla/protobuf/__init__.py` & `classla-2.1/classla/protobuf/__init__.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,52 +1,52 @@
-from __future__ import absolute_import
-
-from io import BytesIO
-import warnings
-
-from google.protobuf.internal.encoder import _EncodeVarint
-from google.protobuf.internal.decoder import _DecodeVarint
-from google.protobuf.message import DecodeError
-from .CoreNLP_pb2 import *
-
-def parseFromDelimitedString(obj, buf, offset=0):
-    """
-    Stanford CoreNLP uses the Java "writeDelimitedTo" function, which
-    writes the size (and offset) of the buffer before writing the object.
-    This function handles parsing this message starting from offset 0.
-
-    @returns how many bytes of @buf were consumed.
-    """
-    size, pos = _DecodeVarint(buf, offset)
-    try:
-        obj.ParseFromString(buf[offset+pos:offset+pos+size])
-    except DecodeError as e:
-        warnings.warn("Failed to decode a serialized output from CoreNLP server. An incomplete or empty object will be returned.", \
-            RuntimeWarning)
-    return pos+size
-
-def writeToDelimitedString(obj, stream=None):
-    """
-    Stanford CoreNLP uses the Java "writeDelimitedTo" function, which
-    writes the size (and offset) of the buffer before writing the object.
-    This function handles parsing this message starting from offset 0.
-
-    @returns how many bytes of @buf were consumed.
-    """
-    if stream is None:
-        stream = BytesIO()
-
-    _EncodeVarint(stream.write, obj.ByteSize(), True)
-    stream.write(obj.SerializeToString())
-    return stream
-
-def to_text(sentence):
-    """
-    Helper routine that converts a Sentence protobuf to a string from
-    its tokens.
-    """
-    text = ""
-    for i, tok in enumerate(sentence.token):
-        if i != 0:
-            text += tok.before
-        text += tok.word
-    return text
+from __future__ import absolute_import
+
+from io import BytesIO
+import warnings
+
+from google.protobuf.internal.encoder import _EncodeVarint
+from google.protobuf.internal.decoder import _DecodeVarint
+from google.protobuf.message import DecodeError
+from .CoreNLP_pb2 import *
+
+def parseFromDelimitedString(obj, buf, offset=0):
+    """
+    Stanford CoreNLP uses the Java "writeDelimitedTo" function, which
+    writes the size (and offset) of the buffer before writing the object.
+    This function handles parsing this message starting from offset 0.
+
+    @returns how many bytes of @buf were consumed.
+    """
+    size, pos = _DecodeVarint(buf, offset)
+    try:
+        obj.ParseFromString(buf[offset+pos:offset+pos+size])
+    except DecodeError as e:
+        warnings.warn("Failed to decode a serialized output from CoreNLP server. An incomplete or empty object will be returned.", \
+            RuntimeWarning)
+    return pos+size
+
+def writeToDelimitedString(obj, stream=None):
+    """
+    Stanford CoreNLP uses the Java "writeDelimitedTo" function, which
+    writes the size (and offset) of the buffer before writing the object.
+    This function handles parsing this message starting from offset 0.
+
+    @returns how many bytes of @buf were consumed.
+    """
+    if stream is None:
+        stream = BytesIO()
+
+    _EncodeVarint(stream.write, obj.ByteSize(), True)
+    stream.write(obj.SerializeToString())
+    return stream
+
+def to_text(sentence):
+    """
+    Helper routine that converts a Sentence protobuf to a string from
+    its tokens.
+    """
+    text = ""
+    for i, tok in enumerate(sentence.token):
+        if i != 0:
+            text += tok.before
+        text += tok.word
+    return text
```

### Comparing `classla-2.0/classla/resources/common.py` & `classla-2.1/classla/resources/common.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,447 +1,447 @@
-"""
-Common utilities for Stanza resources.
-"""
-
-import os
-import requests
-from tqdm import tqdm
-from pathlib import Path
-import json
-import hashlib
-import zipfile
-import shutil
-import logging
-
-from classla.utils.helper_func import make_table
-from classla.pipeline._constants import TOKENIZE, MWT, POS, LEMMA, DEPPARSE, \
-    NER, SENTIMENT
-from classla.pipeline.registry import PIPELINE_NAMES, PROCESSOR_VARIANTS
-from classla._version import __resources_version__
-
-logger = logging.getLogger('classla')
-
-# set home dir for default
-HOME_DIR = str(Path.home())
-STANFORDNLP_RESOURCES_URL = 'https://nlp.stanford.edu/software/stanza/stanza-resources/'
-CLASSLA_RESOURCES_GITHUB = 'https://raw.githubusercontent.com/clarinsi/classla-resources/'
-DEFAULT_RESOURCES_URL = os.getenv('CLASSLA_RESOURCES_URL', CLASSLA_RESOURCES_GITHUB + 'main')
-DEFAULT_RESOURCES_VERSION = os.getenv(
-    'CLASSLA_RESOURCES_VERSION',
-    __resources_version__
-)
-DEFAULT_MODEL_URL = os.getenv('CLASSLA_MODEL_URL', 'default')
-DEFAULT_MODEL_DIR = os.getenv(
-    'CLASSLA_RESOURCES_DIR',
-    os.path.join(HOME_DIR, 'classla_resources')
-)
-
-# given a language and models path, build a default configuration
-def build_default_config(resources, lang, dir, load_list):
-    default_config = {}
-    for item in load_list:
-        processor, package, dependencies = item
-
-        # handle case when processor variants are used
-        if package in PROCESSOR_VARIANTS[processor]:
-            default_config[f"{processor}_with_{package}"] = True
-        # handle case when identity is specified as lemmatizer
-        elif processor == LEMMA and package == 'identity':
-            default_config[f"{LEMMA}_use_identity"] = True
-        elif processor == TOKENIZE:
-            assert 'library' in resources[lang][processor][package], "Tokenizer processor in resources.json should include attribute `library` with value `reldi` or `obeliks`"
-            default_config[f"{processor}_library"] = resources[lang][processor][package]['library']
-            assert 'type' in resources[lang][processor][package], "Tokenizer processor in resources.json should include attribute `type` with value `standard` or `nonstandard`"
-            default_config[f"{processor}_type"] = resources[lang][processor][package]['type']
-        else:
-            if processor == NER:
-                default_config[f"{processor}_forward_charlm_path"] = None
-                default_config[f"{processor}_backward_charlm_path"] = None
-                logger.debug(f'Skipping {NER} charlm...')
-            default_config[f"{processor}_model_path"] = os.path.join(
-                dir, lang, processor, package + '.pt'
-            )
-
-        if not dependencies: continue
-        for dependency in dependencies:
-            dep_processor, dep_model = dependency
-            default_config[f"{processor}_{dep_processor}_path"] = os.path.join(
-                dir, lang, dep_processor, dep_model + '.pt'
-            )
-
-    return default_config
-
-def ensure_dir(dir):
-    """
-    Create dir in case it does not exist.
-    """
-    Path(dir).mkdir(parents=True, exist_ok=True)
-
-def get_md5(path):
-    """
-    Get the MD5 value of a path.
-    """
-    data = open(path, 'rb').read()
-    return hashlib.md5(data).hexdigest()
-
-def unzip_file(filename, zipped_filename):
-    """
-    Unzip only one file and name it as `filename`
-    """
-    logger.debug(f'Unzip: {zipped_filename} to {filename} ...')
-    with zipfile.ZipFile(zipped_filename) as rf:
-        files = zipfile.ZipFile.infolist(rf)
-        for file in files:
-            with open(filename, 'wb') as wf:
-                wf.write(rf.read(file.filename))
-
-def unzip(dir, filename):
-    """
-    Fully unzip a file `filename` that's in a directory `dir`.
-    """
-    logger.debug(f'Unzip: {dir}/{filename}...')
-    with zipfile.ZipFile(os.path.join(dir, filename)) as f:
-        f.extractall(dir)
-
-def get_root_from_zipfile(filename):
-    """
-    Get the root directory from a archived zip file.
-    """
-    try:
-        zf = zipfile.ZipFile(filename, "r")
-    except:
-        raise Exception(f"Failed loading zip file at {filename}.")
-    assert len(zf.filelist) > 0, \
-        f"Zip file at f{filename} seems to be corrupted. Please check it."
-    return os.path.dirname(zf.filelist[0].filename)
-
-def file_exists(path, md5):
-    """
-    Check if the file at `path` exists and match the provided md5 value.
-    """
-    # if os.path.exists(path):
-    #     print(f'PATH: {path} || Written md5: {md5} || Calculated md5: {get_md5(path)}')
-    return os.path.exists(path) and get_md5(path) == md5
-
-def download_file(url, path):
-    """
-    Download a URL into a file as specified by `path`.
-    """
-    verbose = logger.level in [0, 10, 20]
-
-    # Try to get zipped file
-    zipped = True
-    r = requests.get(url + '.zip', stream=True)
-
-    # Zipped file not available
-    if not r.ok:
-        zipped = False
-        r = requests.get(url, stream=True)
-
-    if zipped:
-        normal_path = path
-        path += '.zip'
-    with open(path, 'wb') as f:
-        file_size = int(r.headers.get('content-length'))
-        default_chunk_size = 131072
-        desc = 'Downloading ' + url
-        with tqdm(total=file_size, unit='B', unit_scale=True, \
-            disable=not verbose, desc=desc) as pbar:
-            for chunk in r.iter_content(chunk_size=default_chunk_size):
-                if chunk:
-                    f.write(chunk)
-                    f.flush()
-                    pbar.update(len(chunk))
-
-    if zipped:
-        unzip_file(normal_path, path)
-        os.remove(path)
-
-def request_file(url, path, md5=None):
-    """
-    A complete wrapper over download_file() that also make sure the directory of
-    `path` exists, and that a file matching the md5 value does not exist.
-    """
-    ensure_dir(Path(path).parent)
-    if file_exists(path, md5):
-        logger.info(f'File exists: {path}.')
-        return
-    download_file(url, path)
-    assert(not md5 or file_exists(path, md5))
-
-def sort_processors(processor_list):
-    sorted_list = []
-    for processor in PIPELINE_NAMES:
-        for item in processor_list:
-            if item[0] == processor:
-                sorted_list.append(item)
-    return sorted_list
-
-def maintain_processor_list(resources, lang, package, processors):
-    processor_list = {}
-    # resolve processor models
-    if processors:
-        logger.debug(f'Processing parameter "processors"...')
-        for key, value in processors.items():
-            assert(key in PIPELINE_NAMES)
-            if key == 'srl':
-                assert(value == 'standard_jos'), "You may only run 'srl' with 'standard_jos' type."
-            assert(isinstance(key, str) and isinstance(value, str))
-            # check if keys and values can be found
-            if key in resources[lang] and value in resources[lang][key]:
-                logger.debug(f'Found {key}: {value}.')
-                if 'duplicate' in resources[lang][key][value]:
-                    processor_list[key] = resources[lang][key][value]['duplicate']
-                else:
-                    processor_list[key] = value
-            # allow values to be default in some cases
-            elif key in resources[lang]['default_processors'] and value == 'default':
-                logger.debug(
-                    f'Found {key}: {resources[lang]["default_processors"][key]}.'
-                )
-                processor_list[key] = resources[lang]['default_processors'][key]
-            # allow processors to be set to variants that we didn't implement
-            elif value in PROCESSOR_VARIANTS[key]:
-                logger.debug(
-                    f'Found {key}: {value}. '
-                    f'Using external {value} variant for the {key} processor.'
-                )
-                processor_list[key] = value
-            # allow lemma to be set to "identity"
-            elif key == LEMMA and value == 'identity':
-                logger.debug(
-                    f'Found {key}: {value}. Using identity lemmatizer.'
-                )
-                processor_list[key] = value
-            # not a processor in the officially supported processor list
-            elif key not in resources[lang]:
-                logger.debug(
-                    f'{key}: {value} is not officially supported by Stanza, '
-                    f'loading it anyway.'
-                )
-                processor_list[key] = value
-            # cannot find the package for a processor and warn user
-            else:
-                logger.warning(
-                    f'Can not find {key}: {value} from official model list. '
-                    f'Ignoring it.'
-                )
-    # resolve package
-    if package:
-        logger.debug(f'Processing parameter "package"...')
-        if package == 'default':
-            for key, value in resources[lang]['default_processors'].items():
-                if key not in processor_list:
-                    logger.debug(f'Found {key}: {value}.')
-                    processor_list[key] = value
-        else:
-            flag = False
-            for key in PIPELINE_NAMES:
-                if key not in resources[lang]: continue
-                if package in resources[lang][key]:
-                    flag = True
-                    if key not in processor_list:
-                        logger.debug(f'Found {key}: {package}.')
-                        # Solve duplicates
-                        if 'duplicate' in resources[lang][key][package]:
-                            processor_list[key] = resources[lang][key][package]['duplicate']
-                        else:
-                            processor_list[key] = package
-                    else:
-                        logger.debug(
-                            f'{key}: {package} is overwritten by '
-                            f'{key}: {processors[key]}.'
-                        )
-            if not flag: logger.warning((f'Can not find package: {package}.'))
-    processor_list = [[key, value] for key, value in processor_list.items()]
-    processor_list = sort_processors(processor_list)
-    return processor_list
-
-def add_dependencies(resources, lang, processor_list):
-    default_dependencies = resources[lang]['default_dependencies']
-    for item in processor_list:
-        processor, package = item
-        dependencies = default_dependencies.get(processor, None)
-        # skip dependency checking for external variants of processors and identity lemmatizer
-        if not any([
-                package in PROCESSOR_VARIANTS[processor],
-                processor == LEMMA and package == 'identity'
-            ]):
-            dependencies = resources[lang].get(processor, {}).get(package, {}) \
-                .get('dependencies', dependencies)
-        if dependencies:
-            dependencies = [[dependency['model'], dependency['package']] \
-                for dependency in dependencies]
-        item.append(dependencies)
-    return processor_list
-
-def flatten_processor_list(processor_list):
-    flattened_processor_list = []
-    dependencies_list = []
-    for item in processor_list:
-        processor, package, dependencies = item
-        flattened_processor_list.append([processor, package])
-        if dependencies:
-            dependencies_list += [tuple(dependency) for dependency in dependencies]
-    dependencies_list = [list(item) for item in set(dependencies_list)]
-    for processor, package in dependencies_list:
-        logger.debug(f'Find dependency {processor}: {package}.')
-    flattened_processor_list += dependencies_list
-    return flattened_processor_list
-
-def set_logging_level(logging_level, verbose):
-    # Check verbose for easy logging control
-    if verbose == False:
-        logging_level = 'ERROR'
-    elif verbose == True:
-        logging_level = 'INFO'
-
-    # Set logging level
-    logging_level = logging_level.upper()
-    all_levels = ['DEBUG', 'INFO', 'WARNING', 'WARN', 'ERROR', 'CRITICAL', 'FATAL']
-    if logging_level not in all_levels:
-        raise Exception(
-            f"Unrecognized logging level for pipeline: "
-            f"{logging_level}. Must be one of {', '.join(all_levels)}."
-        )
-    logger.setLevel(logging_level)
-    return logging_level
-
-def process_pipeline_parameters(lang, dir, package, processors):
-    # Check parameter types and convert values to lower case
-    if isinstance(lang, str):
-        lang = lang.strip().lower()
-    elif lang is not None:
-        raise Exception(
-            f"The parameter 'lang' should be str, "
-            f"but got {type(lang).__name__} instead."
-        )
-
-    if isinstance(dir, str):
-        dir = dir.strip()
-    elif dir is not None:
-        raise Exception(
-            f"The parameter 'dir' should be str, "
-            f"but got {type(dir).__name__} instead."
-        )
-
-    if isinstance(package, str):
-        package = package.strip().lower()
-    elif package is not None:
-        raise Exception(
-            f"The parameter 'package' should be str, "
-            f"but got {type(package).__name__} instead."
-        )
-
-    if isinstance(processors, str):
-        # Special case: processors is str, compatible with older verson
-        processors = {
-            processor.strip().lower(): package \
-                for processor in processors.split(',')
-        }
-        package = None
-    elif isinstance(processors, dict):
-        processors = {
-            k.strip().lower(): v.strip().lower() \
-                for k, v in processors.items()
-        }
-    elif processors is not None:
-        raise Exception(
-            f"The parameter 'processors' should be dict or str, "
-            f"but got {type(processors).__name__} instead."
-        )
-
-    return lang, dir, package, processors
-
-# main download function
-def download(
-        lang='sl',
-        dir=DEFAULT_MODEL_DIR,
-        type='default',
-        processors={},
-        logging_level='INFO',
-        verbose=None,
-        resources_url=DEFAULT_RESOURCES_URL,
-        resources_branch=None,
-        resources_version=DEFAULT_RESOURCES_VERSION,
-        model_url=DEFAULT_MODEL_URL
-    ):
-    package = type
-    # set global logging level
-    set_logging_level(logging_level, verbose)
-    # process different pipeline parameters
-    lang, dir, package, processors = process_pipeline_parameters(
-        lang, dir, package, processors
-    )
-
-    if resources_url == DEFAULT_RESOURCES_URL and resources_branch is not None:
-        resources_url = CLASSLA_RESOURCES_GITHUB + resources_branch
-    # Download resources.json to obtain latest packages.
-    logger.debug('Downloading resource file...')
-    # handle short name for resources urls; otherwise treat it as url
-    if resources_url.lower() in ('stanford', 'stanfordnlp'):
-        resources_url = STANFORDNLP_RESOURCES_URL
-    # make request
-    request_file(
-        f'{resources_url}/resources_{resources_version}.json',
-        os.path.join(dir, 'resources.json')
-    )
-    # unpack results
-    try:
-        resources = json.load(open(os.path.join(dir, 'resources.json')))
-    except:
-        raise Exception(
-            f'Cannot load resource file. Please check your network connection, '
-            f'or provided resource url and resource version.'
-        )
-    if lang not in resources:
-        raise Exception(f'Unsupported language: {lang}.')
-    if 'alias' in resources[lang]:
-        logger.info(f'"{lang}" is an alias for "{resources[lang]["alias"]}"')
-        lang = resources[lang]['alias']
-    lang_name = resources[lang]['lang_name'] if 'lang_name' in resources[lang] else ''
-    url = resources['url'] if model_url.lower() == 'default' else model_url
-
-    # Default: download zipfile and unzip
-    # if package == 'default' and (processors is None or len(processors) == 0):
-    #     logger.info(
-    #         f'Downloading default packages for language: {lang} ({lang_name})...'
-    #     )
-    #     request_file(
-    #         f'{url}/{resources_version}/{lang}/default.zip',
-    #         os.path.join(dir, lang, f'default.zip'),
-    #         md5=resources[lang]['default_md5']
-    #     )
-    #     unzip(os.path.join(dir, lang), 'default.zip')
-    # Customize: maintain download list
-    # else:
-    # Translate default packages
-    if package == 'default' and (processors is None or len(processors) == 0):
-        processors = resources[lang]['default_processors']
-
-    download_list = maintain_processor_list(
-        resources, lang, package, processors
-    )
-    download_list = add_dependencies(resources, lang, download_list)
-    download_list = flatten_processor_list(download_list)
-    download_table = make_table(['Processor', 'Package'], download_list)
-    logger.info(
-        f'Downloading these customized packages for language: '
-        f'{lang} ({lang_name})...\n{download_table}'
-    )
-
-    # Download packages
-    for key, value in download_list:
-        if value not in resources[lang][key] or key == 'tokenize':
-            continue
-        try:
-            request_file(
-                f'{url}/{resources[lang][key][value]["link"]}',
-                os.path.join(dir, lang, key, f'{value}.pt'),
-                md5=resources[lang][key][value]['md5']
-            )
-        except KeyError as e:
-            raise Exception(
-                f'Cannot find the following processor and model name combination: '
-                f'{key}, {value}. Please check if you have provided the correct model name.'
-            ) from e
-    logger.info(f'Finished downloading models and saved to {dir}.')
+"""
+Common utilities for Stanza resources.
+"""
+
+import os
+import requests
+from tqdm import tqdm
+from pathlib import Path
+import json
+import hashlib
+import zipfile
+import shutil
+import logging
+
+from classla.utils.helper_func import make_table
+from classla.pipeline._constants import TOKENIZE, MWT, POS, LEMMA, DEPPARSE, \
+    NER, SENTIMENT
+from classla.pipeline.registry import PIPELINE_NAMES, PROCESSOR_VARIANTS
+from classla._version import __resources_version__
+
+logger = logging.getLogger('classla')
+
+# set home dir for default
+HOME_DIR = str(Path.home())
+STANFORDNLP_RESOURCES_URL = 'https://nlp.stanford.edu/software/stanza/stanza-resources/'
+CLASSLA_RESOURCES_GITHUB = 'https://raw.githubusercontent.com/clarinsi/classla-resources/'
+DEFAULT_RESOURCES_URL = os.getenv('CLASSLA_RESOURCES_URL', CLASSLA_RESOURCES_GITHUB + 'main')
+DEFAULT_RESOURCES_VERSION = os.getenv(
+    'CLASSLA_RESOURCES_VERSION',
+    __resources_version__
+)
+DEFAULT_MODEL_URL = os.getenv('CLASSLA_MODEL_URL', 'default')
+DEFAULT_MODEL_DIR = os.getenv(
+    'CLASSLA_RESOURCES_DIR',
+    os.path.join(HOME_DIR, 'classla_resources')
+)
+
+# given a language and models path, build a default configuration
+def build_default_config(resources, lang, dir, load_list):
+    default_config = {}
+    for item in load_list:
+        processor, package, dependencies = item
+
+        # handle case when processor variants are used
+        if package in PROCESSOR_VARIANTS[processor]:
+            default_config[f"{processor}_with_{package}"] = True
+        # handle case when identity is specified as lemmatizer
+        elif processor == LEMMA and package == 'identity':
+            default_config[f"{LEMMA}_use_identity"] = True
+        elif processor == TOKENIZE:
+            assert 'library' in resources[lang][processor][package], "Tokenizer processor in resources.json should include attribute `library` with value `reldi` or `obeliks`"
+            default_config[f"{processor}_library"] = resources[lang][processor][package]['library']
+            assert 'type' in resources[lang][processor][package], "Tokenizer processor in resources.json should include attribute `type` with value `standard` or `nonstandard`"
+            default_config[f"{processor}_type"] = resources[lang][processor][package]['type']
+        else:
+            if processor == NER:
+                default_config[f"{processor}_forward_charlm_path"] = None
+                default_config[f"{processor}_backward_charlm_path"] = None
+                logger.debug(f'Skipping {NER} charlm...')
+            default_config[f"{processor}_model_path"] = os.path.join(
+                dir, lang, processor, package + '.pt'
+            )
+
+        if not dependencies: continue
+        for dependency in dependencies:
+            dep_processor, dep_model = dependency
+            default_config[f"{processor}_{dep_processor}_path"] = os.path.join(
+                dir, lang, dep_processor, dep_model + '.pt'
+            )
+
+    return default_config
+
+def ensure_dir(dir):
+    """
+    Create dir in case it does not exist.
+    """
+    Path(dir).mkdir(parents=True, exist_ok=True)
+
+def get_md5(path):
+    """
+    Get the MD5 value of a path.
+    """
+    data = open(path, 'rb').read()
+    return hashlib.md5(data).hexdigest()
+
+def unzip_file(filename, zipped_filename):
+    """
+    Unzip only one file and name it as `filename`
+    """
+    logger.debug(f'Unzip: {zipped_filename} to {filename} ...')
+    with zipfile.ZipFile(zipped_filename) as rf:
+        files = zipfile.ZipFile.infolist(rf)
+        for file in files:
+            with open(filename, 'wb') as wf:
+                wf.write(rf.read(file.filename))
+
+def unzip(dir, filename):
+    """
+    Fully unzip a file `filename` that's in a directory `dir`.
+    """
+    logger.debug(f'Unzip: {dir}/{filename}...')
+    with zipfile.ZipFile(os.path.join(dir, filename)) as f:
+        f.extractall(dir)
+
+def get_root_from_zipfile(filename):
+    """
+    Get the root directory from a archived zip file.
+    """
+    try:
+        zf = zipfile.ZipFile(filename, "r")
+    except:
+        raise Exception(f"Failed loading zip file at {filename}.")
+    assert len(zf.filelist) > 0, \
+        f"Zip file at f{filename} seems to be corrupted. Please check it."
+    return os.path.dirname(zf.filelist[0].filename)
+
+def file_exists(path, md5):
+    """
+    Check if the file at `path` exists and match the provided md5 value.
+    """
+    # if os.path.exists(path):
+    #     print(f'PATH: {path} || Written md5: {md5} || Calculated md5: {get_md5(path)}')
+    return os.path.exists(path) and get_md5(path) == md5
+
+def download_file(url, path):
+    """
+    Download a URL into a file as specified by `path`.
+    """
+    verbose = logger.level in [0, 10, 20]
+
+    # Try to get zipped file
+    zipped = True
+    r = requests.get(url + '.zip', stream=True)
+
+    # Zipped file not available
+    if not r.ok:
+        zipped = False
+        r = requests.get(url, stream=True)
+
+    if zipped:
+        normal_path = path
+        path += '.zip'
+    with open(path, 'wb') as f:
+        file_size = int(r.headers.get('content-length'))
+        default_chunk_size = 131072
+        desc = 'Downloading ' + url
+        with tqdm(total=file_size, unit='B', unit_scale=True, \
+            disable=not verbose, desc=desc) as pbar:
+            for chunk in r.iter_content(chunk_size=default_chunk_size):
+                if chunk:
+                    f.write(chunk)
+                    f.flush()
+                    pbar.update(len(chunk))
+
+    if zipped:
+        unzip_file(normal_path, path)
+        os.remove(path)
+
+def request_file(url, path, md5=None):
+    """
+    A complete wrapper over download_file() that also make sure the directory of
+    `path` exists, and that a file matching the md5 value does not exist.
+    """
+    ensure_dir(Path(path).parent)
+    if file_exists(path, md5):
+        logger.info(f'File exists: {path}.')
+        return
+    download_file(url, path)
+    assert(not md5 or file_exists(path, md5))
+
+def sort_processors(processor_list):
+    sorted_list = []
+    for processor in PIPELINE_NAMES:
+        for item in processor_list:
+            if item[0] == processor:
+                sorted_list.append(item)
+    return sorted_list
+
+def maintain_processor_list(resources, lang, package, processors):
+    processor_list = {}
+    # resolve processor models
+    if processors:
+        logger.debug(f'Processing parameter "processors"...')
+        for key, value in processors.items():
+            assert(key in PIPELINE_NAMES)
+            if key == 'srl':
+                assert(value == 'standard_jos'), "You may only run 'srl' with 'standard_jos' type."
+            assert(isinstance(key, str) and isinstance(value, str))
+            # check if keys and values can be found
+            if key in resources[lang] and value in resources[lang][key]:
+                logger.debug(f'Found {key}: {value}.')
+                if 'duplicate' in resources[lang][key][value]:
+                    processor_list[key] = resources[lang][key][value]['duplicate']
+                else:
+                    processor_list[key] = value
+            # allow values to be default in some cases
+            elif key in resources[lang]['default_processors'] and value == 'default':
+                logger.debug(
+                    f'Found {key}: {resources[lang]["default_processors"][key]}.'
+                )
+                processor_list[key] = resources[lang]['default_processors'][key]
+            # allow processors to be set to variants that we didn't implement
+            elif value in PROCESSOR_VARIANTS[key]:
+                logger.debug(
+                    f'Found {key}: {value}. '
+                    f'Using external {value} variant for the {key} processor.'
+                )
+                processor_list[key] = value
+            # allow lemma to be set to "identity"
+            elif key == LEMMA and value == 'identity':
+                logger.debug(
+                    f'Found {key}: {value}. Using identity lemmatizer.'
+                )
+                processor_list[key] = value
+            # not a processor in the officially supported processor list
+            elif key not in resources[lang]:
+                logger.debug(
+                    f'{key}: {value} is not officially supported by Stanza, '
+                    f'loading it anyway.'
+                )
+                processor_list[key] = value
+            # cannot find the package for a processor and warn user
+            else:
+                logger.warning(
+                    f'Can not find {key}: {value} from official model list. '
+                    f'Ignoring it.'
+                )
+    # resolve package
+    if package:
+        logger.debug(f'Processing parameter "package"...')
+        if package == 'default':
+            for key, value in resources[lang]['default_processors'].items():
+                if key not in processor_list:
+                    logger.debug(f'Found {key}: {value}.')
+                    processor_list[key] = value
+        else:
+            flag = False
+            for key in PIPELINE_NAMES:
+                if key not in resources[lang]: continue
+                if package in resources[lang][key]:
+                    flag = True
+                    if key not in processor_list:
+                        logger.debug(f'Found {key}: {package}.')
+                        # Solve duplicates
+                        if 'duplicate' in resources[lang][key][package]:
+                            processor_list[key] = resources[lang][key][package]['duplicate']
+                        else:
+                            processor_list[key] = package
+                    else:
+                        logger.debug(
+                            f'{key}: {package} is overwritten by '
+                            f'{key}: {processors[key]}.'
+                        )
+            if not flag: logger.warning((f'Can not find package: {package}.'))
+    processor_list = [[key, value] for key, value in processor_list.items()]
+    processor_list = sort_processors(processor_list)
+    return processor_list
+
+def add_dependencies(resources, lang, processor_list):
+    default_dependencies = resources[lang]['default_dependencies']
+    for item in processor_list:
+        processor, package = item
+        dependencies = default_dependencies.get(processor, None)
+        # skip dependency checking for external variants of processors and identity lemmatizer
+        if not any([
+                package in PROCESSOR_VARIANTS[processor],
+                processor == LEMMA and package == 'identity'
+            ]):
+            dependencies = resources[lang].get(processor, {}).get(package, {}) \
+                .get('dependencies', dependencies)
+        if dependencies:
+            dependencies = [[dependency['model'], dependency['package']] \
+                for dependency in dependencies]
+        item.append(dependencies)
+    return processor_list
+
+def flatten_processor_list(processor_list):
+    flattened_processor_list = []
+    dependencies_list = []
+    for item in processor_list:
+        processor, package, dependencies = item
+        flattened_processor_list.append([processor, package])
+        if dependencies:
+            dependencies_list += [tuple(dependency) for dependency in dependencies]
+    dependencies_list = [list(item) for item in set(dependencies_list)]
+    for processor, package in dependencies_list:
+        logger.debug(f'Find dependency {processor}: {package}.')
+    flattened_processor_list += dependencies_list
+    return flattened_processor_list
+
+def set_logging_level(logging_level, verbose):
+    # Check verbose for easy logging control
+    if verbose == False:
+        logging_level = 'ERROR'
+    elif verbose == True:
+        logging_level = 'INFO'
+
+    # Set logging level
+    logging_level = logging_level.upper()
+    all_levels = ['DEBUG', 'INFO', 'WARNING', 'WARN', 'ERROR', 'CRITICAL', 'FATAL']
+    if logging_level not in all_levels:
+        raise Exception(
+            f"Unrecognized logging level for pipeline: "
+            f"{logging_level}. Must be one of {', '.join(all_levels)}."
+        )
+    logger.setLevel(logging_level)
+    return logging_level
+
+def process_pipeline_parameters(lang, dir, package, processors):
+    # Check parameter types and convert values to lower case
+    if isinstance(lang, str):
+        lang = lang.strip().lower()
+    elif lang is not None:
+        raise Exception(
+            f"The parameter 'lang' should be str, "
+            f"but got {type(lang).__name__} instead."
+        )
+
+    if isinstance(dir, str):
+        dir = dir.strip()
+    elif dir is not None:
+        raise Exception(
+            f"The parameter 'dir' should be str, "
+            f"but got {type(dir).__name__} instead."
+        )
+
+    if isinstance(package, str):
+        package = package.strip().lower()
+    elif package is not None:
+        raise Exception(
+            f"The parameter 'package' should be str, "
+            f"but got {type(package).__name__} instead."
+        )
+
+    if isinstance(processors, str):
+        # Special case: processors is str, compatible with older verson
+        processors = {
+            processor.strip().lower(): package \
+                for processor in processors.split(',')
+        }
+        package = None
+    elif isinstance(processors, dict):
+        processors = {
+            k.strip().lower(): v.strip().lower() \
+                for k, v in processors.items()
+        }
+    elif processors is not None:
+        raise Exception(
+            f"The parameter 'processors' should be dict or str, "
+            f"but got {type(processors).__name__} instead."
+        )
+
+    return lang, dir, package, processors
+
+# main download function
+def download(
+        lang='sl',
+        dir=DEFAULT_MODEL_DIR,
+        type='default',
+        processors={},
+        logging_level='INFO',
+        verbose=None,
+        resources_url=DEFAULT_RESOURCES_URL,
+        resources_branch=None,
+        resources_version=DEFAULT_RESOURCES_VERSION,
+        model_url=DEFAULT_MODEL_URL
+    ):
+    package = type
+    # set global logging level
+    set_logging_level(logging_level, verbose)
+    # process different pipeline parameters
+    lang, dir, package, processors = process_pipeline_parameters(
+        lang, dir, package, processors
+    )
+
+    if resources_url == DEFAULT_RESOURCES_URL and resources_branch is not None:
+        resources_url = CLASSLA_RESOURCES_GITHUB + resources_branch
+    # Download resources.json to obtain latest packages.
+    logger.debug('Downloading resource file...')
+    # handle short name for resources urls; otherwise treat it as url
+    if resources_url.lower() in ('stanford', 'stanfordnlp'):
+        resources_url = STANFORDNLP_RESOURCES_URL
+    # make request
+    request_file(
+        f'{resources_url}/resources_{resources_version}.json',
+        os.path.join(dir, 'resources.json')
+    )
+    # unpack results
+    try:
+        resources = json.load(open(os.path.join(dir, 'resources.json')))
+    except:
+        raise Exception(
+            f'Cannot load resource file. Please check your network connection, '
+            f'or provided resource url and resource version.'
+        )
+    if lang not in resources:
+        raise Exception(f'Unsupported language: {lang}.')
+    if 'alias' in resources[lang]:
+        logger.info(f'"{lang}" is an alias for "{resources[lang]["alias"]}"')
+        lang = resources[lang]['alias']
+    lang_name = resources[lang]['lang_name'] if 'lang_name' in resources[lang] else ''
+    url = resources['url'] if model_url.lower() == 'default' else model_url
+
+    # Default: download zipfile and unzip
+    # if package == 'default' and (processors is None or len(processors) == 0):
+    #     logger.info(
+    #         f'Downloading default packages for language: {lang} ({lang_name})...'
+    #     )
+    #     request_file(
+    #         f'{url}/{resources_version}/{lang}/default.zip',
+    #         os.path.join(dir, lang, f'default.zip'),
+    #         md5=resources[lang]['default_md5']
+    #     )
+    #     unzip(os.path.join(dir, lang), 'default.zip')
+    # Customize: maintain download list
+    # else:
+    # Translate default packages
+    if package == 'default' and (processors is None or len(processors) == 0):
+        processors = resources[lang]['default_processors']
+
+    download_list = maintain_processor_list(
+        resources, lang, package, processors
+    )
+    download_list = add_dependencies(resources, lang, download_list)
+    download_list = flatten_processor_list(download_list)
+    download_table = make_table(['Processor', 'Package'], download_list)
+    logger.info(
+        f'Downloading these customized packages for language: '
+        f'{lang} ({lang_name})...\n{download_table}'
+    )
+
+    # Download packages
+    for key, value in download_list:
+        if value not in resources[lang][key] or key == 'tokenize':
+            continue
+        try:
+            request_file(
+                f'{url}/{resources[lang][key][value]["link"]}',
+                os.path.join(dir, lang, key, f'{value}.pt'),
+                md5=resources[lang][key][value]['md5']
+            )
+        except KeyError as e:
+            raise Exception(
+                f'Cannot find the following processor and model name combination: '
+                f'{key}, {value}. Please check if you have provided the correct model name.'
+            ) from e
+    logger.info(f'Finished downloading models and saved to {dir}.')
```

### Comparing `classla-2.0/classla/server/__init__.py` & `classla-2.1/classla/server/__init__.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-from classla.protobuf import to_text
-from classla.protobuf import Document, Sentence, Token, IndexedWord, Span
-from classla.protobuf import ParseTree, DependencyGraph, CorefChain
-from classla.protobuf import Mention, NERMention, Entity, Relation, RelationTriple, Timex
-from classla.protobuf import Quote, SpeakerInfo
-from classla.protobuf import Operator, Polarity
-from classla.protobuf import SentenceFragment, TokenLocation
-from classla.protobuf import MapStringString, MapIntString
-from .client import CoreNLPClient, AnnotationException, TimeoutException, PermanentlyFailedException, StartServer
-from .annotator import Annotator
+from classla.protobuf import to_text
+from classla.protobuf import Document, Sentence, Token, IndexedWord, Span
+from classla.protobuf import ParseTree, DependencyGraph, CorefChain
+from classla.protobuf import Mention, NERMention, Entity, Relation, RelationTriple, Timex
+from classla.protobuf import Quote, SpeakerInfo
+from classla.protobuf import Operator, Polarity
+from classla.protobuf import SentenceFragment, TokenLocation
+from classla.protobuf import MapStringString, MapIntString
+from .client import CoreNLPClient, AnnotationException, TimeoutException, PermanentlyFailedException, StartServer
+from .annotator import Annotator
```

### Comparing `classla-2.0/classla/server/client.py` & `classla-2.1/classla/server/client.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,670 +1,670 @@
-"""
-Client for accessing Stanford CoreNLP in Python
-"""
-
-import atexit
-import contextlib
-import enum
-import io
-import os
-import re
-import requests
-import logging
-import json
-import shlex
-import socket
-import subprocess
-import time
-import sys
-import uuid
-
-from datetime import datetime
-from pathlib import Path
-
-from six.moves.urllib.parse import urlparse
-
-from classla.protobuf import Document, parseFromDelimitedString, writeToDelimitedString, to_text
-__author__ = 'arunchaganty, kelvinguu, vzhong, wmonroe4'
-
-logger = logging.getLogger('classla')
-
-# pattern tmp props file should follow
-SERVER_PROPS_TMP_FILE_PATTERN = re.compile('corenlp_server-(.*).props')
-
-# Check if str is CoreNLP supported language
-CORENLP_LANGS = ['ar', 'arabic', 'chinese', 'zh', 'english', 'en', 'french', 'fr', 'de', 'german', 'es', 'spanish']
-
-LANGUAGE_SHORTHANDS_TO_FULL = {
-    "ar": "arabic",
-    "zh": "chinese",
-    "en": "english",
-    "fr": "french",
-    "de": "german",
-    "es": "spanish"
-}
-
-
-def is_corenlp_lang(props_str):
-    """ Check if a string references a CoreNLP language """
-    return props_str.lower() in CORENLP_LANGS
-
-
-# Validate CoreNLP properties
-CORENLP_OUTPUT_VALS = ["conll", "conllu", "json", "serialized", "text", "xml", "inlinexml"]
-
-
-def validate_corenlp_props(properties=None, annotators=None, output_format=None):
-    """ Do basic checks to validate CoreNLP properties """
-    if output_format and output_format.lower() not in CORENLP_OUTPUT_VALS:
-        raise ValueError(f"{output_format} not a valid CoreNLP outputFormat value! Choose from: {CORENLP_OUTPUT_VALS}")
-    if type(properties) == dict:
-        if "outputFormat" in properties and properties["outputFormat"].lower() not in CORENLP_OUTPUT_VALS:
-            raise ValueError(f"{properties['outputFormat']} not a valid CoreNLP outputFormat value! Choose from: "
-                             f"{CORENLP_OUTPUT_VALS}")
-
-
-class AnnotationException(Exception):
-    """ Exception raised when there was an error communicating with the CoreNLP server. """
-    pass
-
-
-class TimeoutException(AnnotationException):
-    """ Exception raised when the CoreNLP server timed out. """
-    pass
-
-
-class ShouldRetryException(Exception):
-    """ Exception raised if the service should retry the request. """
-    pass
-
-
-class PermanentlyFailedException(Exception):
-    """ Exception raised if the service should NOT retry the request. """
-    pass
-
-class StartServer(enum.Enum):
-    DONT_START = 0
-    FORCE_START = 1
-    TRY_START = 2
-
-
-def clean_props_file(props_file):
-    # check if there is a temp server props file to remove and remove it
-    if props_file:
-        if os.path.isfile(props_file) and SERVER_PROPS_TMP_FILE_PATTERN.match(os.path.basename(props_file)):
-            os.remove(props_file)
-
-
-class RobustService(object):
-    """ Service that resuscitates itself if it is not available. """
-    CHECK_ALIVE_TIMEOUT = 120
-
-    def __init__(self, start_cmd, stop_cmd, endpoint, stdout=sys.stdout,
-                 stderr=sys.stderr, be_quiet=False, host=None, port=None, ignore_binding_error=False):
-        self.start_cmd = start_cmd and shlex.split(start_cmd)
-        self.stop_cmd = stop_cmd and shlex.split(stop_cmd)
-        self.endpoint = endpoint
-        self.stdout = stdout
-        self.stderr = stderr
-
-        self.server = None
-        self.is_active = False
-        self.be_quiet = be_quiet
-        self.host = host
-        self.port = port
-        self.ignore_binding_error = ignore_binding_error
-        atexit.register(self.atexit_kill)
-
-    def is_alive(self):
-        try:
-            if not self.ignore_binding_error and self.server is not None and self.server.poll() is not None:
-                return False
-            return requests.get(self.endpoint + "/ping").ok
-        except requests.exceptions.ConnectionError as e:
-            raise ShouldRetryException(e)
-
-    def start(self):
-        if self.start_cmd:
-            if self.host and self.port:
-                with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:
-                    try:
-                        sock.bind((self.host, self.port))
-                    except socket.error:
-                        if self.ignore_binding_error:
-                            logger.info(f"Connecting to existing CoreNLP server at {self.host}:{self.port}")
-                            self.server = None
-                            return
-                        else:
-                            raise PermanentlyFailedException("Error: unable to start the CoreNLP server on port %d "
-                                                         "(possibly something is already running there)" % self.port)
-            if self.be_quiet:
-                # Issue #26: subprocess.DEVNULL isn't supported in python 2.7.
-                stderr = open(os.devnull, 'w')
-            else:
-                stderr = self.stderr
-            logger.info(f"Starting server with command: {' '.join(self.start_cmd)}")
-            self.server = subprocess.Popen(self.start_cmd,
-                                           stderr=stderr,
-                                           stdout=stderr)
-
-    def atexit_kill(self):
-        # make some kind of effort to stop the service (such as a
-        # CoreNLP server) at the end of the program.  not waiting so
-        # that the python script exiting isn't delayed
-        if self.server and self.server.poll() is None:
-            self.server.terminate()
-
-    def stop(self):
-        if self.server:
-            self.server.terminate()
-            try:
-                self.server.wait(5)
-            except subprocess.TimeoutExpired:
-                # Resorting to more aggressive measures...
-                self.server.kill()
-                try:
-                    self.server.wait(5)
-                except subprocess.TimeoutExpired:
-                    # oh well
-                    pass
-            self.server = None
-        if self.stop_cmd:
-            subprocess.run(self.stop_cmd, check=True)
-        self.is_active = False
-
-    def __enter__(self):
-        self.start()
-        return self
-
-    def __exit__(self, _, __, ___):
-        self.stop()
-
-    def ensure_alive(self):
-        # Check if the service is active and alive
-        if self.is_active:
-            try:
-                if self.is_alive():
-                    return
-                else:
-                    self.stop()
-            except ShouldRetryException:
-                pass
-
-        # If not, try to start up the service.
-        if self.server is None:
-            self.start()
-
-        # Wait for the service to start up.
-        start_time = time.time()
-        while True:
-            try:
-                if self.is_alive():
-                    break
-            except ShouldRetryException:
-                pass
-
-            if time.time() - start_time < self.CHECK_ALIVE_TIMEOUT:
-                time.sleep(1)
-            else:
-                raise PermanentlyFailedException("Timed out waiting for service to come alive.")
-
-        # At this point we are guaranteed that the service is alive.
-        self.is_active = True
-
-
-def resolve_classpath(classpath=None):
-    """
-    Returns the classpath to use for corenlp.
-
-    Prefers to use the given classpath parameter, if available.  If
-    not, uses the CORENLP_HOME environment variable.  Resolves $CLASSPATH
-    (the exact string) in either the classpath parameter or $CORENLP_HOME.
-    """
-    if classpath == '$CLASSPATH' or (classpath is None and os.getenv("CORENLP_HOME", None) == '$CLASSPATH'):
-        classpath = os.getenv("CLASSPATH")
-    elif classpath is None:
-        classpath = os.getenv("CORENLP_HOME", os.path.join(str(Path.home()), 'stanza_corenlp'))
-
-        assert os.path.exists(classpath), \
-            "Please install CoreNLP by running `classla.install_corenlp()`. If you have installed it, please define " \
-            "$CORENLP_HOME to be location of your CoreNLP distribution or pass in a classpath parameter."
-        classpath = os.path.join(classpath, "*")
-    return classpath
-
-
-class CoreNLPClient(RobustService):
-    """ A client to the Stanford CoreNLP server. """
-
-    DEFAULT_ENDPOINT = "http://localhost:9000"
-    DEFAULT_TIMEOUT = 60000
-    DEFAULT_THREADS = 5
-    DEFAULT_OUTPUT_FORMAT = "serialized"
-    DEFAULT_MEMORY = "5G"
-    DEFAULT_MAX_CHAR_LENGTH = 100000
-
-    def __init__(self, start_server=StartServer.FORCE_START,
-                 endpoint=DEFAULT_ENDPOINT,
-                 timeout=DEFAULT_TIMEOUT,
-                 threads=DEFAULT_THREADS,
-                 annotators=None,
-                 output_format=None,
-                 properties=None,
-                 stdout=sys.stdout,
-                 stderr=sys.stderr,
-                 memory=DEFAULT_MEMORY,
-                 be_quiet=False,
-                 max_char_length=DEFAULT_MAX_CHAR_LENGTH,
-                 preload=True,
-                 classpath=None,
-                 **kwargs):
-
-        # whether or not server should be started by client
-        self.start_server = start_server
-        self.server_props_path = None
-        self.server_start_time = None
-        self.server_host = None
-        self.server_port = None
-        self.server_classpath = None
-        # validate properties
-        validate_corenlp_props(properties=properties, annotators=annotators, output_format=output_format)
-        # set up client defaults
-        self.properties = properties
-        self.annotators = annotators
-        self.output_format = output_format
-        self._setup_client_defaults()
-        # start the server
-        if isinstance(start_server, bool):
-            warning_msg = f"Setting 'start_server' to a boolean value when constructing {self.__class__.__name__} is deprecated and will stop" + \
-                " to function in a future version of classla. Please consider switching to using a value from classla.server.StartServer."
-            logger.warning(warning_msg)
-            start_server = StartServer.FORCE_START if start_server is True else StartServer.DONT_START
-
-        # start the server
-        if start_server is StartServer.FORCE_START or start_server is StartServer.TRY_START:
-            # record info for server start
-            self.server_start_time = datetime.now()
-            # set up default properties for server
-            self._setup_server_defaults()
-            host, port = urlparse(endpoint).netloc.split(":")
-            port = int(port)
-            assert host == "localhost", "If starting a server, endpoint must be localhost"
-            classpath = resolve_classpath(classpath)
-            start_cmd = f"java -Xmx{memory} -cp '{classpath}'  edu.stanford.nlp.pipeline.StanfordCoreNLPServer " \
-                        f"-port {port} -timeout {timeout} -threads {threads} -maxCharLength {max_char_length} " \
-                        f"-quiet {be_quiet} "
-
-            self.server_classpath = classpath
-            self.server_host = host
-            self.server_port = port
-
-            # set up server defaults
-            if self.server_props_path is not None:
-                start_cmd += f" -serverProperties {self.server_props_path}"
-
-            # set annotators for server default
-            if self.annotators is not None:
-                annotators_str = self.annotators if type(annotators) == str else ",".join(annotators)
-                start_cmd += f" -annotators {annotators_str}"
-
-            # specify what to preload, if anything
-            if preload:
-                if type(preload) == bool:
-                    # -preload flag means to preload all default annotators
-                    start_cmd += " -preload"
-                elif type(preload) == list:
-                    # turn list into comma separated list string, only preload these annotators
-                    start_cmd += f" -preload {','.join(preload)}"
-                elif type(preload) == str:
-                    # comma separated list of annotators
-                    start_cmd += f" -preload {preload}"
-
-            # set outputFormat for server default
-            # if no output format requested by user, set to serialized
-            start_cmd += f" -outputFormat {self.output_format}"
-
-            # additional options for server:
-            # - server_id
-            # - ssl
-            # - status_port
-            # - uriContext
-            # - strict
-            # - key
-            # - username
-            # - password
-            # - blockList
-            for kw in ['ssl', 'strict']:
-                if kwargs.get(kw) is not None:
-                    start_cmd += f" -{kw}"
-            for kw in ['status_port', 'uriContext', 'key', 'username', 'password', 'blockList', 'server_id']:
-                if kwargs.get(kw) is not None:
-                    start_cmd += f" -{kw} {kwargs.get(kw)}"
-            stop_cmd = None
-        else:
-            start_cmd = stop_cmd = None
-            host = port = None
-
-        super(CoreNLPClient, self).__init__(start_cmd, stop_cmd, endpoint,
-                                            stdout, stderr, be_quiet, host=host, port=port, ignore_binding_error=(start_server == StartServer.TRY_START))
-
-        self.timeout = timeout
-
-    def _setup_client_defaults(self):
-        """
-        Do some processing of annotators and output_format specified for the client.
-        If interacting with an externally started server, these will be defaults for annotate() calls.
-        :return: None
-        """
-        # normalize annotators to str
-        if self.annotators is not None:
-            self.annotators = self.annotators if type(self.annotators) == str else ",".join(self.annotators)
-
-        # handle case where no output format is specified
-        if self.output_format is None:
-            if type(self.properties) == dict and 'outputFormat' in self.properties:
-                self.output_format = self.properties['outputFormat']
-            else:
-                self.output_format = CoreNLPClient.DEFAULT_OUTPUT_FORMAT
-
-    def _setup_server_defaults(self):
-        """
-        Set up the default properties for the server.
-
-        The properties argument can take on one of 3 value types
-
-        1. File path on system or in CLASSPATH (e.g. /path/to/server.props or StanfordCoreNLP-french.properties
-        2. Name of a Stanford CoreNLP supported language (e.g. french or fr)
-        3. Python dictionary (properties written to tmp file for Java server, erased at end)
-
-        In addition, an annotators list and output_format can be specified directly with arguments. These
-        will overwrite any settings in the specified properties.
-
-        If no properties are specified, the standard Stanford CoreNLP English server will be launched. The outputFormat
-        will be set to 'serialized' and use the ProtobufAnnotationSerializer.
-        """
-
-        # ensure properties is str or dict
-        if self.properties is None or (not isinstance(self.properties, str) and not isinstance(self.properties, dict)):
-            if self.properties is not None:
-                logger.warning('properties passed invalid value (not a str or dict), setting properties = {}')
-            self.properties = {}
-        # check if properties is a string, pass on to server which can handle
-        if isinstance(self.properties, str):
-            # try to translate to Stanford CoreNLP language name, or assume properties is a path
-            if is_corenlp_lang(self.properties):
-                if self.properties.lower() in LANGUAGE_SHORTHANDS_TO_FULL:
-                    self.properties = LANGUAGE_SHORTHANDS_TO_FULL[self.properties]
-                logger.info(
-                    f"Using Stanford CoreNLP default properties for: {self.properties}.  Make sure to have "
-                    f"{self.properties} models jar (available for download here: "
-                    f"https://stanfordnlp.github.io/CoreNLP/) in CLASSPATH")
-            else:
-                if not os.path.isfile(self.properties):
-                    logger.warning(f"{self.properties} does not correspond to a file path. Make sure this file is in "
-                                   f"your CLASSPATH.")
-            self.server_props_path = self.properties
-        elif isinstance(self.properties, dict):
-            # make a copy
-            server_start_properties = dict(self.properties)
-            if self.annotators is not None:
-                server_start_properties['annotators'] = self.annotators
-            if self.output_format is not None and isinstance(self.output_format, str):
-                server_start_properties['outputFormat'] = self.output_format
-            # write desired server start properties to tmp file
-            # set up to erase on exit
-            tmp_path = write_corenlp_props(server_start_properties)
-            logger.info(f"Writing properties to tmp file: {tmp_path}")
-            atexit.register(clean_props_file, tmp_path)
-            self.server_props_path = tmp_path
-
-    def _request(self, buf, properties, reset_default=False, **kwargs):
-        """
-        Send a request to the CoreNLP server.
-
-        :param (str | bytes) buf: data to be sent with the request
-        :param (dict) properties: properties that the server expects
-        :return: request result
-        """
-        self.ensure_alive()
-
-        try:
-            input_format = properties.get("inputFormat", "text")
-            if input_format == "text":
-                ctype = "text/plain; charset=utf-8"
-            elif input_format == "serialized":
-                ctype = "application/x-protobuf"
-            else:
-                raise ValueError("Unrecognized inputFormat " + input_format)
-            # handle auth
-            if 'username' in kwargs and 'password' in kwargs:
-                kwargs['auth'] = requests.auth.HTTPBasicAuth(kwargs['username'], kwargs['password'])
-                kwargs.pop('username')
-                kwargs.pop('password')
-            r = requests.post(self.endpoint,
-                              params={'properties': str(properties), 'resetDefault': str(reset_default).lower()},
-                              data=buf, headers={'content-type': ctype},
-                              timeout=(self.timeout*2)/1000, **kwargs)
-            r.raise_for_status()
-            return r
-        except requests.HTTPError as e:
-            if r.text == "CoreNLP request timed out. Your document may be too long.":
-                raise TimeoutException(r.text)
-            else:
-                raise AnnotationException(r.text)
-
-    def annotate(self, text, annotators=None, output_format=None, properties=None, reset_default=None, **kwargs):
-        """
-        Send a request to the CoreNLP server.
-
-        :param (str | unicode) text: raw text for the CoreNLPServer to parse
-        :param (list | string) annotators: list of annotators to use
-        :param (str) output_format: output type from server: serialized, json, text, conll, conllu, or xml
-        :param (dict) properties: additional request properties (written on top of defaults)
-        :param (bool) reset_default: don't use server defaults
-
-        Precedence for settings:
-
-        1. annotators and output_format args
-        2. Values from properties dict
-        3. Client defaults self.annotators and self.output_format (set during client construction)
-        4. Server defaults
-
-        Additional request parameters (apart from CoreNLP pipeline properties) such as 'username' and 'password'
-        can be specified with the kwargs.
-
-        :return: request result
-        """
-
-        # validate request properties
-        validate_corenlp_props(properties=properties, annotators=annotators, output_format=output_format)
-        # set request properties
-        request_properties = {}
-
-        # start with client defaults
-        if self.annotators is not None:
-            request_properties['annotators'] = self.annotators
-        if self.output_format is not None:
-            request_properties['outputFormat'] = self.output_format
-
-        # add values from properties arg
-        # handle str case
-        if type(properties) == str:
-            if is_corenlp_lang(properties):
-                properties = {'pipelineLanguage': properties.lower()}
-                if reset_default is None:
-                    reset_default = True
-            else:
-                raise ValueError(f"Unrecognized properties keyword {properties}")
-
-        if type(properties) == dict:
-            request_properties.update(properties)
-
-        # if annotators list is specified, override with that
-        # also can use the annotators field the object was created with
-        if annotators is not None and (type(annotators) == str or type(annotators) == list):
-            request_properties['annotators'] = annotators if type(annotators) == str else ",".join(annotators)
-
-        # if output format is specified, override with that
-        if output_format is not None and type(output_format) == str:
-            request_properties['outputFormat'] = output_format
-
-        # make the request
-        # if not explictly set or the case of pipelineLanguage, reset_default should be None
-        if reset_default is None:
-            reset_default = False
-        r = self._request(text.encode('utf-8'), request_properties, reset_default, **kwargs)
-        if request_properties["outputFormat"] == "json":
-            return r.json()
-        elif request_properties["outputFormat"] == "serialized":
-            doc = Document()
-            parseFromDelimitedString(doc, r.content)
-            return doc
-        elif request_properties["outputFormat"] in ["text", "conllu", "conll", "xml"]:
-            return r.text
-        else:
-            return r
-
-    def update(self, doc, annotators=None, properties=None):
-        if properties is None:
-            properties = {}
-            properties.update({
-                'inputFormat': 'serialized',
-                'outputFormat': 'serialized',
-                'serializer': 'edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer'
-            })
-        if annotators:
-            properties['annotators'] = annotators if type(annotators) == str else ",".join(annotators)
-        with io.BytesIO() as stream:
-            writeToDelimitedString(doc, stream)
-            msg = stream.getvalue()
-
-        r = self._request(msg, properties)
-        doc = Document()
-        parseFromDelimitedString(doc, r.content)
-        return doc
-
-    def tokensregex(self, text, pattern, filter=False, to_words=False, annotators=None, properties=None):
-        # this is required for some reason
-        matches = self.__regex('/tokensregex', text, pattern, filter, annotators, properties)
-        if to_words:
-            matches = regex_matches_to_indexed_words(matches)
-        return matches
-
-    def semgrex(self, text, pattern, filter=False, to_words=False, annotators=None, properties=None):
-        matches = self.__regex('/semgrex', text, pattern, filter, annotators, properties)
-        if to_words:
-            matches = regex_matches_to_indexed_words(matches)
-        return matches
-
-    def tregex(self, text, pattern, filter=False, annotators=None, properties=None):
-        return self.__regex('/tregex', text, pattern, filter, annotators, properties)
-
-    def __regex(self, path, text, pattern, filter, annotators=None, properties=None):
-        """
-        Send a regex-related request to the CoreNLP server.
-        :param (str | unicode) path: the path for the regex endpoint
-        :param text: raw text for the CoreNLPServer to apply the regex
-        :param (str | unicode) pattern: regex pattern
-        :param (bool) filter: option to filter sentences that contain matches, if false returns matches
-        :param properties: option to filter sentences that contain matches, if false returns matches
-        :return: request result
-        """
-        self.ensure_alive()
-        if properties is None:
-            properties = {}
-            properties.update({
-                'inputFormat': 'text',
-                'serializer': 'edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer'
-            })
-        if annotators:
-            properties['annotators'] = ",".join(annotators) if isinstance(annotators, list) else annotators
-
-        # force output for regex requests to be json
-        properties['outputFormat'] = 'json'
-
-        # TODO: get rid of this once corenlp 4.0.0 is released?
-        # the "stupid reason" has hopefully been fixed on the corenlp side
-        # but maybe people are married to corenlp 3.9.2 for some reason
-        # HACK: For some stupid reason, CoreNLPServer will timeout if we
-        # need to annotate something from scratch. So, we need to call
-        # this to ensure that the _regex call doesn't timeout.
-        self.annotate(text, properties=properties)
-
-        try:
-            # Error occurs unless put properties in params
-            input_format = properties.get("inputFormat", "text")
-            if input_format == "text":
-                ctype = "text/plain; charset=utf-8"
-            elif input_format == "serialized":
-                ctype = "application/x-protobuf"
-            else:
-                raise ValueError("Unrecognized inputFormat " + input_format)
-            # change request method from `get` to `post` as required by CoreNLP
-            r = requests.post(
-                self.endpoint + path, params={
-                    'pattern': pattern,
-                    'filter': filter,
-                    'properties': str(properties)
-                },
-                data=text.encode('utf-8'),
-                headers={'content-type': ctype},
-                timeout=(self.timeout*2)/1000,
-            )
-            r.raise_for_status()
-            return json.loads(r.text)
-        except requests.HTTPError as e:
-            if r.text.startswith("Timeout"):
-                raise TimeoutException(r.text)
-            else:
-                raise AnnotationException(r.text)
-        except json.JSONDecodeError:
-            raise AnnotationException(r.text)
-
-
-def read_corenlp_props(props_path):
-    """ Read a Stanford CoreNLP properties file into a dict """
-    props_dict = {}
-    if os.path.exists(props_path):
-        with open(props_path) as props_file:
-            entry_lines = \
-                [entry_line for entry_line in props_file.read().split('\n')
-                 if entry_line.strip() and not entry_line.startswith('#')]
-            for entry_line in entry_lines:
-                k = entry_line.split('=')[0]
-                k_len = len(k+"=")
-                v = entry_line[k_len:]
-                props_dict[k.strip()] = v
-        return props_dict
-
-    else:
-        raise RuntimeError(f'Error: Properties file at {props_path} does not exist!')
-
-
-def write_corenlp_props(props_dict, file_path=None):
-    """ Write a Stanford CoreNLP properties dict to a file """
-    if file_path is None:
-        file_path = f"corenlp_server-{uuid.uuid4().hex[:16]}.props"
-        # confirm tmp file path matches pattern
-        assert SERVER_PROPS_TMP_FILE_PATTERN.match(file_path)
-    with open(file_path, 'w') as props_file:
-        for k, v in props_dict.items():
-            if isinstance(v, list):
-                writeable_v = ",".join(v)
-            else:
-                writeable_v = v
-            props_file.write(f'{k} = {writeable_v}\n\n')
-    return file_path
-
-
-def regex_matches_to_indexed_words(matches):
-    """
-    Transforms tokensregex and semgrex matches to indexed words.
-    :param matches: unprocessed regex matches
-    :return: flat array of indexed words
-    """
-    words = [dict(v, **dict([('sentence', i)]))
-             for i, s in enumerate(matches['sentences'])
-             for k, v in s.items() if k != 'length']
-    return words
-
-
-__all__ = ["CoreNLPClient", "AnnotationException", "TimeoutException", "to_text"]
+"""
+Client for accessing Stanford CoreNLP in Python
+"""
+
+import atexit
+import contextlib
+import enum
+import io
+import os
+import re
+import requests
+import logging
+import json
+import shlex
+import socket
+import subprocess
+import time
+import sys
+import uuid
+
+from datetime import datetime
+from pathlib import Path
+
+from six.moves.urllib.parse import urlparse
+
+from classla.protobuf import Document, parseFromDelimitedString, writeToDelimitedString, to_text
+__author__ = 'arunchaganty, kelvinguu, vzhong, wmonroe4'
+
+logger = logging.getLogger('classla')
+
+# pattern tmp props file should follow
+SERVER_PROPS_TMP_FILE_PATTERN = re.compile('corenlp_server-(.*).props')
+
+# Check if str is CoreNLP supported language
+CORENLP_LANGS = ['ar', 'arabic', 'chinese', 'zh', 'english', 'en', 'french', 'fr', 'de', 'german', 'es', 'spanish']
+
+LANGUAGE_SHORTHANDS_TO_FULL = {
+    "ar": "arabic",
+    "zh": "chinese",
+    "en": "english",
+    "fr": "french",
+    "de": "german",
+    "es": "spanish"
+}
+
+
+def is_corenlp_lang(props_str):
+    """ Check if a string references a CoreNLP language """
+    return props_str.lower() in CORENLP_LANGS
+
+
+# Validate CoreNLP properties
+CORENLP_OUTPUT_VALS = ["conll", "conllu", "json", "serialized", "text", "xml", "inlinexml"]
+
+
+def validate_corenlp_props(properties=None, annotators=None, output_format=None):
+    """ Do basic checks to validate CoreNLP properties """
+    if output_format and output_format.lower() not in CORENLP_OUTPUT_VALS:
+        raise ValueError(f"{output_format} not a valid CoreNLP outputFormat value! Choose from: {CORENLP_OUTPUT_VALS}")
+    if type(properties) == dict:
+        if "outputFormat" in properties and properties["outputFormat"].lower() not in CORENLP_OUTPUT_VALS:
+            raise ValueError(f"{properties['outputFormat']} not a valid CoreNLP outputFormat value! Choose from: "
+                             f"{CORENLP_OUTPUT_VALS}")
+
+
+class AnnotationException(Exception):
+    """ Exception raised when there was an error communicating with the CoreNLP server. """
+    pass
+
+
+class TimeoutException(AnnotationException):
+    """ Exception raised when the CoreNLP server timed out. """
+    pass
+
+
+class ShouldRetryException(Exception):
+    """ Exception raised if the service should retry the request. """
+    pass
+
+
+class PermanentlyFailedException(Exception):
+    """ Exception raised if the service should NOT retry the request. """
+    pass
+
+class StartServer(enum.Enum):
+    DONT_START = 0
+    FORCE_START = 1
+    TRY_START = 2
+
+
+def clean_props_file(props_file):
+    # check if there is a temp server props file to remove and remove it
+    if props_file:
+        if os.path.isfile(props_file) and SERVER_PROPS_TMP_FILE_PATTERN.match(os.path.basename(props_file)):
+            os.remove(props_file)
+
+
+class RobustService(object):
+    """ Service that resuscitates itself if it is not available. """
+    CHECK_ALIVE_TIMEOUT = 120
+
+    def __init__(self, start_cmd, stop_cmd, endpoint, stdout=sys.stdout,
+                 stderr=sys.stderr, be_quiet=False, host=None, port=None, ignore_binding_error=False):
+        self.start_cmd = start_cmd and shlex.split(start_cmd)
+        self.stop_cmd = stop_cmd and shlex.split(stop_cmd)
+        self.endpoint = endpoint
+        self.stdout = stdout
+        self.stderr = stderr
+
+        self.server = None
+        self.is_active = False
+        self.be_quiet = be_quiet
+        self.host = host
+        self.port = port
+        self.ignore_binding_error = ignore_binding_error
+        atexit.register(self.atexit_kill)
+
+    def is_alive(self):
+        try:
+            if not self.ignore_binding_error and self.server is not None and self.server.poll() is not None:
+                return False
+            return requests.get(self.endpoint + "/ping").ok
+        except requests.exceptions.ConnectionError as e:
+            raise ShouldRetryException(e)
+
+    def start(self):
+        if self.start_cmd:
+            if self.host and self.port:
+                with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:
+                    try:
+                        sock.bind((self.host, self.port))
+                    except socket.error:
+                        if self.ignore_binding_error:
+                            logger.info(f"Connecting to existing CoreNLP server at {self.host}:{self.port}")
+                            self.server = None
+                            return
+                        else:
+                            raise PermanentlyFailedException("Error: unable to start the CoreNLP server on port %d "
+                                                         "(possibly something is already running there)" % self.port)
+            if self.be_quiet:
+                # Issue #26: subprocess.DEVNULL isn't supported in python 2.7.
+                stderr = open(os.devnull, 'w')
+            else:
+                stderr = self.stderr
+            logger.info(f"Starting server with command: {' '.join(self.start_cmd)}")
+            self.server = subprocess.Popen(self.start_cmd,
+                                           stderr=stderr,
+                                           stdout=stderr)
+
+    def atexit_kill(self):
+        # make some kind of effort to stop the service (such as a
+        # CoreNLP server) at the end of the program.  not waiting so
+        # that the python script exiting isn't delayed
+        if self.server and self.server.poll() is None:
+            self.server.terminate()
+
+    def stop(self):
+        if self.server:
+            self.server.terminate()
+            try:
+                self.server.wait(5)
+            except subprocess.TimeoutExpired:
+                # Resorting to more aggressive measures...
+                self.server.kill()
+                try:
+                    self.server.wait(5)
+                except subprocess.TimeoutExpired:
+                    # oh well
+                    pass
+            self.server = None
+        if self.stop_cmd:
+            subprocess.run(self.stop_cmd, check=True)
+        self.is_active = False
+
+    def __enter__(self):
+        self.start()
+        return self
+
+    def __exit__(self, _, __, ___):
+        self.stop()
+
+    def ensure_alive(self):
+        # Check if the service is active and alive
+        if self.is_active:
+            try:
+                if self.is_alive():
+                    return
+                else:
+                    self.stop()
+            except ShouldRetryException:
+                pass
+
+        # If not, try to start up the service.
+        if self.server is None:
+            self.start()
+
+        # Wait for the service to start up.
+        start_time = time.time()
+        while True:
+            try:
+                if self.is_alive():
+                    break
+            except ShouldRetryException:
+                pass
+
+            if time.time() - start_time < self.CHECK_ALIVE_TIMEOUT:
+                time.sleep(1)
+            else:
+                raise PermanentlyFailedException("Timed out waiting for service to come alive.")
+
+        # At this point we are guaranteed that the service is alive.
+        self.is_active = True
+
+
+def resolve_classpath(classpath=None):
+    """
+    Returns the classpath to use for corenlp.
+
+    Prefers to use the given classpath parameter, if available.  If
+    not, uses the CORENLP_HOME environment variable.  Resolves $CLASSPATH
+    (the exact string) in either the classpath parameter or $CORENLP_HOME.
+    """
+    if classpath == '$CLASSPATH' or (classpath is None and os.getenv("CORENLP_HOME", None) == '$CLASSPATH'):
+        classpath = os.getenv("CLASSPATH")
+    elif classpath is None:
+        classpath = os.getenv("CORENLP_HOME", os.path.join(str(Path.home()), 'stanza_corenlp'))
+
+        assert os.path.exists(classpath), \
+            "Please install CoreNLP by running `classla.install_corenlp()`. If you have installed it, please define " \
+            "$CORENLP_HOME to be location of your CoreNLP distribution or pass in a classpath parameter."
+        classpath = os.path.join(classpath, "*")
+    return classpath
+
+
+class CoreNLPClient(RobustService):
+    """ A client to the Stanford CoreNLP server. """
+
+    DEFAULT_ENDPOINT = "http://localhost:9000"
+    DEFAULT_TIMEOUT = 60000
+    DEFAULT_THREADS = 5
+    DEFAULT_OUTPUT_FORMAT = "serialized"
+    DEFAULT_MEMORY = "5G"
+    DEFAULT_MAX_CHAR_LENGTH = 100000
+
+    def __init__(self, start_server=StartServer.FORCE_START,
+                 endpoint=DEFAULT_ENDPOINT,
+                 timeout=DEFAULT_TIMEOUT,
+                 threads=DEFAULT_THREADS,
+                 annotators=None,
+                 output_format=None,
+                 properties=None,
+                 stdout=sys.stdout,
+                 stderr=sys.stderr,
+                 memory=DEFAULT_MEMORY,
+                 be_quiet=False,
+                 max_char_length=DEFAULT_MAX_CHAR_LENGTH,
+                 preload=True,
+                 classpath=None,
+                 **kwargs):
+
+        # whether or not server should be started by client
+        self.start_server = start_server
+        self.server_props_path = None
+        self.server_start_time = None
+        self.server_host = None
+        self.server_port = None
+        self.server_classpath = None
+        # validate properties
+        validate_corenlp_props(properties=properties, annotators=annotators, output_format=output_format)
+        # set up client defaults
+        self.properties = properties
+        self.annotators = annotators
+        self.output_format = output_format
+        self._setup_client_defaults()
+        # start the server
+        if isinstance(start_server, bool):
+            warning_msg = f"Setting 'start_server' to a boolean value when constructing {self.__class__.__name__} is deprecated and will stop" + \
+                " to function in a future version of classla. Please consider switching to using a value from classla.server.StartServer."
+            logger.warning(warning_msg)
+            start_server = StartServer.FORCE_START if start_server is True else StartServer.DONT_START
+
+        # start the server
+        if start_server is StartServer.FORCE_START or start_server is StartServer.TRY_START:
+            # record info for server start
+            self.server_start_time = datetime.now()
+            # set up default properties for server
+            self._setup_server_defaults()
+            host, port = urlparse(endpoint).netloc.split(":")
+            port = int(port)
+            assert host == "localhost", "If starting a server, endpoint must be localhost"
+            classpath = resolve_classpath(classpath)
+            start_cmd = f"java -Xmx{memory} -cp '{classpath}'  edu.stanford.nlp.pipeline.StanfordCoreNLPServer " \
+                        f"-port {port} -timeout {timeout} -threads {threads} -maxCharLength {max_char_length} " \
+                        f"-quiet {be_quiet} "
+
+            self.server_classpath = classpath
+            self.server_host = host
+            self.server_port = port
+
+            # set up server defaults
+            if self.server_props_path is not None:
+                start_cmd += f" -serverProperties {self.server_props_path}"
+
+            # set annotators for server default
+            if self.annotators is not None:
+                annotators_str = self.annotators if type(annotators) == str else ",".join(annotators)
+                start_cmd += f" -annotators {annotators_str}"
+
+            # specify what to preload, if anything
+            if preload:
+                if type(preload) == bool:
+                    # -preload flag means to preload all default annotators
+                    start_cmd += " -preload"
+                elif type(preload) == list:
+                    # turn list into comma separated list string, only preload these annotators
+                    start_cmd += f" -preload {','.join(preload)}"
+                elif type(preload) == str:
+                    # comma separated list of annotators
+                    start_cmd += f" -preload {preload}"
+
+            # set outputFormat for server default
+            # if no output format requested by user, set to serialized
+            start_cmd += f" -outputFormat {self.output_format}"
+
+            # additional options for server:
+            # - server_id
+            # - ssl
+            # - status_port
+            # - uriContext
+            # - strict
+            # - key
+            # - username
+            # - password
+            # - blockList
+            for kw in ['ssl', 'strict']:
+                if kwargs.get(kw) is not None:
+                    start_cmd += f" -{kw}"
+            for kw in ['status_port', 'uriContext', 'key', 'username', 'password', 'blockList', 'server_id']:
+                if kwargs.get(kw) is not None:
+                    start_cmd += f" -{kw} {kwargs.get(kw)}"
+            stop_cmd = None
+        else:
+            start_cmd = stop_cmd = None
+            host = port = None
+
+        super(CoreNLPClient, self).__init__(start_cmd, stop_cmd, endpoint,
+                                            stdout, stderr, be_quiet, host=host, port=port, ignore_binding_error=(start_server == StartServer.TRY_START))
+
+        self.timeout = timeout
+
+    def _setup_client_defaults(self):
+        """
+        Do some processing of annotators and output_format specified for the client.
+        If interacting with an externally started server, these will be defaults for annotate() calls.
+        :return: None
+        """
+        # normalize annotators to str
+        if self.annotators is not None:
+            self.annotators = self.annotators if type(self.annotators) == str else ",".join(self.annotators)
+
+        # handle case where no output format is specified
+        if self.output_format is None:
+            if type(self.properties) == dict and 'outputFormat' in self.properties:
+                self.output_format = self.properties['outputFormat']
+            else:
+                self.output_format = CoreNLPClient.DEFAULT_OUTPUT_FORMAT
+
+    def _setup_server_defaults(self):
+        """
+        Set up the default properties for the server.
+
+        The properties argument can take on one of 3 value types
+
+        1. File path on system or in CLASSPATH (e.g. /path/to/server.props or StanfordCoreNLP-french.properties
+        2. Name of a Stanford CoreNLP supported language (e.g. french or fr)
+        3. Python dictionary (properties written to tmp file for Java server, erased at end)
+
+        In addition, an annotators list and output_format can be specified directly with arguments. These
+        will overwrite any settings in the specified properties.
+
+        If no properties are specified, the standard Stanford CoreNLP English server will be launched. The outputFormat
+        will be set to 'serialized' and use the ProtobufAnnotationSerializer.
+        """
+
+        # ensure properties is str or dict
+        if self.properties is None or (not isinstance(self.properties, str) and not isinstance(self.properties, dict)):
+            if self.properties is not None:
+                logger.warning('properties passed invalid value (not a str or dict), setting properties = {}')
+            self.properties = {}
+        # check if properties is a string, pass on to server which can handle
+        if isinstance(self.properties, str):
+            # try to translate to Stanford CoreNLP language name, or assume properties is a path
+            if is_corenlp_lang(self.properties):
+                if self.properties.lower() in LANGUAGE_SHORTHANDS_TO_FULL:
+                    self.properties = LANGUAGE_SHORTHANDS_TO_FULL[self.properties]
+                logger.info(
+                    f"Using Stanford CoreNLP default properties for: {self.properties}.  Make sure to have "
+                    f"{self.properties} models jar (available for download here: "
+                    f"https://stanfordnlp.github.io/CoreNLP/) in CLASSPATH")
+            else:
+                if not os.path.isfile(self.properties):
+                    logger.warning(f"{self.properties} does not correspond to a file path. Make sure this file is in "
+                                   f"your CLASSPATH.")
+            self.server_props_path = self.properties
+        elif isinstance(self.properties, dict):
+            # make a copy
+            server_start_properties = dict(self.properties)
+            if self.annotators is not None:
+                server_start_properties['annotators'] = self.annotators
+            if self.output_format is not None and isinstance(self.output_format, str):
+                server_start_properties['outputFormat'] = self.output_format
+            # write desired server start properties to tmp file
+            # set up to erase on exit
+            tmp_path = write_corenlp_props(server_start_properties)
+            logger.info(f"Writing properties to tmp file: {tmp_path}")
+            atexit.register(clean_props_file, tmp_path)
+            self.server_props_path = tmp_path
+
+    def _request(self, buf, properties, reset_default=False, **kwargs):
+        """
+        Send a request to the CoreNLP server.
+
+        :param (str | bytes) buf: data to be sent with the request
+        :param (dict) properties: properties that the server expects
+        :return: request result
+        """
+        self.ensure_alive()
+
+        try:
+            input_format = properties.get("inputFormat", "text")
+            if input_format == "text":
+                ctype = "text/plain; charset=utf-8"
+            elif input_format == "serialized":
+                ctype = "application/x-protobuf"
+            else:
+                raise ValueError("Unrecognized inputFormat " + input_format)
+            # handle auth
+            if 'username' in kwargs and 'password' in kwargs:
+                kwargs['auth'] = requests.auth.HTTPBasicAuth(kwargs['username'], kwargs['password'])
+                kwargs.pop('username')
+                kwargs.pop('password')
+            r = requests.post(self.endpoint,
+                              params={'properties': str(properties), 'resetDefault': str(reset_default).lower()},
+                              data=buf, headers={'content-type': ctype},
+                              timeout=(self.timeout*2)/1000, **kwargs)
+            r.raise_for_status()
+            return r
+        except requests.HTTPError as e:
+            if r.text == "CoreNLP request timed out. Your document may be too long.":
+                raise TimeoutException(r.text)
+            else:
+                raise AnnotationException(r.text)
+
+    def annotate(self, text, annotators=None, output_format=None, properties=None, reset_default=None, **kwargs):
+        """
+        Send a request to the CoreNLP server.
+
+        :param (str | unicode) text: raw text for the CoreNLPServer to parse
+        :param (list | string) annotators: list of annotators to use
+        :param (str) output_format: output type from server: serialized, json, text, conll, conllu, or xml
+        :param (dict) properties: additional request properties (written on top of defaults)
+        :param (bool) reset_default: don't use server defaults
+
+        Precedence for settings:
+
+        1. annotators and output_format args
+        2. Values from properties dict
+        3. Client defaults self.annotators and self.output_format (set during client construction)
+        4. Server defaults
+
+        Additional request parameters (apart from CoreNLP pipeline properties) such as 'username' and 'password'
+        can be specified with the kwargs.
+
+        :return: request result
+        """
+
+        # validate request properties
+        validate_corenlp_props(properties=properties, annotators=annotators, output_format=output_format)
+        # set request properties
+        request_properties = {}
+
+        # start with client defaults
+        if self.annotators is not None:
+            request_properties['annotators'] = self.annotators
+        if self.output_format is not None:
+            request_properties['outputFormat'] = self.output_format
+
+        # add values from properties arg
+        # handle str case
+        if type(properties) == str:
+            if is_corenlp_lang(properties):
+                properties = {'pipelineLanguage': properties.lower()}
+                if reset_default is None:
+                    reset_default = True
+            else:
+                raise ValueError(f"Unrecognized properties keyword {properties}")
+
+        if type(properties) == dict:
+            request_properties.update(properties)
+
+        # if annotators list is specified, override with that
+        # also can use the annotators field the object was created with
+        if annotators is not None and (type(annotators) == str or type(annotators) == list):
+            request_properties['annotators'] = annotators if type(annotators) == str else ",".join(annotators)
+
+        # if output format is specified, override with that
+        if output_format is not None and type(output_format) == str:
+            request_properties['outputFormat'] = output_format
+
+        # make the request
+        # if not explictly set or the case of pipelineLanguage, reset_default should be None
+        if reset_default is None:
+            reset_default = False
+        r = self._request(text.encode('utf-8'), request_properties, reset_default, **kwargs)
+        if request_properties["outputFormat"] == "json":
+            return r.json()
+        elif request_properties["outputFormat"] == "serialized":
+            doc = Document()
+            parseFromDelimitedString(doc, r.content)
+            return doc
+        elif request_properties["outputFormat"] in ["text", "conllu", "conll", "xml"]:
+            return r.text
+        else:
+            return r
+
+    def update(self, doc, annotators=None, properties=None):
+        if properties is None:
+            properties = {}
+            properties.update({
+                'inputFormat': 'serialized',
+                'outputFormat': 'serialized',
+                'serializer': 'edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer'
+            })
+        if annotators:
+            properties['annotators'] = annotators if type(annotators) == str else ",".join(annotators)
+        with io.BytesIO() as stream:
+            writeToDelimitedString(doc, stream)
+            msg = stream.getvalue()
+
+        r = self._request(msg, properties)
+        doc = Document()
+        parseFromDelimitedString(doc, r.content)
+        return doc
+
+    def tokensregex(self, text, pattern, filter=False, to_words=False, annotators=None, properties=None):
+        # this is required for some reason
+        matches = self.__regex('/tokensregex', text, pattern, filter, annotators, properties)
+        if to_words:
+            matches = regex_matches_to_indexed_words(matches)
+        return matches
+
+    def semgrex(self, text, pattern, filter=False, to_words=False, annotators=None, properties=None):
+        matches = self.__regex('/semgrex', text, pattern, filter, annotators, properties)
+        if to_words:
+            matches = regex_matches_to_indexed_words(matches)
+        return matches
+
+    def tregex(self, text, pattern, filter=False, annotators=None, properties=None):
+        return self.__regex('/tregex', text, pattern, filter, annotators, properties)
+
+    def __regex(self, path, text, pattern, filter, annotators=None, properties=None):
+        """
+        Send a regex-related request to the CoreNLP server.
+        :param (str | unicode) path: the path for the regex endpoint
+        :param text: raw text for the CoreNLPServer to apply the regex
+        :param (str | unicode) pattern: regex pattern
+        :param (bool) filter: option to filter sentences that contain matches, if false returns matches
+        :param properties: option to filter sentences that contain matches, if false returns matches
+        :return: request result
+        """
+        self.ensure_alive()
+        if properties is None:
+            properties = {}
+            properties.update({
+                'inputFormat': 'text',
+                'serializer': 'edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer'
+            })
+        if annotators:
+            properties['annotators'] = ",".join(annotators) if isinstance(annotators, list) else annotators
+
+        # force output for regex requests to be json
+        properties['outputFormat'] = 'json'
+
+        # TODO: get rid of this once corenlp 4.0.0 is released?
+        # the "stupid reason" has hopefully been fixed on the corenlp side
+        # but maybe people are married to corenlp 3.9.2 for some reason
+        # HACK: For some stupid reason, CoreNLPServer will timeout if we
+        # need to annotate something from scratch. So, we need to call
+        # this to ensure that the _regex call doesn't timeout.
+        self.annotate(text, properties=properties)
+
+        try:
+            # Error occurs unless put properties in params
+            input_format = properties.get("inputFormat", "text")
+            if input_format == "text":
+                ctype = "text/plain; charset=utf-8"
+            elif input_format == "serialized":
+                ctype = "application/x-protobuf"
+            else:
+                raise ValueError("Unrecognized inputFormat " + input_format)
+            # change request method from `get` to `post` as required by CoreNLP
+            r = requests.post(
+                self.endpoint + path, params={
+                    'pattern': pattern,
+                    'filter': filter,
+                    'properties': str(properties)
+                },
+                data=text.encode('utf-8'),
+                headers={'content-type': ctype},
+                timeout=(self.timeout*2)/1000,
+            )
+            r.raise_for_status()
+            return json.loads(r.text)
+        except requests.HTTPError as e:
+            if r.text.startswith("Timeout"):
+                raise TimeoutException(r.text)
+            else:
+                raise AnnotationException(r.text)
+        except json.JSONDecodeError:
+            raise AnnotationException(r.text)
+
+
+def read_corenlp_props(props_path):
+    """ Read a Stanford CoreNLP properties file into a dict """
+    props_dict = {}
+    if os.path.exists(props_path):
+        with open(props_path) as props_file:
+            entry_lines = \
+                [entry_line for entry_line in props_file.read().split('\n')
+                 if entry_line.strip() and not entry_line.startswith('#')]
+            for entry_line in entry_lines:
+                k = entry_line.split('=')[0]
+                k_len = len(k+"=")
+                v = entry_line[k_len:]
+                props_dict[k.strip()] = v
+        return props_dict
+
+    else:
+        raise RuntimeError(f'Error: Properties file at {props_path} does not exist!')
+
+
+def write_corenlp_props(props_dict, file_path=None):
+    """ Write a Stanford CoreNLP properties dict to a file """
+    if file_path is None:
+        file_path = f"corenlp_server-{uuid.uuid4().hex[:16]}.props"
+        # confirm tmp file path matches pattern
+        assert SERVER_PROPS_TMP_FILE_PATTERN.match(file_path)
+    with open(file_path, 'w') as props_file:
+        for k, v in props_dict.items():
+            if isinstance(v, list):
+                writeable_v = ",".join(v)
+            else:
+                writeable_v = v
+            props_file.write(f'{k} = {writeable_v}\n\n')
+    return file_path
+
+
+def regex_matches_to_indexed_words(matches):
+    """
+    Transforms tokensregex and semgrex matches to indexed words.
+    :param matches: unprocessed regex matches
+    :return: flat array of indexed words
+    """
+    words = [dict(v, **dict([('sentence', i)]))
+             for i, s in enumerate(matches['sentences'])
+             for k, v in s.items() if k != 'length']
+    return words
+
+
+__all__ = ["CoreNLPClient", "AnnotationException", "TimeoutException", "to_text"]
```

### Comparing `classla-2.0/classla/server/main.py` & `classla-2.1/classla/server/main.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,72 +1,72 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Simple shell program to pipe in 
-"""
-
-import corenlp
-
-import json
-import re
-import csv
-import sys
-from collections import namedtuple, OrderedDict
-
-FLOAT_RE = re.compile(r"\d*\.\d+")
-INT_RE = re.compile(r"\d+")
-
-def dictstr(arg):
-    """
-    Parse a key=value string as a tuple (key, value) that can be provided as an argument to dict()
-    """
-    key, value = arg.split("=")
-
-    if value.lower() == "true" or value.lower() == "false":
-        value = bool(value)
-    elif INT_RE.match(value):
-        value = int(value)
-    elif FLOAT_RE.match(value):
-        value = float(value)
-    return (key, value)
-
-
-def do_annotate(args):
-    args.props = dict(args.props) if args.props else {}
-    if args.sentence_mode:
-        args.props["ssplit.isOneSentence"] = True
-
-    with corenlp.CoreNLPClient(annotators=args.annotators, properties=args.props, be_quiet=not args.verbose_server) as client:
-        for line in args.input:
-            if line.startswith("#"): continue
-
-            ann = client.annotate(line.strip(), output_format=args.format)
-
-            if args.format == "json":
-                if args.sentence_mode:
-                    ann = ann["sentences"][0]
-
-                args.output.write(json.dumps(ann))
-                args.output.write("\n")
-
-def main():
-    import argparse
-    parser = argparse.ArgumentParser(description='Annotate data')
-    parser.add_argument('-i', '--input', type=argparse.FileType('r'), default=sys.stdin, help="Input file to process; each line contains one document (default: stdin)")
-    parser.add_argument('-o', '--output', type=argparse.FileType('w'), default=sys.stdout, help="File to write annotations to (default: stdout)")
-    parser.add_argument('-f', '--format', choices=["json",], default="json", help="Output format")
-    parser.add_argument('-a', '--annotators', nargs="+", type=str, default=["tokenize ssplit lemma pos"], help="A list of annotators")
-    parser.add_argument('-s', '--sentence-mode', action="store_true",help="Assume each line of input is a sentence.")
-    parser.add_argument('-v', '--verbose-server', action="store_true",help="Server is made verbose")
-    parser.add_argument('-m', '--memory', type=str, default="4G", help="Memory to use for the server")
-    parser.add_argument('-p', '--props', nargs="+", type=dictstr, help="Properties as a list of key=value pairs")
-    parser.set_defaults(func=do_annotate)
-
-    ARGS = parser.parse_args()
-    if ARGS.func is None:
-        parser.print_help()
-        sys.exit(1)
-    else:
-        ARGS.func(ARGS)
-
-if __name__ == "__main__":
-    main()
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Simple shell program to pipe in 
+"""
+
+import corenlp
+
+import json
+import re
+import csv
+import sys
+from collections import namedtuple, OrderedDict
+
+FLOAT_RE = re.compile(r"\d*\.\d+")
+INT_RE = re.compile(r"\d+")
+
+def dictstr(arg):
+    """
+    Parse a key=value string as a tuple (key, value) that can be provided as an argument to dict()
+    """
+    key, value = arg.split("=")
+
+    if value.lower() == "true" or value.lower() == "false":
+        value = bool(value)
+    elif INT_RE.match(value):
+        value = int(value)
+    elif FLOAT_RE.match(value):
+        value = float(value)
+    return (key, value)
+
+
+def do_annotate(args):
+    args.props = dict(args.props) if args.props else {}
+    if args.sentence_mode:
+        args.props["ssplit.isOneSentence"] = True
+
+    with corenlp.CoreNLPClient(annotators=args.annotators, properties=args.props, be_quiet=not args.verbose_server) as client:
+        for line in args.input:
+            if line.startswith("#"): continue
+
+            ann = client.annotate(line.strip(), output_format=args.format)
+
+            if args.format == "json":
+                if args.sentence_mode:
+                    ann = ann["sentences"][0]
+
+                args.output.write(json.dumps(ann))
+                args.output.write("\n")
+
+def main():
+    import argparse
+    parser = argparse.ArgumentParser(description='Annotate data')
+    parser.add_argument('-i', '--input', type=argparse.FileType('r'), default=sys.stdin, help="Input file to process; each line contains one document (default: stdin)")
+    parser.add_argument('-o', '--output', type=argparse.FileType('w'), default=sys.stdout, help="File to write annotations to (default: stdout)")
+    parser.add_argument('-f', '--format', choices=["json",], default="json", help="Output format")
+    parser.add_argument('-a', '--annotators', nargs="+", type=str, default=["tokenize ssplit lemma pos"], help="A list of annotators")
+    parser.add_argument('-s', '--sentence-mode', action="store_true",help="Assume each line of input is a sentence.")
+    parser.add_argument('-v', '--verbose-server', action="store_true",help="Server is made verbose")
+    parser.add_argument('-m', '--memory', type=str, default="4G", help="Memory to use for the server")
+    parser.add_argument('-p', '--props', nargs="+", type=dictstr, help="Properties as a list of key=value pairs")
+    parser.set_defaults(func=do_annotate)
+
+    ARGS = parser.parse_args()
+    if ARGS.func is None:
+        parser.print_help()
+        sys.exit(1)
+    else:
+        ARGS.func(ARGS)
+
+if __name__ == "__main__":
+    main()
```

### Comparing `classla-2.0/classla/utils/conll.py` & `classla-2.1/classla/utils/conll.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,201 +1,201 @@
-"""
-Utility functions for the loading and conversion of CoNLL-format files.
-"""
-import os
-import io
-
-FIELD_NUM = 10
-
-ID = 'id'
-TEXT = 'text'
-LEMMA = 'lemma'
-UPOS = 'upos'
-XPOS = 'xpos'
-FEATS = 'feats'
-HEAD = 'head'
-DEPREL = 'deprel'
-DEPS = 'deps'
-MISC = 'misc'
-NER = 'ner'
-SRL = 'srl'
-FIELD_TO_IDX = {ID: 0, TEXT: 1, LEMMA: 2, UPOS: 3, XPOS: 4, FEATS: 5, HEAD: 6, DEPREL: 7, DEPS: 8, MISC: 9}
-
-
-class ListMetadata:
-    def __init__(self, array, metadata):
-        self.list = array
-        self.metadata = metadata
-
-    def __iter__(self):
-        for el in self.list:
-            yield el
-
-
-class CoNLL:
-    @staticmethod
-    def load_conll(f, ignore_gapping=True, generate_raw_text=False):
-        """ Load the file or string into the CoNLL-U format data.
-        Input: file or string reader, where the data is in CoNLL-U format.
-        Output: a list of list of list for each token in each sentence in the data, where the innermost list represents
-        all fields of a token.
-        """
-        doc, sent, metadata, raw_text = [], [], '', ''
-        for line in f:
-            raw_line = line
-            line = line.strip()
-            if len(line) == 0:
-                if len(sent) > 0:
-                    if generate_raw_text:
-                        doc.append((sent, metadata, raw_text))
-                    else:
-                        doc.append((sent, metadata))
-                    metadata = ''
-                    sent = []
-            else:
-                if line.startswith('#'): # skip comment line
-                    metadata += raw_line
-                    if generate_raw_text and line.startswith('# text = '):
-                        raw_text = line[9:]
-                    continue
-                array = line.split('\t')
-                if ignore_gapping and '.' in array[0]:
-                    continue
-                assert len(array) == FIELD_NUM, \
-                        f"Cannot parse CoNLL line: expecting {FIELD_NUM} fields, {len(array)} found."
-                sent += [array]
-        if len(sent) > 0:
-            if generate_raw_text:
-                doc.append((sent, metadata, raw_text))
-            else:
-                doc.append((sent, metadata))
-        return doc
-
-    @staticmethod
-    def convert_conll(doc_conll, generate_raw_text=False):
-        """ Convert the CoNLL-U format input data to a dictionary format output data.
-        Input: list of token fields loaded from the CoNLL-U format data, where the outmost list represents a list of sentences, and the inside list represents all fields of a token.
-        Output: a list of list of dictionaries for each token in each sentence in the document.
-        """
-        doc_dict = []
-        metadata_list = []
-        raw_text_list = []
-        for sent in doc_conll:
-            if generate_raw_text:
-                sent_conll, metadata, raw_text = sent
-                raw_text_list.append(raw_text)
-            else:
-                sent_conll, metadata = sent
-            sent_dict = []
-            for token_conll in sent_conll:
-                token_dict = CoNLL.convert_conll_token(token_conll)
-                sent_dict.append(token_dict)
-            metadata_list.append(metadata)
-            doc_dict.append(sent_dict)
-        if generate_raw_text:
-            return doc_dict, metadata_list, ' '.join(raw_text_list)
-        return doc_dict, metadata_list
-
-    @staticmethod
-    def convert_conll_token(token_conll):
-        """ Convert the CoNLL-U format input token to the dictionary format output token.
-        Input: a list of all CoNLL-U fields for the token.
-        Output: a dictionary that maps from field name to value.
-        """
-        token_dict = {}
-        for field in FIELD_TO_IDX:
-            value = token_conll[FIELD_TO_IDX[field]]
-            if value != '_':
-                if field == HEAD:
-                    token_dict[field] = int(value)
-                elif field == ID:
-                    token_dict[field] = tuple(int(x) for x in value.split('-'))
-                else:
-                    token_dict[field] = value
-            # special case if text is '_'
-            if token_conll[FIELD_TO_IDX[TEXT]] == '_':
-                token_dict[TEXT] = token_conll[FIELD_TO_IDX[TEXT]]
-                token_dict[LEMMA] = token_conll[FIELD_TO_IDX[LEMMA]]
-        return token_dict
-
-    @staticmethod
-    def conll2dict(input_file=None, input_str=None, ignore_gapping=True, generate_raw_text=False):
-        """ Load the CoNLL-U format data from file or string into lists of dictionaries.
-        """
-        assert any([input_file, input_str]) and not all([input_file, input_str]), 'either input input file or input string'
-        if input_str:
-            infile = io.StringIO(input_str)
-        else:
-            infile = open(input_file)
-        doc_conll = CoNLL.load_conll(infile, ignore_gapping, generate_raw_text)
-        doc_dict = CoNLL.convert_conll(doc_conll, generate_raw_text)
-        return doc_dict
-
-    @staticmethod
-    def convert_dict(doc_dict):
-        """ Convert the dictionary format input data to the CoNLL-U format output data. This is the reverse function of
-        `convert_conll`.
-        Input: dictionary format data, which is a list of list of dictionaries for each token in each sentence in the data.
-        Output: CoNLL-U format data, which is a list of list of list for each token in each sentence in the data.
-        """
-        doc_conll = []
-        for sent_dict, metadata in doc_dict:
-            sent_conll = []
-            for token_dict in sent_dict:
-                token_conll = CoNLL.convert_token_dict(token_dict)
-                sent_conll.append(token_conll)
-            doc_conll.append((sent_conll, metadata))
-        return doc_conll
-
-    @staticmethod
-    def convert_token_dict(token_dict):
-        """ Convert the dictionary format input token to the CoNLL-U format output token. This is the reverse function of
-        `convert_conll_token`.
-        Input: dictionary format token, which is a dictionaries for the token.
-        Output: CoNLL-U format token, which is a list for the token.
-        """
-        token_conll = ['_' for i in range(FIELD_NUM)]
-        misc_dict = {}
-        for key in token_dict:
-            if key == ID:
-                token_conll[FIELD_TO_IDX[key]] = '-'.join([str(x) for x in token_dict[key]]) if isinstance(token_dict[key], tuple) else str(token_dict[key])
-            elif key == NER:
-                misc_dict['NER'] = str(token_dict[key])
-            elif key == SRL:
-                misc_dict['SRL'] = str(token_dict[key])
-            elif key == MISC:
-                for misc in str(token_dict[key]).split('|'):
-                    misc_key, misc_val = misc.split('=')
-                    if misc_key not in [NER, SRL]:
-                        misc_dict[misc_key] = misc_val
-            elif key in FIELD_TO_IDX:
-                token_conll[FIELD_TO_IDX[key]] = str(token_dict[key])
-        token_conll[FIELD_TO_IDX[MISC]] = '|'.join([k + '=' + v for k, v in sorted(misc_dict.items(), key=lambda item: item[0].lower())]) if misc_dict else '_'
-        return token_conll
-
-    @staticmethod
-    def conll_as_string(doc):
-        """ Dump the loaded CoNLL-U format list data to string. """
-        return_string = ""
-        for sent, meta in doc:
-            return_string += meta if meta else ''
-            for ln in sent:
-                return_string += ("\t".join(ln)+"\n")
-            return_string += "\n"
-        return return_string
-
-    @staticmethod
-    def dict2conll(doc_dict, filename):
-        """ Convert the dictionary format input data to the CoNLL-U format output data and write to a file.
-        """
-        doc_conll = CoNLL.convert_dict(doc_dict)
-        conll_string = CoNLL.conll_as_string(doc_conll)
-        with open(filename, 'w') as outfile:
-            outfile.write(conll_string)
-        return
-
-    @staticmethod
-    def write_doc2conll(doc, filename):
-        """ Writes the doc as a conll file to the given filename
-        """
-        with open(filename, 'w', encoding='utf-8') as outfile:
-            outfile.write(CoNLL.conll_as_string(CoNLL.convert_dict(doc.to_dict())))
+"""
+Utility functions for the loading and conversion of CoNLL-format files.
+"""
+import os
+import io
+
+FIELD_NUM = 10
+
+ID = 'id'
+TEXT = 'text'
+LEMMA = 'lemma'
+UPOS = 'upos'
+XPOS = 'xpos'
+FEATS = 'feats'
+HEAD = 'head'
+DEPREL = 'deprel'
+DEPS = 'deps'
+MISC = 'misc'
+NER = 'ner'
+SRL = 'srl'
+FIELD_TO_IDX = {ID: 0, TEXT: 1, LEMMA: 2, UPOS: 3, XPOS: 4, FEATS: 5, HEAD: 6, DEPREL: 7, DEPS: 8, MISC: 9}
+
+
+class ListMetadata:
+    def __init__(self, array, metadata):
+        self.list = array
+        self.metadata = metadata
+
+    def __iter__(self):
+        for el in self.list:
+            yield el
+
+
+class CoNLL:
+    @staticmethod
+    def load_conll(f, ignore_gapping=True, generate_raw_text=False):
+        """ Load the file or string into the CoNLL-U format data.
+        Input: file or string reader, where the data is in CoNLL-U format.
+        Output: a list of list of list for each token in each sentence in the data, where the innermost list represents
+        all fields of a token.
+        """
+        doc, sent, metadata, raw_text = [], [], '', ''
+        for line in f:
+            raw_line = line
+            line = line.strip()
+            if len(line) == 0:
+                if len(sent) > 0:
+                    if generate_raw_text:
+                        doc.append((sent, metadata, raw_text))
+                    else:
+                        doc.append((sent, metadata))
+                    metadata = ''
+                    sent = []
+            else:
+                if line.startswith('#'): # skip comment line
+                    metadata += raw_line
+                    if generate_raw_text and line.startswith('# text = '):
+                        raw_text = line[9:]
+                    continue
+                array = line.split('\t')
+                if ignore_gapping and '.' in array[0]:
+                    continue
+                assert len(array) == FIELD_NUM, \
+                        f"Cannot parse CoNLL line: expecting {FIELD_NUM} fields, {len(array)} found."
+                sent += [array]
+        if len(sent) > 0:
+            if generate_raw_text:
+                doc.append((sent, metadata, raw_text))
+            else:
+                doc.append((sent, metadata))
+        return doc
+
+    @staticmethod
+    def convert_conll(doc_conll, generate_raw_text=False):
+        """ Convert the CoNLL-U format input data to a dictionary format output data.
+        Input: list of token fields loaded from the CoNLL-U format data, where the outmost list represents a list of sentences, and the inside list represents all fields of a token.
+        Output: a list of list of dictionaries for each token in each sentence in the document.
+        """
+        doc_dict = []
+        metadata_list = []
+        raw_text_list = []
+        for sent in doc_conll:
+            if generate_raw_text:
+                sent_conll, metadata, raw_text = sent
+                raw_text_list.append(raw_text)
+            else:
+                sent_conll, metadata = sent
+            sent_dict = []
+            for token_conll in sent_conll:
+                token_dict = CoNLL.convert_conll_token(token_conll)
+                sent_dict.append(token_dict)
+            metadata_list.append(metadata)
+            doc_dict.append(sent_dict)
+        if generate_raw_text:
+            return doc_dict, metadata_list, ' '.join(raw_text_list)
+        return doc_dict, metadata_list
+
+    @staticmethod
+    def convert_conll_token(token_conll):
+        """ Convert the CoNLL-U format input token to the dictionary format output token.
+        Input: a list of all CoNLL-U fields for the token.
+        Output: a dictionary that maps from field name to value.
+        """
+        token_dict = {}
+        for field in FIELD_TO_IDX:
+            value = token_conll[FIELD_TO_IDX[field]]
+            if value != '_':
+                if field == HEAD:
+                    token_dict[field] = int(value)
+                elif field == ID:
+                    token_dict[field] = tuple(int(x) for x in value.split('-'))
+                else:
+                    token_dict[field] = value
+            # special case if text is '_'
+            if token_conll[FIELD_TO_IDX[TEXT]] == '_':
+                token_dict[TEXT] = token_conll[FIELD_TO_IDX[TEXT]]
+                token_dict[LEMMA] = token_conll[FIELD_TO_IDX[LEMMA]]
+        return token_dict
+
+    @staticmethod
+    def conll2dict(input_file=None, input_str=None, ignore_gapping=True, generate_raw_text=False):
+        """ Load the CoNLL-U format data from file or string into lists of dictionaries.
+        """
+        assert any([input_file, input_str]) and not all([input_file, input_str]), 'either input input file or input string'
+        if input_str:
+            infile = io.StringIO(input_str)
+        else:
+            infile = open(input_file)
+        doc_conll = CoNLL.load_conll(infile, ignore_gapping, generate_raw_text)
+        doc_dict = CoNLL.convert_conll(doc_conll, generate_raw_text)
+        return doc_dict
+
+    @staticmethod
+    def convert_dict(doc_dict):
+        """ Convert the dictionary format input data to the CoNLL-U format output data. This is the reverse function of
+        `convert_conll`.
+        Input: dictionary format data, which is a list of list of dictionaries for each token in each sentence in the data.
+        Output: CoNLL-U format data, which is a list of list of list for each token in each sentence in the data.
+        """
+        doc_conll = []
+        for sent_dict, metadata in doc_dict:
+            sent_conll = []
+            for token_dict in sent_dict:
+                token_conll = CoNLL.convert_token_dict(token_dict)
+                sent_conll.append(token_conll)
+            doc_conll.append((sent_conll, metadata))
+        return doc_conll
+
+    @staticmethod
+    def convert_token_dict(token_dict):
+        """ Convert the dictionary format input token to the CoNLL-U format output token. This is the reverse function of
+        `convert_conll_token`.
+        Input: dictionary format token, which is a dictionaries for the token.
+        Output: CoNLL-U format token, which is a list for the token.
+        """
+        token_conll = ['_' for i in range(FIELD_NUM)]
+        misc_dict = {}
+        for key in token_dict:
+            if key == ID:
+                token_conll[FIELD_TO_IDX[key]] = '-'.join([str(x) for x in token_dict[key]]) if isinstance(token_dict[key], tuple) else str(token_dict[key])
+            elif key == NER:
+                misc_dict['NER'] = str(token_dict[key])
+            elif key == SRL:
+                misc_dict['SRL'] = str(token_dict[key])
+            elif key == MISC:
+                for misc in str(token_dict[key]).split('|'):
+                    misc_key, misc_val = misc.split('=')
+                    if misc_key not in [NER, SRL]:
+                        misc_dict[misc_key] = misc_val
+            elif key in FIELD_TO_IDX:
+                token_conll[FIELD_TO_IDX[key]] = str(token_dict[key])
+        token_conll[FIELD_TO_IDX[MISC]] = '|'.join([k + '=' + v for k, v in sorted(misc_dict.items(), key=lambda item: item[0].lower())]) if misc_dict else '_'
+        return token_conll
+
+    @staticmethod
+    def conll_as_string(doc):
+        """ Dump the loaded CoNLL-U format list data to string. """
+        return_string = ""
+        for sent, meta in doc:
+            return_string += meta if meta else ''
+            for ln in sent:
+                return_string += ("\t".join(ln)+"\n")
+            return_string += "\n"
+        return return_string
+
+    @staticmethod
+    def dict2conll(doc_dict, filename):
+        """ Convert the dictionary format input data to the CoNLL-U format output data and write to a file.
+        """
+        doc_conll = CoNLL.convert_dict(doc_dict)
+        conll_string = CoNLL.conll_as_string(doc_conll)
+        with open(filename, 'w') as outfile:
+            outfile.write(conll_string)
+        return
+
+    @staticmethod
+    def write_doc2conll(doc, filename):
+        """ Writes the doc as a conll file to the given filename
+        """
+        with open(filename, 'w', encoding='utf-8') as outfile:
+            outfile.write(CoNLL.conll_as_string(CoNLL.convert_dict(doc.to_dict())))
```

### Comparing `classla-2.0/classla/utils/conll18_ud_eval.py` & `classla-2.1/classla/utils/conll18_ud_eval.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,585 +1,585 @@
-#!/usr/bin/env python3
-
-# Compatible with Python 2.7 and 3.2+, can be used either as a module
-# or a standalone executable.
-#
-# Copyright 2017, 2018 Institute of Formal and Applied Linguistics (UFAL),
-# Faculty of Mathematics and Physics, Charles University, Czech Republic.
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-# Authors: Milan Straka, Martin Popel <surname@ufal.mff.cuni.cz>
-#
-# Changelog:
-# - [12 Apr 2018] Version 0.9: Initial release.
-# - [19 Apr 2018] Version 1.0: Fix bug in MLAS (duplicate entries in functional_children).
-#                              Add --counts option.
-# - [02 May 2018] Version 1.1: When removing spaces to match gold and system characters,
-#                              consider all Unicode characters of category Zs instead of
-#                              just ASCII space.
-# - [25 Jun 2018] Version 1.2: Use python3 in the she-bang (instead of python).
-#                              In Python2, make the whole computation use `unicode` strings.
-
-# Command line usage
-# ------------------
-# conll18_ud_eval.py [-v] gold_conllu_file system_conllu_file
-#
-# - if no -v is given, only the official CoNLL18 UD Shared Task evaluation metrics
-#   are printed
-# - if -v is given, more metrics are printed (as precision, recall, F1 score,
-#   and in case the metric is computed on aligned words also accuracy on these):
-#   - Tokens: how well do the gold tokens match system tokens
-#   - Sentences: how well do the gold sentences match system sentences
-#   - Words: how well can the gold words be aligned to system words
-#   - UPOS: using aligned words, how well does UPOS match
-#   - XPOS: using aligned words, how well does XPOS match
-#   - UFeats: using aligned words, how well does universal FEATS match
-#   - AllTags: using aligned words, how well does UPOS+XPOS+FEATS match
-#   - Lemmas: using aligned words, how well does LEMMA match
-#   - UAS: using aligned words, how well does HEAD match
-#   - LAS: using aligned words, how well does HEAD+DEPREL(ignoring subtypes) match
-#   - CLAS: using aligned words with content DEPREL, how well does
-#       HEAD+DEPREL(ignoring subtypes) match
-#   - MLAS: using aligned words with content DEPREL, how well does
-#       HEAD+DEPREL(ignoring subtypes)+UPOS+UFEATS+FunctionalChildren(DEPREL+UPOS+UFEATS) match
-#   - BLEX: using aligned words with content DEPREL, how well does
-#       HEAD+DEPREL(ignoring subtypes)+LEMMAS match
-# - if -c is given, raw counts of correct/gold_total/system_total/aligned words are printed
-#   instead of precision/recall/F1/AlignedAccuracy for all metrics.
-
-# API usage
-# ---------
-# - load_conllu(file)
-#   - loads CoNLL-U file from given file object to an internal representation
-#   - the file object should return str in both Python 2 and Python 3
-#   - raises UDError exception if the given file cannot be loaded
-# - evaluate(gold_ud, system_ud)
-#   - evaluate the given gold and system CoNLL-U files (loaded with load_conllu)
-#   - raises UDError if the concatenated tokens of gold and system file do not match
-#   - returns a dictionary with the metrics described above, each metric having
-#     three fields: precision, recall and f1
-
-# Description of token matching
-# -----------------------------
-# In order to match tokens of gold file and system file, we consider the text
-# resulting from concatenation of gold tokens and text resulting from
-# concatenation of system tokens. These texts should match -- if they do not,
-# the evaluation fails.
-#
-# If the texts do match, every token is represented as a range in this original
-# text, and tokens are equal only if their range is the same.
-
-# Description of word matching
-# ----------------------------
-# When matching words of gold file and system file, we first match the tokens.
-# The words which are also tokens are matched as tokens, but words in multi-word
-# tokens have to be handled differently.
-#
-# To handle multi-word tokens, we start by finding "multi-word spans".
-# Multi-word span is a span in the original text such that
-# - it contains at least one multi-word token
-# - all multi-word tokens in the span (considering both gold and system ones)
-#   are completely inside the span (i.e., they do not "stick out")
-# - the multi-word span is as small as possible
-#
-# For every multi-word span, we align the gold and system words completely
-# inside this span using LCS on their FORMs. The words not intersecting
-# (even partially) any multi-word span are then aligned as tokens.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import argparse
-import io
-import sys
-import unicodedata
-import unittest
-
-# CoNLL-U column names
-ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC = range(10)
-
-# Content and functional relations
-CONTENT_DEPRELS = {
-    "nsubj", "obj", "iobj", "csubj", "ccomp", "xcomp", "obl", "vocative",
-    "expl", "dislocated", "advcl", "advmod", "discourse", "nmod", "appos",
-    "nummod", "acl", "amod", "conj", "fixed", "flat", "compound", "list",
-    "parataxis", "orphan", "goeswith", "reparandum", "root", "dep"
-}
-
-FUNCTIONAL_DEPRELS = {
-    "aux", "cop", "mark", "det", "clf", "case", "cc"
-}
-
-UNIVERSAL_FEATURES = {
-    "PronType", "NumType", "Poss", "Reflex", "Foreign", "Abbr", "Gender",
-    "Animacy", "Number", "Case", "Definite", "Degree", "VerbForm", "Mood",
-    "Tense", "Aspect", "Voice", "Evident", "Polarity", "Person", "Polite"
-}
-
-# UD Error is used when raising exceptions in this module
-class UDError(Exception):
-    pass
-
-# Conversion methods handling `str` <-> `unicode` conversions in Python2
-def _decode(text):
-    return text if sys.version_info[0] >= 3 or not isinstance(text, str) else text.decode("utf-8")
-
-def _encode(text):
-    return text if sys.version_info[0] >= 3 or not isinstance(text, unicode) else text.encode("utf-8")
-
-# Load given CoNLL-U file into internal representation
-def load_conllu(file):
-    # Internal representation classes
-    class UDRepresentation:
-        def __init__(self):
-            # Characters of all the tokens in the whole file.
-            # Whitespace between tokens is not included.
-            self.characters = []
-            # List of UDSpan instances with start&end indices into `characters`.
-            self.tokens = []
-            # List of UDWord instances.
-            self.words = []
-            # List of UDSpan instances with start&end indices into `characters`.
-            self.sentences = []
-    class UDSpan:
-        def __init__(self, start, end):
-            self.start = start
-            # Note that self.end marks the first position **after the end** of span,
-            # so we can use characters[start:end] or range(start, end).
-            self.end = end
-    class UDWord:
-        def __init__(self, span, columns, is_multiword):
-            # Span of this word (or MWT, see below) within ud_representation.characters.
-            self.span = span
-            # 10 columns of the CoNLL-U file: ID, FORM, LEMMA,...
-            self.columns = columns
-            # is_multiword==True means that this word is part of a multi-word token.
-            # In that case, self.span marks the span of the whole multi-word token.
-            self.is_multiword = is_multiword
-            # Reference to the UDWord instance representing the HEAD (or None if root).
-            self.parent = None
-            # List of references to UDWord instances representing functional-deprel children.
-            self.functional_children = []
-            # Only consider universal FEATS.
-            self.columns[FEATS] = "|".join(sorted(feat for feat in columns[FEATS].split("|")
-                                                  if feat.split("=", 1)[0] in UNIVERSAL_FEATURES))
-            # Let's ignore language-specific deprel subtypes.
-            self.columns[DEPREL] = columns[DEPREL].split(":")[0]
-            # Precompute which deprels are CONTENT_DEPRELS and which FUNCTIONAL_DEPRELS
-            self.is_content_deprel = self.columns[DEPREL] in CONTENT_DEPRELS
-            self.is_functional_deprel = self.columns[DEPREL] in FUNCTIONAL_DEPRELS
-
-    ud = UDRepresentation()
-
-    # Load the CoNLL-U file
-    index, sentence_start = 0, None
-    while True:
-        line = file.readline()
-        if not line:
-            break
-        line = _decode(line.rstrip("\r\n"))
-
-        # Handle sentence start boundaries
-        if sentence_start is None:
-            # Skip comments
-            if line.startswith("#"):
-                continue
-            # Start a new sentence
-            ud.sentences.append(UDSpan(index, 0))
-            sentence_start = len(ud.words)
-        if not line:
-            # Add parent and children UDWord links and check there are no cycles
-            def process_word(word):
-                if word.parent == "remapping":
-                    raise UDError("There is a cycle in a sentence")
-                if word.parent is None:
-                    if word.columns[HEAD] == '_':
-                        return
-                    head = int(word.columns[HEAD])
-                    if head < 0 or head > len(ud.words) - sentence_start:
-                        raise UDError("HEAD '{}' points outside of the sentence".format(_encode(word.columns[HEAD])))
-                    if head:
-                        parent = ud.words[sentence_start + head - 1]
-                        word.parent = "remapping"
-                        process_word(parent)
-                        word.parent = parent
-
-            for word in ud.words[sentence_start:]:
-                process_word(word)
-            # func_children cannot be assigned within process_word
-            # because it is called recursively and may result in adding one child twice.
-            for word in ud.words[sentence_start:]:
-                if word.parent and word.is_functional_deprel:
-                    word.parent.functional_children.append(word)
-
-            # Check there is a single root node
-            # if sentence_start < len(ud.words) and len([word for word in ud.words[sentence_start:] if word.parent is None]) != 1:
-            #     raise UDError("There are multiple roots in a sentence")
-
-            # End the sentence
-            ud.sentences[-1].end = index
-            sentence_start = None
-            continue
-
-        # Read next token/word
-        columns = line.split("\t")
-        if len(columns) != 10:
-            raise UDError("The CoNLL-U line does not contain 10 tab-separated columns: '{}'".format(_encode(line)))
-
-        # Skip empty nodes
-        if "." in columns[ID]:
-            continue
-
-        # Delete spaces from FORM, so gold.characters == system.characters
-        # even if one of them tokenizes the space. Use any Unicode character
-        # with category Zs.
-        columns[FORM] = "".join(filter(lambda c: unicodedata.category(c) != "Zs", columns[FORM]))
-        if not columns[FORM]:
-            raise UDError("There is an empty FORM in the CoNLL-U file")
-
-        # Save token
-        ud.characters.extend(columns[FORM])
-        ud.tokens.append(UDSpan(index, index + len(columns[FORM])))
-        index += len(columns[FORM])
-
-        # Handle multi-word tokens to save word(s)
-        if "-" in columns[ID]:
-            try:
-                start, end = map(int, columns[ID].split("-"))
-            except:
-                raise UDError("Cannot parse multi-word token ID '{}'".format(_encode(columns[ID])))
-
-            for _ in range(start, end + 1):
-                word_line = _decode(file.readline().rstrip("\r\n"))
-                word_columns = word_line.split("\t")
-                if len(word_columns) != 10:
-                    raise UDError("The CoNLL-U line does not contain 10 tab-separated columns: '{}'".format(_encode(word_line)))
-                ud.words.append(UDWord(ud.tokens[-1], word_columns, is_multiword=True))
-        # Basic tokens/words
-        else:
-            try:
-                word_id = int(columns[ID])
-            except:
-                raise UDError("Cannot parse word ID '{}'".format(_encode(columns[ID])))
-            if word_id != len(ud.words) - sentence_start + 1:
-                raise UDError("Incorrect word ID '{}' for word '{}', expected '{}'".format(
-                    _encode(columns[ID]), _encode(columns[FORM]), len(ud.words) - sentence_start + 1))
-
-            if columns[HEAD] != '_':
-                try:
-                    head_id = int(columns[HEAD])
-                except:
-                    raise UDError("Cannot parse HEAD '{}'".format(_encode(columns[HEAD])))
-                if head_id < 0:
-                    raise UDError("HEAD cannot be negative")
-
-            ud.words.append(UDWord(ud.tokens[-1], columns, is_multiword=False))
-
-    if sentence_start is not None:
-        raise UDError("The CoNLL-U file does not end with empty line")
-
-    return ud
-
-# Evaluate the gold and system treebanks (loaded using load_conllu).
-def evaluate(gold_ud, system_ud):
-    class Score:
-        def __init__(self, gold_total, system_total, correct, aligned_total=None):
-            self.correct = correct
-            self.gold_total = gold_total
-            self.system_total = system_total
-            self.aligned_total = aligned_total
-            self.precision = correct / system_total if system_total else 0.0
-            self.recall = correct / gold_total if gold_total else 0.0
-            self.f1 = 2 * correct / (system_total + gold_total) if system_total + gold_total else 0.0
-            self.aligned_accuracy = correct / aligned_total if aligned_total else aligned_total
-    class AlignmentWord:
-        def __init__(self, gold_word, system_word):
-            self.gold_word = gold_word
-            self.system_word = system_word
-    class Alignment:
-        def __init__(self, gold_words, system_words):
-            self.gold_words = gold_words
-            self.system_words = system_words
-            self.matched_words = []
-            self.matched_words_map = {}
-        def append_aligned_words(self, gold_word, system_word):
-            self.matched_words.append(AlignmentWord(gold_word, system_word))
-            self.matched_words_map[system_word] = gold_word
-
-    def spans_score(gold_spans, system_spans):
-        correct, gi, si = 0, 0, 0
-        while gi < len(gold_spans) and si < len(system_spans):
-            if system_spans[si].start < gold_spans[gi].start:
-                si += 1
-            elif gold_spans[gi].start < system_spans[si].start:
-                gi += 1
-            else:
-                correct += gold_spans[gi].end == system_spans[si].end
-                si += 1
-                gi += 1
-
-        return Score(len(gold_spans), len(system_spans), correct)
-
-    def alignment_score(alignment, key_fn=None, filter_fn=None):
-        if filter_fn is not None:
-            gold = sum(1 for gold in alignment.gold_words if filter_fn(gold))
-            system = sum(1 for system in alignment.system_words if filter_fn(system))
-            aligned = sum(1 for word in alignment.matched_words if filter_fn(word.gold_word))
-        else:
-            gold = len(alignment.gold_words)
-            system = len(alignment.system_words)
-            aligned = len(alignment.matched_words)
-
-        if key_fn is None:
-            # Return score for whole aligned words
-            return Score(gold, system, aligned)
-
-        def gold_aligned_gold(word):
-            return word
-        def gold_aligned_system(word):
-            return alignment.matched_words_map.get(word, "NotAligned") if word is not None else None
-        correct = 0
-        for words in alignment.matched_words:
-            if filter_fn is None or filter_fn(words.gold_word):
-                if key_fn(words.gold_word, gold_aligned_gold) == key_fn(words.system_word, gold_aligned_system):
-                    correct += 1
-
-        return Score(gold, system, correct, aligned)
-
-    def beyond_end(words, i, multiword_span_end):
-        if i >= len(words):
-            return True
-        if words[i].is_multiword:
-            return words[i].span.start >= multiword_span_end
-        return words[i].span.end > multiword_span_end
-
-    def extend_end(word, multiword_span_end):
-        if word.is_multiword and word.span.end > multiword_span_end:
-            return word.span.end
-        return multiword_span_end
-
-    def find_multiword_span(gold_words, system_words, gi, si):
-        # We know gold_words[gi].is_multiword or system_words[si].is_multiword.
-        # Find the start of the multiword span (gs, ss), so the multiword span is minimal.
-        # Initialize multiword_span_end characters index.
-        if gold_words[gi].is_multiword:
-            multiword_span_end = gold_words[gi].span.end
-            if not system_words[si].is_multiword and system_words[si].span.start < gold_words[gi].span.start:
-                si += 1
-        else: # if system_words[si].is_multiword
-            multiword_span_end = system_words[si].span.end
-            if not gold_words[gi].is_multiword and gold_words[gi].span.start < system_words[si].span.start:
-                gi += 1
-        gs, ss = gi, si
-
-        # Find the end of the multiword span
-        # (so both gi and si are pointing to the word following the multiword span end).
-        while not beyond_end(gold_words, gi, multiword_span_end) or \
-              not beyond_end(system_words, si, multiword_span_end):
-            if gi < len(gold_words) and (si >= len(system_words) or
-                                         gold_words[gi].span.start <= system_words[si].span.start):
-                multiword_span_end = extend_end(gold_words[gi], multiword_span_end)
-                gi += 1
-            else:
-                multiword_span_end = extend_end(system_words[si], multiword_span_end)
-                si += 1
-        return gs, ss, gi, si
-
-    def compute_lcs(gold_words, system_words, gi, si, gs, ss):
-        lcs = [[0] * (si - ss) for i in range(gi - gs)]
-        for g in reversed(range(gi - gs)):
-            for s in reversed(range(si - ss)):
-                if gold_words[gs + g].columns[FORM].lower() == system_words[ss + s].columns[FORM].lower():
-                    lcs[g][s] = 1 + (lcs[g+1][s+1] if g+1 < gi-gs and s+1 < si-ss else 0)
-                lcs[g][s] = max(lcs[g][s], lcs[g+1][s] if g+1 < gi-gs else 0)
-                lcs[g][s] = max(lcs[g][s], lcs[g][s+1] if s+1 < si-ss else 0)
-        return lcs
-
-    def align_words(gold_words, system_words):
-        alignment = Alignment(gold_words, system_words)
-
-        gi, si = 0, 0
-        while gi < len(gold_words) and si < len(system_words):
-            if gold_words[gi].is_multiword or system_words[si].is_multiword:
-                # A: Multi-word tokens => align via LCS within the whole "multiword span".
-                gs, ss, gi, si = find_multiword_span(gold_words, system_words, gi, si)
-
-                if si > ss and gi > gs:
-                    lcs = compute_lcs(gold_words, system_words, gi, si, gs, ss)
-
-                    # Store aligned words
-                    s, g = 0, 0
-                    while g < gi - gs and s < si - ss:
-                        if gold_words[gs + g].columns[FORM].lower() == system_words[ss + s].columns[FORM].lower():
-                            alignment.append_aligned_words(gold_words[gs+g], system_words[ss+s])
-                            g += 1
-                            s += 1
-                        elif lcs[g][s] == (lcs[g+1][s] if g+1 < gi-gs else 0):
-                            g += 1
-                        else:
-                            s += 1
-            else:
-                # B: No multi-word token => align according to spans.
-                if (gold_words[gi].span.start, gold_words[gi].span.end) == (system_words[si].span.start, system_words[si].span.end):
-                    alignment.append_aligned_words(gold_words[gi], system_words[si])
-                    gi += 1
-                    si += 1
-                elif gold_words[gi].span.start <= system_words[si].span.start:
-                    gi += 1
-                else:
-                    si += 1
-
-        return alignment
-
-    # Check that the underlying character sequences do match.
-    if gold_ud.characters != system_ud.characters:
-        index = 0
-        while index < len(gold_ud.characters) and index < len(system_ud.characters) and \
-                gold_ud.characters[index] == system_ud.characters[index]:
-            index += 1
-
-        raise UDError(
-            "The concatenation of tokens in gold file and in system file differ!\n" +
-            "First 20 differing characters in gold file: '{}' and system file: '{}'".format(
-                "".join(map(_encode, gold_ud.characters[index:index + 20])),
-                "".join(map(_encode, system_ud.characters[index:index + 20]))
-            )
-        )
-
-    # Align words
-    alignment = align_words(gold_ud.words, system_ud.words)
-
-    # Compute the F1-scores
-    return {
-        "Tokens": spans_score(gold_ud.tokens, system_ud.tokens),
-        "Sentences": spans_score(gold_ud.sentences, system_ud.sentences),
-        "Words": alignment_score(alignment),
-        "UPOS": alignment_score(alignment, lambda w, _: w.columns[UPOS]),
-        "XPOS": alignment_score(alignment, lambda w, _: w.columns[XPOS]),
-        "UFeats": alignment_score(alignment, lambda w, _: w.columns[FEATS]),
-        "AllTags": alignment_score(alignment, lambda w, _: (w.columns[UPOS], w.columns[XPOS], w.columns[FEATS])),
-        "Lemmas": alignment_score(alignment, lambda w, ga: w.columns[LEMMA] if ga(w).columns[LEMMA] != "_" else "_"),
-        "UAS": alignment_score(alignment, lambda w, ga: ga(w.parent)),
-        "LAS": alignment_score(alignment, lambda w, ga: (ga(w.parent), w.columns[DEPREL])),
-        "CLAS": alignment_score(alignment, lambda w, ga: (ga(w.parent), w.columns[DEPREL]),
-                                filter_fn=lambda w: w.is_content_deprel),
-        "MLAS": alignment_score(alignment, lambda w, ga: (ga(w.parent), w.columns[DEPREL], w.columns[UPOS], w.columns[FEATS],
-                                                         [(ga(c), c.columns[DEPREL], c.columns[UPOS], c.columns[FEATS])
-                                                          for c in w.functional_children]),
-                                filter_fn=lambda w: w.is_content_deprel),
-        "BLEX": alignment_score(alignment, lambda w, ga: (ga(w.parent), w.columns[DEPREL],
-                                                          w.columns[LEMMA] if ga(w).columns[LEMMA] != "_" else "_"),
-                                filter_fn=lambda w: w.is_content_deprel),
-    }
-
-
-def load_conllu_file(path):
-    _file = open(path, mode="r", **({"encoding": "utf-8"} if sys.version_info >= (3, 0) else {}))
-    return load_conllu(_file)
-
-def evaluate_wrapper(args):
-    # Load CoNLL-U files
-    gold_ud = load_conllu_file(args.gold_file)
-    system_ud = load_conllu_file(args.system_file)
-    return evaluate(gold_ud, system_ud)
-
-def main():
-    # Parse arguments
-    parser = argparse.ArgumentParser()
-    parser.add_argument("gold_file", type=str,
-                        help="Name of the CoNLL-U file with the gold data.")
-    parser.add_argument("system_file", type=str,
-                        help="Name of the CoNLL-U file with the predicted data.")
-    parser.add_argument("--verbose", "-v", default=False, action="store_true",
-                        help="Print all metrics.")
-    parser.add_argument("--counts", "-c", default=False, action="store_true",
-                        help="Print raw counts of correct/gold/system/aligned words instead of prec/rec/F1 for all metrics.")
-    args = parser.parse_args()
-
-    # Evaluate
-    evaluation = evaluate_wrapper(args)
-
-    # Print the evaluation
-    if not args.verbose and not args.counts:
-        print("LAS F1 Score: {:.2f}".format(100 * evaluation["LAS"].f1))
-        print("MLAS Score: {:.2f}".format(100 * evaluation["MLAS"].f1))
-        print("BLEX Score: {:.2f}".format(100 * evaluation["BLEX"].f1))
-    else:
-        if args.counts:
-            print("Metric     | Correct   |      Gold | Predicted | Aligned")
-        else:
-            print("Metric     | Precision |    Recall |  F1 Score | AligndAcc")
-        print("-----------+-----------+-----------+-----------+-----------")
-        for metric in["Tokens", "Sentences", "Words", "UPOS", "XPOS", "UFeats", "AllTags", "Lemmas", "UAS", "LAS", "CLAS", "MLAS", "BLEX"]:
-            if args.counts:
-                print("{:11}|{:10} |{:10} |{:10} |{:10}".format(
-                    metric,
-                    evaluation[metric].correct,
-                    evaluation[metric].gold_total,
-                    evaluation[metric].system_total,
-                    evaluation[metric].aligned_total or (evaluation[metric].correct if metric == "Words" else "")
-                ))
-            else:
-                print("{:11}|{:10.2f} |{:10.2f} |{:10.2f} |{}".format(
-                    metric,
-                    100 * evaluation[metric].precision,
-                    100 * evaluation[metric].recall,
-                    100 * evaluation[metric].f1,
-                    "{:10.2f}".format(100 * evaluation[metric].aligned_accuracy) if evaluation[metric].aligned_accuracy is not None else ""
-                ))
-
-if __name__ == "__main__":
-    main()
-
-# Tests, which can be executed with `python -m unittest conll18_ud_eval`.
-class TestAlignment(unittest.TestCase):
-    @staticmethod
-    def _load_words(words):
-        """Prepare fake CoNLL-U files with fake HEAD to prevent multiple roots errors."""
-        lines, num_words = [], 0
-        for w in words:
-            parts = w.split(" ")
-            if len(parts) == 1:
-                num_words += 1
-                lines.append("{}\t{}\t_\t_\t_\t_\t{}\t_\t_\t_".format(num_words, parts[0], int(num_words>1)))
-            else:
-                lines.append("{}-{}\t{}\t_\t_\t_\t_\t_\t_\t_\t_".format(num_words + 1, num_words + len(parts) - 1, parts[0]))
-                for part in parts[1:]:
-                    num_words += 1
-                    lines.append("{}\t{}\t_\t_\t_\t_\t{}\t_\t_\t_".format(num_words, part, int(num_words>1)))
-        return load_conllu((io.StringIO if sys.version_info >= (3, 0) else io.BytesIO)("\n".join(lines+["\n"])))
-
-    def _test_exception(self, gold, system):
-        self.assertRaises(UDError, evaluate, self._load_words(gold), self._load_words(system))
-
-    def _test_ok(self, gold, system, correct):
-        metrics = evaluate(self._load_words(gold), self._load_words(system))
-        gold_words = sum((max(1, len(word.split(" ")) - 1) for word in gold))
-        system_words = sum((max(1, len(word.split(" ")) - 1) for word in system))
-        self.assertEqual((metrics["Words"].precision, metrics["Words"].recall, metrics["Words"].f1),
-                         (correct / system_words, correct / gold_words, 2 * correct / (gold_words + system_words)))
-
-    def test_exception(self):
-        self._test_exception(["a"], ["b"])
-
-    def test_equal(self):
-        self._test_ok(["a"], ["a"], 1)
-        self._test_ok(["a", "b", "c"], ["a", "b", "c"], 3)
-
-    def test_equal_with_multiword(self):
-        self._test_ok(["abc a b c"], ["a", "b", "c"], 3)
-        self._test_ok(["a", "bc b c", "d"], ["a", "b", "c", "d"], 4)
-        self._test_ok(["abcd a b c d"], ["ab a b", "cd c d"], 4)
-        self._test_ok(["abc a b c", "de d e"], ["a", "bcd b c d", "e"], 5)
-
-    def test_alignment(self):
-        self._test_ok(["abcd"], ["a", "b", "c", "d"], 0)
-        self._test_ok(["abc", "d"], ["a", "b", "c", "d"], 1)
-        self._test_ok(["a", "bc", "d"], ["a", "b", "c", "d"], 2)
-        self._test_ok(["a", "bc b c", "d"], ["a", "b", "cd"], 2)
-        self._test_ok(["abc a BX c", "def d EX f"], ["ab a b", "cd c d", "ef e f"], 4)
-        self._test_ok(["ab a b", "cd bc d"], ["a", "bc", "d"], 2)
-        self._test_ok(["a", "bc b c", "d"], ["ab AX BX", "cd CX a"], 1)
+#!/usr/bin/env python3
+
+# Compatible with Python 2.7 and 3.2+, can be used either as a module
+# or a standalone executable.
+#
+# Copyright 2017, 2018 Institute of Formal and Applied Linguistics (UFAL),
+# Faculty of Mathematics and Physics, Charles University, Czech Republic.
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+# Authors: Milan Straka, Martin Popel <surname@ufal.mff.cuni.cz>
+#
+# Changelog:
+# - [12 Apr 2018] Version 0.9: Initial release.
+# - [19 Apr 2018] Version 1.0: Fix bug in MLAS (duplicate entries in functional_children).
+#                              Add --counts option.
+# - [02 May 2018] Version 1.1: When removing spaces to match gold and system characters,
+#                              consider all Unicode characters of category Zs instead of
+#                              just ASCII space.
+# - [25 Jun 2018] Version 1.2: Use python3 in the she-bang (instead of python).
+#                              In Python2, make the whole computation use `unicode` strings.
+
+# Command line usage
+# ------------------
+# conll18_ud_eval.py [-v] gold_conllu_file system_conllu_file
+#
+# - if no -v is given, only the official CoNLL18 UD Shared Task evaluation metrics
+#   are printed
+# - if -v is given, more metrics are printed (as precision, recall, F1 score,
+#   and in case the metric is computed on aligned words also accuracy on these):
+#   - Tokens: how well do the gold tokens match system tokens
+#   - Sentences: how well do the gold sentences match system sentences
+#   - Words: how well can the gold words be aligned to system words
+#   - UPOS: using aligned words, how well does UPOS match
+#   - XPOS: using aligned words, how well does XPOS match
+#   - UFeats: using aligned words, how well does universal FEATS match
+#   - AllTags: using aligned words, how well does UPOS+XPOS+FEATS match
+#   - Lemmas: using aligned words, how well does LEMMA match
+#   - UAS: using aligned words, how well does HEAD match
+#   - LAS: using aligned words, how well does HEAD+DEPREL(ignoring subtypes) match
+#   - CLAS: using aligned words with content DEPREL, how well does
+#       HEAD+DEPREL(ignoring subtypes) match
+#   - MLAS: using aligned words with content DEPREL, how well does
+#       HEAD+DEPREL(ignoring subtypes)+UPOS+UFEATS+FunctionalChildren(DEPREL+UPOS+UFEATS) match
+#   - BLEX: using aligned words with content DEPREL, how well does
+#       HEAD+DEPREL(ignoring subtypes)+LEMMAS match
+# - if -c is given, raw counts of correct/gold_total/system_total/aligned words are printed
+#   instead of precision/recall/F1/AlignedAccuracy for all metrics.
+
+# API usage
+# ---------
+# - load_conllu(file)
+#   - loads CoNLL-U file from given file object to an internal representation
+#   - the file object should return str in both Python 2 and Python 3
+#   - raises UDError exception if the given file cannot be loaded
+# - evaluate(gold_ud, system_ud)
+#   - evaluate the given gold and system CoNLL-U files (loaded with load_conllu)
+#   - raises UDError if the concatenated tokens of gold and system file do not match
+#   - returns a dictionary with the metrics described above, each metric having
+#     three fields: precision, recall and f1
+
+# Description of token matching
+# -----------------------------
+# In order to match tokens of gold file and system file, we consider the text
+# resulting from concatenation of gold tokens and text resulting from
+# concatenation of system tokens. These texts should match -- if they do not,
+# the evaluation fails.
+#
+# If the texts do match, every token is represented as a range in this original
+# text, and tokens are equal only if their range is the same.
+
+# Description of word matching
+# ----------------------------
+# When matching words of gold file and system file, we first match the tokens.
+# The words which are also tokens are matched as tokens, but words in multi-word
+# tokens have to be handled differently.
+#
+# To handle multi-word tokens, we start by finding "multi-word spans".
+# Multi-word span is a span in the original text such that
+# - it contains at least one multi-word token
+# - all multi-word tokens in the span (considering both gold and system ones)
+#   are completely inside the span (i.e., they do not "stick out")
+# - the multi-word span is as small as possible
+#
+# For every multi-word span, we align the gold and system words completely
+# inside this span using LCS on their FORMs. The words not intersecting
+# (even partially) any multi-word span are then aligned as tokens.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import argparse
+import io
+import sys
+import unicodedata
+import unittest
+
+# CoNLL-U column names
+ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC = range(10)
+
+# Content and functional relations
+CONTENT_DEPRELS = {
+    "nsubj", "obj", "iobj", "csubj", "ccomp", "xcomp", "obl", "vocative",
+    "expl", "dislocated", "advcl", "advmod", "discourse", "nmod", "appos",
+    "nummod", "acl", "amod", "conj", "fixed", "flat", "compound", "list",
+    "parataxis", "orphan", "goeswith", "reparandum", "root", "dep"
+}
+
+FUNCTIONAL_DEPRELS = {
+    "aux", "cop", "mark", "det", "clf", "case", "cc"
+}
+
+UNIVERSAL_FEATURES = {
+    "PronType", "NumType", "Poss", "Reflex", "Foreign", "Abbr", "Gender",
+    "Animacy", "Number", "Case", "Definite", "Degree", "VerbForm", "Mood",
+    "Tense", "Aspect", "Voice", "Evident", "Polarity", "Person", "Polite"
+}
+
+# UD Error is used when raising exceptions in this module
+class UDError(Exception):
+    pass
+
+# Conversion methods handling `str` <-> `unicode` conversions in Python2
+def _decode(text):
+    return text if sys.version_info[0] >= 3 or not isinstance(text, str) else text.decode("utf-8")
+
+def _encode(text):
+    return text if sys.version_info[0] >= 3 or not isinstance(text, unicode) else text.encode("utf-8")
+
+# Load given CoNLL-U file into internal representation
+def load_conllu(file):
+    # Internal representation classes
+    class UDRepresentation:
+        def __init__(self):
+            # Characters of all the tokens in the whole file.
+            # Whitespace between tokens is not included.
+            self.characters = []
+            # List of UDSpan instances with start&end indices into `characters`.
+            self.tokens = []
+            # List of UDWord instances.
+            self.words = []
+            # List of UDSpan instances with start&end indices into `characters`.
+            self.sentences = []
+    class UDSpan:
+        def __init__(self, start, end):
+            self.start = start
+            # Note that self.end marks the first position **after the end** of span,
+            # so we can use characters[start:end] or range(start, end).
+            self.end = end
+    class UDWord:
+        def __init__(self, span, columns, is_multiword):
+            # Span of this word (or MWT, see below) within ud_representation.characters.
+            self.span = span
+            # 10 columns of the CoNLL-U file: ID, FORM, LEMMA,...
+            self.columns = columns
+            # is_multiword==True means that this word is part of a multi-word token.
+            # In that case, self.span marks the span of the whole multi-word token.
+            self.is_multiword = is_multiword
+            # Reference to the UDWord instance representing the HEAD (or None if root).
+            self.parent = None
+            # List of references to UDWord instances representing functional-deprel children.
+            self.functional_children = []
+            # Only consider universal FEATS.
+            self.columns[FEATS] = "|".join(sorted(feat for feat in columns[FEATS].split("|")
+                                                  if feat.split("=", 1)[0] in UNIVERSAL_FEATURES))
+            # Let's ignore language-specific deprel subtypes.
+            self.columns[DEPREL] = columns[DEPREL].split(":")[0]
+            # Precompute which deprels are CONTENT_DEPRELS and which FUNCTIONAL_DEPRELS
+            self.is_content_deprel = self.columns[DEPREL] in CONTENT_DEPRELS
+            self.is_functional_deprel = self.columns[DEPREL] in FUNCTIONAL_DEPRELS
+
+    ud = UDRepresentation()
+
+    # Load the CoNLL-U file
+    index, sentence_start = 0, None
+    while True:
+        line = file.readline()
+        if not line:
+            break
+        line = _decode(line.rstrip("\r\n"))
+
+        # Handle sentence start boundaries
+        if sentence_start is None:
+            # Skip comments
+            if line.startswith("#"):
+                continue
+            # Start a new sentence
+            ud.sentences.append(UDSpan(index, 0))
+            sentence_start = len(ud.words)
+        if not line:
+            # Add parent and children UDWord links and check there are no cycles
+            def process_word(word):
+                if word.parent == "remapping":
+                    raise UDError("There is a cycle in a sentence")
+                if word.parent is None:
+                    if word.columns[HEAD] == '_':
+                        return
+                    head = int(word.columns[HEAD])
+                    if head < 0 or head > len(ud.words) - sentence_start:
+                        raise UDError("HEAD '{}' points outside of the sentence".format(_encode(word.columns[HEAD])))
+                    if head:
+                        parent = ud.words[sentence_start + head - 1]
+                        word.parent = "remapping"
+                        process_word(parent)
+                        word.parent = parent
+
+            for word in ud.words[sentence_start:]:
+                process_word(word)
+            # func_children cannot be assigned within process_word
+            # because it is called recursively and may result in adding one child twice.
+            for word in ud.words[sentence_start:]:
+                if word.parent and word.is_functional_deprel:
+                    word.parent.functional_children.append(word)
+
+            # Check there is a single root node
+            # if sentence_start < len(ud.words) and len([word for word in ud.words[sentence_start:] if word.parent is None]) != 1:
+            #     raise UDError("There are multiple roots in a sentence")
+
+            # End the sentence
+            ud.sentences[-1].end = index
+            sentence_start = None
+            continue
+
+        # Read next token/word
+        columns = line.split("\t")
+        if len(columns) != 10:
+            raise UDError("The CoNLL-U line does not contain 10 tab-separated columns: '{}'".format(_encode(line)))
+
+        # Skip empty nodes
+        if "." in columns[ID]:
+            continue
+
+        # Delete spaces from FORM, so gold.characters == system.characters
+        # even if one of them tokenizes the space. Use any Unicode character
+        # with category Zs.
+        columns[FORM] = "".join(filter(lambda c: unicodedata.category(c) != "Zs", columns[FORM]))
+        if not columns[FORM]:
+            raise UDError("There is an empty FORM in the CoNLL-U file")
+
+        # Save token
+        ud.characters.extend(columns[FORM])
+        ud.tokens.append(UDSpan(index, index + len(columns[FORM])))
+        index += len(columns[FORM])
+
+        # Handle multi-word tokens to save word(s)
+        if "-" in columns[ID]:
+            try:
+                start, end = map(int, columns[ID].split("-"))
+            except:
+                raise UDError("Cannot parse multi-word token ID '{}'".format(_encode(columns[ID])))
+
+            for _ in range(start, end + 1):
+                word_line = _decode(file.readline().rstrip("\r\n"))
+                word_columns = word_line.split("\t")
+                if len(word_columns) != 10:
+                    raise UDError("The CoNLL-U line does not contain 10 tab-separated columns: '{}'".format(_encode(word_line)))
+                ud.words.append(UDWord(ud.tokens[-1], word_columns, is_multiword=True))
+        # Basic tokens/words
+        else:
+            try:
+                word_id = int(columns[ID])
+            except:
+                raise UDError("Cannot parse word ID '{}'".format(_encode(columns[ID])))
+            if word_id != len(ud.words) - sentence_start + 1:
+                raise UDError("Incorrect word ID '{}' for word '{}', expected '{}'".format(
+                    _encode(columns[ID]), _encode(columns[FORM]), len(ud.words) - sentence_start + 1))
+
+            if columns[HEAD] != '_':
+                try:
+                    head_id = int(columns[HEAD])
+                except:
+                    raise UDError("Cannot parse HEAD '{}'".format(_encode(columns[HEAD])))
+                if head_id < 0:
+                    raise UDError("HEAD cannot be negative")
+
+            ud.words.append(UDWord(ud.tokens[-1], columns, is_multiword=False))
+
+    if sentence_start is not None:
+        raise UDError("The CoNLL-U file does not end with empty line")
+
+    return ud
+
+# Evaluate the gold and system treebanks (loaded using load_conllu).
+def evaluate(gold_ud, system_ud):
+    class Score:
+        def __init__(self, gold_total, system_total, correct, aligned_total=None):
+            self.correct = correct
+            self.gold_total = gold_total
+            self.system_total = system_total
+            self.aligned_total = aligned_total
+            self.precision = correct / system_total if system_total else 0.0
+            self.recall = correct / gold_total if gold_total else 0.0
+            self.f1 = 2 * correct / (system_total + gold_total) if system_total + gold_total else 0.0
+            self.aligned_accuracy = correct / aligned_total if aligned_total else aligned_total
+    class AlignmentWord:
+        def __init__(self, gold_word, system_word):
+            self.gold_word = gold_word
+            self.system_word = system_word
+    class Alignment:
+        def __init__(self, gold_words, system_words):
+            self.gold_words = gold_words
+            self.system_words = system_words
+            self.matched_words = []
+            self.matched_words_map = {}
+        def append_aligned_words(self, gold_word, system_word):
+            self.matched_words.append(AlignmentWord(gold_word, system_word))
+            self.matched_words_map[system_word] = gold_word
+
+    def spans_score(gold_spans, system_spans):
+        correct, gi, si = 0, 0, 0
+        while gi < len(gold_spans) and si < len(system_spans):
+            if system_spans[si].start < gold_spans[gi].start:
+                si += 1
+            elif gold_spans[gi].start < system_spans[si].start:
+                gi += 1
+            else:
+                correct += gold_spans[gi].end == system_spans[si].end
+                si += 1
+                gi += 1
+
+        return Score(len(gold_spans), len(system_spans), correct)
+
+    def alignment_score(alignment, key_fn=None, filter_fn=None):
+        if filter_fn is not None:
+            gold = sum(1 for gold in alignment.gold_words if filter_fn(gold))
+            system = sum(1 for system in alignment.system_words if filter_fn(system))
+            aligned = sum(1 for word in alignment.matched_words if filter_fn(word.gold_word))
+        else:
+            gold = len(alignment.gold_words)
+            system = len(alignment.system_words)
+            aligned = len(alignment.matched_words)
+
+        if key_fn is None:
+            # Return score for whole aligned words
+            return Score(gold, system, aligned)
+
+        def gold_aligned_gold(word):
+            return word
+        def gold_aligned_system(word):
+            return alignment.matched_words_map.get(word, "NotAligned") if word is not None else None
+        correct = 0
+        for words in alignment.matched_words:
+            if filter_fn is None or filter_fn(words.gold_word):
+                if key_fn(words.gold_word, gold_aligned_gold) == key_fn(words.system_word, gold_aligned_system):
+                    correct += 1
+
+        return Score(gold, system, correct, aligned)
+
+    def beyond_end(words, i, multiword_span_end):
+        if i >= len(words):
+            return True
+        if words[i].is_multiword:
+            return words[i].span.start >= multiword_span_end
+        return words[i].span.end > multiword_span_end
+
+    def extend_end(word, multiword_span_end):
+        if word.is_multiword and word.span.end > multiword_span_end:
+            return word.span.end
+        return multiword_span_end
+
+    def find_multiword_span(gold_words, system_words, gi, si):
+        # We know gold_words[gi].is_multiword or system_words[si].is_multiword.
+        # Find the start of the multiword span (gs, ss), so the multiword span is minimal.
+        # Initialize multiword_span_end characters index.
+        if gold_words[gi].is_multiword:
+            multiword_span_end = gold_words[gi].span.end
+            if not system_words[si].is_multiword and system_words[si].span.start < gold_words[gi].span.start:
+                si += 1
+        else: # if system_words[si].is_multiword
+            multiword_span_end = system_words[si].span.end
+            if not gold_words[gi].is_multiword and gold_words[gi].span.start < system_words[si].span.start:
+                gi += 1
+        gs, ss = gi, si
+
+        # Find the end of the multiword span
+        # (so both gi and si are pointing to the word following the multiword span end).
+        while not beyond_end(gold_words, gi, multiword_span_end) or \
+              not beyond_end(system_words, si, multiword_span_end):
+            if gi < len(gold_words) and (si >= len(system_words) or
+                                         gold_words[gi].span.start <= system_words[si].span.start):
+                multiword_span_end = extend_end(gold_words[gi], multiword_span_end)
+                gi += 1
+            else:
+                multiword_span_end = extend_end(system_words[si], multiword_span_end)
+                si += 1
+        return gs, ss, gi, si
+
+    def compute_lcs(gold_words, system_words, gi, si, gs, ss):
+        lcs = [[0] * (si - ss) for i in range(gi - gs)]
+        for g in reversed(range(gi - gs)):
+            for s in reversed(range(si - ss)):
+                if gold_words[gs + g].columns[FORM].lower() == system_words[ss + s].columns[FORM].lower():
+                    lcs[g][s] = 1 + (lcs[g+1][s+1] if g+1 < gi-gs and s+1 < si-ss else 0)
+                lcs[g][s] = max(lcs[g][s], lcs[g+1][s] if g+1 < gi-gs else 0)
+                lcs[g][s] = max(lcs[g][s], lcs[g][s+1] if s+1 < si-ss else 0)
+        return lcs
+
+    def align_words(gold_words, system_words):
+        alignment = Alignment(gold_words, system_words)
+
+        gi, si = 0, 0
+        while gi < len(gold_words) and si < len(system_words):
+            if gold_words[gi].is_multiword or system_words[si].is_multiword:
+                # A: Multi-word tokens => align via LCS within the whole "multiword span".
+                gs, ss, gi, si = find_multiword_span(gold_words, system_words, gi, si)
+
+                if si > ss and gi > gs:
+                    lcs = compute_lcs(gold_words, system_words, gi, si, gs, ss)
+
+                    # Store aligned words
+                    s, g = 0, 0
+                    while g < gi - gs and s < si - ss:
+                        if gold_words[gs + g].columns[FORM].lower() == system_words[ss + s].columns[FORM].lower():
+                            alignment.append_aligned_words(gold_words[gs+g], system_words[ss+s])
+                            g += 1
+                            s += 1
+                        elif lcs[g][s] == (lcs[g+1][s] if g+1 < gi-gs else 0):
+                            g += 1
+                        else:
+                            s += 1
+            else:
+                # B: No multi-word token => align according to spans.
+                if (gold_words[gi].span.start, gold_words[gi].span.end) == (system_words[si].span.start, system_words[si].span.end):
+                    alignment.append_aligned_words(gold_words[gi], system_words[si])
+                    gi += 1
+                    si += 1
+                elif gold_words[gi].span.start <= system_words[si].span.start:
+                    gi += 1
+                else:
+                    si += 1
+
+        return alignment
+
+    # Check that the underlying character sequences do match.
+    if gold_ud.characters != system_ud.characters:
+        index = 0
+        while index < len(gold_ud.characters) and index < len(system_ud.characters) and \
+                gold_ud.characters[index] == system_ud.characters[index]:
+            index += 1
+
+        raise UDError(
+            "The concatenation of tokens in gold file and in system file differ!\n" +
+            "First 20 differing characters in gold file: '{}' and system file: '{}'".format(
+                "".join(map(_encode, gold_ud.characters[index:index + 20])),
+                "".join(map(_encode, system_ud.characters[index:index + 20]))
+            )
+        )
+
+    # Align words
+    alignment = align_words(gold_ud.words, system_ud.words)
+
+    # Compute the F1-scores
+    return {
+        "Tokens": spans_score(gold_ud.tokens, system_ud.tokens),
+        "Sentences": spans_score(gold_ud.sentences, system_ud.sentences),
+        "Words": alignment_score(alignment),
+        "UPOS": alignment_score(alignment, lambda w, _: w.columns[UPOS]),
+        "XPOS": alignment_score(alignment, lambda w, _: w.columns[XPOS]),
+        "UFeats": alignment_score(alignment, lambda w, _: w.columns[FEATS]),
+        "AllTags": alignment_score(alignment, lambda w, _: (w.columns[UPOS], w.columns[XPOS], w.columns[FEATS])),
+        "Lemmas": alignment_score(alignment, lambda w, ga: w.columns[LEMMA] if ga(w).columns[LEMMA] != "_" else "_"),
+        "UAS": alignment_score(alignment, lambda w, ga: ga(w.parent)),
+        "LAS": alignment_score(alignment, lambda w, ga: (ga(w.parent), w.columns[DEPREL])),
+        "CLAS": alignment_score(alignment, lambda w, ga: (ga(w.parent), w.columns[DEPREL]),
+                                filter_fn=lambda w: w.is_content_deprel),
+        "MLAS": alignment_score(alignment, lambda w, ga: (ga(w.parent), w.columns[DEPREL], w.columns[UPOS], w.columns[FEATS],
+                                                         [(ga(c), c.columns[DEPREL], c.columns[UPOS], c.columns[FEATS])
+                                                          for c in w.functional_children]),
+                                filter_fn=lambda w: w.is_content_deprel),
+        "BLEX": alignment_score(alignment, lambda w, ga: (ga(w.parent), w.columns[DEPREL],
+                                                          w.columns[LEMMA] if ga(w).columns[LEMMA] != "_" else "_"),
+                                filter_fn=lambda w: w.is_content_deprel),
+    }
+
+
+def load_conllu_file(path):
+    _file = open(path, mode="r", **({"encoding": "utf-8"} if sys.version_info >= (3, 0) else {}))
+    return load_conllu(_file)
+
+def evaluate_wrapper(args):
+    # Load CoNLL-U files
+    gold_ud = load_conllu_file(args.gold_file)
+    system_ud = load_conllu_file(args.system_file)
+    return evaluate(gold_ud, system_ud)
+
+def main():
+    # Parse arguments
+    parser = argparse.ArgumentParser()
+    parser.add_argument("gold_file", type=str,
+                        help="Name of the CoNLL-U file with the gold data.")
+    parser.add_argument("system_file", type=str,
+                        help="Name of the CoNLL-U file with the predicted data.")
+    parser.add_argument("--verbose", "-v", default=False, action="store_true",
+                        help="Print all metrics.")
+    parser.add_argument("--counts", "-c", default=False, action="store_true",
+                        help="Print raw counts of correct/gold/system/aligned words instead of prec/rec/F1 for all metrics.")
+    args = parser.parse_args()
+
+    # Evaluate
+    evaluation = evaluate_wrapper(args)
+
+    # Print the evaluation
+    if not args.verbose and not args.counts:
+        print("LAS F1 Score: {:.2f}".format(100 * evaluation["LAS"].f1))
+        print("MLAS Score: {:.2f}".format(100 * evaluation["MLAS"].f1))
+        print("BLEX Score: {:.2f}".format(100 * evaluation["BLEX"].f1))
+    else:
+        if args.counts:
+            print("Metric     | Correct   |      Gold | Predicted | Aligned")
+        else:
+            print("Metric     | Precision |    Recall |  F1 Score | AligndAcc")
+        print("-----------+-----------+-----------+-----------+-----------")
+        for metric in["Tokens", "Sentences", "Words", "UPOS", "XPOS", "UFeats", "AllTags", "Lemmas", "UAS", "LAS", "CLAS", "MLAS", "BLEX"]:
+            if args.counts:
+                print("{:11}|{:10} |{:10} |{:10} |{:10}".format(
+                    metric,
+                    evaluation[metric].correct,
+                    evaluation[metric].gold_total,
+                    evaluation[metric].system_total,
+                    evaluation[metric].aligned_total or (evaluation[metric].correct if metric == "Words" else "")
+                ))
+            else:
+                print("{:11}|{:10.2f} |{:10.2f} |{:10.2f} |{}".format(
+                    metric,
+                    100 * evaluation[metric].precision,
+                    100 * evaluation[metric].recall,
+                    100 * evaluation[metric].f1,
+                    "{:10.2f}".format(100 * evaluation[metric].aligned_accuracy) if evaluation[metric].aligned_accuracy is not None else ""
+                ))
+
+if __name__ == "__main__":
+    main()
+
+# Tests, which can be executed with `python -m unittest conll18_ud_eval`.
+class TestAlignment(unittest.TestCase):
+    @staticmethod
+    def _load_words(words):
+        """Prepare fake CoNLL-U files with fake HEAD to prevent multiple roots errors."""
+        lines, num_words = [], 0
+        for w in words:
+            parts = w.split(" ")
+            if len(parts) == 1:
+                num_words += 1
+                lines.append("{}\t{}\t_\t_\t_\t_\t{}\t_\t_\t_".format(num_words, parts[0], int(num_words>1)))
+            else:
+                lines.append("{}-{}\t{}\t_\t_\t_\t_\t_\t_\t_\t_".format(num_words + 1, num_words + len(parts) - 1, parts[0]))
+                for part in parts[1:]:
+                    num_words += 1
+                    lines.append("{}\t{}\t_\t_\t_\t_\t{}\t_\t_\t_".format(num_words, part, int(num_words>1)))
+        return load_conllu((io.StringIO if sys.version_info >= (3, 0) else io.BytesIO)("\n".join(lines+["\n"])))
+
+    def _test_exception(self, gold, system):
+        self.assertRaises(UDError, evaluate, self._load_words(gold), self._load_words(system))
+
+    def _test_ok(self, gold, system, correct):
+        metrics = evaluate(self._load_words(gold), self._load_words(system))
+        gold_words = sum((max(1, len(word.split(" ")) - 1) for word in gold))
+        system_words = sum((max(1, len(word.split(" ")) - 1) for word in system))
+        self.assertEqual((metrics["Words"].precision, metrics["Words"].recall, metrics["Words"].f1),
+                         (correct / system_words, correct / gold_words, 2 * correct / (gold_words + system_words)))
+
+    def test_exception(self):
+        self._test_exception(["a"], ["b"])
+
+    def test_equal(self):
+        self._test_ok(["a"], ["a"], 1)
+        self._test_ok(["a", "b", "c"], ["a", "b", "c"], 3)
+
+    def test_equal_with_multiword(self):
+        self._test_ok(["abc a b c"], ["a", "b", "c"], 3)
+        self._test_ok(["a", "bc b c", "d"], ["a", "b", "c", "d"], 4)
+        self._test_ok(["abcd a b c d"], ["ab a b", "cd c d"], 4)
+        self._test_ok(["abc a b c", "de d e"], ["a", "bcd b c d", "e"], 5)
+
+    def test_alignment(self):
+        self._test_ok(["abcd"], ["a", "b", "c", "d"], 0)
+        self._test_ok(["abc", "d"], ["a", "b", "c", "d"], 1)
+        self._test_ok(["a", "bc", "d"], ["a", "b", "c", "d"], 2)
+        self._test_ok(["a", "bc b c", "d"], ["a", "b", "cd"], 2)
+        self._test_ok(["abc a BX c", "def d EX f"], ["ab a b", "cd c d", "ef e f"], 4)
+        self._test_ok(["ab a b", "cd bc d"], ["a", "bc", "d"], 2)
+        self._test_ok(["a", "bc b c", "d"], ["ab AX BX", "cd CX a"], 1)
```

### Comparing `classla-2.0/classla/utils/contract_mwt.py` & `classla-2.1/classla/utils/contract_mwt.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,40 +1,40 @@
-import sys
-
-def contract_mwt(ignore_gapping=True):
-    with open(sys.argv[2], 'w') as fout:
-        with open(sys.argv[1], 'r') as fin:
-            idx = 0
-            mwt_begin = 0
-            mwt_end = -1
-            for line in fin:
-                line = line.strip()
-    
-                if line.startswith('#'):
-                    print(line, file=fout)
-                    continue
-                elif len(line) <= 0:
-                    print(line, file=fout)
-                    idx = 0
-                    mwt_begin = 0
-                    mwt_end = -1
-                    continue
-    
-                line = line.split('\t')
-
-                # ignore gapping word
-                if ignore_gapping and '.' in line[0]:
-                    continue
-
-                idx += 1
-                if '-' in line[0]:
-                    mwt_begin, mwt_end = [int(x) for x in line[0].split('-')]
-                    print("{}\t{}\t{}".format(idx, "\t".join(line[1:-1]), "MWT=Yes" if line[-1] == '_' else line[-1] + "|MWT=Yes"), file=fout)
-                    idx -= 1
-                elif mwt_begin <= idx <= mwt_end:
-                    continue
-                else:
-                    print("{}\t{}".format(idx, "\t".join(line[1:])), file=fout)
-
-if __name__ == '__main__':
-    contract_mwt()
-
+import sys
+
+def contract_mwt(ignore_gapping=True):
+    with open(sys.argv[2], 'w') as fout:
+        with open(sys.argv[1], 'r') as fin:
+            idx = 0
+            mwt_begin = 0
+            mwt_end = -1
+            for line in fin:
+                line = line.strip()
+    
+                if line.startswith('#'):
+                    print(line, file=fout)
+                    continue
+                elif len(line) <= 0:
+                    print(line, file=fout)
+                    idx = 0
+                    mwt_begin = 0
+                    mwt_end = -1
+                    continue
+    
+                line = line.split('\t')
+
+                # ignore gapping word
+                if ignore_gapping and '.' in line[0]:
+                    continue
+
+                idx += 1
+                if '-' in line[0]:
+                    mwt_begin, mwt_end = [int(x) for x in line[0].split('-')]
+                    print("{}\t{}\t{}".format(idx, "\t".join(line[1:-1]), "MWT=Yes" if line[-1] == '_' else line[-1] + "|MWT=Yes"), file=fout)
+                    idx -= 1
+                elif mwt_begin <= idx <= mwt_end:
+                    continue
+                else:
+                    print("{}\t{}".format(idx, "\t".join(line[1:])), file=fout)
+
+if __name__ == '__main__':
+    contract_mwt()
+
```

### Comparing `classla-2.0/classla/utils/obeliks.py` & `classla-2.1/classla/utils/obeliks.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,55 +1,55 @@
-"""
-Utilities related to using reldi in the pipeline.
-"""
-import re
-
-import obeliks
-
-
-def check_reldi():
-    """
-    Import necessary components from reldi to perform tokenization.
-    """
-    try:
-        import classla
-    except ImportError:
-        raise ImportError(
-            "Reldi is used but not present on your machine. Make sure you run 'git submodule init' and 'git submodule update' commands."
-        )
-    return True
-
-
-class ObeliksTrainer():
-    def __init__(self, lang='sl', type='standard', annotate_pos=False):
-        """ Construct a reldi-based tokenizer by loading the reldi pipeline.
-        """
-        if lang not in ['sl']:
-            raise Exception("Obeliks tokenizer is currently only allowed in Slovene.")
-
-    @staticmethod
-    def tokenize(document):
-        """ Tokenize a document with the obeliks tokenizer and add results to appropriate format.
-        """
-        raw_text = '\n'.join(document) if isinstance(document, list) else document
-
-        document = []
-        metadocument = []
-
-        if raw_text == '' or raw_text.isspace():
-            return raw_text, document, metadocument
-
-        for doc in obeliks.run(raw_text, object_output=True):
-            for sentence in doc:
-                for word in sentence['sentence']:
-                    if word['lemma'] == '_':
-                        del (word['lemma'])
-                    if word['xpos'] == '_':
-                        del (word['xpos'])
-                    if word['upos'] == '_':
-                        del (word['upos'])
-                    if word['misc'] == '_':
-                        del (word['misc'])
-                document.append(sentence['sentence'])
-                metadocument.append(sentence['metadata'])
-
-        return raw_text, document, metadocument
+"""
+Utilities related to using reldi in the pipeline.
+"""
+import re
+
+import obeliks
+
+
+def check_reldi():
+    """
+    Import necessary components from reldi to perform tokenization.
+    """
+    try:
+        import classla
+    except ImportError:
+        raise ImportError(
+            "Reldi is used but not present on your machine. Make sure you run 'git submodule init' and 'git submodule update' commands."
+        )
+    return True
+
+
+class ObeliksTrainer():
+    def __init__(self, lang='sl', type='standard', annotate_pos=False):
+        """ Construct a reldi-based tokenizer by loading the reldi pipeline.
+        """
+        if lang not in ['sl']:
+            raise Exception("Obeliks tokenizer is currently only allowed in Slovene.")
+
+    @staticmethod
+    def tokenize(document):
+        """ Tokenize a document with the obeliks tokenizer and add results to appropriate format.
+        """
+        raw_text = '\n'.join(document) if isinstance(document, list) else document
+
+        document = []
+        metadocument = []
+
+        if raw_text == '' or raw_text.isspace():
+            return raw_text, document, metadocument
+
+        for doc in obeliks.run(raw_text, object_output=True):
+            for sentence in doc:
+                for word in sentence['sentence']:
+                    if word['lemma'] == '_':
+                        del (word['lemma'])
+                    if word['xpos'] == '_':
+                        del (word['xpos'])
+                    if word['upos'] == '_':
+                        del (word['upos'])
+                    if word['misc'] == '_':
+                        del (word['misc'])
+                document.append(sentence['sentence'])
+                metadocument.append(sentence['metadata'])
+
+        return raw_text, document, metadocument
```

### Comparing `classla-2.0/classla/utils/postprocess_vietnamese_tokenizer_data.py` & `classla-2.1/classla/utils/postprocess_vietnamese_tokenizer_data.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,64 +1,64 @@
-import argparse
-import re
-import sys
-from collections import Counter
-import json
-
-def para_to_chunks(text, char_level_pred):
-    chunks = []
-    preds = []
-    lastchunk = ''
-    lastpred = ''
-    for idx in range(len(text)):
-        if re.match(r'^\w$', text[idx], flags=re.UNICODE):
-            lastchunk += text[idx]
-        else:
-            if len(lastchunk) > 0 and not re.match(r'^\W+$', lastchunk, flags=re.UNICODE):
-                chunks += [lastchunk]
-                assert len(lastpred) > 0
-                preds += [int(lastpred)]
-                lastchunk = ''
-            if not re.match(r'^\s$', text[idx], flags=re.UNICODE):
-                # punctuation
-                chunks += [text[idx]]
-                preds += [int(char_level_pred[idx])]
-            else:
-                # prepend leading white spaces to chunks so we can tell the difference between "2 , 2" and "2,2"
-                lastchunk += text[idx]
-        lastpred = char_level_pred[idx]
-
-    if len(lastchunk) > 0:
-        chunks += [lastchunk]
-        preds += [int(lastpred)]
-
-    return list(zip(chunks, preds))
-
-def paras_to_chunks(text, char_level_pred):
-    return [para_to_chunks(re.sub(r'\s', ' ', pt.rstrip()), pc) for pt, pc in zip(text.split('\n\n'), char_level_pred.split('\n\n'))]
-
-if __name__ == '__main__':
-    parser = argparse.ArgumentParser()
-
-    parser.add_argument('plaintext_file', type=str, help="Plaintext file containing the raw input")
-    parser.add_argument('--char_level_pred', type=str, default=None, help="Plaintext file containing character-level predictions")
-    parser.add_argument('-o', '--output', default=None, type=str, help="Output file name; output to the console if not specified (the default)")
-
-    args = parser.parse_args()
-
-    with open(args.plaintext_file, 'r') as f:
-        text = ''.join(f.readlines()).rstrip()
-        text = '\n\n'.join([x for x in text.split('\n\n')])
-
-    if args.char_level_pred is not None:
-        with open(args.char_level_pred, 'r') as f:
-            char_level_pred = ''.join(f.readlines())
-    else:
-        char_level_pred = '\n\n'.join(['0' * len(x) for x in text.split('\n\n')])
-
-    assert len(text) == len(char_level_pred), 'Text has {} characters but there are {} char-level labels!'.format(len(text), len(char_level_pred))
-
-    output = sys.stdout if args.output is None else open(args.output, 'w')
-
-    json.dump(paras_to_chunks(text, char_level_pred), output)
-
-    output.close()
+import argparse
+import re
+import sys
+from collections import Counter
+import json
+
+def para_to_chunks(text, char_level_pred):
+    chunks = []
+    preds = []
+    lastchunk = ''
+    lastpred = ''
+    for idx in range(len(text)):
+        if re.match(r'^\w$', text[idx], flags=re.UNICODE):
+            lastchunk += text[idx]
+        else:
+            if len(lastchunk) > 0 and not re.match(r'^\W+$', lastchunk, flags=re.UNICODE):
+                chunks += [lastchunk]
+                assert len(lastpred) > 0
+                preds += [int(lastpred)]
+                lastchunk = ''
+            if not re.match(r'^\s$', text[idx], flags=re.UNICODE):
+                # punctuation
+                chunks += [text[idx]]
+                preds += [int(char_level_pred[idx])]
+            else:
+                # prepend leading white spaces to chunks so we can tell the difference between "2 , 2" and "2,2"
+                lastchunk += text[idx]
+        lastpred = char_level_pred[idx]
+
+    if len(lastchunk) > 0:
+        chunks += [lastchunk]
+        preds += [int(lastpred)]
+
+    return list(zip(chunks, preds))
+
+def paras_to_chunks(text, char_level_pred):
+    return [para_to_chunks(re.sub(r'\s', ' ', pt.rstrip()), pc) for pt, pc in zip(text.split('\n\n'), char_level_pred.split('\n\n'))]
+
+if __name__ == '__main__':
+    parser = argparse.ArgumentParser()
+
+    parser.add_argument('plaintext_file', type=str, help="Plaintext file containing the raw input")
+    parser.add_argument('--char_level_pred', type=str, default=None, help="Plaintext file containing character-level predictions")
+    parser.add_argument('-o', '--output', default=None, type=str, help="Output file name; output to the console if not specified (the default)")
+
+    args = parser.parse_args()
+
+    with open(args.plaintext_file, 'r') as f:
+        text = ''.join(f.readlines()).rstrip()
+        text = '\n\n'.join([x for x in text.split('\n\n')])
+
+    if args.char_level_pred is not None:
+        with open(args.char_level_pred, 'r') as f:
+            char_level_pred = ''.join(f.readlines())
+    else:
+        char_level_pred = '\n\n'.join(['0' * len(x) for x in text.split('\n\n')])
+
+    assert len(text) == len(char_level_pred), 'Text has {} characters but there are {} char-level labels!'.format(len(text), len(char_level_pred))
+
+    output = sys.stdout if args.output is None else open(args.output, 'w')
+
+    json.dump(paras_to_chunks(text, char_level_pred), output)
+
+    output.close()
```

### Comparing `classla-2.0/classla/utils/prepare_ner_data.py` & `classla-2.1/classla/utils/prepare_ner_data.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,71 +1,71 @@
-"""
-This script converts NER data from the CoNLL03 format to the latest CoNLL-U format. The script assumes that in the 
-input column format data, the token is always in the first column, while the NER tag is always in the last column.
-"""
-
-import argparse
-import json
-
-MIN_NUM_FIELD = 2
-MAX_NUM_FIELD = 5
-
-DOC_START_TOKEN = '-DOCSTART-'
-
-def parse_args():
-    parser = argparse.ArgumentParser(description="Convert the conll03 format data into conllu format.")
-    parser.add_argument('input', help='Input conll03 format data filename.')
-    parser.add_argument('output', help='Output json filename.')
-    args = parser.parse_args()
-    return args
-
-def main():
-    args = parse_args()
-
-    sentences = load_conll03(args.input)
-    print("{} examples loaded from {}".format(len(sentences), args.input))
-    
-    document = []
-    for (words, tags) in sentences:
-        sent = []
-        for w, t in zip(words, tags):
-            sent += [{'text': w, 'ner': t}]
-        document += [sent]
-
-    with open(args.output, 'w') as outfile:
-        json.dump(document, outfile)
-    print("Generated json file {}.".format(args.output))
-
-def load_conll03(filename, skip_doc_start=True):
-    cached_lines = []
-    examples = []
-    with open(filename) as infile:
-        for line in infile:
-            line = line.strip()
-            if skip_doc_start and DOC_START_TOKEN in line:
-                continue
-            if len(line) > 0:
-                array = line.split()
-                if len(array) < MIN_NUM_FIELD:
-                    continue
-                else:
-                    cached_lines.append(line)
-            elif len(cached_lines) > 0:
-                example = process_cache(cached_lines)
-                examples.append(example)
-                cached_lines = []
-        if len(cached_lines) > 0:
-            examples.append(process_cache(cached_lines))
-    return examples
-
-def process_cache(cached_lines):
-    tokens = []
-    ner_tags = []
-    for line in cached_lines:
-        array = line.split()
-        assert len(array) >= MIN_NUM_FIELD and len(array) <= MAX_NUM_FIELD
-        tokens.append(array[0])
-        ner_tags.append(array[-1])
-    return (tokens, ner_tags)
-
-if __name__ == '__main__':
-    main()
+"""
+This script converts NER data from the CoNLL03 format to the latest CoNLL-U format. The script assumes that in the 
+input column format data, the token is always in the first column, while the NER tag is always in the last column.
+"""
+
+import argparse
+import json
+
+MIN_NUM_FIELD = 2
+MAX_NUM_FIELD = 5
+
+DOC_START_TOKEN = '-DOCSTART-'
+
+def parse_args():
+    parser = argparse.ArgumentParser(description="Convert the conll03 format data into conllu format.")
+    parser.add_argument('input', help='Input conll03 format data filename.')
+    parser.add_argument('output', help='Output json filename.')
+    args = parser.parse_args()
+    return args
+
+def main():
+    args = parse_args()
+
+    sentences = load_conll03(args.input)
+    print("{} examples loaded from {}".format(len(sentences), args.input))
+    
+    document = []
+    for (words, tags) in sentences:
+        sent = []
+        for w, t in zip(words, tags):
+            sent += [{'text': w, 'ner': t}]
+        document += [sent]
+
+    with open(args.output, 'w') as outfile:
+        json.dump(document, outfile)
+    print("Generated json file {}.".format(args.output))
+
+def load_conll03(filename, skip_doc_start=True):
+    cached_lines = []
+    examples = []
+    with open(filename) as infile:
+        for line in infile:
+            line = line.strip()
+            if skip_doc_start and DOC_START_TOKEN in line:
+                continue
+            if len(line) > 0:
+                array = line.split()
+                if len(array) < MIN_NUM_FIELD:
+                    continue
+                else:
+                    cached_lines.append(line)
+            elif len(cached_lines) > 0:
+                example = process_cache(cached_lines)
+                examples.append(example)
+                cached_lines = []
+        if len(cached_lines) > 0:
+            examples.append(process_cache(cached_lines))
+    return examples
+
+def process_cache(cached_lines):
+    tokens = []
+    ner_tags = []
+    for line in cached_lines:
+        array = line.split()
+        assert len(array) >= MIN_NUM_FIELD and len(array) <= MAX_NUM_FIELD
+        tokens.append(array[0])
+        ner_tags.append(array[-1])
+    return (tokens, ner_tags)
+
+if __name__ == '__main__':
+    main()
```

### Comparing `classla-2.0/classla/utils/prepare_tokenizer_data.py` & `classla-2.1/classla/utils/prepare_tokenizer_data.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,143 +1,143 @@
-import argparse
-import os
-import re
-import sys
-
-"""
-Data is output in 4 files:
-
-a file containing the mwt information
-a file containing the words and sentences in conllu format
-a file containing the raw text of each paragraph
-a file of 0,1,2 indicating word break or sentence break on a character level for the raw text
-  1: end of word
-  2: end of sentence
-"""
-
-PARAGRAPH_BREAK = re.compile(r'\n\s*\n')
-
-parser = argparse.ArgumentParser()
-
-parser.add_argument('plaintext_file', type=str, help="Plaintext file containing the raw input")
-parser.add_argument('conllu_file', type=str, help="CoNLL-U file containing tokens and sentence breaks")
-parser.add_argument('-o', '--output', default=None, type=str, help="Output file name; output to the console if not specified (the default)")
-parser.add_argument('-m', '--mwt_output', default=None, type=str, help="Output file name for MWT expansions; output to the console if not specified (the default)")
-
-args = parser.parse_args()
-
-with open(args.plaintext_file, 'r') as f:
-    text = ''.join(f.readlines())
-textlen = len(text)
-
-if args.output is None:
-    output = sys.stdout
-else:
-    outdir = os.path.split(args.output)[0]
-    os.makedirs(outdir, exist_ok=True)
-    output = open(args.output, 'w')
-
-index = 0 # character offset in rawtext
-
-def is_para_break(index, text):
-    """ Detect if a paragraph break can be found, and return the length of the paragraph break sequence. """
-    if text[index] == '\n':
-        para_break = PARAGRAPH_BREAK.match(text[index:])
-        if para_break:
-            break_len = len(para_break.group(0))
-            return True, break_len
-    return False, 0
-
-def find_next_word(index, text, word, output):
-    """
-    Locate the next word in the text. In case a paragraph break is found, also write paragraph break to labels.
-    """
-    idx = 0
-    word_sofar = ''
-    yeah=False
-    while index < len(text) and idx < len(word):
-        para_break, break_len = is_para_break(index, text)
-        if para_break:
-            # multiple newlines found, paragraph break
-            if len(word_sofar) > 0:
-                assert re.match(r'^\s+$', word_sofar), 'Found non-empty string at the end of a paragraph that doesn\'t match any token: |{}|'.format(word_sofar)
-                word_sofar = ''
-
-            output.write('\n\n')
-            index += break_len - 1
-        elif re.match(r'^\s$', text[index]) and not re.match(r'^\s$', word[idx]):
-            # whitespace found, and whitespace is not part of a word
-            word_sofar += text[index]
-        else:
-            # non-whitespace char, or a whitespace char that's part of a word
-            word_sofar += text[index]
-            assert text[index].replace('\n', ' ') == word[idx], "Character mismatch: raw text contains |%s| but the next word is |%s|." % (word_sofar, word)
-            idx += 1
-        index += 1
-    return index, word_sofar
-
-mwt_expansions = []
-with open(args.conllu_file, 'r') as f:
-    buf = ''
-    mwtbegin = 0
-    mwtend = -1
-    expanded = []
-    last_comments = ""
-    for line in f:
-        line = line.strip()
-        if len(line):
-            if line[0] == "#":
-                # comment, don't do anything
-                if len(last_comments) == 0:
-                    last_comments = line
-                continue
-
-            line = line.split('\t')
-            if '.' in line[0]:
-                # the tokenizer doesn't deal with ellipsis
-                continue
-
-            word = line[1]
-            if '-' in line[0]:
-                # multiword token
-                mwtbegin, mwtend = [int(x) for x in line[0].split('-')]
-                lastmwt = word
-                expanded = []
-            elif mwtbegin <= int(line[0]) < mwtend:
-                expanded += [word]
-                continue
-            elif int(line[0]) == mwtend:
-                expanded += [word]
-                expanded = [x.lower() for x in expanded] # evaluation doesn't care about case
-                mwt_expansions += [(lastmwt, tuple(expanded))]
-                if lastmwt[0].islower() and not expanded[0][0].islower():
-                    print('Sentence ID with potential wrong MWT expansion: ', last_comments, file=sys.stderr)
-                mwtbegin = 0
-                mwtend = -1
-                lastmwt = None
-                continue
-
-            if len(buf):
-                output.write(buf)
-            index, word_found = find_next_word(index, text, word, output)
-            buf = '0' * (len(word_found)-1) + ('1' if '-' not in line[0] else '3')
-        else:
-            # sentence break found
-            if len(buf):
-                assert int(buf[-1]) >= 1
-                output.write(buf[:-1] + '{}'.format(int(buf[-1]) + 1))
-                buf = ''
-
-            last_comments = ''
-
-output.close()
-
-from collections import Counter
-mwts = Counter(mwt_expansions)
-if args.mwt_output is None:
-    print('MWTs:', mwts)
-else:
-    import json
-    with open(args.mwt_output, 'w') as f:
-        json.dump(list(mwts.items()), f)
-
-    print('{} unique MWTs found in data'.format(len(mwts)))
+import argparse
+import os
+import re
+import sys
+
+"""
+Data is output in 4 files:
+
+a file containing the mwt information
+a file containing the words and sentences in conllu format
+a file containing the raw text of each paragraph
+a file of 0,1,2 indicating word break or sentence break on a character level for the raw text
+  1: end of word
+  2: end of sentence
+"""
+
+PARAGRAPH_BREAK = re.compile(r'\n\s*\n')
+
+parser = argparse.ArgumentParser()
+
+parser.add_argument('plaintext_file', type=str, help="Plaintext file containing the raw input")
+parser.add_argument('conllu_file', type=str, help="CoNLL-U file containing tokens and sentence breaks")
+parser.add_argument('-o', '--output', default=None, type=str, help="Output file name; output to the console if not specified (the default)")
+parser.add_argument('-m', '--mwt_output', default=None, type=str, help="Output file name for MWT expansions; output to the console if not specified (the default)")
+
+args = parser.parse_args()
+
+with open(args.plaintext_file, 'r') as f:
+    text = ''.join(f.readlines())
+textlen = len(text)
+
+if args.output is None:
+    output = sys.stdout
+else:
+    outdir = os.path.split(args.output)[0]
+    os.makedirs(outdir, exist_ok=True)
+    output = open(args.output, 'w')
+
+index = 0 # character offset in rawtext
+
+def is_para_break(index, text):
+    """ Detect if a paragraph break can be found, and return the length of the paragraph break sequence. """
+    if text[index] == '\n':
+        para_break = PARAGRAPH_BREAK.match(text[index:])
+        if para_break:
+            break_len = len(para_break.group(0))
+            return True, break_len
+    return False, 0
+
+def find_next_word(index, text, word, output):
+    """
+    Locate the next word in the text. In case a paragraph break is found, also write paragraph break to labels.
+    """
+    idx = 0
+    word_sofar = ''
+    yeah=False
+    while index < len(text) and idx < len(word):
+        para_break, break_len = is_para_break(index, text)
+        if para_break:
+            # multiple newlines found, paragraph break
+            if len(word_sofar) > 0:
+                assert re.match(r'^\s+$', word_sofar), 'Found non-empty string at the end of a paragraph that doesn\'t match any token: |{}|'.format(word_sofar)
+                word_sofar = ''
+
+            output.write('\n\n')
+            index += break_len - 1
+        elif re.match(r'^\s$', text[index]) and not re.match(r'^\s$', word[idx]):
+            # whitespace found, and whitespace is not part of a word
+            word_sofar += text[index]
+        else:
+            # non-whitespace char, or a whitespace char that's part of a word
+            word_sofar += text[index]
+            assert text[index].replace('\n', ' ') == word[idx], "Character mismatch: raw text contains |%s| but the next word is |%s|." % (word_sofar, word)
+            idx += 1
+        index += 1
+    return index, word_sofar
+
+mwt_expansions = []
+with open(args.conllu_file, 'r') as f:
+    buf = ''
+    mwtbegin = 0
+    mwtend = -1
+    expanded = []
+    last_comments = ""
+    for line in f:
+        line = line.strip()
+        if len(line):
+            if line[0] == "#":
+                # comment, don't do anything
+                if len(last_comments) == 0:
+                    last_comments = line
+                continue
+
+            line = line.split('\t')
+            if '.' in line[0]:
+                # the tokenizer doesn't deal with ellipsis
+                continue
+
+            word = line[1]
+            if '-' in line[0]:
+                # multiword token
+                mwtbegin, mwtend = [int(x) for x in line[0].split('-')]
+                lastmwt = word
+                expanded = []
+            elif mwtbegin <= int(line[0]) < mwtend:
+                expanded += [word]
+                continue
+            elif int(line[0]) == mwtend:
+                expanded += [word]
+                expanded = [x.lower() for x in expanded] # evaluation doesn't care about case
+                mwt_expansions += [(lastmwt, tuple(expanded))]
+                if lastmwt[0].islower() and not expanded[0][0].islower():
+                    print('Sentence ID with potential wrong MWT expansion: ', last_comments, file=sys.stderr)
+                mwtbegin = 0
+                mwtend = -1
+                lastmwt = None
+                continue
+
+            if len(buf):
+                output.write(buf)
+            index, word_found = find_next_word(index, text, word, output)
+            buf = '0' * (len(word_found)-1) + ('1' if '-' not in line[0] else '3')
+        else:
+            # sentence break found
+            if len(buf):
+                assert int(buf[-1]) >= 1
+                output.write(buf[:-1] + '{}'.format(int(buf[-1]) + 1))
+                buf = ''
+
+            last_comments = ''
+
+output.close()
+
+from collections import Counter
+mwts = Counter(mwt_expansions)
+if args.mwt_output is None:
+    print('MWTs:', mwts)
+else:
+    import json
+    with open(args.mwt_output, 'w') as f:
+        json.dump(list(mwts.items()), f)
+
+    print('{} unique MWTs found in data'.format(len(mwts)))
```

### Comparing `classla-2.0/classla.egg-info/PKG-INFO` & `classla-2.1/README.md`

 * *Files 16% similar despite different names*

```diff
@@ -1,345 +1,339 @@
-Metadata-Version: 2.1
-Name: classla
-Version: 2.0
-Summary: Adapted Stanford NLP Python Library with improvements for specific languages.
-Home-page: https://github.com/clarinsi/classla-stanfordnlp.git
-Author: CLARIN.SI
-Author-email: info@clarin.si
-License: Apache License 2.0
-Keywords: natural-language-processing nlp natural-language-understanding stanford-nlp deep-learning clarinsi
-Classifier: Development Status :: 4 - Beta
-Classifier: Intended Audience :: Developers
-Classifier: Intended Audience :: Education
-Classifier: Intended Audience :: Science/Research
-Classifier: Intended Audience :: Information Technology
-Classifier: Topic :: Scientific/Engineering
-Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
-Classifier: Topic :: Scientific/Engineering :: Information Analysis
-Classifier: Topic :: Text Processing
-Classifier: Topic :: Text Processing :: Linguistic
-Classifier: Topic :: Software Development
-Classifier: Topic :: Software Development :: Libraries
-Classifier: Programming Language :: Python :: 3.6
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Requires-Python: >=3.6
-Description-Content-Type: text/markdown
-Provides-Extra: dev
-Provides-Extra: test
-License-File: LICENSE
-
-# A [CLASSLA](http://www.clarin.si/info/k-centre/) Fork of [Stanza](https://github.com/stanfordnlp/stanza) for Processing Slovenian, Croatian, Serbian, Macedonian and Bulgarian
-
-## Description
-
-This pipeline allows for processing of standard Slovenian, Croatian, Serbian and Bulgarian on the levels of
-
-- tokenization and sentence splitting
-- part-of-speech tagging
-- lemmatization
-- dependency parsing
-- named entity recognition
-
-It also allows for (alpha) processing of standard Macedonian on the levels of 
-
-- tokenization and sentence splitting
-- part-of-speech tagging
-- lemmatization
-
-Finally, it allows for processing of non-standard (Internet) Slovenian, Croatian and Serbian on the same levels as standard language (all models are tailored to non-standard language except for dependency parsing where the standard module is used).
-
-## Differences to Stanza
-
-The differences of this pipeline to the original Stanza pipeline are the following:
-
-- usage of language-specific rule-based tokenizers and sentence splitters, [obeliks](https://pypi.org/project/obeliks/) for standard Slovenian and [reldi-tokeniser](https://pypi.org/project/reldi-tokeniser/) for the remaining varieties and languages (Stanza uses inferior machine-learning-based tokenization and sentence splitting trained on UD data)
-- default pre-tagging and pre-lemmatization on the level of tokenizers for the following phenomena: punctuation, symbol, e-mail, URL, mention, hashtag, emoticon, emoji (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#usage-of-tagging-control-via-the-tokenizer))
-- optional control of the tagger for Slovenian via an inflectional lexicon on the levels of XPOS, UPOS, FEATS (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#usage-of-inflectional-lexicon))
-- closed class handling depending on the usage of the options described in the last two bullets, as documented [here](https://github.com/clarinsi/classla/blob/master/README.closed_classes.md)
-- usage of external inflectional lexicons for lookup lemmatization, seq2seq being used very infrequently on OOVs only (Stanza uses only UD training data for lookup lemmatization)
-- morphosyntactic tagging models based on larger quantities of training data than is available in UD (training data that are morphosyntactically tagged, but not UD-parsed)
-- lemmatization models based on larger quantities of training data than is available in UD (training data that are lemmatized, but not UD-parsed)
-- optional JOS-project-based parsing of Slovenian (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#jos-dependency-parsing-system))
-- named entity recognition models for all languages except Macedonian (Stanza does not cover named entity recognition for any of the languages supported by classla)
-- Macedonian models (Macedonian is not available in UD yet)
-- non-standard models for Croatian, Slovenian, Serbian (there is no UD data for these varieties)
-
-The above modifications led to some important improvements in the tool’s performance in comparison to original Stanza. For standard Slovenian, for example, running the full classla pipeline increases sentence segmentation F1 scores to 99.52 (94.29% error reduction), lemmatization to 99.17 (68.8% error reduction), XPOS tagging  to 97.38 (46.75% error reduction), UPOS tagging to 98.69 (23.4% error reduction), and LAS to 92.05 (23.56% error reduction).  See official [Stanza performance](https://stanfordnlp.github.io/stanza/performance.html) (evaluated on different data splits) for comparison.
-
-## Installation
-### pip
-We recommend that you install CLASSLA via pip, the Python package manager. To install, run:
-```bash
-pip install classla
-```
-This will also resolve all dependencies.
-
-__NOTE TO EXISTING USERS__: Once you install this classla version, you will HAVE TO re-download the models. All previously downloaded models will not be used anymore. We suggest you delete the old models. Their default location is at `~/classla_resources`.
-
-## Running CLASSLA
-
-### Getting started
-
-To run the CLASSLA pipeline for the first time on processing standard Slovenian, follow these steps:
-
-```
->>> import classla
->>> classla.download('sl')                            # download standard models for Slovenian, use hr for Croatian, sr for Serbian, bg for Bulgarian, mk for Macedonian
->>> nlp = classla.Pipeline('sl')                      # initialize the default Slovenian pipeline, use hr for Croatian, sr for Serbian, bg for Bulgarian, mk for Macedonian
->>> doc = nlp("France Prešeren je rojen v Vrbi.")     # run the pipeline
->>> print(doc.to_conll())                             # print the output in CoNLL-U format
-# newpar id = 1
-# sent_id = 1.1
-# text = France Prešeren je rojen v Vrbi.
-1	France	France	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	4	nsubj	_	NER=B-PER
-2	Prešeren	Prešeren	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat_name	_	NER=I-PER
-3	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	cop	_	NER=O
-4	rojen	rojen	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part	0	root	_	NER=O
-5	v	v	ADP	Sl	Case=Loc	6	case	_	NER=O
-6	Vrbi	Vrba	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
-7	.	.	PUNCT	Z	_	4	punct	_	NER=O
-
-```
-You can find examples of standard language processing for [Croatian](#example-of-standard-croatian), [Serbian](#example-of-standard-serbian), [Macedonian](#example-of-standard-macedonian) and [Bulgarian](#example-of-standard-bulgarian) at the end of this document.
-
-### Processing non-standard language
-
-Processing non-standard Slovenian differs to the above standard example just by an additional argument ```type="nonstandard"```:
-
-```
->>> import classla
->>> classla.download('sl', type='nonstandard')        # download non-standard models for Slovenian, use hr for Croatian and sr for Serbian
->>> nlp = classla.Pipeline('sl', type='nonstandard')  # initialize the default non-standard Slovenian pipeline, use hr for Croatian and sr for Serbian
->>> doc = nlp("kva smo mi zurali zadnje leto v zagrebu...")     # run the pipeline
->>> print(doc.to_conll())                             # print the output in CoNLL-U format 
-1	kva	kaj	PRON	Pq-nsa	Case=Acc|Gender=Neut|Number=Sing|PronType=Int	4	obj	_	NER=O
-2	smo	biti	AUX	Va-r1p-n	Mood=Ind|Number=Plur|Person=1|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
-3	mi	jaz	PRON	Pp1mpn	Case=Nom|Gender=Masc|Number=Plur|Person=1|PronType=Prs	nsubj	_	NER=O
-4	zurali	žurati	VERB	Vmpp-pm	Aspect=Imp|Gender=Masc|Number=Plur|VerbForm=Part	root	_	NER=O
-5	zadnje	zadnji	ADJ	Agpnsa	Case=Acc|Degree=Pos|Gender=Neut|Number=Sing	6	amod	_	NER=O
-6	leto	leto	NOUN	Ncnsa	Case=Acc|Gender=Neut|Number=Sing	4	obl	NER=O
-7	v	v	ADP	Sl	Case=Loc	8	case	_	NER=O
-8	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	4	obl	NER=B-LOC|SpaceAfter=No
-9	...	.	PUNCT	Z	_	4	punct	_	NER=O
-
-```
-
-You can find examples of non-standard language processing for [Croatian](#example-of-non-standard-croatian) and [Serbian](#example-of-non-standard-serbian)  at the end of this document.
-
-For additional usage examples you can also consult the ```pipeline_demo.py``` file.
-
-## Processors
-
-The CLASSLA pipeline is built from multiple units. These units are called processors. By default CLASSLA runs the ```tokenize```, ```ner```, ```pos```, ```lemma``` and ```depparse``` processors.
-
-You can specify which processors CLASSLA should run, via the ```processors``` attribute as in the following example, performing tokenization, named entity recognition, part-of-speech tagging and lemmatization.
-
-```python
->>> nlp = classla.Pipeline('sl', processors='tokenize,ner,pos,lemma')
-```
-
-Another popular option might be to perform tokenization, part-of-speech tagging, lemmatization and dependency parsing.
-
-```python
->>> nlp = classla.Pipeline('sl', processors='tokenize,pos,lemma,depparse')
-```
-
-### Tokenization and sentence splitting
-
-The tokenization and sentence splitting processor ```tokenize``` is the first processor and is required for any further processing.
-
-In case you already have tokenized text, you should separate tokens via spaces and pass the attribute ```tokenize_pretokenized=True```.
-
-By default CLASSLA uses a rule-based tokenizer - [obeliks](https://github.com/clarinsi/obeliks) for Slovenian standard language pipeline. In other cases we use [reldi-tokeniser](https://github.com/clarinsi/reldi-tokeniser).
-
-<!--Most important attributes:
-```
-tokenize_pretokenized   - [boolean]     ignores tokenizer
-```-->
-
-### Part-of-speech tagging
-
-The POS tagging processor ```pos``` will general output that contains morphosyntactic description following the [MULTEXT-East standard](http://nl.ijs.si/ME/V6/msd/html/msd.lang-specific.html) and universal part-of-speech tags and universal features following the [Universal Dependencies standard](https://universaldependencies.org). This processing requires the usage of the ```tokenize``` processor.
-
-<!--Most important attributes:
-```
-pos_model_path          - [str]         alternative path to model file
-pos_pretrain_path       - [str]         alternative path to pretrain file
-```-->
-
-### Lemmatization
-
-The lemmatization processor ```lemma``` will produce lemmas (basic forms) for each token in the input. It requires the usage of both the ```tokenize``` and ```pos``` processors.
-
-### Dependency parsing
-
-The dependency parsing processor ```depparse``` performs syntactic dependency parsing of sentences following the [Universal Dependencies formalism](https://universaldependencies.org/introduction.html#:~:text=Universal%20Dependencies%20(UD)%20is%20a,from%20a%20language%20typology%20perspective.). It requires the ```tokenize``` and ```pos``` processors.
-
-### Named entity recognition
-
-The named entity recognition processor ```ner``` identifies named entities in text following the [IOB2](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)) format. It requires only the ```tokenize``` processor.
-
-## Citing
-
-If you use this tool, please cite the following paper:
-
-```
-@inproceedings{ljubesic-dobrovoljc-2019-neural,
-    title = "What does Neural Bring? Analysing Improvements in Morphosyntactic Annotation and Lemmatisation of {S}lovenian, {C}roatian and {S}erbian",
-    author = "Ljube{\v{s}}i{\'c}, Nikola  and
-      Dobrovoljc, Kaja",
-    booktitle = "Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing",
-    month = aug,
-    year = "2019",
-    address = "Florence, Italy",
-    publisher = "Association for Computational Linguistics",
-    url = "https://www.aclweb.org/anthology/W19-3704",
-    doi = "10.18653/v1/W19-3704",
-    pages = "29--34"
-    }
-```
-
-## Croatian examples
-
-### Example of standard Croatian 
-
-```
->>> import classla
->>> nlp = classla.Pipeline('hr') # run classla.download('hr') beforehand if necessary
->>> doc = nlp("Ante Starčević rođen je u Velikom Žitniku.")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = Ante Starčević rođen je u Velikom Žitniku.
-1	Ante	Ante	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj_pass	_	NER=B-PER
-2	Starčević	Starčević	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
-3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
-4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux_pass	_	NER=O
-5	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
-6	Velikom	velik	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	7	amod	_	NER=B-LOC
-7	Žitniku	Žitnik	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	3	obl	_	NER=I-LOC|SpaceAfter=No
-8	.	.	PUNCT	Z	_	3	punct	_	NER=O
-
-```
-### Example of non-standard Croatian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('hr', type='nonstandard') # run classla.download('hr', type='nonstandard') beforehand if necessary
->>> doc = nlp("kaj sam ja tulumaril jucer u ljubljani...")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = kaj sam ja tulumaril jucer u ljubljani...
-1	kaj	što	PRON	Pi3n-a	Case=Acc|Gender=Neut|PronType=Int,Rel	4	obj	_	NER=O
-2	sam	biti	AUX	Var1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
-3	ja	ja	PRON	Pp1-sn	Case=Nom|Number=Sing|Person=1|PronType=Prs	4	nsubj	_	NER=O
-4	tulumaril	tulumariti	VERB	Vmp-sm	Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	0	root	_	NER=O
-5	jucer	jučer	ADV	Rgp	Degree=Pos	4	advmod	_	NER=O
-6	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
-7	ljubljani	Ljubljana	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
-8	...	.	PUNCT	Z	_	4	punct	_	NER=O
-
-```
-
-## Serbian examples
-
-### Example of standard Serbian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('sr') # run classla.download('sr') beforehand if necessary
->>> doc = nlp("Slobodan Jovanović rođen je u Novom Sadu.")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = Slobodan Jovanović rođen je u Novom Sadu.
-1	Slobodan	Slobodan	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj	_	NER=B-PER
-2	Jovanović	Jovanović	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
-3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
-4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux	_	NER=O
-5	u	u	ADP	Sl	Case=Loc	6	case	_	NER=O
-6	Novom	nov	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	3	obl	_	NER=B-LOC
-7	Sadu	Sad	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	6	flat	_	NER=I-LOC|SpaceAfter=No
-8	.	.	PUNCT	Z	_	3	punct	_	NER=O
-
-```
-
-### Example of non-standard Serbian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('sr', type='nonstandard') # run classla.download('sr', type='nonstandard') beforehand if necessary
->>> doc = nlp("ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu...")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu...
-1	ne	ne	PART	Qz	Polarity=Neg	2	advmod	_	NER=O
-2	mogu	moći	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	root	_	NER=O
-3	da	da	SCONJ	Cs	_	4	mark	_	NER=O
-4	verujem	verovati	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	2	xcomp	_	NER=O
-5	kakvo	kakav	DET	Pi-nsn	Case=Nom|Gender=Neut|Number=Sing|PronType=Int,Rel	ccomp	_	NER=O
-6	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	aux	_	NER=O
-7	zezanje	zezanje	NOUN	Ncnsn	Case=Nom|Gender=Neut|Number=Sing	5	nsubj	NER=O
-8	bilo	biti	AUX	Vap-sn	Gender=Neut|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	5	cop	_	NER=O
-9	prosle	prošli	ADJ	Agpfsgy	Case=Gen|Definite=Def|Degree=Pos|Gender=Fem|Number=Sing	10	amod	_	NER=O
-10	godine	godina	NOUN	Ncfsg	Case=Gen|Gender=Fem|Number=Sing	8	obl	_	NER=O
-11	u	u	ADP	Sl	Case=Loc	12	case	_	NER=O
-12	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	8	obl	NER=B-LOC|SpaceAfter=No
-13	...	.	PUNCT	Z	_	2	punct	_	NER=O
-
-```
-
-## Bulgarian examples
-
-### Example of standard Bulgarian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('bg') # run classla.download('bg') beforehand if necessary
->>> doc = nlp("Алеко Константинов е роден в Свищов.")
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = Алеко Константинов е роден в Свищов.
-1	Алеко	алеко	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	nsubj:pass	_	NER=B-PER
-2	Константинов	константинов	PROPN	Hmsi	Definite=Ind|Gender=Masc|Number=Sing	flat	_	NER=I-PER
-3	е	съм	AUX	Vxitf-r3s	Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act	4	aux:pass	_	NER=O
-4	роден	родя-(се)	VERB	Vpptcv--smi	Aspect=Perf|Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
-5	в	в	ADP	R	_	6	case	_	NER=O
-6	Свищов	свищов	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	iobj	NER=B-LOC|SpaceAfter=No
-7	.	.	PUNCT	punct	_	4	punct	_	NER=O
-
-```
-
-## Macedonian examples
-
-### Example of standard Macedonian
-
-```
->>> import classla
->>> nlp = classla.Pipeline('mk') # run classla.download('mk') beforehand if necessary
->>> doc = nlp('Крсте Петков Мисирков е роден во Постол.')
->>> print(doc.to_conll())
-# newpar id = 1
-# sent_id = 1.1
-# text = Крсте Петков Мисирков е роден во Постол.
-1	Крсте	крсте	ADJ	Afpms-n	Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
-2	Петков	петков	NOUN	Ncmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
-3	Мисирков	мисирков	NOUN	Ncmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
-4	е	сум	AUX	Vapip3s-n	Aspect=Prog|Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres	_	_
-5	роден	роден	ADJ	Ap-ms-n	Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part	_	_	_	_
-6	во	во	ADP	Sps	AdpType=Prep	_	_	_	_
-7	Постол	постол	NOUN	Ncmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	SpaceAfter=No
-8	.	.	PUNCT	Z	_	_	_	_	_
-
-```
-
-## Training instructions
-
-[Training instructions](https://github.com/clarinsi/classla-stanfordnlp/blob/master/README.train.md)
-
-## Superuser instructions
-
-[Superuser instructions](https://github.com/clarinsi/classla-stanfordnlp/blob/master/README.superuser.md)
+# A [CLASSLA](http://www.clarin.si/info/k-centre/) Fork of [Stanza](https://github.com/stanfordnlp/stanza) for Processing Slovenian, Croatian, Serbian, Macedonian and Bulgarian
+
+## Description
+
+This pipeline allows for processing of standard Slovenian, Croatian, Serbian and Bulgarian on the levels of
+
+- tokenization and sentence splitting
+- part-of-speech tagging
+- lemmatization
+- dependency parsing
+- named entity recognition
+
+It also allows for (alpha) processing of standard Macedonian on the levels of 
+
+- tokenization and sentence splitting
+- part-of-speech tagging
+- lemmatization
+
+Finally, it allows for processing of non-standard (Internet) Slovenian, Croatian and Serbian on the same levels as standard language (all models are tailored to non-standard language except for dependency parsing where the standard module is used).
+
+## Differences to Stanza
+
+The differences of this pipeline to the original Stanza pipeline are the following:
+
+- usage of language-specific rule-based tokenizers and sentence splitters, [obeliks](https://pypi.org/project/obeliks/) for standard Slovenian and [reldi-tokeniser](https://pypi.org/project/reldi-tokeniser/) for the remaining varieties and languages (Stanza uses inferior machine-learning-based tokenization and sentence splitting trained on UD data)
+- default pre-tagging and pre-lemmatization on the level of tokenizers for the following phenomena: punctuation, symbol, e-mail, URL, mention, hashtag, emoticon, emoji (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#usage-of-tagging-control-via-the-tokenizer))
+- optional control of the tagger for Slovenian via an inflectional lexicon on the levels of XPOS, UPOS, FEATS (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#usage-of-inflectional-lexicon))
+- closed class handling depending on the usage of the options described in the last two bullets, as documented [here](https://github.com/clarinsi/classla/blob/master/README.closed_classes.md)
+- usage of external inflectional lexicons for lookup lemmatization, seq2seq being used very infrequently on OOVs only (Stanza uses only UD training data for lookup lemmatization)
+- morphosyntactic tagging models based on larger quantities of training data than is available in UD (training data that are morphosyntactically tagged, but not UD-parsed)
+- lemmatization models based on larger quantities of training data than is available in UD (training data that are lemmatized, but not UD-parsed)
+- optional JOS-project-based parsing of Slovenian (usage documented [here](https://github.com/clarinsi/classla/blob/master/README.superuser.md#jos-dependency-parsing-system))
+- named entity recognition models for all languages except Macedonian (Stanza does not cover named entity recognition for any of the languages supported by classla)
+- Macedonian models (Macedonian is not available in UD yet)
+- non-standard models for Croatian, Slovenian, Serbian (there is no UD data for these varieties)
+
+The above modifications led to some important improvements in the tool’s performance in comparison to original Stanza. For standard Slovenian, for example, running the full classla pipeline increases sentence segmentation F1 scores to 99.52 (94.29% error reduction), lemmatization to 99.17 (68.8% error reduction), XPOS tagging  to 97.38 (46.75% error reduction), UPOS tagging to 98.69 (23.4% error reduction), and LAS to 92.05 (23.56% error reduction).  See official [Stanza performance](https://stanfordnlp.github.io/stanza/performance.html) (evaluated on different data splits) for comparison.
+
+## Installation
+### pip
+We recommend that you install CLASSLA via pip, the Python package manager. To install, run:
+```bash
+pip install classla
+```
+This will also resolve all dependencies.
+
+__NOTE TO EXISTING USERS__: Once you install this classla version, you will HAVE TO re-download the models. All previously downloaded models will not be used anymore. We suggest you delete the old models. Their default location is at `~/classla_resources`.
+
+## Running CLASSLA
+
+### Getting started
+
+To run the CLASSLA pipeline for the first time on processing standard Slovenian, follow these steps:
+
+```
+>>> import classla
+>>> classla.download('sl')                            # download standard models for Slovenian, use hr for Croatian, sr for Serbian, bg for Bulgarian, mk for Macedonian
+>>> nlp = classla.Pipeline('sl')                      # initialize the default Slovenian pipeline, use hr for Croatian, sr for Serbian, bg for Bulgarian, mk for Macedonian
+>>> doc = nlp("France Prešeren je rojen v Vrbi.")     # run the pipeline
+>>> print(doc.to_conll())                             # print the output in CoNLL-U format
+# newpar id = 1
+# sent_id = 1.1
+# text = France Prešeren je rojen v Vrbi.
+1	France	France	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	4	nsubj	_	NER=B-PER
+2	Prešeren	Prešeren	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat:name	_	NER=I-PER
+3	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	cop	_	NER=O
+4	rojen	rojen	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part	0	root	_	NER=O
+5	v	v	ADP	Sl	Case=Loc	6	case	_	NER=O
+6	Vrbi	Vrba	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
+7	.	.	PUNCT	Z	_	4	punct	_	NER=O
+
+```
+You can find examples of standard language processing for [Croatian](#example-of-standard-croatian), [Serbian](#example-of-standard-serbian), [Macedonian](#example-of-standard-macedonian) and [Bulgarian](#example-of-standard-bulgarian) at the end of this document.
+
+### Processing non-standard language
+
+Processing non-standard Slovenian differs to the above standard example just by an additional argument ```type="nonstandard"```:
+
+```
+>>> import classla
+>>> classla.download('sl', type='nonstandard')        # download non-standard models for Slovenian, use hr for Croatian and sr for Serbian
+>>> nlp = classla.Pipeline('sl', type='nonstandard')  # initialize the default non-standard Slovenian pipeline, use hr for Croatian and sr for Serbian
+>>> doc = nlp("kva smo mi zurali zadnje leto v zagrebu...")     # run the pipeline
+>>> print(doc.to_conll())                             # print the output in CoNLL-U format
+# newpar id = 1
+# sent_id = 1.1
+# text = kva smo mi zurali zadnje leto v zagrebu...
+1	kva	kaj	PRON	Pq-nsa	Case=Acc|Gender=Neut|Number=Sing|PronType=Int	4	obj	_	NER=O
+2	smo	biti	AUX	Va-r1p-n	Mood=Ind|Number=Plur|Person=1|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
+3	mi	jaz	PRON	Pp1mpn	Case=Nom|Gender=Masc|Number=Plur|Person=1|PronType=Prs	4	nsubj	_	NER=O
+4	zurali	zurati	VERB	Vmpp-pm	Aspect=Imp|Gender=Masc|Number=Plur|VerbForm=Part	0	root	_	NER=O
+5	zadnje	zadnji	ADJ	Agpnsa	Case=Acc|Degree=Pos|Gender=Neut|Number=Sing	6	amod	_	NER=O
+6	leto	leto	NOUN	Ncnsa	Case=Acc|Gender=Neut|Number=Sing	4	obl	_	NER=O
+7	v	v	ADP	Sl	Case=Loc	8	case	_	NER=O
+8	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
+9	...	...	PUNCT	Z	_	4	punct	_	NER=O
+
+```
+
+You can find examples of non-standard language processing for [Croatian](#example-of-non-standard-croatian) and [Serbian](#example-of-non-standard-serbian)  at the end of this document.
+
+For additional usage examples you can also consult the ```pipeline_demo.py``` file.
+
+### Processing online texts
+
+A special web processing mode for processing texts obtained from the internet can be activated with the ```type="web"``` argument:
+
+```
+>>> import classla
+>>> classla.download('sl', type='web')        # download web models for Slovenian, use hr for Croatian and sr for Serbian
+>>> nlp = classla.Pipeline('sl', type='web')  # initialize the default Slovenian web pipeline, use hr for Croatian and sr for Serbian
+>>> doc = nlp("Kdor hoce prenesti preko racunalnika http://t.co/LwWyzs0cA0")     # run the pipeline
+>>> print(doc.to_conll())                             # print the output in CoNLL-U format
+# newpar id = 1
+# sent_id = 1.1
+# text = Kdor hoce prenesti preko racunalnika http://t.co/LwWyzs0cA0
+1	Kdor	kdor	PRON	Pr-msn	Case=Nom|Gender=Masc|Number=Sing|PronType=Rel	2	nsubj	_	NER=O
+2	hoce	hoteti	VERB	Vmpr3s-n	Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	0	root	_	NER=O
+3	prenesti	prenesti	VERB	Vmen	Aspect=Perf|VerbForm=Inf	2	xcomp	_	NER=O
+4	preko	preko	ADP	Sg	Case=Gen	5	case	_	NER=O
+5	racunalnika	računalnik	NOUN	Ncmsg	Case=Gen|Gender=Masc|Number=Sing	3	obl	_	NER=O
+6	http://t.co/LwWyzs0cA0	http://t.co/LwWyzs0cA0	SYM	Xw	_	5	nmod	_	NER=O
+```
+
+## Processors
+
+The CLASSLA pipeline is built from multiple units. These units are called processors. By default CLASSLA runs the ```tokenize```, ```ner```, ```pos```, ```lemma``` and ```depparse``` processors.
+
+You can specify which processors CLASSLA should run, via the ```processors``` attribute as in the following example, performing tokenization, named entity recognition, part-of-speech tagging and lemmatization.
+
+```python
+>>> nlp = classla.Pipeline('sl', processors='tokenize,ner,pos,lemma')
+```
+
+Another popular option might be to perform tokenization, part-of-speech tagging, lemmatization and dependency parsing.
+
+```python
+>>> nlp = classla.Pipeline('sl', processors='tokenize,pos,lemma,depparse')
+```
+
+### Tokenization and sentence splitting
+
+The tokenization and sentence splitting processor ```tokenize``` is the first processor and is required for any further processing.
+
+In case you already have tokenized text, you should separate tokens via spaces and pass the attribute ```tokenize_pretokenized=True```.
+
+By default CLASSLA uses a rule-based tokenizer - [obeliks](https://github.com/clarinsi/obeliks) for Slovenian standard language pipeline. In other cases we use [reldi-tokeniser](https://github.com/clarinsi/reldi-tokeniser).
+
+<!--Most important attributes:
+```
+tokenize_pretokenized   - [boolean]     ignores tokenizer
+```-->
+
+### Part-of-speech tagging
+
+The POS tagging processor ```pos``` will general output that contains morphosyntactic description following the [MULTEXT-East standard](http://nl.ijs.si/ME/V6/msd/html/msd.lang-specific.html) and universal part-of-speech tags and universal features following the [Universal Dependencies standard](https://universaldependencies.org). This processing requires the usage of the ```tokenize``` processor.
+
+<!--Most important attributes:
+```
+pos_model_path          - [str]         alternative path to model file
+pos_pretrain_path       - [str]         alternative path to pretrain file
+```-->
+
+### Lemmatization
+
+The lemmatization processor ```lemma``` will produce lemmas (basic forms) for each token in the input. It requires the usage of both the ```tokenize``` and ```pos``` processors.
+
+### Dependency parsing
+
+The dependency parsing processor ```depparse``` performs syntactic dependency parsing of sentences following the [Universal Dependencies formalism](https://universaldependencies.org/introduction.html#:~:text=Universal%20Dependencies%20(UD)%20is%20a,from%20a%20language%20typology%20perspective.). It requires the ```tokenize``` and ```pos``` processors.
+
+### Named entity recognition
+
+The named entity recognition processor ```ner``` identifies named entities in text following the [IOB2](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)) format. It requires only the ```tokenize``` processor.
+
+## Citing
+
+If you use this tool, please cite the following paper:
+
+```
+@inproceedings{ljubesic-dobrovoljc-2019-neural,
+    title = "What does Neural Bring? Analysing Improvements in Morphosyntactic Annotation and Lemmatisation of {S}lovenian, {C}roatian and {S}erbian",
+    author = "Ljube{\v{s}}i{\'c}, Nikola  and
+      Dobrovoljc, Kaja",
+    booktitle = "Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing",
+    month = aug,
+    year = "2019",
+    address = "Florence, Italy",
+    publisher = "Association for Computational Linguistics",
+    url = "https://www.aclweb.org/anthology/W19-3704",
+    doi = "10.18653/v1/W19-3704",
+    pages = "29--34"
+    }
+```
+
+## Croatian examples
+
+### Example of standard Croatian 
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('hr') # run classla.download('hr') beforehand if necessary
+>>> doc = nlp("Ante Starčević rođen je u Velikom Žitniku.")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = Ante Starčević rođen je u Velikom Žitniku.
+1	Ante	Ante	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj	_	NER=B-PER
+2	Starčević	Starčević	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
+3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
+4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux	_	NER=O
+5	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
+6	Velikom	velik	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	7	amod	_	NER=B-LOC
+7	Žitniku	Žitnik	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	3	obl	_	NER=I-LOC|SpaceAfter=No
+8	.	.	PUNCT	Z	_	3	punct	_	NER=O
+
+```
+### Example of non-standard Croatian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('hr', type='nonstandard') # run classla.download('hr', type='nonstandard') beforehand if necessary
+>>> doc = nlp("kaj sam ja tulumaril jucer u ljubljani...")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = kaj sam ja tulumaril jucer u ljubljani...
+1	kaj	što	PRON	Pq3n-a	Case=Acc|Gender=Neut|PronType=Int,Rel	4	obj	_	NER=O
+2	sam	biti	AUX	Var1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
+3	ja	ja	PRON	Pp1-sn	Case=Nom|Number=Sing|Person=1|PronType=Prs	4	nsubj	_	NER=O
+4	tulumaril	tulumariti	VERB	Vmp-sm	Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	0	root	_	NER=O
+5	jucer	jučer	ADV	Rgp	Degree=Pos	4	advmod	_	NER=O
+6	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
+7	ljubljani	Ljubljana	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
+8	...	...	PUNCT	Z	_	4	punct	_	NER=O
+
+```
+
+## Serbian examples
+
+### Example of standard Serbian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('sr') # run classla.download('sr') beforehand if necessary
+>>> doc = nlp("Slobodan Jovanović rođen je u Novom Sadu.")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = Slobodan Jovanović rođen je u Novom Sadu.
+1	Slobodan	Slobodan	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj	_	NER=B-PER
+2	Jovanović	Jovanović	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
+3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
+4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux	_	NER=O
+5	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
+6	Novom	nov	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	7	amod	_	NER=B-LOC
+7	Sadu	Sad	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	3	obl	_	NER=I-LOC|SpaceAfter=No
+8	.	.	PUNCT	Z	_	3	punct	_	NER=O
+
+```
+
+### Example of non-standard Serbian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('sr', type='nonstandard') # run classla.download('sr', type='nonstandard') beforehand if necessary
+>>> doc = nlp("ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu...")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu...
+1	ne	ne	PART	Qz	Polarity=Neg	2	advmod	_	NER=O
+2	mogu	moći	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	0	root	_	NER=O
+3	da	da	SCONJ	Cs	_	4	mark	_	NER=O
+4	verujem	verovati	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	2	xcomp	_	NER=O
+5	kakvo	kakav	DET	Pi-nsn	Case=Nom|Gender=Neut|Number=Sing|PronType=Int,Rel	4	ccomp	_	NER=O
+6	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	5	aux	_	NER=O
+7	zezanje	zezanje	NOUN	Ncnsn	Case=Nom|Gender=Neut|Number=Sing	8	nsubj	_	NER=O
+8	bilo	biti	AUX	Vap-sn	Gender=Neut|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	5	cop	_	NER=O
+9	prosle	prošli	ADJ	Agpfsgy	Case=Gen|Definite=Def|Degree=Pos|Gender=Fem|Number=Sing	10	amod	_	NER=O
+10	godine	godina	NOUN	Ncfsg	Case=Gen|Gender=Fem|Number=Sing	8	obl	_	NER=O
+11	u	u	ADP	Sl	Case=Loc	12	case	_	NER=O
+12	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	8	obl	_	NER=B-LOC|SpaceAfter=No
+13	...	...	PUNCT	Z	_	2	punct	_	NER=O
+
+```
+
+## Bulgarian examples
+
+### Example of standard Bulgarian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('bg') # run classla.download('bg') beforehand if necessary
+>>> doc = nlp("Алеко Константинов е роден в Свищов.")
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = Алеко Константинов е роден в Свищов.
+1	Алеко	алеко	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	nsubj:pass	_	NER=B-PER
+2	Константинов	константинов	PROPN	Hmsi	Definite=Ind|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
+3	е	съм	AUX	Vxitf-r3s	Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act	4	aux:pass	_	NER=O
+4	роден	родя-(се)	VERB	Vpptcv--smi	Aspect=Perf|Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
+5	в	в	ADP	R	_	6	case	_	NER=O
+6	Свищов	свищов	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	iobj	_	NER=B-LOC|SpaceAfter=No
+7	.	.	PUNCT	punct	_	4	punct	_	NER=O
+
+```
+
+## Macedonian examples
+
+### Example of standard Macedonian
+
+```
+>>> import classla
+>>> nlp = classla.Pipeline('mk') # run classla.download('mk') beforehand if necessary
+>>> doc = nlp('Крсте Петков Мисирков е роден во Постол.')
+>>> print(doc.to_conll())
+# newpar id = 1
+# sent_id = 1.1
+# text = Крсте Петков Мисирков е роден во Постол.
+1	Крсте	Крсте	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
+2	Петков	Петков	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
+3	Мисирков	Мисирков	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
+4	е	сум	AUX	Vapip3s-n	Aspect=Prog|Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres	_	_	_	_
+5	роден	роден	ADJ	Ap-ms-n	Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part	_	_	_	_
+6	во	во	ADP	Sps	AdpType=Prep	_	_	_	_
+7	Постол	Постол	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	SpaceAfter=No
+8	.	.	PUNCT	Z	_	_	_	_	_
+
+```
+
+## Training instructions
+
+[Training instructions](https://github.com/clarinsi/classla-stanfordnlp/blob/master/README.train.md)
+
+## Superuser instructions
+
+[Superuser instructions](https://github.com/clarinsi/classla-stanfordnlp/blob/master/README.superuser.md)
```

### Comparing `classla-2.0/classla.egg-info/SOURCES.txt` & `classla-2.1/classla.egg-info/SOURCES.txt`

 * *Files identical despite different names*

### Comparing `classla-2.0/tests/__init__.py` & `classla-2.1/tests_classla/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,120 +1,119 @@
-"""
-Utilities for testing
-"""
-
-import os
-import re
-from pathlib import Path
-
-# Environment Variables
-# set this to specify working directory of tests
-TEST_HOME_VAR = 'STANZA_TEST_HOME'
-
-# Global Variables
-# test working directory base name must be stanza_test
-TEST_DIR_BASE_NAME = 'stanza_test'
-
-# check the working dir is set and compliant
-assert os.getenv(TEST_HOME_VAR) is not None, \
-    f'Please set {TEST_HOME_VAR} environment variable for test working dir, base name must be: {TEST_DIR_BASE_NAME}'
-TEST_WORKING_DIR = os.getenv(TEST_HOME_VAR)
-assert os.path.basename(TEST_WORKING_DIR) == TEST_DIR_BASE_NAME, \
-    f'Base name of test home dir must be: {TEST_DIR_BASE_NAME}'
-
-HOME_DIR = str(Path.home())
-
-TEST_MODELS_DIR = os.path.join(HOME_DIR, f'{TEST_WORKING_DIR}/models')
-
-DEFAULT_MODEL_DIR = os.getenv(
-    'CLASSLA_RESOURCES_DIR',
-    os.path.join(HOME_DIR, 'classla_resources')
-)
-
-# server resources
-SERVER_TEST_PROPS = f'{TEST_WORKING_DIR}/scripts/external_server.properties'
-
-# langauge resources
-LANGUAGE_RESOURCES = {}
-
-TOKENIZE_MODEL = 'tokenizer.pt'
-MWT_MODEL = 'mwt_expander.pt'
-POS_MODEL = 'tagger.pt'
-POS_PRETRAIN = 'pretrain.pt'
-LEMMA_MODEL = 'lemmatizer.pt'
-DEPPARSE_MODEL = 'parser.pt'
-DEPPARSE_PRETRAIN = 'pretrain.pt'
-
-MODEL_FILES = [TOKENIZE_MODEL, MWT_MODEL, POS_MODEL, POS_PRETRAIN, LEMMA_MODEL, DEPPARSE_MODEL, DEPPARSE_PRETRAIN]
-
-# English resources
-EN_KEY = 'en'
-EN_SHORTHAND = 'en_ewt'
-# models
-EN_MODELS_DIR = f'{TEST_WORKING_DIR}/models/{EN_SHORTHAND}_models'
-EN_MODEL_FILES = [f'{EN_MODELS_DIR}/{EN_SHORTHAND}_{model_fname}' for model_fname in MODEL_FILES]
-
-# French resources
-FR_KEY = 'fr'
-FR_SHORTHAND = 'fr_gsd'
-# regression file paths
-FR_TEST_IN = f'{TEST_WORKING_DIR}/in/fr_gsd.test.txt'
-FR_TEST_OUT = f'{TEST_WORKING_DIR}/out/fr_gsd.test.txt.out'
-FR_TEST_GOLD_OUT = f'{TEST_WORKING_DIR}/out/fr_gsd.test.txt.out.gold'
-# models
-FR_MODELS_DIR = f'{TEST_WORKING_DIR}/models/{FR_SHORTHAND}_models'
-FR_MODEL_FILES = [f'{FR_MODELS_DIR}/{FR_SHORTHAND}_{model_fname}' for model_fname in MODEL_FILES]
-
-# Other language resources
-AR_SHORTHAND = 'ar_padt'
-DE_SHORTHAND = 'de_gsd'
-KK_SHORTHAND = 'kk_ktb'
-KO_SHORTHAND = 'ko_gsd'
-
-
-# utils for clean up
-# only allow removal of dirs/files in this approved list
-REMOVABLE_PATHS = ['en_ewt_models', 'en_ewt_tokenizer.pt', 'en_ewt_mwt_expander.pt', 'en_ewt_tagger.pt',
-                   'en_ewt.pretrain.pt', 'en_ewt_lemmatizer.pt', 'en_ewt_parser.pt', 'fr_gsd_models',
-                   'fr_gsd_tokenizer.pt', 'fr_gsd_mwt_expander.pt', 'fr_gsd_tagger.pt', 'fr_gsd.pretrain.pt',
-                   'fr_gsd_lemmatizer.pt', 'fr_gsd_parser.pt', 'ar_padt_models', 'ar_padt_tokenizer.pt',
-                   'ar_padt_mwt_expander.pt', 'ar_padt_tagger.pt', 'ar_padt.pretrain.pt', 'ar_padt_lemmatizer.pt',
-                   'ar_padt_parser.pt', 'de_gsd_models', 'de_gsd_tokenizer.pt', 'de_gsd_mwt_expander.pt',
-                   'de_gsd_tagger.pt', 'de_gsd.pretrain.pt', 'de_gsd_lemmatizer.pt', 'de_gsd_parser.pt',
-                   'kk_ktb_models', 'kk_ktb_tokenizer.pt', 'kk_ktb_mwt_expander.pt', 'kk_ktb_tagger.pt',
-                   'kk_ktb.pretrain.pt', 'kk_ktb_lemmatizer.pt', 'kk_ktb_parser.pt', 'ko_gsd_models',
-                   'ko_gsd_tokenizer.pt', 'ko_gsd_mwt_expander.pt', 'ko_gsd_tagger.pt', 'ko_gsd.pretrain.pt',
-                   'ko_gsd_lemmatizer.pt', 'ko_gsd_parser.pt']
-
-
-def safe_rm(path_to_rm):
-    """
-    Safely remove a directory of files or a file
-    1.) check path exists, files are files, dirs are dirs
-    2.) only remove things on approved list REMOVABLE_PATHS
-    3.) assert no longer exists
-    """
-    # just return if path doesn't exist
-    if not os.path.exists(path_to_rm):
-        return
-    # handle directory
-    if os.path.isdir(path_to_rm):
-        files_to_rm = [f'{path_to_rm}/{fname}' for fname in os.listdir(path_to_rm)]
-        dir_to_rm = path_to_rm
-    else:
-        files_to_rm = [path_to_rm]
-        dir_to_rm = None
-    # clear out files
-    for file_to_rm in files_to_rm:
-        if os.path.isfile(file_to_rm) and os.path.basename(file_to_rm) in REMOVABLE_PATHS:
-            os.remove(file_to_rm)
-            assert not os.path.exists(file_to_rm), f'Error removing: {file_to_rm}'
-    # clear out directory
-    if dir_to_rm is not None and os.path.isdir(dir_to_rm):
-        os.rmdir(dir_to_rm)
-        assert not os.path.exists(dir_to_rm), f'Error removing: {dir_to_rm}'
-
-def compare_ignoring_whitespace(predicted, expected):
-    predicted = re.sub('[ \t]+', ' ', predicted.strip())
-    expected = re.sub('[ \t]+', ' ', expected.strip())
-    assert predicted == expected
-
+"""
+Utilities for testing
+"""
+
+import os
+import re
+
+os.environ['CLASSLA_TEST_HOME'] = 'classla_test'
+
+# Environment Variables
+# set this to specify working directory of tests
+TEST_HOME_VAR = 'CLASSLA_TEST_HOME'
+
+# Global Variables
+# test working directory base name must be classla_test
+TEST_DIR_BASE_NAME = 'classla_test'
+
+# check the working dir is set and compliant
+assert os.getenv(TEST_HOME_VAR) is not None, \
+    f'Please set {TEST_HOME_VAR} environment variable for test working dir, base name must be: {TEST_DIR_BASE_NAME}'
+TEST_WORKING_DIR = os.getenv(TEST_HOME_VAR)
+assert os.path.basename(TEST_WORKING_DIR) == TEST_DIR_BASE_NAME, \
+    f'Base name of test home dir must be: {TEST_DIR_BASE_NAME}'
+
+TEST_MODELS_DIR = f'{TEST_WORKING_DIR}/models'
+
+# server resources
+SERVER_TEST_PROPS = f'{TEST_WORKING_DIR}/scripts/external_server.properties'
+
+# langauge resources
+LANGUAGE_RESOURCES = {}
+
+TOKENIZE_MODEL = 'tokenizer.pt'
+MWT_MODEL = 'mwt_expander.pt'
+POS_MODEL = 'tagger.pt'
+POS_PRETRAIN = 'pretrain.pt'
+LEMMA_MODEL = 'lemmatizer.pt'
+DEPPARSE_MODEL = 'parser.pt'
+DEPPARSE_PRETRAIN = 'pretrain.pt'
+
+MODEL_FILES = [TOKENIZE_MODEL, MWT_MODEL, POS_MODEL, POS_PRETRAIN, LEMMA_MODEL, DEPPARSE_MODEL, DEPPARSE_PRETRAIN]
+
+# English resources
+EN_KEY = 'en'
+EN_SHORTHAND = 'en_ewt'
+# models
+EN_MODELS_DIR = f'{TEST_WORKING_DIR}/models/{EN_SHORTHAND}_models'
+EN_MODEL_FILES = [f'{EN_MODELS_DIR}/{EN_SHORTHAND}_{model_fname}' for model_fname in MODEL_FILES]
+
+# French resources
+FR_KEY = 'fr'
+FR_SHORTHAND = 'fr_gsd'
+# regression file paths
+FR_TEST_IN = f'{TEST_WORKING_DIR}/in/fr_gsd.test.txt'
+FR_TEST_OUT = f'{TEST_WORKING_DIR}/out/fr_gsd.test.txt.out'
+FR_TEST_GOLD_OUT = f'{TEST_WORKING_DIR}/out/fr_gsd.test.txt.out.gold'
+# models
+FR_MODELS_DIR = f'{TEST_WORKING_DIR}/models/{FR_SHORTHAND}_models'
+FR_MODEL_FILES = [f'{FR_MODELS_DIR}/{FR_SHORTHAND}_{model_fname}' for model_fname in MODEL_FILES]
+
+# Other language resources
+AR_SHORTHAND = 'ar_padt'
+DE_SHORTHAND = 'de_gsd'
+KK_SHORTHAND = 'kk_ktb'
+KO_SHORTHAND = 'ko_gsd'
+
+
+# Other language resources
+SL_SHORTHAND = 'ssj'
+SL_JOS_SHORTHAND = 'ssj_jos'
+SL_NS_SHORTHAND = 'nonstandard'
+
+# utils for clean up
+# only allow removal of dirs/files in this approved list
+REMOVABLE_PATHS = ['en_ewt_models', 'en_ewt_tokenizer.pt', 'en_ewt_mwt_expander.pt', 'en_ewt_tagger.pt',
+                   'en_ewt.pretrain.pt', 'en_ewt_lemmatizer.pt', 'en_ewt_parser.pt', 'fr_gsd_models',
+                   'fr_gsd_tokenizer.pt', 'fr_gsd_mwt_expander.pt', 'fr_gsd_tagger.pt', 'fr_gsd.pretrain.pt',
+                   'fr_gsd_lemmatizer.pt', 'fr_gsd_parser.pt', 'ar_padt_models', 'ar_padt_tokenizer.pt',
+                   'ar_padt_mwt_expander.pt', 'ar_padt_tagger.pt', 'ar_padt.pretrain.pt', 'ar_padt_lemmatizer.pt',
+                   'ar_padt_parser.pt', 'de_gsd_models', 'de_gsd_tokenizer.pt', 'de_gsd_mwt_expander.pt',
+                   'de_gsd_tagger.pt', 'de_gsd.pretrain.pt', 'de_gsd_lemmatizer.pt', 'de_gsd_parser.pt',
+                   'kk_ktb_models', 'kk_ktb_tokenizer.pt', 'kk_ktb_mwt_expander.pt', 'kk_ktb_tagger.pt',
+                   'kk_ktb.pretrain.pt', 'kk_ktb_lemmatizer.pt', 'kk_ktb_parser.pt', 'ko_gsd_models',
+                   'ko_gsd_tokenizer.pt', 'ko_gsd_mwt_expander.pt', 'ko_gsd_tagger.pt', 'ko_gsd.pretrain.pt',
+                   'ko_gsd_lemmatizer.pt', 'ko_gsd_parser.pt']
+
+
+def safe_rm(path_to_rm):
+    """
+    Safely remove a directory of files or a file
+    1.) check path exists, files are files, dirs are dirs
+    2.) only remove things on approved list REMOVABLE_PATHS
+    3.) assert no longer exists
+    """
+    # just return if path doesn't exist
+    if not os.path.exists(path_to_rm):
+        return
+    # handle directory
+    if os.path.isdir(path_to_rm):
+        files_to_rm = [f'{path_to_rm}/{fname}' for fname in os.listdir(path_to_rm)]
+        dir_to_rm = path_to_rm
+    else:
+        files_to_rm = [path_to_rm]
+        dir_to_rm = None
+    # clear out files
+    for file_to_rm in files_to_rm:
+        if os.path.isfile(file_to_rm) and os.path.basename(file_to_rm) in REMOVABLE_PATHS:
+            os.remove(file_to_rm)
+            assert not os.path.exists(file_to_rm), f'Error removing: {file_to_rm}'
+    # clear out directory
+    if dir_to_rm is not None and os.path.isdir(dir_to_rm):
+        os.rmdir(dir_to_rm)
+        assert not os.path.exists(dir_to_rm), f'Error removing: {dir_to_rm}'
+
+def compare_ignoring_whitespace(predicted, expected):
+    predicted = re.sub('[ \t]+', ' ', predicted.strip())
+    expected = re.sub('[ \t]+', ' ', expected.strip())
+    assert predicted == expected
+
```

### Comparing `classla-2.0/tests/test_client.py` & `classla-2.1/tests/test_client.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,182 +1,182 @@
-"""
-Tests that call a running CoreNLPClient.
-"""
-
-import pytest
-import classla.server as corenlp
-import classla.server.client as client
-import shlex
-import subprocess
-import time
-
-from tests import *
-
-# set the marker for this module
-pytestmark = [pytest.mark.travis, pytest.mark.client]
-
-TEXT = "Chris wrote a simple sentence that he parsed with Stanford CoreNLP.\n"
-
-MAX_REQUEST_ATTEMPTS = 5
-
-EN_GOLD = """
-Sentence #1 (12 tokens):
-Chris wrote a simple sentence that he parsed with Stanford CoreNLP.
-
-Tokens:
-[Text=Chris CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=NNP]
-[Text=wrote CharacterOffsetBegin=6 CharacterOffsetEnd=11 PartOfSpeech=VBD]
-[Text=a CharacterOffsetBegin=12 CharacterOffsetEnd=13 PartOfSpeech=DT]
-[Text=simple CharacterOffsetBegin=14 CharacterOffsetEnd=20 PartOfSpeech=JJ]
-[Text=sentence CharacterOffsetBegin=21 CharacterOffsetEnd=29 PartOfSpeech=NN]
-[Text=that CharacterOffsetBegin=30 CharacterOffsetEnd=34 PartOfSpeech=WDT]
-[Text=he CharacterOffsetBegin=35 CharacterOffsetEnd=37 PartOfSpeech=PRP]
-[Text=parsed CharacterOffsetBegin=38 CharacterOffsetEnd=44 PartOfSpeech=VBD]
-[Text=with CharacterOffsetBegin=45 CharacterOffsetEnd=49 PartOfSpeech=IN]
-[Text=Stanford CharacterOffsetBegin=50 CharacterOffsetEnd=58 PartOfSpeech=NNP]
-[Text=CoreNLP CharacterOffsetBegin=59 CharacterOffsetEnd=66 PartOfSpeech=NNP]
-[Text=. CharacterOffsetBegin=66 CharacterOffsetEnd=67 PartOfSpeech=.]
-""".strip()
-
-
-@pytest.fixture(scope="module")
-def corenlp_client():
-    """ Client to run tests on """
-    client = corenlp.CoreNLPClient(annotators='tokenize,ssplit,pos,lemma,ner,depparse',
-                                   server_id='stanza_main_test_server')
-    yield client
-    client.stop()
-
-
-def test_connect(corenlp_client):
-    corenlp_client.ensure_alive()
-    assert corenlp_client.is_active
-    assert corenlp_client.is_alive()
-
-
-def test_context_manager():
-    with corenlp.CoreNLPClient(annotators="tokenize,ssplit",
-                               endpoint="http://localhost:9001") as context_client:
-        ann = context_client.annotate(TEXT)
-        assert corenlp.to_text(ann.sentence[0]) == TEXT[:-1]
-
-def test_no_duplicate_servers():
-    """We expect a second server on the same port to fail"""
-    with pytest.raises(corenlp.PermanentlyFailedException):
-        with corenlp.CoreNLPClient(annotators="tokenize,ssplit") as duplicate_server:
-            raise RuntimeError("This should have failed")
-
-def test_annotate(corenlp_client):
-    ann = corenlp_client.annotate(TEXT)
-    assert corenlp.to_text(ann.sentence[0]) == TEXT[:-1]
-
-
-def test_update(corenlp_client):
-    ann = corenlp_client.annotate(TEXT)
-    ann = corenlp_client.update(ann)
-    assert corenlp.to_text(ann.sentence[0]) == TEXT[:-1]
-
-
-def test_tokensregex(corenlp_client):
-    pattern = '([ner: PERSON]+) /wrote/ /an?/ []{0,3} /sentence|article/'
-    matches = corenlp_client.tokensregex(TEXT, pattern)
-    assert len(matches["sentences"]) == 1
-    assert matches["sentences"][0]["length"] == 1
-    assert matches == {
-        "sentences": [{
-            "0": {
-                "text": "Chris wrote a simple sentence",
-                "begin": 0,
-                "end": 5,
-                "1": {
-                    "text": "Chris",
-                    "begin": 0,
-                    "end": 1
-                }},
-            "length": 1
-        },]}
-
-
-def test_semgrex(corenlp_client):
-    pattern = '{word:wrote} >nsubj {}=subject >obj {}=object'
-    matches = corenlp_client.semgrex(TEXT, pattern, to_words=True)
-    assert matches == [
-        {
-            "text": "wrote",
-            "begin": 1,
-            "end": 2,
-            "$subject": {
-                "text": "Chris",
-                "begin": 0,
-                "end": 1
-            },
-            "$object": {
-                "text": "sentence",
-                "begin": 4,
-                "end": 5
-            },
-            "sentence": 0,}]
-
-
-def test_external_server_legacy_start_server():
-    """ Test starting up an external server and accessing with a client with start_server=False """
-    corenlp_home = client.resolve_classpath(None)
-    start_cmd = f'java -Xmx5g -cp "{corenlp_home}" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 ' \
-                f'-timeout 60000 -server_id stanza_external_server -serverProperties {SERVER_TEST_PROPS}'
-    start_cmd = start_cmd and shlex.split(start_cmd)
-    external_server_process = subprocess.Popen(start_cmd)
-    with corenlp.CoreNLPClient(start_server=False, endpoint="http://localhost:9001") as external_server_client:
-        ann = external_server_client.annotate(TEXT, annotators='tokenize,ssplit,pos', output_format='text')
-    assert external_server_process
-    external_server_process.terminate()
-    external_server_process.wait(5)
-    assert ann.strip() == EN_GOLD
-
-def test_external_server():
-    """ Test starting up an external server and accessing with a client with start_server=StartServer.DONT_START """
-    corenlp_home = os.getenv('CORENLP_HOME')
-    start_cmd = f'java -Xmx5g -cp "{corenlp_home}/*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 ' \
-                f'-timeout 60000 -server_id stanza_external_server -serverProperties {SERVER_TEST_PROPS}'
-    start_cmd = start_cmd and shlex.split(start_cmd)
-    external_server_process = subprocess.Popen(start_cmd)
-    with corenlp.CoreNLPClient(start_server=corenlp.StartServer.DONT_START, endpoint="http://localhost:9001") as external_server_client:
-        ann = external_server_client.annotate(TEXT, annotators='tokenize,ssplit,pos', output_format='text')
-    assert external_server_process
-    external_server_process.terminate()
-    external_server_process.wait(5)
-    assert ann.strip() == EN_GOLD
-
-def test_external_server_try_start_with_external():
-    """ Test starting up an external server and accessing with a client with start_server=StartServer.TRY_START """
-    corenlp_home = os.getenv('CORENLP_HOME')
-    start_cmd = f'java -Xmx5g -cp "{corenlp_home}/*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 ' \
-                f'-timeout 60000 -server_id stanza_external_server -serverProperties {SERVER_TEST_PROPS}'
-    start_cmd = start_cmd and shlex.split(start_cmd)
-    external_server_process = subprocess.Popen(start_cmd)
-    with corenlp.CoreNLPClient(start_server=corenlp.StartServer.TRY_START, endpoint="http://localhost:9001") as external_server_client:
-        ann = external_server_client.annotate(TEXT, annotators='tokenize,ssplit,pos', output_format='text')
-    assert external_server_process
-    external_server_process.terminate()
-    external_server_process.wait(5)
-    assert ann.strip() == EN_GOLD
-
-def test_external_server_try_start():
-    """ Test starting up a server with a client with start_server=StartServer.TRY_START """
-    corenlp_home = os.getenv('CORENLP_HOME')
-    with corenlp.CoreNLPClient(start_server=corenlp.StartServer.TRY_START, endpoint="http://localhost:9001") as external_server_client:
-        ann = external_server_client.annotate(TEXT, annotators='tokenize,ssplit,pos', output_format='text')
-    assert ann.strip() == EN_GOLD
-
-def test_external_server_force_start():
-    """ Test starting up an external server and accessing with a client with start_server=StartServer.FORCE_START """
-    corenlp_home = os.getenv('CORENLP_HOME')
-    start_cmd = f'java -Xmx5g -cp "{corenlp_home}/*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 ' \
-                f'-timeout 60000 -server_id stanza_external_server -serverProperties {SERVER_TEST_PROPS}'
-    start_cmd = start_cmd and shlex.split(start_cmd)
-    external_server_process = subprocess.Popen(start_cmd)
-    time.sleep(5) # wait and make sure the external CoreNLP server is up and running
-    with pytest.raises(corenlp.PermanentlyFailedException):
-        with corenlp.CoreNLPClient(start_server=corenlp.StartServer.FORCE_START, endpoint="http://localhost:9001") as external_server_client:
-            ann = external_server_client.annotate(TEXT, annotators='tokenize,ssplit,pos', output_format='text')
-    assert external_server_process
-    external_server_process.terminate()
-    external_server_process.wait(5)
+"""
+Tests that call a running CoreNLPClient.
+"""
+
+import pytest
+import classla.server as corenlp
+import classla.server.client as client
+import shlex
+import subprocess
+import time
+
+from tests import *
+
+# set the marker for this module
+pytestmark = [pytest.mark.travis, pytest.mark.client]
+
+TEXT = "Chris wrote a simple sentence that he parsed with Stanford CoreNLP.\n"
+
+MAX_REQUEST_ATTEMPTS = 5
+
+EN_GOLD = """
+Sentence #1 (12 tokens):
+Chris wrote a simple sentence that he parsed with Stanford CoreNLP.
+
+Tokens:
+[Text=Chris CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=NNP]
+[Text=wrote CharacterOffsetBegin=6 CharacterOffsetEnd=11 PartOfSpeech=VBD]
+[Text=a CharacterOffsetBegin=12 CharacterOffsetEnd=13 PartOfSpeech=DT]
+[Text=simple CharacterOffsetBegin=14 CharacterOffsetEnd=20 PartOfSpeech=JJ]
+[Text=sentence CharacterOffsetBegin=21 CharacterOffsetEnd=29 PartOfSpeech=NN]
+[Text=that CharacterOffsetBegin=30 CharacterOffsetEnd=34 PartOfSpeech=WDT]
+[Text=he CharacterOffsetBegin=35 CharacterOffsetEnd=37 PartOfSpeech=PRP]
+[Text=parsed CharacterOffsetBegin=38 CharacterOffsetEnd=44 PartOfSpeech=VBD]
+[Text=with CharacterOffsetBegin=45 CharacterOffsetEnd=49 PartOfSpeech=IN]
+[Text=Stanford CharacterOffsetBegin=50 CharacterOffsetEnd=58 PartOfSpeech=NNP]
+[Text=CoreNLP CharacterOffsetBegin=59 CharacterOffsetEnd=66 PartOfSpeech=NNP]
+[Text=. CharacterOffsetBegin=66 CharacterOffsetEnd=67 PartOfSpeech=.]
+""".strip()
+
+
+@pytest.fixture(scope="module")
+def corenlp_client():
+    """ Client to run tests on """
+    client = corenlp.CoreNLPClient(annotators='tokenize,ssplit,pos,lemma,ner,depparse',
+                                   server_id='stanza_main_test_server')
+    yield client
+    client.stop()
+
+
+def test_connect(corenlp_client):
+    corenlp_client.ensure_alive()
+    assert corenlp_client.is_active
+    assert corenlp_client.is_alive()
+
+
+def test_context_manager():
+    with corenlp.CoreNLPClient(annotators="tokenize,ssplit",
+                               endpoint="http://localhost:9001") as context_client:
+        ann = context_client.annotate(TEXT)
+        assert corenlp.to_text(ann.sentence[0]) == TEXT[:-1]
+
+def test_no_duplicate_servers():
+    """We expect a second server on the same port to fail"""
+    with pytest.raises(corenlp.PermanentlyFailedException):
+        with corenlp.CoreNLPClient(annotators="tokenize,ssplit") as duplicate_server:
+            raise RuntimeError("This should have failed")
+
+def test_annotate(corenlp_client):
+    ann = corenlp_client.annotate(TEXT)
+    assert corenlp.to_text(ann.sentence[0]) == TEXT[:-1]
+
+
+def test_update(corenlp_client):
+    ann = corenlp_client.annotate(TEXT)
+    ann = corenlp_client.update(ann)
+    assert corenlp.to_text(ann.sentence[0]) == TEXT[:-1]
+
+
+def test_tokensregex(corenlp_client):
+    pattern = '([ner: PERSON]+) /wrote/ /an?/ []{0,3} /sentence|article/'
+    matches = corenlp_client.tokensregex(TEXT, pattern)
+    assert len(matches["sentences"]) == 1
+    assert matches["sentences"][0]["length"] == 1
+    assert matches == {
+        "sentences": [{
+            "0": {
+                "text": "Chris wrote a simple sentence",
+                "begin": 0,
+                "end": 5,
+                "1": {
+                    "text": "Chris",
+                    "begin": 0,
+                    "end": 1
+                }},
+            "length": 1
+        },]}
+
+
+def test_semgrex(corenlp_client):
+    pattern = '{word:wrote} >nsubj {}=subject >obj {}=object'
+    matches = corenlp_client.semgrex(TEXT, pattern, to_words=True)
+    assert matches == [
+        {
+            "text": "wrote",
+            "begin": 1,
+            "end": 2,
+            "$subject": {
+                "text": "Chris",
+                "begin": 0,
+                "end": 1
+            },
+            "$object": {
+                "text": "sentence",
+                "begin": 4,
+                "end": 5
+            },
+            "sentence": 0,}]
+
+
+def test_external_server_legacy_start_server():
+    """ Test starting up an external server and accessing with a client with start_server=False """
+    corenlp_home = client.resolve_classpath(None)
+    start_cmd = f'java -Xmx5g -cp "{corenlp_home}" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 ' \
+                f'-timeout 60000 -server_id stanza_external_server -serverProperties {SERVER_TEST_PROPS}'
+    start_cmd = start_cmd and shlex.split(start_cmd)
+    external_server_process = subprocess.Popen(start_cmd)
+    with corenlp.CoreNLPClient(start_server=False, endpoint="http://localhost:9001") as external_server_client:
+        ann = external_server_client.annotate(TEXT, annotators='tokenize,ssplit,pos', output_format='text')
+    assert external_server_process
+    external_server_process.terminate()
+    external_server_process.wait(5)
+    assert ann.strip() == EN_GOLD
+
+def test_external_server():
+    """ Test starting up an external server and accessing with a client with start_server=StartServer.DONT_START """
+    corenlp_home = os.getenv('CORENLP_HOME')
+    start_cmd = f'java -Xmx5g -cp "{corenlp_home}/*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 ' \
+                f'-timeout 60000 -server_id stanza_external_server -serverProperties {SERVER_TEST_PROPS}'
+    start_cmd = start_cmd and shlex.split(start_cmd)
+    external_server_process = subprocess.Popen(start_cmd)
+    with corenlp.CoreNLPClient(start_server=corenlp.StartServer.DONT_START, endpoint="http://localhost:9001") as external_server_client:
+        ann = external_server_client.annotate(TEXT, annotators='tokenize,ssplit,pos', output_format='text')
+    assert external_server_process
+    external_server_process.terminate()
+    external_server_process.wait(5)
+    assert ann.strip() == EN_GOLD
+
+def test_external_server_try_start_with_external():
+    """ Test starting up an external server and accessing with a client with start_server=StartServer.TRY_START """
+    corenlp_home = os.getenv('CORENLP_HOME')
+    start_cmd = f'java -Xmx5g -cp "{corenlp_home}/*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 ' \
+                f'-timeout 60000 -server_id stanza_external_server -serverProperties {SERVER_TEST_PROPS}'
+    start_cmd = start_cmd and shlex.split(start_cmd)
+    external_server_process = subprocess.Popen(start_cmd)
+    with corenlp.CoreNLPClient(start_server=corenlp.StartServer.TRY_START, endpoint="http://localhost:9001") as external_server_client:
+        ann = external_server_client.annotate(TEXT, annotators='tokenize,ssplit,pos', output_format='text')
+    assert external_server_process
+    external_server_process.terminate()
+    external_server_process.wait(5)
+    assert ann.strip() == EN_GOLD
+
+def test_external_server_try_start():
+    """ Test starting up a server with a client with start_server=StartServer.TRY_START """
+    corenlp_home = os.getenv('CORENLP_HOME')
+    with corenlp.CoreNLPClient(start_server=corenlp.StartServer.TRY_START, endpoint="http://localhost:9001") as external_server_client:
+        ann = external_server_client.annotate(TEXT, annotators='tokenize,ssplit,pos', output_format='text')
+    assert ann.strip() == EN_GOLD
+
+def test_external_server_force_start():
+    """ Test starting up an external server and accessing with a client with start_server=StartServer.FORCE_START """
+    corenlp_home = os.getenv('CORENLP_HOME')
+    start_cmd = f'java -Xmx5g -cp "{corenlp_home}/*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 ' \
+                f'-timeout 60000 -server_id stanza_external_server -serverProperties {SERVER_TEST_PROPS}'
+    start_cmd = start_cmd and shlex.split(start_cmd)
+    external_server_process = subprocess.Popen(start_cmd)
+    time.sleep(5) # wait and make sure the external CoreNLP server is up and running
+    with pytest.raises(corenlp.PermanentlyFailedException):
+        with corenlp.CoreNLPClient(start_server=corenlp.StartServer.FORCE_START, endpoint="http://localhost:9001") as external_server_client:
+            ann = external_server_client.annotate(TEXT, annotators='tokenize,ssplit,pos', output_format='text')
+    assert external_server_process
+    external_server_process.terminate()
+    external_server_process.wait(5)
```

### Comparing `classla-2.0/tests/test_data_conversion.py` & `classla-2.1/tests/test_data_conversion.py`

 * *Ordering differences only*

 * *Files 6% similar despite different names*

```diff
@@ -1,35 +1,35 @@
-"""
-Basic tests of the data conversion
-"""
-import pytest
-
-import classla
-from classla.utils.conll import CoNLL
-from classla.models.common.doc import Document
-from tests import *
-
-pytestmark = pytest.mark.pipeline
-
-# data for testing
-CONLL = [[['1', 'Nous', 'il', 'PRON', '_', 'Number=Plur|Person=1|PronType=Prs', '3', 'nsubj', '_', 'start_char=0|end_char=4'], ['2', 'avons', 'avoir', 'AUX', '_', 'Mood=Ind|Number=Plur|Person=1|Tense=Pres|VerbForm=Fin', '3', 'aux:tense', '_', 'start_char=5|end_char=10'], ['3', 'atteint', 'atteindre', 'VERB', '_', 'Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part', '0', 'root', '_', 'start_char=11|end_char=18'], ['4', 'la', 'le', 'DET', '_', 'Definite=Def|Gender=Fem|Number=Sing|PronType=Art', '5', 'det', '_', 'start_char=19|end_char=21'], ['5', 'fin', 'fin', 'NOUN', '_', 'Gender=Fem|Number=Sing', '3', 'obj', '_', 'start_char=22|end_char=25'], ['6-7', 'du', '_', '_', '_', '_', '_', '_', '_', 'start_char=26|end_char=28'], ['6', 'de', 'de', 'ADP', '_', '_', '8', 'case', '_', '_'], ['7', 'le', 'le', 'DET', '_', 'Definite=Def|Gender=Masc|Number=Sing|PronType=Art', '8', 'det', '_', '_'], ['8', 'sentier', 'sentier', 'NOUN', '_', 'Gender=Masc|Number=Sing', '5', 'nmod', '_', 'start_char=29|end_char=36'], ['9', '.', '.', 'PUNCT', '_', '_', '3', 'punct', '_', 'start_char=36|end_char=37']]]
-DICT = [[{'id': (1,), 'text': 'Nous', 'lemma': 'il', 'upos': 'PRON', 'feats': 'Number=Plur|Person=1|PronType=Prs', 'head': 3, 'deprel': 'nsubj', 'misc': 'start_char=0|end_char=4'}, {'id': (2,), 'text': 'avons', 'lemma': 'avoir', 'upos': 'AUX', 'feats': 'Mood=Ind|Number=Plur|Person=1|Tense=Pres|VerbForm=Fin', 'head': 3, 'deprel': 'aux:tense', 'misc': 'start_char=5|end_char=10'}, {'id': (3,), 'text': 'atteint', 'lemma': 'atteindre', 'upos': 'VERB', 'feats': 'Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part', 'head': 0, 'deprel': 'root', 'misc': 'start_char=11|end_char=18'}, {'id': (4,), 'text': 'la', 'lemma': 'le', 'upos': 'DET', 'feats': 'Definite=Def|Gender=Fem|Number=Sing|PronType=Art', 'head': 5, 'deprel': 'det', 'misc': 'start_char=19|end_char=21'}, {'id': (5,), 'text': 'fin', 'lemma': 'fin', 'upos': 'NOUN', 'feats': 'Gender=Fem|Number=Sing', 'head': 3, 'deprel': 'obj', 'misc': 'start_char=22|end_char=25'}, {'id': (6, 7), 'text': 'du', 'misc': 'start_char=26|end_char=28'}, {'id': (6,), 'text': 'de', 'lemma': 'de', 'upos': 'ADP', 'head': 8, 'deprel': 'case'}, {'id': (7,), 'text': 'le', 'lemma': 'le', 'upos': 'DET', 'feats': 'Definite=Def|Gender=Masc|Number=Sing|PronType=Art', 'head': 8, 'deprel': 'det'}, {'id': (8,), 'text': 'sentier', 'lemma': 'sentier', 'upos': 'NOUN', 'feats': 'Gender=Masc|Number=Sing', 'head': 5, 'deprel': 'nmod', 'misc': 'start_char=29|end_char=36'}, {'id': (9,), 'text': '.', 'lemma': '.', 'upos': 'PUNCT', 'head': 3, 'deprel': 'punct', 'misc': 'start_char=36|end_char=37'}]] 
-
-def test_conll_to_dict():
-    dicts = CoNLL.convert_conll(CONLL)
-    assert dicts == DICT
-
-def test_dict_to_conll():
-    conll = CoNLL.convert_dict(DICT)
-    assert conll == CONLL
-
-def test_dict_to_doc_and_doc_to_dict():
-    doc = Document(DICT)
-    dicts = doc.to_dict()
-    dicts_tupleid = []
-    for sentence in dicts:
-        items = []
-        for item in sentence:
-            item['id'] = item['id'] if isinstance(item['id'], tuple) else (item['id'], )
-            items.append(item)
-        dicts_tupleid.append(items)
-    assert dicts_tupleid == DICT
+"""
+Basic tests of the data conversion
+"""
+import pytest
+
+import classla
+from classla.utils.conll import CoNLL
+from classla.models.common.doc import Document
+from tests import *
+
+pytestmark = pytest.mark.pipeline
+
+# data for testing
+CONLL = [[['1', 'Nous', 'il', 'PRON', '_', 'Number=Plur|Person=1|PronType=Prs', '3', 'nsubj', '_', 'start_char=0|end_char=4'], ['2', 'avons', 'avoir', 'AUX', '_', 'Mood=Ind|Number=Plur|Person=1|Tense=Pres|VerbForm=Fin', '3', 'aux:tense', '_', 'start_char=5|end_char=10'], ['3', 'atteint', 'atteindre', 'VERB', '_', 'Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part', '0', 'root', '_', 'start_char=11|end_char=18'], ['4', 'la', 'le', 'DET', '_', 'Definite=Def|Gender=Fem|Number=Sing|PronType=Art', '5', 'det', '_', 'start_char=19|end_char=21'], ['5', 'fin', 'fin', 'NOUN', '_', 'Gender=Fem|Number=Sing', '3', 'obj', '_', 'start_char=22|end_char=25'], ['6-7', 'du', '_', '_', '_', '_', '_', '_', '_', 'start_char=26|end_char=28'], ['6', 'de', 'de', 'ADP', '_', '_', '8', 'case', '_', '_'], ['7', 'le', 'le', 'DET', '_', 'Definite=Def|Gender=Masc|Number=Sing|PronType=Art', '8', 'det', '_', '_'], ['8', 'sentier', 'sentier', 'NOUN', '_', 'Gender=Masc|Number=Sing', '5', 'nmod', '_', 'start_char=29|end_char=36'], ['9', '.', '.', 'PUNCT', '_', '_', '3', 'punct', '_', 'start_char=36|end_char=37']]]
+DICT = [[{'id': (1,), 'text': 'Nous', 'lemma': 'il', 'upos': 'PRON', 'feats': 'Number=Plur|Person=1|PronType=Prs', 'head': 3, 'deprel': 'nsubj', 'misc': 'start_char=0|end_char=4'}, {'id': (2,), 'text': 'avons', 'lemma': 'avoir', 'upos': 'AUX', 'feats': 'Mood=Ind|Number=Plur|Person=1|Tense=Pres|VerbForm=Fin', 'head': 3, 'deprel': 'aux:tense', 'misc': 'start_char=5|end_char=10'}, {'id': (3,), 'text': 'atteint', 'lemma': 'atteindre', 'upos': 'VERB', 'feats': 'Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part', 'head': 0, 'deprel': 'root', 'misc': 'start_char=11|end_char=18'}, {'id': (4,), 'text': 'la', 'lemma': 'le', 'upos': 'DET', 'feats': 'Definite=Def|Gender=Fem|Number=Sing|PronType=Art', 'head': 5, 'deprel': 'det', 'misc': 'start_char=19|end_char=21'}, {'id': (5,), 'text': 'fin', 'lemma': 'fin', 'upos': 'NOUN', 'feats': 'Gender=Fem|Number=Sing', 'head': 3, 'deprel': 'obj', 'misc': 'start_char=22|end_char=25'}, {'id': (6, 7), 'text': 'du', 'misc': 'start_char=26|end_char=28'}, {'id': (6,), 'text': 'de', 'lemma': 'de', 'upos': 'ADP', 'head': 8, 'deprel': 'case'}, {'id': (7,), 'text': 'le', 'lemma': 'le', 'upos': 'DET', 'feats': 'Definite=Def|Gender=Masc|Number=Sing|PronType=Art', 'head': 8, 'deprel': 'det'}, {'id': (8,), 'text': 'sentier', 'lemma': 'sentier', 'upos': 'NOUN', 'feats': 'Gender=Masc|Number=Sing', 'head': 5, 'deprel': 'nmod', 'misc': 'start_char=29|end_char=36'}, {'id': (9,), 'text': '.', 'lemma': '.', 'upos': 'PUNCT', 'head': 3, 'deprel': 'punct', 'misc': 'start_char=36|end_char=37'}]] 
+
+def test_conll_to_dict():
+    dicts = CoNLL.convert_conll(CONLL)
+    assert dicts == DICT
+
+def test_dict_to_conll():
+    conll = CoNLL.convert_dict(DICT)
+    assert conll == CONLL
+
+def test_dict_to_doc_and_doc_to_dict():
+    doc = Document(DICT)
+    dicts = doc.to_dict()
+    dicts_tupleid = []
+    for sentence in dicts:
+        items = []
+        for item in sentence:
+            item['id'] = item['id'] if isinstance(item['id'], tuple) else (item['id'], )
+            items.append(item)
+        dicts_tupleid.append(items)
+    assert dicts_tupleid == DICT
```

### Comparing `classla-2.0/tests/test_decorators.py` & `classla-2.1/tests/test_decorators.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,126 +1,126 @@
-"""
-Basic tests of the depparse processor boolean flags
-"""
-import pytest
-
-import classla
-from classla.models.common.doc import Document
-from classla.pipeline.core import PipelineRequirementsException
-from classla.pipeline.processor import Processor, ProcessorVariant, register_processor, register_processor_variant, ProcessorRegisterException
-from classla.utils.conll import CoNLL
-from tests import *
-
-pytestmark = pytest.mark.pipeline
-
-# data for testing
-EN_DOC = "This is a test sentence. This is another!"
-
-EN_DOC_LOWERCASE_TOKENS = '''<Token id=1;words=[<Word id=1;text=this>]>
-<Token id=2;words=[<Word id=2;text=is>]>
-<Token id=3;words=[<Word id=3;text=a>]>
-<Token id=4;words=[<Word id=4;text=test>]>
-<Token id=5;words=[<Word id=5;text=sentence>]>
-<Token id=6;words=[<Word id=6;text=.>]>
-
-<Token id=1;words=[<Word id=1;text=this>]>
-<Token id=2;words=[<Word id=2;text=is>]>
-<Token id=3;words=[<Word id=3;text=another>]>
-<Token id=4;words=[<Word id=4;text=!>]>'''
-
-EN_DOC_LOL_TOKENS = '''<Token id=1;words=[<Word id=1;text=LOL>]>
-<Token id=2;words=[<Word id=2;text=LOL>]>
-<Token id=3;words=[<Word id=3;text=LOL>]>
-<Token id=4;words=[<Word id=4;text=LOL>]>
-<Token id=5;words=[<Word id=5;text=LOL>]>
-<Token id=6;words=[<Word id=6;text=LOL>]>
-<Token id=7;words=[<Word id=7;text=LOL>]>
-<Token id=8;words=[<Word id=8;text=LOL>]>'''
-
-EN_DOC_COOL_LEMMAS = '''<Token id=1;words=[<Word id=1;text=This;lemma=cool;upos=PRON;xpos=DT;feats=Number=Sing|PronType=Dem>]>
-<Token id=2;words=[<Word id=2;text=is;lemma=cool;upos=AUX;xpos=VBZ;feats=Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin>]>
-<Token id=3;words=[<Word id=3;text=a;lemma=cool;upos=DET;xpos=DT;feats=Definite=Ind|PronType=Art>]>
-<Token id=4;words=[<Word id=4;text=test;lemma=cool;upos=NOUN;xpos=NN;feats=Number=Sing>]>
-<Token id=5;words=[<Word id=5;text=sentence;lemma=cool;upos=NOUN;xpos=NN;feats=Number=Sing>]>
-<Token id=6;words=[<Word id=6;text=.;lemma=cool;upos=PUNCT;xpos=.>]>
-
-<Token id=1;words=[<Word id=1;text=This;lemma=cool;upos=PRON;xpos=DT;feats=Number=Sing|PronType=Dem>]>
-<Token id=2;words=[<Word id=2;text=is;lemma=cool;upos=AUX;xpos=VBZ;feats=Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin>]>
-<Token id=3;words=[<Word id=3;text=another;lemma=cool;upos=DET;xpos=DT>]>
-<Token id=4;words=[<Word id=4;text=!;lemma=cool;upos=PUNCT;xpos=.>]>'''
-
-@register_processor("lowercase")
-class LowercaseProcessor(Processor):
-    ''' Processor that lowercases all text '''
-    _requires = set(['tokenize'])
-    _provides = set(['lowercase'])
-
-    def __init__(self, config, pipeline, use_gpu):
-        pass
-
-    def _set_up_model(self, *args):
-        pass
-
-    def process(self, doc):
-        doc.text = doc.text.lower()
-        for sent in doc.sentences:
-            for tok in sent.tokens:
-                tok.text = tok.text.lower()
-
-            for word in sent.words:
-                word.text = word.text.lower()
-
-        return doc
-
-def test_register_processor():
-    nlp = classla.Pipeline(dir=TEST_MODELS_DIR, lang='en', processors='tokenize,lowercase')
-    doc = nlp(EN_DOC)
-    assert EN_DOC_LOWERCASE_TOKENS == '\n\n'.join(sent.tokens_string() for sent in doc.sentences)
-
-def test_register_nonprocessor():
-    with pytest.raises(ProcessorRegisterException):
-        @register_processor("nonprocessor")
-        class NonProcessor:
-            pass
-
-@register_processor_variant("tokenize", "lol")
-class LOLTokenizer(ProcessorVariant):
-    ''' An alternative tokenizer that splits text by space and replaces all tokens with LOL '''
-
-    def __init__(self, lang):
-        pass
-
-    def process(self, text):
-        sentence = [{'id': (i+1, ), 'text': 'LOL'} for i, tok in enumerate(text.split())]
-        return Document([sentence], text)
-
-def test_register_processor_variant():
-    nlp = classla.Pipeline(dir=TEST_MODELS_DIR, lang='en', processors={"tokenize": "lol"}, package=None)
-    doc = nlp(EN_DOC)
-    assert EN_DOC_LOL_TOKENS == '\n\n'.join(sent.tokens_string() for sent in doc.sentences)
-
-@register_processor_variant("lemma", "cool")
-class CoolLemmatizer(ProcessorVariant):
-    ''' An alternative lemmatizer that lemmatizes every word to "cool". '''
-
-    OVERRIDE = True
-
-    def __init__(self, lang):
-        pass
-
-    def process(self, document):
-        for sentence in document.sentences:
-            for word in sentence.words:
-                word.lemma = "cool"
-
-        return document
-
-def test_register_processor_variant_with_override():
-    nlp = classla.Pipeline(dir=TEST_MODELS_DIR, lang='en', processors={"tokenize": "ewt", "pos": "ewt", "lemma": "cool"}, package=None)
-    doc = nlp(EN_DOC)
-    assert EN_DOC_COOL_LEMMAS == '\n\n'.join(sent.tokens_string() for sent in doc.sentences)
-
-def test_register_nonprocessor_variant():
-    with pytest.raises(ProcessorRegisterException):
-        @register_processor_variant("tokenize", "nonvariant")
-        class NonVariant:
-            pass
+"""
+Basic tests of the depparse processor boolean flags
+"""
+import pytest
+
+import classla
+from classla.models.common.doc import Document
+from classla.pipeline.core import PipelineRequirementsException
+from classla.pipeline.processor import Processor, ProcessorVariant, register_processor, register_processor_variant, ProcessorRegisterException
+from classla.utils.conll import CoNLL
+from tests import *
+
+pytestmark = pytest.mark.pipeline
+
+# data for testing
+EN_DOC = "This is a test sentence. This is another!"
+
+EN_DOC_LOWERCASE_TOKENS = '''<Token id=1;words=[<Word id=1;text=this>]>
+<Token id=2;words=[<Word id=2;text=is>]>
+<Token id=3;words=[<Word id=3;text=a>]>
+<Token id=4;words=[<Word id=4;text=test>]>
+<Token id=5;words=[<Word id=5;text=sentence>]>
+<Token id=6;words=[<Word id=6;text=.>]>
+
+<Token id=1;words=[<Word id=1;text=this>]>
+<Token id=2;words=[<Word id=2;text=is>]>
+<Token id=3;words=[<Word id=3;text=another>]>
+<Token id=4;words=[<Word id=4;text=!>]>'''
+
+EN_DOC_LOL_TOKENS = '''<Token id=1;words=[<Word id=1;text=LOL>]>
+<Token id=2;words=[<Word id=2;text=LOL>]>
+<Token id=3;words=[<Word id=3;text=LOL>]>
+<Token id=4;words=[<Word id=4;text=LOL>]>
+<Token id=5;words=[<Word id=5;text=LOL>]>
+<Token id=6;words=[<Word id=6;text=LOL>]>
+<Token id=7;words=[<Word id=7;text=LOL>]>
+<Token id=8;words=[<Word id=8;text=LOL>]>'''
+
+EN_DOC_COOL_LEMMAS = '''<Token id=1;words=[<Word id=1;text=This;lemma=cool;upos=PRON;xpos=DT;feats=Number=Sing|PronType=Dem>]>
+<Token id=2;words=[<Word id=2;text=is;lemma=cool;upos=AUX;xpos=VBZ;feats=Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin>]>
+<Token id=3;words=[<Word id=3;text=a;lemma=cool;upos=DET;xpos=DT;feats=Definite=Ind|PronType=Art>]>
+<Token id=4;words=[<Word id=4;text=test;lemma=cool;upos=NOUN;xpos=NN;feats=Number=Sing>]>
+<Token id=5;words=[<Word id=5;text=sentence;lemma=cool;upos=NOUN;xpos=NN;feats=Number=Sing>]>
+<Token id=6;words=[<Word id=6;text=.;lemma=cool;upos=PUNCT;xpos=.>]>
+
+<Token id=1;words=[<Word id=1;text=This;lemma=cool;upos=PRON;xpos=DT;feats=Number=Sing|PronType=Dem>]>
+<Token id=2;words=[<Word id=2;text=is;lemma=cool;upos=AUX;xpos=VBZ;feats=Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin>]>
+<Token id=3;words=[<Word id=3;text=another;lemma=cool;upos=DET;xpos=DT>]>
+<Token id=4;words=[<Word id=4;text=!;lemma=cool;upos=PUNCT;xpos=.>]>'''
+
+@register_processor("lowercase")
+class LowercaseProcessor(Processor):
+    ''' Processor that lowercases all text '''
+    _requires = set(['tokenize'])
+    _provides = set(['lowercase'])
+
+    def __init__(self, config, pipeline, use_gpu):
+        pass
+
+    def _set_up_model(self, *args):
+        pass
+
+    def process(self, doc):
+        doc.text = doc.text.lower()
+        for sent in doc.sentences:
+            for tok in sent.tokens:
+                tok.text = tok.text.lower()
+
+            for word in sent.words:
+                word.text = word.text.lower()
+
+        return doc
+
+def test_register_processor():
+    nlp = classla.Pipeline(dir=TEST_MODELS_DIR, lang='en', processors='tokenize,lowercase')
+    doc = nlp(EN_DOC)
+    assert EN_DOC_LOWERCASE_TOKENS == '\n\n'.join(sent.tokens_string() for sent in doc.sentences)
+
+def test_register_nonprocessor():
+    with pytest.raises(ProcessorRegisterException):
+        @register_processor("nonprocessor")
+        class NonProcessor:
+            pass
+
+@register_processor_variant("tokenize", "lol")
+class LOLTokenizer(ProcessorVariant):
+    ''' An alternative tokenizer that splits text by space and replaces all tokens with LOL '''
+
+    def __init__(self, lang):
+        pass
+
+    def process(self, text):
+        sentence = [{'id': (i+1, ), 'text': 'LOL'} for i, tok in enumerate(text.split())]
+        return Document([sentence], text)
+
+def test_register_processor_variant():
+    nlp = classla.Pipeline(dir=TEST_MODELS_DIR, lang='en', processors={"tokenize": "lol"}, package=None)
+    doc = nlp(EN_DOC)
+    assert EN_DOC_LOL_TOKENS == '\n\n'.join(sent.tokens_string() for sent in doc.sentences)
+
+@register_processor_variant("lemma", "cool")
+class CoolLemmatizer(ProcessorVariant):
+    ''' An alternative lemmatizer that lemmatizes every word to "cool". '''
+
+    OVERRIDE = True
+
+    def __init__(self, lang):
+        pass
+
+    def process(self, document):
+        for sentence in document.sentences:
+            for word in sentence.words:
+                word.lemma = "cool"
+
+        return document
+
+def test_register_processor_variant_with_override():
+    nlp = classla.Pipeline(dir=TEST_MODELS_DIR, lang='en', processors={"tokenize": "ewt", "pos": "ewt", "lemma": "cool"}, package=None)
+    doc = nlp(EN_DOC)
+    assert EN_DOC_COOL_LEMMAS == '\n\n'.join(sent.tokens_string() for sent in doc.sentences)
+
+def test_register_nonprocessor_variant():
+    with pytest.raises(ProcessorRegisterException):
+        @register_processor_variant("tokenize", "nonvariant")
+        class NonVariant:
+            pass
```

### Comparing `classla-2.0/tests/test_depparse.py` & `classla-2.1/tests/test_depparse.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,85 +1,85 @@
-"""
-Basic tests of the depparse processor boolean flags
-"""
-import pytest
-
-import classla
-from classla.pipeline.core import PipelineRequirementsException
-from classla.utils.conll import CoNLL
-from tests import *
-
-pytestmark = pytest.mark.pipeline
-
-# data for testing
-EN_DOC = "Barack Obama was born in Hawaii.  He was elected president in 2008.  Obama attended Harvard."
-
-EN_DOC_CONLLU_PRETAGGED = """
-1	Barack	_	PROPN	NNP	Number=Sing	0	_	_	_
-2	Obama	_	PROPN	NNP	Number=Sing	1	_	_	_
-3	was	_	AUX	VBD	Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin	2	_	_	_
-4	born	_	VERB	VBN	Tense=Past|VerbForm=Part|Voice=Pass	3	_	_	_
-5	in	_	ADP	IN	_	4	_	_	_
-6	Hawaii	_	PROPN	NNP	Number=Sing	5	_	_	_
-7	.	_	PUNCT	.	_	6	_	_	_
-
-1	He	_	PRON	PRP	Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs	0	_	_	_
-2	was	_	AUX	VBD	Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin	1	_	_	_
-3	elected	_	VERB	VBN	Tense=Past|VerbForm=Part|Voice=Pass	2	_	_	_
-4	president	_	PROPN	NNP	Number=Sing	3	_	_	_
-5	in	_	ADP	IN	_	4	_	_	_
-6	2008	_	NUM	CD	NumType=Card	5	_	_	_
-7	.	_	PUNCT	.	_	6	_	_	_
-
-1	Obama	_	PROPN	NNP	Number=Sing	0	_	_	_
-2	attended	_	VERB	VBD	Mood=Ind|Tense=Past|VerbForm=Fin	1	_	_	_
-3	Harvard	_	PROPN	NNP	Number=Sing	2	_	_	_
-4	.	_	PUNCT	.	_	3	_	_	_
-
-
-""".lstrip()
-
-EN_DOC_DEPENDENCY_PARSES_GOLD = """
-('Barack', 4, 'nsubj:pass')
-('Obama', 1, 'flat')
-('was', 4, 'aux:pass')
-('born', 0, 'root')
-('in', 6, 'case')
-('Hawaii', 4, 'obl')
-('.', 4, 'punct')
-
-('He', 3, 'nsubj:pass')
-('was', 3, 'aux:pass')
-('elected', 0, 'root')
-('president', 3, 'xcomp')
-('in', 6, 'case')
-('2008', 3, 'obl')
-('.', 3, 'punct')
-
-('Obama', 2, 'nsubj')
-('attended', 0, 'root')
-('Harvard', 2, 'obj')
-('.', 2, 'punct')
-""".strip()
-
-
-def test_depparse():
-    nlp = classla.Pipeline(dir=TEST_MODELS_DIR, lang='en')
-    doc = nlp(EN_DOC)
-    assert EN_DOC_DEPENDENCY_PARSES_GOLD == '\n\n'.join([sent.dependencies_string() for sent in doc.sentences])
-
-
-def test_depparse_with_pretagged_doc():
-    nlp = classla.Pipeline(**{'processors': 'depparse', 'dir': TEST_MODELS_DIR, 'lang': 'en',
-                                  'depparse_pretagged': True})
-
-    doc, metasentences = CoNLL.conll2dict(input_file=EN_DOC_CONLLU_PRETAGGED)
-    doc = classla.Document(doc, metasentences=metasentences)
-    processed_doc = nlp(doc)
-
-    assert EN_DOC_DEPENDENCY_PARSES_GOLD == '\n\n'.join(
-        [sent.dependencies_string() for sent in processed_doc.sentences])
-
-
-def test_raises_requirements_exception_if_pretagged_not_passed():
-    with pytest.raises(PipelineRequirementsException):
-        classla.Pipeline(**{'processors': 'depparse', 'dir': TEST_MODELS_DIR, 'lang': 'en'})
+"""
+Basic tests of the depparse processor boolean flags
+"""
+import pytest
+
+import classla
+from classla.pipeline.core import PipelineRequirementsException
+from classla.utils.conll import CoNLL
+from tests import *
+
+pytestmark = pytest.mark.pipeline
+
+# data for testing
+EN_DOC = "Barack Obama was born in Hawaii.  He was elected president in 2008.  Obama attended Harvard."
+
+EN_DOC_CONLLU_PRETAGGED = """
+1	Barack	_	PROPN	NNP	Number=Sing	0	_	_	_
+2	Obama	_	PROPN	NNP	Number=Sing	1	_	_	_
+3	was	_	AUX	VBD	Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin	2	_	_	_
+4	born	_	VERB	VBN	Tense=Past|VerbForm=Part|Voice=Pass	3	_	_	_
+5	in	_	ADP	IN	_	4	_	_	_
+6	Hawaii	_	PROPN	NNP	Number=Sing	5	_	_	_
+7	.	_	PUNCT	.	_	6	_	_	_
+
+1	He	_	PRON	PRP	Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs	0	_	_	_
+2	was	_	AUX	VBD	Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin	1	_	_	_
+3	elected	_	VERB	VBN	Tense=Past|VerbForm=Part|Voice=Pass	2	_	_	_
+4	president	_	PROPN	NNP	Number=Sing	3	_	_	_
+5	in	_	ADP	IN	_	4	_	_	_
+6	2008	_	NUM	CD	NumType=Card	5	_	_	_
+7	.	_	PUNCT	.	_	6	_	_	_
+
+1	Obama	_	PROPN	NNP	Number=Sing	0	_	_	_
+2	attended	_	VERB	VBD	Mood=Ind|Tense=Past|VerbForm=Fin	1	_	_	_
+3	Harvard	_	PROPN	NNP	Number=Sing	2	_	_	_
+4	.	_	PUNCT	.	_	3	_	_	_
+
+
+""".lstrip()
+
+EN_DOC_DEPENDENCY_PARSES_GOLD = """
+('Barack', 4, 'nsubj:pass')
+('Obama', 1, 'flat')
+('was', 4, 'aux:pass')
+('born', 0, 'root')
+('in', 6, 'case')
+('Hawaii', 4, 'obl')
+('.', 4, 'punct')
+
+('He', 3, 'nsubj:pass')
+('was', 3, 'aux:pass')
+('elected', 0, 'root')
+('president', 3, 'xcomp')
+('in', 6, 'case')
+('2008', 3, 'obl')
+('.', 3, 'punct')
+
+('Obama', 2, 'nsubj')
+('attended', 0, 'root')
+('Harvard', 2, 'obj')
+('.', 2, 'punct')
+""".strip()
+
+
+def test_depparse():
+    nlp = classla.Pipeline(dir=TEST_MODELS_DIR, lang='en')
+    doc = nlp(EN_DOC)
+    assert EN_DOC_DEPENDENCY_PARSES_GOLD == '\n\n'.join([sent.dependencies_string() for sent in doc.sentences])
+
+
+def test_depparse_with_pretagged_doc():
+    nlp = classla.Pipeline(**{'processors': 'depparse', 'dir': TEST_MODELS_DIR, 'lang': 'en',
+                                  'depparse_pretagged': True})
+
+    doc, metasentences = CoNLL.conll2dict(input_file=EN_DOC_CONLLU_PRETAGGED)
+    doc = classla.Document(doc, metasentences=metasentences)
+    processed_doc = nlp(doc)
+
+    assert EN_DOC_DEPENDENCY_PARSES_GOLD == '\n\n'.join(
+        [sent.dependencies_string() for sent in processed_doc.sentences])
+
+
+def test_raises_requirements_exception_if_pretagged_not_passed():
+    with pytest.raises(PipelineRequirementsException):
+        classla.Pipeline(**{'processors': 'depparse', 'dir': TEST_MODELS_DIR, 'lang': 'en'})
```

### Comparing `classla-2.0/tests/test_depparse_data.py` & `classla-2.1/tests/test_depparse_data.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,62 +1,62 @@
-"""
-Test some pieces of the depparse dataloader
-"""
-import pytest
-from classla.models.depparse.data import data_to_batches
-
-pytestmark = [pytest.mark.travis, pytest.mark.pipeline]
-
-def make_fake_data(*lengths):
-    data = []
-    for i, length in enumerate(lengths):
-        word = chr(ord('A') + i)
-        chunk = [[word] * length]
-        data.append(chunk)
-    return data
-
-def check_batches(batched_data, expected_sizes, expected_order):
-    for chunk, size in zip(batched_data, expected_sizes):
-        assert sum(len(x[0]) for x in chunk) == size
-    word_order = []
-    for chunk in batched_data:
-        for sentence in chunk:
-            word_order.append(sentence[0][0])
-    assert word_order == expected_order
-
-def test_data_to_batches_eval_mode():
-    """
-    Tests the chunking of batches in eval_mode
-
-    A few options are tested, such as whther or not to sort and the maximum sentence size
-    """
-    data = make_fake_data(1, 2, 3)
-    batched_data = data_to_batches(data, batch_size=5, eval_mode=True, sort_during_eval=True, max_sentence_size=None)
-    check_batches(batched_data[0], [5, 1], ['C', 'B', 'A'])
-
-    data = make_fake_data(1, 2, 6)
-    batched_data = data_to_batches(data, batch_size=5, eval_mode=True, sort_during_eval=True, max_sentence_size=None)
-    check_batches(batched_data[0], [6, 3], ['C', 'B', 'A'])
-
-    data = make_fake_data(3, 2, 1)
-    batched_data = data_to_batches(data, batch_size=5, eval_mode=True, sort_during_eval=True, max_sentence_size=None)
-    check_batches(batched_data[0], [5, 1], ['A', 'B', 'C'])
-
-    data = make_fake_data(3, 5, 2)
-    batched_data = data_to_batches(data, batch_size=5, eval_mode=True, sort_during_eval=True, max_sentence_size=None)
-    check_batches(batched_data[0], [5, 5], ['B', 'A', 'C'])
-
-    data = make_fake_data(3, 5, 2)
-    batched_data = data_to_batches(data, batch_size=5, eval_mode=True, sort_during_eval=False, max_sentence_size=3)
-    check_batches(batched_data[0], [3, 5, 2], ['A', 'B', 'C'])
-
-    data = make_fake_data(4, 1, 1)
-    batched_data = data_to_batches(data, batch_size=5, eval_mode=True, sort_during_eval=False, max_sentence_size=3)
-    check_batches(batched_data[0], [4, 2], ['A', 'B', 'C'])
-
-    data = make_fake_data(1, 4, 1)
-    batched_data = data_to_batches(data, batch_size=5, eval_mode=True, sort_during_eval=False, max_sentence_size=3)
-    check_batches(batched_data[0], [1, 4, 1], ['A', 'B', 'C'])
-
-if __name__ == '__main__':
-    test_data_to_batches()
-
+"""
+Test some pieces of the depparse dataloader
+"""
+import pytest
+from classla.models.depparse.data import data_to_batches
+
+pytestmark = [pytest.mark.travis, pytest.mark.pipeline]
+
+def make_fake_data(*lengths):
+    data = []
+    for i, length in enumerate(lengths):
+        word = chr(ord('A') + i)
+        chunk = [[word] * length]
+        data.append(chunk)
+    return data
+
+def check_batches(batched_data, expected_sizes, expected_order):
+    for chunk, size in zip(batched_data, expected_sizes):
+        assert sum(len(x[0]) for x in chunk) == size
+    word_order = []
+    for chunk in batched_data:
+        for sentence in chunk:
+            word_order.append(sentence[0][0])
+    assert word_order == expected_order
+
+def test_data_to_batches_eval_mode():
+    """
+    Tests the chunking of batches in eval_mode
+
+    A few options are tested, such as whther or not to sort and the maximum sentence size
+    """
+    data = make_fake_data(1, 2, 3)
+    batched_data = data_to_batches(data, batch_size=5, eval_mode=True, sort_during_eval=True, max_sentence_size=None)
+    check_batches(batched_data[0], [5, 1], ['C', 'B', 'A'])
+
+    data = make_fake_data(1, 2, 6)
+    batched_data = data_to_batches(data, batch_size=5, eval_mode=True, sort_during_eval=True, max_sentence_size=None)
+    check_batches(batched_data[0], [6, 3], ['C', 'B', 'A'])
+
+    data = make_fake_data(3, 2, 1)
+    batched_data = data_to_batches(data, batch_size=5, eval_mode=True, sort_during_eval=True, max_sentence_size=None)
+    check_batches(batched_data[0], [5, 1], ['A', 'B', 'C'])
+
+    data = make_fake_data(3, 5, 2)
+    batched_data = data_to_batches(data, batch_size=5, eval_mode=True, sort_during_eval=True, max_sentence_size=None)
+    check_batches(batched_data[0], [5, 5], ['B', 'A', 'C'])
+
+    data = make_fake_data(3, 5, 2)
+    batched_data = data_to_batches(data, batch_size=5, eval_mode=True, sort_during_eval=False, max_sentence_size=3)
+    check_batches(batched_data[0], [3, 5, 2], ['A', 'B', 'C'])
+
+    data = make_fake_data(4, 1, 1)
+    batched_data = data_to_batches(data, batch_size=5, eval_mode=True, sort_during_eval=False, max_sentence_size=3)
+    check_batches(batched_data[0], [4, 2], ['A', 'B', 'C'])
+
+    data = make_fake_data(1, 4, 1)
+    batched_data = data_to_batches(data, batch_size=5, eval_mode=True, sort_during_eval=False, max_sentence_size=3)
+    check_batches(batched_data[0], [1, 4, 1], ['A', 'B', 'C'])
+
+if __name__ == '__main__':
+    test_data_to_batches()
+
```

### Comparing `classla-2.0/tests/test_doc.py` & `classla-2.1/tests/test_doc.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,57 +1,57 @@
-import pytest
-
-import classla
-from tests import *
-from classla.models.common.doc import Document, ID, TEXT, NER
-
-pytestmark = [pytest.mark.travis, pytest.mark.pipeline]
-
-@pytest.fixture
-def sentences_dict():
-    return [[{ID: 1, TEXT: "unban"},
-             {ID: 2, TEXT: "mox"},
-             {ID: 3, TEXT: "opal"}],
-            [{ID: 4, TEXT: "ban"},
-             {ID: 5, TEXT: "Lurrus"}]]
-
-@pytest.fixture
-def doc(sentences_dict):
-    doc = Document(sentences_dict)
-    return doc
-
-def test_basic_values(doc, sentences_dict):
-    """
-    Test that sentences & token text are properly set when constructing a doc
-    """
-    assert len(doc.sentences) == len(sentences_dict)
-
-    for sentence, raw_sentence in zip(doc.sentences, sentences_dict):
-        assert sentence.doc == doc
-        assert len(sentence.tokens) == len(raw_sentence)
-        for token, raw_token in zip(sentence.tokens, raw_sentence):
-            assert token.text == raw_token[TEXT]
-
-def test_set_sentence(doc):
-    """
-    Test setting a field on the sentences themselves
-    """
-    doc.set(fields="sentiment",
-            contents=["4", "0"],
-            to_sentence=True)
-
-    assert doc.sentences[0].sentiment == "4"
-    assert doc.sentences[1].sentiment == "0"
-
-def test_set_tokens(doc):
-    """
-    Test setting values on tokens
-    """
-    ner_contents = ["O", "ARTIFACT", "ARTIFACT", "O", "CAT"]
-    doc.set(fields=NER,
-            contents=ner_contents,
-            to_token=True)
-
-    result = doc.get(NER, from_token=True)
-    assert result == ner_contents
-
-
+import pytest
+
+import classla
+from tests import *
+from classla.models.common.doc import Document, ID, TEXT, NER
+
+pytestmark = [pytest.mark.travis, pytest.mark.pipeline]
+
+@pytest.fixture
+def sentences_dict():
+    return [[{ID: 1, TEXT: "unban"},
+             {ID: 2, TEXT: "mox"},
+             {ID: 3, TEXT: "opal"}],
+            [{ID: 4, TEXT: "ban"},
+             {ID: 5, TEXT: "Lurrus"}]]
+
+@pytest.fixture
+def doc(sentences_dict):
+    doc = Document(sentences_dict)
+    return doc
+
+def test_basic_values(doc, sentences_dict):
+    """
+    Test that sentences & token text are properly set when constructing a doc
+    """
+    assert len(doc.sentences) == len(sentences_dict)
+
+    for sentence, raw_sentence in zip(doc.sentences, sentences_dict):
+        assert sentence.doc == doc
+        assert len(sentence.tokens) == len(raw_sentence)
+        for token, raw_token in zip(sentence.tokens, raw_sentence):
+            assert token.text == raw_token[TEXT]
+
+def test_set_sentence(doc):
+    """
+    Test setting a field on the sentences themselves
+    """
+    doc.set(fields="sentiment",
+            contents=["4", "0"],
+            to_sentence=True)
+
+    assert doc.sentences[0].sentiment == "4"
+    assert doc.sentences[1].sentiment == "0"
+
+def test_set_tokens(doc):
+    """
+    Test setting values on tokens
+    """
+    ner_contents = ["O", "ARTIFACT", "ARTIFACT", "O", "CAT"]
+    doc.set(fields=NER,
+            contents=ner_contents,
+            to_token=True)
+
+    result = doc.get(NER, from_token=True)
+    assert result == ner_contents
+
+
```

### Comparing `classla-2.0/tests/test_installation.py` & `classla-2.1/tests/test_installation.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,39 +1,39 @@
-"""
-Test installation functions.
-"""
-
-import os
-import pytest
-import shutil
-import tempfile
-
-import classla
-
-pytestmark = [pytest.mark.travis, pytest.mark.client]
-
-def test_install_corenlp():
-    # we do not reset the CORENLP_HOME variable since this may impact the 
-    # client tests
-    with tempfile.TemporaryDirectory(dir=".") as test_dir:
-
-        # the download method doesn't install over existing directories
-        shutil.rmtree(test_dir)
-        classla.install_corenlp(dir=test_dir, url='http://nlp.stanford.edu/software/')
-
-        assert os.path.isdir(test_dir), "Installation destination directory not found."
-        jar_files = [f for f in os.listdir(test_dir) \
-                     if f.endswith('.jar') and f.startswith('stanford-corenlp')]
-        assert len(jar_files) > 0, \
-            "Cannot find stanford-corenlp jar files in the installation directory."
-        assert not os.path.exists(os.path.join(test_dir, 'corenlp.zip')), \
-            "Downloaded zip file was not removed."
-    
-def test_download_corenlp_models():
-    model_name = "arabic"
-    version = "4.1.0"
-
-    with tempfile.TemporaryDirectory(dir=".") as test_dir:
-        classla.download_corenlp_models(model=model_name, version=version, dir=test_dir)
-
-        dest_file = os.path.join(test_dir, f"stanford-corenlp-{version}-models-{model_name}.jar")
-        assert os.path.isfile(dest_file), "Downloaded model file not found."
+"""
+Test installation functions.
+"""
+
+import os
+import pytest
+import shutil
+import tempfile
+
+import classla
+
+pytestmark = [pytest.mark.travis, pytest.mark.client]
+
+def test_install_corenlp():
+    # we do not reset the CORENLP_HOME variable since this may impact the 
+    # client tests
+    with tempfile.TemporaryDirectory(dir=".") as test_dir:
+
+        # the download method doesn't install over existing directories
+        shutil.rmtree(test_dir)
+        classla.install_corenlp(dir=test_dir, url='http://nlp.stanford.edu/software/')
+
+        assert os.path.isdir(test_dir), "Installation destination directory not found."
+        jar_files = [f for f in os.listdir(test_dir) \
+                     if f.endswith('.jar') and f.startswith('stanford-corenlp')]
+        assert len(jar_files) > 0, \
+            "Cannot find stanford-corenlp jar files in the installation directory."
+        assert not os.path.exists(os.path.join(test_dir, 'corenlp.zip')), \
+            "Downloaded zip file was not removed."
+    
+def test_download_corenlp_models():
+    model_name = "arabic"
+    version = "4.1.0"
+
+    with tempfile.TemporaryDirectory(dir=".") as test_dir:
+        classla.download_corenlp_models(model=model_name, version=version, dir=test_dir)
+
+        dest_file = os.path.join(test_dir, f"stanford-corenlp-{version}-models-{model_name}.jar")
+        assert os.path.isfile(dest_file), "Downloaded model file not found."
```

### Comparing `classla-2.0/tests/test_lemmatizer.py` & `classla-2.1/tests/test_lemmatizer.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,51 +1,51 @@
-"""
-Basic testing of lemmatization
-"""
-
-import pytest
-import classla
-
-from tests import *
-
-pytestmark = pytest.mark.pipeline
-
-EN_DOC = "Joe Smith was born in California."
-
-EN_DOC_IDENTITY_GOLD = """
-Joe Joe
-Smith Smith
-was was
-born born
-in in
-California California
-. .
-""".strip()
-
-EN_DOC_LEMMATIZER_MODEL_GOLD = """
-Joe Joe
-Smith Smith
-was be
-born bear
-in in
-California California
-. .
-""".strip()
-
-
-def test_identity_lemmatizer():
-    nlp = classla.Pipeline(**{'processors': 'tokenize,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en',
-                                  'lemma_use_identity': True})
-    doc = nlp(EN_DOC)
-    word_lemma_pairs = []
-    for w in doc.iter_words():
-        word_lemma_pairs += [f"{w.text} {w.lemma}"]
-    assert EN_DOC_IDENTITY_GOLD == "\n".join(word_lemma_pairs)
-
-def test_full_lemmatizer():
-    nlp = classla.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en'})
-    doc = nlp(EN_DOC)
-    word_lemma_pairs = []
-    for w in doc.iter_words():
-        word_lemma_pairs += [f"{w.text} {w.lemma}"]
-    assert EN_DOC_LEMMATIZER_MODEL_GOLD == "\n".join(word_lemma_pairs)
-
+"""
+Basic testing of lemmatization
+"""
+
+import pytest
+import classla
+
+from tests import *
+
+pytestmark = pytest.mark.pipeline
+
+EN_DOC = "Joe Smith was born in California."
+
+EN_DOC_IDENTITY_GOLD = """
+Joe Joe
+Smith Smith
+was was
+born born
+in in
+California California
+. .
+""".strip()
+
+EN_DOC_LEMMATIZER_MODEL_GOLD = """
+Joe Joe
+Smith Smith
+was be
+born bear
+in in
+California California
+. .
+""".strip()
+
+
+def test_identity_lemmatizer():
+    nlp = classla.Pipeline(**{'processors': 'tokenize,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en',
+                                  'lemma_use_identity': True})
+    doc = nlp(EN_DOC)
+    word_lemma_pairs = []
+    for w in doc.iter_words():
+        word_lemma_pairs += [f"{w.text} {w.lemma}"]
+    assert EN_DOC_IDENTITY_GOLD == "\n".join(word_lemma_pairs)
+
+def test_full_lemmatizer():
+    nlp = classla.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en'})
+    doc = nlp(EN_DOC)
+    word_lemma_pairs = []
+    for w in doc.iter_words():
+        word_lemma_pairs += [f"{w.text} {w.lemma}"]
+    assert EN_DOC_LEMMATIZER_MODEL_GOLD == "\n".join(word_lemma_pairs)
+
```

### Comparing `classla-2.0/tests/test_mwt_expander.py` & `classla-2.1/tests/test_mwt_expander.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,84 +1,84 @@
-"""
-Basic testing of multi-word-token expansion
-"""
-
-import pytest
-import classla
-
-from tests import *
-
-pytestmark = pytest.mark.pipeline
-
-# mwt data for testing
-FR_MWT_SENTENCE = "Alors encore inconnu du grand public, Emmanuel Macron devient en 2014 ministre de l'Économie, de " \
-                  "l'Industrie et du Numérique."
-
-
-FR_MWT_TOKEN_TO_WORDS_GOLD = """
-token: Alors    		words: [<Word id=1;text=Alors>]
-token: encore   		words: [<Word id=2;text=encore>]
-token: inconnu  		words: [<Word id=3;text=inconnu>]
-token: du       		words: [<Word id=4;text=de>, <Word id=5;text=le>]
-token: grand    		words: [<Word id=6;text=grand>]
-token: public   		words: [<Word id=7;text=public>]
-token: ,        		words: [<Word id=8;text=,>]
-token: Emmanuel 		words: [<Word id=9;text=Emmanuel>]
-token: Macron   		words: [<Word id=10;text=Macron>]
-token: devient  		words: [<Word id=11;text=devient>]
-token: en       		words: [<Word id=12;text=en>]
-token: 2014     		words: [<Word id=13;text=2014>]
-token: ministre 		words: [<Word id=14;text=ministre>]
-token: de       		words: [<Word id=15;text=de>]
-token: l'       		words: [<Word id=16;text=l'>]
-token: Économie 		words: [<Word id=17;text=Économie>]
-token: ,        		words: [<Word id=18;text=,>]
-token: de       		words: [<Word id=19;text=de>]
-token: l'       		words: [<Word id=20;text=l'>]
-token: Industrie		words: [<Word id=21;text=Industrie>]
-token: et       		words: [<Word id=22;text=et>]
-token: du       		words: [<Word id=23;text=de>, <Word id=24;text=le>]
-token: Numérique		words: [<Word id=25;text=Numérique>]
-token: .        		words: [<Word id=26;text=.>]
-""".strip()
-
-FR_MWT_WORD_TO_TOKEN_GOLD = """
-word: Alors    		token parent:1-Alors
-word: encore   		token parent:2-encore
-word: inconnu  		token parent:3-inconnu
-word: de       		token parent:4-5-du
-word: le       		token parent:4-5-du
-word: grand    		token parent:6-grand
-word: public   		token parent:7-public
-word: ,        		token parent:8-,
-word: Emmanuel 		token parent:9-Emmanuel
-word: Macron   		token parent:10-Macron
-word: devient  		token parent:11-devient
-word: en       		token parent:12-en
-word: 2014     		token parent:13-2014
-word: ministre 		token parent:14-ministre
-word: de       		token parent:15-de
-word: l'       		token parent:16-l'
-word: Économie 		token parent:17-Économie
-word: ,        		token parent:18-,
-word: de       		token parent:19-de
-word: l'       		token parent:20-l'
-word: Industrie		token parent:21-Industrie
-word: et       		token parent:22-et
-word: de       		token parent:23-24-du
-word: le       		token parent:23-24-du
-word: Numérique		token parent:25-Numérique
-word: .        		token parent:26-.
-""".strip()
-
-
-def test_mwt():
-    pipeline = classla.Pipeline(processors='tokenize,mwt', dir=TEST_MODELS_DIR, lang='fr')
-    doc = pipeline(FR_MWT_SENTENCE)
-    token_to_words = "\n".join(
-        [f'token: {token.text.ljust(9)}\t\twords: [{", ".join([word.pretty_print() for word in token.words])}]' for sent in doc.sentences for token in sent.tokens]
-    ).strip()
-    word_to_token = "\n".join(
-        [f'word: {word.text.ljust(9)}\t\ttoken parent:{"-".join([str(x) for x in word.parent.id])}-{word.parent.text}'
-         for sent in doc.sentences for word in sent.words]).strip()
-    assert token_to_words == FR_MWT_TOKEN_TO_WORDS_GOLD
-    assert word_to_token == FR_MWT_WORD_TO_TOKEN_GOLD
+"""
+Basic testing of multi-word-token expansion
+"""
+
+import pytest
+import classla
+
+from tests import *
+
+pytestmark = pytest.mark.pipeline
+
+# mwt data for testing
+FR_MWT_SENTENCE = "Alors encore inconnu du grand public, Emmanuel Macron devient en 2014 ministre de l'Économie, de " \
+                  "l'Industrie et du Numérique."
+
+
+FR_MWT_TOKEN_TO_WORDS_GOLD = """
+token: Alors    		words: [<Word id=1;text=Alors>]
+token: encore   		words: [<Word id=2;text=encore>]
+token: inconnu  		words: [<Word id=3;text=inconnu>]
+token: du       		words: [<Word id=4;text=de>, <Word id=5;text=le>]
+token: grand    		words: [<Word id=6;text=grand>]
+token: public   		words: [<Word id=7;text=public>]
+token: ,        		words: [<Word id=8;text=,>]
+token: Emmanuel 		words: [<Word id=9;text=Emmanuel>]
+token: Macron   		words: [<Word id=10;text=Macron>]
+token: devient  		words: [<Word id=11;text=devient>]
+token: en       		words: [<Word id=12;text=en>]
+token: 2014     		words: [<Word id=13;text=2014>]
+token: ministre 		words: [<Word id=14;text=ministre>]
+token: de       		words: [<Word id=15;text=de>]
+token: l'       		words: [<Word id=16;text=l'>]
+token: Économie 		words: [<Word id=17;text=Économie>]
+token: ,        		words: [<Word id=18;text=,>]
+token: de       		words: [<Word id=19;text=de>]
+token: l'       		words: [<Word id=20;text=l'>]
+token: Industrie		words: [<Word id=21;text=Industrie>]
+token: et       		words: [<Word id=22;text=et>]
+token: du       		words: [<Word id=23;text=de>, <Word id=24;text=le>]
+token: Numérique		words: [<Word id=25;text=Numérique>]
+token: .        		words: [<Word id=26;text=.>]
+""".strip()
+
+FR_MWT_WORD_TO_TOKEN_GOLD = """
+word: Alors    		token parent:1-Alors
+word: encore   		token parent:2-encore
+word: inconnu  		token parent:3-inconnu
+word: de       		token parent:4-5-du
+word: le       		token parent:4-5-du
+word: grand    		token parent:6-grand
+word: public   		token parent:7-public
+word: ,        		token parent:8-,
+word: Emmanuel 		token parent:9-Emmanuel
+word: Macron   		token parent:10-Macron
+word: devient  		token parent:11-devient
+word: en       		token parent:12-en
+word: 2014     		token parent:13-2014
+word: ministre 		token parent:14-ministre
+word: de       		token parent:15-de
+word: l'       		token parent:16-l'
+word: Économie 		token parent:17-Économie
+word: ,        		token parent:18-,
+word: de       		token parent:19-de
+word: l'       		token parent:20-l'
+word: Industrie		token parent:21-Industrie
+word: et       		token parent:22-et
+word: de       		token parent:23-24-du
+word: le       		token parent:23-24-du
+word: Numérique		token parent:25-Numérique
+word: .        		token parent:26-.
+""".strip()
+
+
+def test_mwt():
+    pipeline = classla.Pipeline(processors='tokenize,mwt', dir=TEST_MODELS_DIR, lang='fr')
+    doc = pipeline(FR_MWT_SENTENCE)
+    token_to_words = "\n".join(
+        [f'token: {token.text.ljust(9)}\t\twords: [{", ".join([word.pretty_print() for word in token.words])}]' for sent in doc.sentences for token in sent.tokens]
+    ).strip()
+    word_to_token = "\n".join(
+        [f'word: {word.text.ljust(9)}\t\ttoken parent:{"-".join([str(x) for x in word.parent.id])}-{word.parent.text}'
+         for sent in doc.sentences for word in sent.words]).strip()
+    assert token_to_words == FR_MWT_TOKEN_TO_WORDS_GOLD
+    assert word_to_token == FR_MWT_WORD_TO_TOKEN_GOLD
```

### Comparing `classla-2.0/tests/test_ner_tagger.py` & `classla-2.1/tests/test_ner_tagger.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,41 +1,41 @@
-"""
-Basic testing of the NER tagger.
-"""
-
-import pytest
-import classla
-
-from tests import *
-from classla.models.ner.scorer import score_by_token, score_by_entity
-
-pytestmark = pytest.mark.pipeline
-
-EN_DOC = "Chris Manning is a good man. He works in Stanford University."
-
-EN_DOC_GOLD = """
-<Span text=Chris Manning;type=PERSON;start_char=0;end_char=13>
-<Span text=Stanford University;type=ORG;start_char=41;end_char=60>
-""".strip()
-
-
-def test_ner():
-    nlp = classla.Pipeline(**{'processors': 'tokenize,ner', 'dir': TEST_MODELS_DIR, 'lang': 'en', 'logging_level': 'error'})
-    doc = nlp(EN_DOC)
-    assert EN_DOC_GOLD == '\n'.join([ent.pretty_print() for ent in doc.ents])
-
-
-def test_ner_scorer():
-    pred_sequences = [['O', 'S-LOC', 'O', 'O', 'B-PER', 'E-PER'],
-                    ['O', 'S-MISC', 'O', 'E-ORG', 'O', 'B-PER', 'I-PER', 'E-PER']]
-    gold_sequences = [['O', 'B-LOC', 'E-LOC', 'O', 'B-PER', 'E-PER'],
-                    ['O', 'S-MISC', 'B-ORG', 'E-ORG', 'O', 'B-PER', 'E-PER', 'S-LOC']]
-    
-    token_p, token_r, token_f = score_by_token(pred_sequences, gold_sequences)
-    assert pytest.approx(token_p, abs=0.00001) == 0.625
-    assert pytest.approx(token_r, abs=0.00001) == 0.5
-    assert pytest.approx(token_f, abs=0.00001) == 0.55555
-
-    entity_p, entity_r, entity_f = score_by_entity(pred_sequences, gold_sequences)
-    assert pytest.approx(entity_p, abs=0.00001) == 0.4
-    assert pytest.approx(entity_r, abs=0.00001) == 0.33333
+"""
+Basic testing of the NER tagger.
+"""
+
+import pytest
+import classla
+
+from tests import *
+from classla.models.ner.scorer import score_by_token, score_by_entity
+
+pytestmark = pytest.mark.pipeline
+
+EN_DOC = "Chris Manning is a good man. He works in Stanford University."
+
+EN_DOC_GOLD = """
+<Span text=Chris Manning;type=PERSON;start_char=0;end_char=13>
+<Span text=Stanford University;type=ORG;start_char=41;end_char=60>
+""".strip()
+
+
+def test_ner():
+    nlp = classla.Pipeline(**{'processors': 'tokenize,ner', 'dir': TEST_MODELS_DIR, 'lang': 'en', 'logging_level': 'error'})
+    doc = nlp(EN_DOC)
+    assert EN_DOC_GOLD == '\n'.join([ent.pretty_print() for ent in doc.ents])
+
+
+def test_ner_scorer():
+    pred_sequences = [['O', 'S-LOC', 'O', 'O', 'B-PER', 'E-PER'],
+                    ['O', 'S-MISC', 'O', 'E-ORG', 'O', 'B-PER', 'I-PER', 'E-PER']]
+    gold_sequences = [['O', 'B-LOC', 'E-LOC', 'O', 'B-PER', 'E-PER'],
+                    ['O', 'S-MISC', 'B-ORG', 'E-ORG', 'O', 'B-PER', 'E-PER', 'S-LOC']]
+    
+    token_p, token_r, token_f = score_by_token(pred_sequences, gold_sequences)
+    assert pytest.approx(token_p, abs=0.00001) == 0.625
+    assert pytest.approx(token_r, abs=0.00001) == 0.5
+    assert pytest.approx(token_f, abs=0.00001) == 0.55555
+
+    entity_p, entity_r, entity_f = score_by_entity(pred_sequences, gold_sequences)
+    assert pytest.approx(entity_p, abs=0.00001) == 0.4
+    assert pytest.approx(entity_r, abs=0.00001) == 0.33333
     assert pytest.approx(entity_f, abs=0.00001) == 0.36363
```

### Comparing `classla-2.0/tests/test_pretrain.py` & `classla-2.1/tests/test_pretrain.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,53 +1,53 @@
-import os
-import tempfile
-
-import pytest
-import numpy as np
-
-from classla.models.common import pretrain
-from tests import *
-
-pytestmark = [pytest.mark.travis, pytest.mark.pipeline]
-
-def check_pretrain(pt):
-    # 4 base vectors, plus the 3 vectors actually present in the file
-    assert len(pt.vocab) == 7
-    assert 'unban' in pt.vocab
-    assert 'mox' in pt.vocab
-    assert 'opal' in pt.vocab
-
-    expected = np.array([[ 0.,  0.,  0.,  0.,],
-                         [ 0.,  0.,  0.,  0.,],
-                         [ 0.,  0.,  0.,  0.,],
-                         [ 0.,  0.,  0.,  0.,],
-                         [ 1.,  2.,  3.,  4.,],
-                         [ 5.,  6.,  7.,  8.,],
-                         [ 9., 10., 11., 12.,]])
-    np.testing.assert_allclose(pt.emb, expected)    
-
-def test_text_pretrain():
-    pt = pretrain.Pretrain(vec_filename=f'{TEST_WORKING_DIR}/in/tiny_emb.txt', save_to_file=False)
-    check_pretrain(pt)
-
-def test_xz_pretrain():
-    pt = pretrain.Pretrain(vec_filename=f'{TEST_WORKING_DIR}/in/tiny_emb.xz', save_to_file=False)
-    check_pretrain(pt)
-
-def test_resave_pretrain():
-    """
-    Test saving a pretrain and then loading from the existing file
-    """
-    test_pt_file = tempfile.NamedTemporaryFile(dir=f'{TEST_WORKING_DIR}/out', suffix=".pt", delete=False)
-    try:
-        test_pt_file.close()
-        # note that this tests the ability to save a pretrain and the
-        # ability to fall back when the existing pretrain isn't working
-        pt = pretrain.Pretrain(filename=test_pt_file.name,
-                               vec_filename=f'{TEST_WORKING_DIR}/in/tiny_emb.xz')
-        check_pretrain(pt)
-
-        pt2 = pretrain.Pretrain(filename=test_pt_file.name,
-                               vec_filename=f'unban_mox_opal')
-        check_pretrain(pt2)
-    finally:
-        os.unlink(test_pt_file.name)
+import os
+import tempfile
+
+import pytest
+import numpy as np
+
+from classla.models.common import pretrain
+from tests import *
+
+pytestmark = [pytest.mark.travis, pytest.mark.pipeline]
+
+def check_pretrain(pt):
+    # 4 base vectors, plus the 3 vectors actually present in the file
+    assert len(pt.vocab) == 7
+    assert 'unban' in pt.vocab
+    assert 'mox' in pt.vocab
+    assert 'opal' in pt.vocab
+
+    expected = np.array([[ 0.,  0.,  0.,  0.,],
+                         [ 0.,  0.,  0.,  0.,],
+                         [ 0.,  0.,  0.,  0.,],
+                         [ 0.,  0.,  0.,  0.,],
+                         [ 1.,  2.,  3.,  4.,],
+                         [ 5.,  6.,  7.,  8.,],
+                         [ 9., 10., 11., 12.,]])
+    np.testing.assert_allclose(pt.emb, expected)    
+
+def test_text_pretrain():
+    pt = pretrain.Pretrain(vec_filename=f'{TEST_WORKING_DIR}/in/tiny_emb.txt', save_to_file=False)
+    check_pretrain(pt)
+
+def test_xz_pretrain():
+    pt = pretrain.Pretrain(vec_filename=f'{TEST_WORKING_DIR}/in/tiny_emb.xz', save_to_file=False)
+    check_pretrain(pt)
+
+def test_resave_pretrain():
+    """
+    Test saving a pretrain and then loading from the existing file
+    """
+    test_pt_file = tempfile.NamedTemporaryFile(dir=f'{TEST_WORKING_DIR}/out', suffix=".pt", delete=False)
+    try:
+        test_pt_file.close()
+        # note that this tests the ability to save a pretrain and the
+        # ability to fall back when the existing pretrain isn't working
+        pt = pretrain.Pretrain(filename=test_pt_file.name,
+                               vec_filename=f'{TEST_WORKING_DIR}/in/tiny_emb.xz')
+        check_pretrain(pt)
+
+        pt2 = pretrain.Pretrain(filename=test_pt_file.name,
+                               vec_filename=f'unban_mox_opal')
+        check_pretrain(pt2)
+    finally:
+        os.unlink(test_pt_file.name)
```

### Comparing `classla-2.0/tests/test_protobuf.py` & `classla-2.1/tests/test_protobuf.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,151 +1,151 @@
-"""
-Tests to read a stored protobuf.
-Also serves as an example of how to parse sentences, tokens, pos, lemma,
-ner, dependencies and mentions.
-
-The test corresponds to annotations for the following sentence:
-    Chris wrote a simple sentence that he parsed with Stanford CoreNLP.
-"""
-import os
-import pytest
-
-from pytest import fixture
-from classla.protobuf import Document, Sentence, Token, DependencyGraph,\
-                             CorefChain
-from classla.protobuf import parseFromDelimitedString, writeToDelimitedString, to_text
-
-# set the marker for this module
-pytestmark = [pytest.mark.travis, pytest.mark.client]
-
-# Text that was annotated
-TEXT = "Chris wrote a simple sentence that he parsed with Stanford CoreNLP.\n"
-
-
-@fixture
-def doc_pb():
-    test_dir = os.path.dirname(os.path.abspath(__file__))
-    test_data = os.path.join(test_dir, 'data', 'test.dat')
-    with open(test_data, 'rb') as f:
-        buf = f.read()
-    doc = Document()
-    parseFromDelimitedString(doc, buf)
-    return doc
-
-
-def test_parse_protobuf(doc_pb):
-    assert doc_pb.ByteSize() == 4709
-
-
-def test_write_protobuf(doc_pb):
-    stream = writeToDelimitedString(doc_pb)
-    buf = stream.getvalue()
-    stream.close()
-
-    doc_pb_ = Document()
-    parseFromDelimitedString(doc_pb_, buf)
-    assert doc_pb == doc_pb_
-
-
-def test_document_text(doc_pb):
-    assert doc_pb.text == TEXT
-
-
-def test_sentences(doc_pb):
-    assert len(doc_pb.sentence) == 1
-
-    sentence = doc_pb.sentence[0]
-    assert isinstance(sentence, Sentence)
-    # check sentence length
-    assert sentence.characterOffsetEnd - sentence.characterOffsetBegin == 67
-    # Note that the sentence text should actually be recovered from the tokens.
-    assert sentence.text == ''
-    assert to_text(sentence) == TEXT[:-1]
-
-
-def test_tokens(doc_pb):
-    sentence = doc_pb.sentence[0]
-    tokens = sentence.token
-    assert len(tokens) == 12
-    assert isinstance(tokens[0], Token)
-
-    # Word
-    words = "Chris wrote a simple sentence that he parsed with Stanford CoreNLP .".split()
-    words_ = [t.word for t in tokens]
-    assert  words_ == words
-
-    # Lemma
-    lemmas = "Chris write a simple sentence that he parse with Stanford CoreNLP .".split()
-    lemmas_ = [t.lemma for t in tokens]
-    assert lemmas_ == lemmas
-
-    # POS
-    pos = "NNP VBD DT JJ NN IN PRP VBD IN NNP NNP .".split()
-    pos_ = [t.pos for t in tokens]
-    assert pos_ == pos
-
-    # NER
-    ner = "PERSON O O O O O O O O ORGANIZATION O O".split()
-    ner_ = [t.ner for t in tokens]
-    assert ner_ == ner
-
-    # character offsets
-    begin = [int(i) for i in "0 6 12 14 21 30 35 38 45 50 59 66".split()]
-    end =   [int(i) for i in "5 11 13 20 29 34 37 44 49 58 66 67".split()]
-    begin_ = [t.beginChar for t in tokens]
-    end_ = [t.endChar for t in tokens]
-    assert begin_ == begin
-    assert end_ == end
-
-
-def test_dependency_parse(doc_pb):
-    """
-    Extract the dependency parse from the annotation.
-    """
-    sentence = doc_pb.sentence[0]
-
-    # You can choose from the following types of dependencies.
-    # In general, you'll want enhancedPlusPlus
-    assert sentence.basicDependencies.ByteSize() > 0
-    assert sentence.enhancedDependencies.ByteSize() > 0
-    assert sentence.enhancedPlusPlusDependencies.ByteSize() > 0
-
-    tree = sentence.enhancedPlusPlusDependencies
-    isinstance(tree, DependencyGraph)
-    # Indices are 1-indexd with 0 being the "pseudo root"
-    assert tree.root  # 'wrote' is the root. == [2]
-    # There are as many nodes as there are tokens.
-    assert len(tree.node) == len(sentence.token)
-
-    # Enhanced++ depdencies often contain additional edges and are
-    # not trees -- here, 'parsed' would also have an edge to
-    # 'sentence'
-    assert len(tree.edge) == 12
-
-    # This edge goes from "wrote" to "Chirs"
-    edge = tree.edge[0]
-    assert edge.source == 2
-    assert edge.target == 1
-    assert edge.dep == "nsubj"
-
-
-def test_coref_chain(doc_pb):
-    """
-    Extract the corefence chains from the annotation.
-    """
-    # Coreference chains span sentences and are stored in the
-    # document.
-    chains = doc_pb.corefChain
-
-    # In this document there is 1 chain with Chris and he.
-    assert len(chains) == 1
-    chain = chains[0]
-    assert isinstance(chain, CorefChain)
-    assert chain.mention[0].beginIndex == 0  # 'Chris'
-    assert chain.mention[0].endIndex == 1
-    assert chain.mention[0].gender == "MALE"
-
-    assert chain.mention[1].beginIndex == 6  # 'he'
-    assert chain.mention[1].endIndex == 7
-    assert chain.mention[1].gender == "MALE"
-
-    assert chain.representative == 0  # Head of the chain is 'Chris'
+"""
+Tests to read a stored protobuf.
+Also serves as an example of how to parse sentences, tokens, pos, lemma,
+ner, dependencies and mentions.
+
+The test corresponds to annotations for the following sentence:
+    Chris wrote a simple sentence that he parsed with Stanford CoreNLP.
+"""
+import os
+import pytest
+
+from pytest import fixture
+from classla.protobuf import Document, Sentence, Token, DependencyGraph,\
+                             CorefChain
+from classla.protobuf import parseFromDelimitedString, writeToDelimitedString, to_text
+
+# set the marker for this module
+pytestmark = [pytest.mark.travis, pytest.mark.client]
+
+# Text that was annotated
+TEXT = "Chris wrote a simple sentence that he parsed with Stanford CoreNLP.\n"
+
+
+@fixture
+def doc_pb():
+    test_dir = os.path.dirname(os.path.abspath(__file__))
+    test_data = os.path.join(test_dir, 'data', 'test.dat')
+    with open(test_data, 'rb') as f:
+        buf = f.read()
+    doc = Document()
+    parseFromDelimitedString(doc, buf)
+    return doc
+
+
+def test_parse_protobuf(doc_pb):
+    assert doc_pb.ByteSize() == 4709
+
+
+def test_write_protobuf(doc_pb):
+    stream = writeToDelimitedString(doc_pb)
+    buf = stream.getvalue()
+    stream.close()
+
+    doc_pb_ = Document()
+    parseFromDelimitedString(doc_pb_, buf)
+    assert doc_pb == doc_pb_
+
+
+def test_document_text(doc_pb):
+    assert doc_pb.text == TEXT
+
+
+def test_sentences(doc_pb):
+    assert len(doc_pb.sentence) == 1
+
+    sentence = doc_pb.sentence[0]
+    assert isinstance(sentence, Sentence)
+    # check sentence length
+    assert sentence.characterOffsetEnd - sentence.characterOffsetBegin == 67
+    # Note that the sentence text should actually be recovered from the tokens.
+    assert sentence.text == ''
+    assert to_text(sentence) == TEXT[:-1]
+
+
+def test_tokens(doc_pb):
+    sentence = doc_pb.sentence[0]
+    tokens = sentence.token
+    assert len(tokens) == 12
+    assert isinstance(tokens[0], Token)
+
+    # Word
+    words = "Chris wrote a simple sentence that he parsed with Stanford CoreNLP .".split()
+    words_ = [t.word for t in tokens]
+    assert  words_ == words
+
+    # Lemma
+    lemmas = "Chris write a simple sentence that he parse with Stanford CoreNLP .".split()
+    lemmas_ = [t.lemma for t in tokens]
+    assert lemmas_ == lemmas
+
+    # POS
+    pos = "NNP VBD DT JJ NN IN PRP VBD IN NNP NNP .".split()
+    pos_ = [t.pos for t in tokens]
+    assert pos_ == pos
+
+    # NER
+    ner = "PERSON O O O O O O O O ORGANIZATION O O".split()
+    ner_ = [t.ner for t in tokens]
+    assert ner_ == ner
+
+    # character offsets
+    begin = [int(i) for i in "0 6 12 14 21 30 35 38 45 50 59 66".split()]
+    end =   [int(i) for i in "5 11 13 20 29 34 37 44 49 58 66 67".split()]
+    begin_ = [t.beginChar for t in tokens]
+    end_ = [t.endChar for t in tokens]
+    assert begin_ == begin
+    assert end_ == end
+
+
+def test_dependency_parse(doc_pb):
+    """
+    Extract the dependency parse from the annotation.
+    """
+    sentence = doc_pb.sentence[0]
+
+    # You can choose from the following types of dependencies.
+    # In general, you'll want enhancedPlusPlus
+    assert sentence.basicDependencies.ByteSize() > 0
+    assert sentence.enhancedDependencies.ByteSize() > 0
+    assert sentence.enhancedPlusPlusDependencies.ByteSize() > 0
+
+    tree = sentence.enhancedPlusPlusDependencies
+    isinstance(tree, DependencyGraph)
+    # Indices are 1-indexd with 0 being the "pseudo root"
+    assert tree.root  # 'wrote' is the root. == [2]
+    # There are as many nodes as there are tokens.
+    assert len(tree.node) == len(sentence.token)
+
+    # Enhanced++ depdencies often contain additional edges and are
+    # not trees -- here, 'parsed' would also have an edge to
+    # 'sentence'
+    assert len(tree.edge) == 12
+
+    # This edge goes from "wrote" to "Chirs"
+    edge = tree.edge[0]
+    assert edge.source == 2
+    assert edge.target == 1
+    assert edge.dep == "nsubj"
+
+
+def test_coref_chain(doc_pb):
+    """
+    Extract the corefence chains from the annotation.
+    """
+    # Coreference chains span sentences and are stored in the
+    # document.
+    chains = doc_pb.corefChain
+
+    # In this document there is 1 chain with Chris and he.
+    assert len(chains) == 1
+    chain = chains[0]
+    assert isinstance(chain, CorefChain)
+    assert chain.mention[0].beginIndex == 0  # 'Chris'
+    assert chain.mention[0].endIndex == 1
+    assert chain.mention[0].gender == "MALE"
+
+    assert chain.mention[1].beginIndex == 6  # 'he'
+    assert chain.mention[1].endIndex == 7
+    assert chain.mention[1].gender == "MALE"
+
+    assert chain.representative == 0  # Head of the chain is 'Chris'
```

### Comparing `classla-2.0/tests/test_server_misc.py` & `classla-2.1/tests/test_server_misc.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,96 +1,96 @@
-"""
-Misc tests for the server
-"""
-
-import pytest
-import re
-import stanza.server as corenlp
-from tests import compare_ignoring_whitespace
-
-pytestmark = pytest.mark.client
-
-EN_DOC = "Joe Smith lives in California."
-
-EN_DOC_GOLD = """
-Sentence #1 (6 tokens):
-Joe Smith lives in California.
-
-Tokens:
-[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP Lemma=Joe NamedEntityTag=PERSON]
-[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP Lemma=Smith NamedEntityTag=PERSON]
-[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ Lemma=live NamedEntityTag=O]
-[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN Lemma=in NamedEntityTag=O]
-[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP Lemma=California NamedEntityTag=STATE_OR_PROVINCE]
-[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=. Lemma=. NamedEntityTag=O]
-
-Dependency Parse (enhanced plus plus dependencies):
-root(ROOT-0, lives-3)
-compound(Smith-2, Joe-1)
-nsubj(lives-3, Smith-2)
-case(California-5, in-4)
-obl:in(lives-3, California-5)
-punct(lives-3, .-6)
-
-Extracted the following NER entity mentions:
-Joe Smith       PERSON  PERSON:0.9972202681743931
-California      STATE_OR_PROVINCE       LOCATION:0.9990868267559281
-
-Extracted the following KBP triples:
-1.0     Joe Smith       per:statesorprovinces_of_residence      California
-"""
-
-
-EN_DOC_POS_ONLY_GOLD = """
-Sentence #1 (6 tokens):
-Joe Smith lives in California.
-
-Tokens:
-[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP]
-[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP]
-[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ]
-[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN]
-[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP]
-[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=.]
-"""
-
-def test_english_request():
-    """ Test case of starting server with Spanish defaults, and then requesting default English properties """
-    with corenlp.CoreNLPClient(properties='spanish', server_id='test_spanish_english_request') as client:
-        ann = client.annotate(EN_DOC, properties='english', output_format='text')
-        compare_ignoring_whitespace(ann, EN_DOC_GOLD)
-
-    # Rerun the test with a server created in English mode to verify
-    # that the expected output is what the defaults actually give us
-    with corenlp.CoreNLPClient(properties='english', server_id='test_english_request') as client:
-        ann = client.annotate(EN_DOC, output_format='text')
-        compare_ignoring_whitespace(ann, EN_DOC_GOLD)
-
-
-def test_default_annotators():
-    """
-    Test case of creating a client with start_server=False and a set of annotators
-    The annotators should be used instead of the server's default annotators
-    """
-    with corenlp.CoreNLPClient(server_id='test_default_annotators',
-                               output_format='text',
-                               annotators=['tokenize','ssplit','pos','lemma','ner','depparse']) as client:
-        with corenlp.CoreNLPClient(start_server=False,
-                                   output_format='text',
-                                   annotators=['tokenize','ssplit','pos']) as client2:
-            ann = client2.annotate(EN_DOC)
-
-expected_codepoints = ((0, 1), (2, 4), (5, 8), (9, 15), (16, 20))
-expected_characters = ((0, 1), (2, 4), (5, 10), (11, 17), (18, 22))
-codepoint_doc = "I am 𝒚̂𝒊 random text"
-
-def test_codepoints():
-    """ Test case of asking for codepoints from the English tokenizer """
-    with corenlp.CoreNLPClient(annotators=['tokenize','ssplit'], # 'depparse','coref'],
-                               properties={'tokenize.codepoint': 'true'}) as client:
-        ann = client.annotate(codepoint_doc)
-        for i, (codepoints, characters) in enumerate(zip(expected_codepoints, expected_characters)):
-            token = ann.sentence[0].token[i]
-            assert token.codepointOffsetBegin == codepoints[0]
-            assert token.codepointOffsetEnd == codepoints[1]
-            assert token.beginChar == characters[0]
-            assert token.endChar == characters[1]
+"""
+Misc tests for the server
+"""
+
+import pytest
+import re
+import stanza.server as corenlp
+from tests import compare_ignoring_whitespace
+
+pytestmark = pytest.mark.client
+
+EN_DOC = "Joe Smith lives in California."
+
+EN_DOC_GOLD = """
+Sentence #1 (6 tokens):
+Joe Smith lives in California.
+
+Tokens:
+[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP Lemma=Joe NamedEntityTag=PERSON]
+[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP Lemma=Smith NamedEntityTag=PERSON]
+[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ Lemma=live NamedEntityTag=O]
+[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN Lemma=in NamedEntityTag=O]
+[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP Lemma=California NamedEntityTag=STATE_OR_PROVINCE]
+[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=. Lemma=. NamedEntityTag=O]
+
+Dependency Parse (enhanced plus plus dependencies):
+root(ROOT-0, lives-3)
+compound(Smith-2, Joe-1)
+nsubj(lives-3, Smith-2)
+case(California-5, in-4)
+obl:in(lives-3, California-5)
+punct(lives-3, .-6)
+
+Extracted the following NER entity mentions:
+Joe Smith       PERSON  PERSON:0.9972202681743931
+California      STATE_OR_PROVINCE       LOCATION:0.9990868267559281
+
+Extracted the following KBP triples:
+1.0     Joe Smith       per:statesorprovinces_of_residence      California
+"""
+
+
+EN_DOC_POS_ONLY_GOLD = """
+Sentence #1 (6 tokens):
+Joe Smith lives in California.
+
+Tokens:
+[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP]
+[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP]
+[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ]
+[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN]
+[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP]
+[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=.]
+"""
+
+def test_english_request():
+    """ Test case of starting server with Spanish defaults, and then requesting default English properties """
+    with corenlp.CoreNLPClient(properties='spanish', server_id='test_spanish_english_request') as client:
+        ann = client.annotate(EN_DOC, properties='english', output_format='text')
+        compare_ignoring_whitespace(ann, EN_DOC_GOLD)
+
+    # Rerun the test with a server created in English mode to verify
+    # that the expected output is what the defaults actually give us
+    with corenlp.CoreNLPClient(properties='english', server_id='test_english_request') as client:
+        ann = client.annotate(EN_DOC, output_format='text')
+        compare_ignoring_whitespace(ann, EN_DOC_GOLD)
+
+
+def test_default_annotators():
+    """
+    Test case of creating a client with start_server=False and a set of annotators
+    The annotators should be used instead of the server's default annotators
+    """
+    with corenlp.CoreNLPClient(server_id='test_default_annotators',
+                               output_format='text',
+                               annotators=['tokenize','ssplit','pos','lemma','ner','depparse']) as client:
+        with corenlp.CoreNLPClient(start_server=False,
+                                   output_format='text',
+                                   annotators=['tokenize','ssplit','pos']) as client2:
+            ann = client2.annotate(EN_DOC)
+
+expected_codepoints = ((0, 1), (2, 4), (5, 8), (9, 15), (16, 20))
+expected_characters = ((0, 1), (2, 4), (5, 10), (11, 17), (18, 22))
+codepoint_doc = "I am 𝒚̂𝒊 random text"
+
+def test_codepoints():
+    """ Test case of asking for codepoints from the English tokenizer """
+    with corenlp.CoreNLPClient(annotators=['tokenize','ssplit'], # 'depparse','coref'],
+                               properties={'tokenize.codepoint': 'true'}) as client:
+        ann = client.annotate(codepoint_doc)
+        for i, (codepoints, characters) in enumerate(zip(expected_codepoints, expected_characters)):
+            token = ann.sentence[0].token[i]
+            assert token.codepointOffsetBegin == codepoints[0]
+            assert token.codepointOffsetEnd == codepoints[1]
+            assert token.beginChar == characters[0]
+            assert token.endChar == characters[1]
```

### Comparing `classla-2.0/tests/test_server_request.py` & `classla-2.1/tests/test_server_request.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,223 +1,223 @@
-"""
-Tests for setting request properties of servers
-"""
-
-import json
-import pytest
-import classla.server as corenlp
-
-from classla.protobuf import Document
-from tests import TEST_WORKING_DIR, compare_ignoring_whitespace
-
-pytestmark = pytest.mark.client
-
-EN_DOC = "Joe Smith lives in California."
-
-# results with an example properties file
-EN_DOC_GOLD = """
-Sentence #1 (6 tokens):
-Joe Smith lives in California.
-
-Tokens:
-[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP]
-[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP]
-[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ]
-[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN]
-[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP]
-[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=.]
-"""
-
-GERMAN_DOC = "Angela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland."
-
-GERMAN_DOC_GOLD = """
-Sentence #1 (10 tokens):
-Angela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland.
-
-Tokens:
-[Text=Angela CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=PROPN]
-[Text=Merkel CharacterOffsetBegin=7 CharacterOffsetEnd=13 PartOfSpeech=PROPN]
-[Text=ist CharacterOffsetBegin=14 CharacterOffsetEnd=17 PartOfSpeech=AUX]
-[Text=seit CharacterOffsetBegin=18 CharacterOffsetEnd=22 PartOfSpeech=ADP]
-[Text=2005 CharacterOffsetBegin=23 CharacterOffsetEnd=27 PartOfSpeech=NUM]
-[Text=Bundeskanzlerin CharacterOffsetBegin=28 CharacterOffsetEnd=43 PartOfSpeech=NOUN]
-[Text=der CharacterOffsetBegin=44 CharacterOffsetEnd=47 PartOfSpeech=DET]
-[Text=Bundesrepublik CharacterOffsetBegin=48 CharacterOffsetEnd=62 PartOfSpeech=PROPN]
-[Text=Deutschland CharacterOffsetBegin=63 CharacterOffsetEnd=74 PartOfSpeech=PROPN]
-[Text=. CharacterOffsetBegin=74 CharacterOffsetEnd=75 PartOfSpeech=PUNCT]
-"""
-
-FRENCH_CUSTOM_PROPS = {'annotators': 'tokenize,ssplit,mwt,pos,parse',
-                       'tokenize.language': 'fr',
-                       'pos.model': 'edu/stanford/nlp/models/pos-tagger/french-ud.tagger',
-                       'parse.model': 'edu/stanford/nlp/models/srparser/frenchSR.ser.gz',
-                       'mwt.mappingFile': 'edu/stanford/nlp/models/mwt/french/french-mwt.tsv',
-                       'mwt.pos.model': 'edu/stanford/nlp/models/mwt/french/french-mwt.tagger',
-                       'mwt.statisticalMappingFile': 'edu/stanford/nlp/models/mwt/french/french-mwt-statistical.tsv',
-                       'mwt.preserveCasing': 'false',
-                       'outputFormat': 'text'}
-
-FRENCH_EXTRA_PROPS = {'annotators': 'tokenize,ssplit,mwt,pos,depparse',
-                      'tokenize.language': 'fr',
-                      'pos.model': 'edu/stanford/nlp/models/pos-tagger/french-ud.tagger',
-                      'mwt.mappingFile': 'edu/stanford/nlp/models/mwt/french/french-mwt.tsv',
-                      'mwt.pos.model': 'edu/stanford/nlp/models/mwt/french/french-mwt.tagger',
-                      'mwt.statisticalMappingFile': 'edu/stanford/nlp/models/mwt/french/french-mwt-statistical.tsv',
-                      'mwt.preserveCasing': 'false',
-                      'depparse.model': 'edu/stanford/nlp/models/parser/nndep/UD_French.gz'}
-
-FRENCH_DOC = "Cette enquête préliminaire fait suite aux révélations de l’hebdomadaire quelques jours plus tôt."
-
-FRENCH_CUSTOM_GOLD = """
-Sentence #1 (16 tokens):
-Cette enquête préliminaire fait suite aux révélations de l’hebdomadaire quelques jours plus tôt.
-
-Tokens:
-[Text=Cette CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=DET]
-[Text=enquête CharacterOffsetBegin=6 CharacterOffsetEnd=13 PartOfSpeech=NOUN]
-[Text=préliminaire CharacterOffsetBegin=14 CharacterOffsetEnd=26 PartOfSpeech=ADJ]
-[Text=fait CharacterOffsetBegin=27 CharacterOffsetEnd=31 PartOfSpeech=VERB]
-[Text=suite CharacterOffsetBegin=32 CharacterOffsetEnd=37 PartOfSpeech=NOUN]
-[Text=à CharacterOffsetBegin=38 CharacterOffsetEnd=41 PartOfSpeech=ADP]
-[Text=les CharacterOffsetBegin=38 CharacterOffsetEnd=41 PartOfSpeech=DET]
-[Text=révélations CharacterOffsetBegin=42 CharacterOffsetEnd=53 PartOfSpeech=NOUN]
-[Text=de CharacterOffsetBegin=54 CharacterOffsetEnd=56 PartOfSpeech=ADP]
-[Text=l’ CharacterOffsetBegin=57 CharacterOffsetEnd=59 PartOfSpeech=NOUN]
-[Text=hebdomadaire CharacterOffsetBegin=59 CharacterOffsetEnd=71 PartOfSpeech=ADJ]
-[Text=quelques CharacterOffsetBegin=72 CharacterOffsetEnd=80 PartOfSpeech=DET]
-[Text=jours CharacterOffsetBegin=81 CharacterOffsetEnd=86 PartOfSpeech=NOUN]
-[Text=plus CharacterOffsetBegin=87 CharacterOffsetEnd=91 PartOfSpeech=ADV]
-[Text=tôt CharacterOffsetBegin=92 CharacterOffsetEnd=95 PartOfSpeech=ADV]
-[Text=. CharacterOffsetBegin=95 CharacterOffsetEnd=96 PartOfSpeech=PUNCT]
-
-Constituency parse: 
-(ROOT
-  (SENT
-    (NP (DET Cette)
-      (MWN (NOUN enquête) (ADJ préliminaire)))
-    (VN
-      (MWV (VERB fait) (NOUN suite)))
-    (PP (ADP à)
-      (NP (DET les) (NOUN révélations)
-        (PP (ADP de)
-          (NP (NOUN l’)
-            (AP (ADJ hebdomadaire))))))
-    (NP (DET quelques) (NOUN jours))
-    (AdP (ADV plus) (ADV tôt))
-    (PUNCT .)))
-"""
-
-FRENCH_EXTRA_GOLD = """
-Sentence #1 (16 tokens):
-Cette enquête préliminaire fait suite aux révélations de l’hebdomadaire quelques jours plus tôt.
-
-Tokens:
-[Text=Cette CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=DET]
-[Text=enquête CharacterOffsetBegin=6 CharacterOffsetEnd=13 PartOfSpeech=NOUN]
-[Text=préliminaire CharacterOffsetBegin=14 CharacterOffsetEnd=26 PartOfSpeech=ADJ]
-[Text=fait CharacterOffsetBegin=27 CharacterOffsetEnd=31 PartOfSpeech=VERB]
-[Text=suite CharacterOffsetBegin=32 CharacterOffsetEnd=37 PartOfSpeech=NOUN]
-[Text=à CharacterOffsetBegin=38 CharacterOffsetEnd=41 PartOfSpeech=ADP]
-[Text=les CharacterOffsetBegin=38 CharacterOffsetEnd=41 PartOfSpeech=DET]
-[Text=révélations CharacterOffsetBegin=42 CharacterOffsetEnd=53 PartOfSpeech=NOUN]
-[Text=de CharacterOffsetBegin=54 CharacterOffsetEnd=56 PartOfSpeech=ADP]
-[Text=l’ CharacterOffsetBegin=57 CharacterOffsetEnd=59 PartOfSpeech=NOUN]
-[Text=hebdomadaire CharacterOffsetBegin=59 CharacterOffsetEnd=71 PartOfSpeech=ADJ]
-[Text=quelques CharacterOffsetBegin=72 CharacterOffsetEnd=80 PartOfSpeech=DET]
-[Text=jours CharacterOffsetBegin=81 CharacterOffsetEnd=86 PartOfSpeech=NOUN]
-[Text=plus CharacterOffsetBegin=87 CharacterOffsetEnd=91 PartOfSpeech=ADV]
-[Text=tôt CharacterOffsetBegin=92 CharacterOffsetEnd=95 PartOfSpeech=ADV]
-[Text=. CharacterOffsetBegin=95 CharacterOffsetEnd=96 PartOfSpeech=PUNCT]
-
-Dependency Parse (enhanced plus plus dependencies):
-root(ROOT-0, fait-4)
-det(enquête-2, Cette-1)
-nsubj(fait-4, enquête-2)
-amod(enquête-2, préliminaire-3)
-obj(fait-4, suite-5)
-case(révélations-8, à-6)
-det(révélations-8, les-7)
-obl:à(fait-4, révélations-8)
-case(l’-10, de-9)
-nmod:de(révélations-8, l’-10)
-amod(révélations-8, hebdomadaire-11)
-det(jours-13, quelques-12)
-obl(fait-4, jours-13)
-advmod(tôt-15, plus-14)
-advmod(jours-13, tôt-15)
-punct(fait-4, .-16)
-"""
-
-FRENCH_JSON_GOLD = json.loads(open(f'{TEST_WORKING_DIR}/out/example_french.json').read())
-
-ES_DOC = 'Andrés Manuel López Obrador es el presidente de México.'
-
-ES_PROPS = {'annotators': 'tokenize,ssplit,mwt,pos,depparse', 'tokenize.language': 'es',
-            'pos.model': 'edu/stanford/nlp/models/pos-tagger/spanish-ud.tagger',
-            'mwt.mappingFile': 'edu/stanford/nlp/models/mwt/spanish/spanish-mwt.tsv',
-            'depparse.model': 'edu/stanford/nlp/models/parser/nndep/UD_Spanish.gz'}
-
-ES_PROPS_GOLD = """
-Sentence #1 (10 tokens):
-Andrés Manuel López Obrador es el presidente de México.
-
-Tokens:
-[Text=Andrés CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=PROPN]
-[Text=Manuel CharacterOffsetBegin=7 CharacterOffsetEnd=13 PartOfSpeech=PROPN]
-[Text=López CharacterOffsetBegin=14 CharacterOffsetEnd=19 PartOfSpeech=PROPN]
-[Text=Obrador CharacterOffsetBegin=20 CharacterOffsetEnd=27 PartOfSpeech=PROPN]
-[Text=es CharacterOffsetBegin=28 CharacterOffsetEnd=30 PartOfSpeech=AUX]
-[Text=el CharacterOffsetBegin=31 CharacterOffsetEnd=33 PartOfSpeech=DET]
-[Text=presidente CharacterOffsetBegin=34 CharacterOffsetEnd=44 PartOfSpeech=NOUN]
-[Text=de CharacterOffsetBegin=45 CharacterOffsetEnd=47 PartOfSpeech=ADP]
-[Text=México CharacterOffsetBegin=48 CharacterOffsetEnd=54 PartOfSpeech=PROPN]
-[Text=. CharacterOffsetBegin=54 CharacterOffsetEnd=55 PartOfSpeech=PUNCT]
-
-Dependency Parse (enhanced plus plus dependencies):
-root(ROOT-0, presidente-7)
-nsubj(presidente-7, Andrés-1)
-flat(Andrés-1, Manuel-2)
-flat(Andrés-1, López-3)
-flat(Andrés-1, Obrador-4)
-cop(presidente-7, es-5)
-det(presidente-7, el-6)
-case(México-9, de-8)
-nmod:de(presidente-7, México-9)
-punct(presidente-7, .-10)
-"""
-
-
-@pytest.fixture(scope="module")
-def corenlp_client():
-    """ Client to run tests on """
-    client = corenlp.CoreNLPClient(annotators='tokenize,ssplit,pos', server_id='stanza_request_tests_server')
-    yield client
-    client.stop()
-
-
-def test_basic(corenlp_client):
-    """ Basic test of making a request, test default output format is a Document """
-    ann = corenlp_client.annotate(EN_DOC, output_format="text")
-    assert ann.strip() == EN_DOC_GOLD.strip()
-    ann = corenlp_client.annotate(EN_DOC)
-    assert isinstance(ann, Document)
-
-
-def test_python_dict(corenlp_client):
-    """ Test using a Python dictionary to specify all request properties """
-    ann = corenlp_client.annotate(ES_DOC, properties=ES_PROPS, output_format="text")
-    assert ann.strip() == ES_PROPS_GOLD.strip()
-    ann = corenlp_client.annotate(FRENCH_DOC, properties=FRENCH_CUSTOM_PROPS)
-    assert ann.strip() == FRENCH_CUSTOM_GOLD.strip()
-
-
-def test_lang_setting(corenlp_client):
-    """ Test using a Stanford CoreNLP supported languages as a properties key """
-    ann = corenlp_client.annotate(GERMAN_DOC, properties="german", output_format="text")
-    compare_ignoring_whitespace(ann, GERMAN_DOC_GOLD)
-
-
-def test_annotators_and_output_format(corenlp_client):
-    """ Test setting the annotators and output_format """
-    ann = corenlp_client.annotate(FRENCH_DOC, properties=FRENCH_EXTRA_PROPS,
-                                  annotators="tokenize,ssplit,mwt,pos", output_format="json")
-    assert FRENCH_JSON_GOLD == ann
+"""
+Tests for setting request properties of servers
+"""
+
+import json
+import pytest
+import classla.server as corenlp
+
+from classla.protobuf import Document
+from tests import TEST_WORKING_DIR, compare_ignoring_whitespace
+
+pytestmark = pytest.mark.client
+
+EN_DOC = "Joe Smith lives in California."
+
+# results with an example properties file
+EN_DOC_GOLD = """
+Sentence #1 (6 tokens):
+Joe Smith lives in California.
+
+Tokens:
+[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP]
+[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP]
+[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ]
+[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN]
+[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP]
+[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=.]
+"""
+
+GERMAN_DOC = "Angela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland."
+
+GERMAN_DOC_GOLD = """
+Sentence #1 (10 tokens):
+Angela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland.
+
+Tokens:
+[Text=Angela CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=PROPN]
+[Text=Merkel CharacterOffsetBegin=7 CharacterOffsetEnd=13 PartOfSpeech=PROPN]
+[Text=ist CharacterOffsetBegin=14 CharacterOffsetEnd=17 PartOfSpeech=AUX]
+[Text=seit CharacterOffsetBegin=18 CharacterOffsetEnd=22 PartOfSpeech=ADP]
+[Text=2005 CharacterOffsetBegin=23 CharacterOffsetEnd=27 PartOfSpeech=NUM]
+[Text=Bundeskanzlerin CharacterOffsetBegin=28 CharacterOffsetEnd=43 PartOfSpeech=NOUN]
+[Text=der CharacterOffsetBegin=44 CharacterOffsetEnd=47 PartOfSpeech=DET]
+[Text=Bundesrepublik CharacterOffsetBegin=48 CharacterOffsetEnd=62 PartOfSpeech=PROPN]
+[Text=Deutschland CharacterOffsetBegin=63 CharacterOffsetEnd=74 PartOfSpeech=PROPN]
+[Text=. CharacterOffsetBegin=74 CharacterOffsetEnd=75 PartOfSpeech=PUNCT]
+"""
+
+FRENCH_CUSTOM_PROPS = {'annotators': 'tokenize,ssplit,mwt,pos,parse',
+                       'tokenize.language': 'fr',
+                       'pos.model': 'edu/stanford/nlp/models/pos-tagger/french-ud.tagger',
+                       'parse.model': 'edu/stanford/nlp/models/srparser/frenchSR.ser.gz',
+                       'mwt.mappingFile': 'edu/stanford/nlp/models/mwt/french/french-mwt.tsv',
+                       'mwt.pos.model': 'edu/stanford/nlp/models/mwt/french/french-mwt.tagger',
+                       'mwt.statisticalMappingFile': 'edu/stanford/nlp/models/mwt/french/french-mwt-statistical.tsv',
+                       'mwt.preserveCasing': 'false',
+                       'outputFormat': 'text'}
+
+FRENCH_EXTRA_PROPS = {'annotators': 'tokenize,ssplit,mwt,pos,depparse',
+                      'tokenize.language': 'fr',
+                      'pos.model': 'edu/stanford/nlp/models/pos-tagger/french-ud.tagger',
+                      'mwt.mappingFile': 'edu/stanford/nlp/models/mwt/french/french-mwt.tsv',
+                      'mwt.pos.model': 'edu/stanford/nlp/models/mwt/french/french-mwt.tagger',
+                      'mwt.statisticalMappingFile': 'edu/stanford/nlp/models/mwt/french/french-mwt-statistical.tsv',
+                      'mwt.preserveCasing': 'false',
+                      'depparse.model': 'edu/stanford/nlp/models/parser/nndep/UD_French.gz'}
+
+FRENCH_DOC = "Cette enquête préliminaire fait suite aux révélations de l’hebdomadaire quelques jours plus tôt."
+
+FRENCH_CUSTOM_GOLD = """
+Sentence #1 (16 tokens):
+Cette enquête préliminaire fait suite aux révélations de l’hebdomadaire quelques jours plus tôt.
+
+Tokens:
+[Text=Cette CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=DET]
+[Text=enquête CharacterOffsetBegin=6 CharacterOffsetEnd=13 PartOfSpeech=NOUN]
+[Text=préliminaire CharacterOffsetBegin=14 CharacterOffsetEnd=26 PartOfSpeech=ADJ]
+[Text=fait CharacterOffsetBegin=27 CharacterOffsetEnd=31 PartOfSpeech=VERB]
+[Text=suite CharacterOffsetBegin=32 CharacterOffsetEnd=37 PartOfSpeech=NOUN]
+[Text=à CharacterOffsetBegin=38 CharacterOffsetEnd=41 PartOfSpeech=ADP]
+[Text=les CharacterOffsetBegin=38 CharacterOffsetEnd=41 PartOfSpeech=DET]
+[Text=révélations CharacterOffsetBegin=42 CharacterOffsetEnd=53 PartOfSpeech=NOUN]
+[Text=de CharacterOffsetBegin=54 CharacterOffsetEnd=56 PartOfSpeech=ADP]
+[Text=l’ CharacterOffsetBegin=57 CharacterOffsetEnd=59 PartOfSpeech=NOUN]
+[Text=hebdomadaire CharacterOffsetBegin=59 CharacterOffsetEnd=71 PartOfSpeech=ADJ]
+[Text=quelques CharacterOffsetBegin=72 CharacterOffsetEnd=80 PartOfSpeech=DET]
+[Text=jours CharacterOffsetBegin=81 CharacterOffsetEnd=86 PartOfSpeech=NOUN]
+[Text=plus CharacterOffsetBegin=87 CharacterOffsetEnd=91 PartOfSpeech=ADV]
+[Text=tôt CharacterOffsetBegin=92 CharacterOffsetEnd=95 PartOfSpeech=ADV]
+[Text=. CharacterOffsetBegin=95 CharacterOffsetEnd=96 PartOfSpeech=PUNCT]
+
+Constituency parse: 
+(ROOT
+  (SENT
+    (NP (DET Cette)
+      (MWN (NOUN enquête) (ADJ préliminaire)))
+    (VN
+      (MWV (VERB fait) (NOUN suite)))
+    (PP (ADP à)
+      (NP (DET les) (NOUN révélations)
+        (PP (ADP de)
+          (NP (NOUN l’)
+            (AP (ADJ hebdomadaire))))))
+    (NP (DET quelques) (NOUN jours))
+    (AdP (ADV plus) (ADV tôt))
+    (PUNCT .)))
+"""
+
+FRENCH_EXTRA_GOLD = """
+Sentence #1 (16 tokens):
+Cette enquête préliminaire fait suite aux révélations de l’hebdomadaire quelques jours plus tôt.
+
+Tokens:
+[Text=Cette CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=DET]
+[Text=enquête CharacterOffsetBegin=6 CharacterOffsetEnd=13 PartOfSpeech=NOUN]
+[Text=préliminaire CharacterOffsetBegin=14 CharacterOffsetEnd=26 PartOfSpeech=ADJ]
+[Text=fait CharacterOffsetBegin=27 CharacterOffsetEnd=31 PartOfSpeech=VERB]
+[Text=suite CharacterOffsetBegin=32 CharacterOffsetEnd=37 PartOfSpeech=NOUN]
+[Text=à CharacterOffsetBegin=38 CharacterOffsetEnd=41 PartOfSpeech=ADP]
+[Text=les CharacterOffsetBegin=38 CharacterOffsetEnd=41 PartOfSpeech=DET]
+[Text=révélations CharacterOffsetBegin=42 CharacterOffsetEnd=53 PartOfSpeech=NOUN]
+[Text=de CharacterOffsetBegin=54 CharacterOffsetEnd=56 PartOfSpeech=ADP]
+[Text=l’ CharacterOffsetBegin=57 CharacterOffsetEnd=59 PartOfSpeech=NOUN]
+[Text=hebdomadaire CharacterOffsetBegin=59 CharacterOffsetEnd=71 PartOfSpeech=ADJ]
+[Text=quelques CharacterOffsetBegin=72 CharacterOffsetEnd=80 PartOfSpeech=DET]
+[Text=jours CharacterOffsetBegin=81 CharacterOffsetEnd=86 PartOfSpeech=NOUN]
+[Text=plus CharacterOffsetBegin=87 CharacterOffsetEnd=91 PartOfSpeech=ADV]
+[Text=tôt CharacterOffsetBegin=92 CharacterOffsetEnd=95 PartOfSpeech=ADV]
+[Text=. CharacterOffsetBegin=95 CharacterOffsetEnd=96 PartOfSpeech=PUNCT]
+
+Dependency Parse (enhanced plus plus dependencies):
+root(ROOT-0, fait-4)
+det(enquête-2, Cette-1)
+nsubj(fait-4, enquête-2)
+amod(enquête-2, préliminaire-3)
+obj(fait-4, suite-5)
+case(révélations-8, à-6)
+det(révélations-8, les-7)
+obl:à(fait-4, révélations-8)
+case(l’-10, de-9)
+nmod:de(révélations-8, l’-10)
+amod(révélations-8, hebdomadaire-11)
+det(jours-13, quelques-12)
+obl(fait-4, jours-13)
+advmod(tôt-15, plus-14)
+advmod(jours-13, tôt-15)
+punct(fait-4, .-16)
+"""
+
+FRENCH_JSON_GOLD = json.loads(open(f'{TEST_WORKING_DIR}/out/example_french.json').read())
+
+ES_DOC = 'Andrés Manuel López Obrador es el presidente de México.'
+
+ES_PROPS = {'annotators': 'tokenize,ssplit,mwt,pos,depparse', 'tokenize.language': 'es',
+            'pos.model': 'edu/stanford/nlp/models/pos-tagger/spanish-ud.tagger',
+            'mwt.mappingFile': 'edu/stanford/nlp/models/mwt/spanish/spanish-mwt.tsv',
+            'depparse.model': 'edu/stanford/nlp/models/parser/nndep/UD_Spanish.gz'}
+
+ES_PROPS_GOLD = """
+Sentence #1 (10 tokens):
+Andrés Manuel López Obrador es el presidente de México.
+
+Tokens:
+[Text=Andrés CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=PROPN]
+[Text=Manuel CharacterOffsetBegin=7 CharacterOffsetEnd=13 PartOfSpeech=PROPN]
+[Text=López CharacterOffsetBegin=14 CharacterOffsetEnd=19 PartOfSpeech=PROPN]
+[Text=Obrador CharacterOffsetBegin=20 CharacterOffsetEnd=27 PartOfSpeech=PROPN]
+[Text=es CharacterOffsetBegin=28 CharacterOffsetEnd=30 PartOfSpeech=AUX]
+[Text=el CharacterOffsetBegin=31 CharacterOffsetEnd=33 PartOfSpeech=DET]
+[Text=presidente CharacterOffsetBegin=34 CharacterOffsetEnd=44 PartOfSpeech=NOUN]
+[Text=de CharacterOffsetBegin=45 CharacterOffsetEnd=47 PartOfSpeech=ADP]
+[Text=México CharacterOffsetBegin=48 CharacterOffsetEnd=54 PartOfSpeech=PROPN]
+[Text=. CharacterOffsetBegin=54 CharacterOffsetEnd=55 PartOfSpeech=PUNCT]
+
+Dependency Parse (enhanced plus plus dependencies):
+root(ROOT-0, presidente-7)
+nsubj(presidente-7, Andrés-1)
+flat(Andrés-1, Manuel-2)
+flat(Andrés-1, López-3)
+flat(Andrés-1, Obrador-4)
+cop(presidente-7, es-5)
+det(presidente-7, el-6)
+case(México-9, de-8)
+nmod:de(presidente-7, México-9)
+punct(presidente-7, .-10)
+"""
+
+
+@pytest.fixture(scope="module")
+def corenlp_client():
+    """ Client to run tests on """
+    client = corenlp.CoreNLPClient(annotators='tokenize,ssplit,pos', server_id='stanza_request_tests_server')
+    yield client
+    client.stop()
+
+
+def test_basic(corenlp_client):
+    """ Basic test of making a request, test default output format is a Document """
+    ann = corenlp_client.annotate(EN_DOC, output_format="text")
+    assert ann.strip() == EN_DOC_GOLD.strip()
+    ann = corenlp_client.annotate(EN_DOC)
+    assert isinstance(ann, Document)
+
+
+def test_python_dict(corenlp_client):
+    """ Test using a Python dictionary to specify all request properties """
+    ann = corenlp_client.annotate(ES_DOC, properties=ES_PROPS, output_format="text")
+    assert ann.strip() == ES_PROPS_GOLD.strip()
+    ann = corenlp_client.annotate(FRENCH_DOC, properties=FRENCH_CUSTOM_PROPS)
+    assert ann.strip() == FRENCH_CUSTOM_GOLD.strip()
+
+
+def test_lang_setting(corenlp_client):
+    """ Test using a Stanford CoreNLP supported languages as a properties key """
+    ann = corenlp_client.annotate(GERMAN_DOC, properties="german", output_format="text")
+    compare_ignoring_whitespace(ann, GERMAN_DOC_GOLD)
+
+
+def test_annotators_and_output_format(corenlp_client):
+    """ Test setting the annotators and output_format """
+    ann = corenlp_client.annotate(FRENCH_DOC, properties=FRENCH_EXTRA_PROPS,
+                                  annotators="tokenize,ssplit,mwt,pos", output_format="json")
+    assert FRENCH_JSON_GOLD == ann
```

### Comparing `classla-2.0/tests/test_server_start.py` & `classla-2.1/tests/test_server_start.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,214 +1,214 @@
-"""
-Tests for starting a server in Python code
-"""
-
-import pytest
-import classla.server as corenlp
-from classla.server.client import AnnotationException
-import time
-
-from tests import *
-
-pytestmark = pytest.mark.client
-
-EN_DOC = "Joe Smith lives in California."
-
-# results on EN_DOC with standard StanfordCoreNLP defaults
-EN_PRELOAD_GOLD = """
-Sentence #1 (6 tokens):
-Joe Smith lives in California.
-
-Tokens:
-[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP Lemma=Joe NamedEntityTag=PERSON]
-[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP Lemma=Smith NamedEntityTag=PERSON]
-[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ Lemma=live NamedEntityTag=O]
-[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN Lemma=in NamedEntityTag=O]
-[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP Lemma=California NamedEntityTag=STATE_OR_PROVINCE]
-[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=. Lemma=. NamedEntityTag=O]
-
-Dependency Parse (enhanced plus plus dependencies):
-root(ROOT-0, lives-3)
-compound(Smith-2, Joe-1)
-nsubj(lives-3, Smith-2)
-case(California-5, in-4)
-obl:in(lives-3, California-5)
-punct(lives-3, .-6)
-
-Extracted the following NER entity mentions:
-Joe Smith       PERSON              PERSON:0.9972202681743931
-California      STATE_OR_PROVINCE   LOCATION:0.9990868267559281
-
-Extracted the following KBP triples:
-1.0 Joe Smith per:statesorprovinces_of_residence California
-"""
-
-# results with an example properties file
-EN_PROPS_FILE_GOLD = """
-Sentence #1 (6 tokens):
-Joe Smith lives in California.
-
-Tokens:
-[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP]
-[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP]
-[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ]
-[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN]
-[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP]
-[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=.]
-"""
-
-GERMAN_DOC = "Angela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland."
-
-# results with standard German properties
-GERMAN_FULL_PROPS_GOLD = """
-Sentence #1 (10 tokens):
-Angela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland.
-
-Tokens:
-[Text=Angela CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=PROPN Lemma=angela NamedEntityTag=PERSON]
-[Text=Merkel CharacterOffsetBegin=7 CharacterOffsetEnd=13 PartOfSpeech=PROPN Lemma=merkel NamedEntityTag=PERSON]
-[Text=ist CharacterOffsetBegin=14 CharacterOffsetEnd=17 PartOfSpeech=AUX Lemma=ist NamedEntityTag=O]
-[Text=seit CharacterOffsetBegin=18 CharacterOffsetEnd=22 PartOfSpeech=ADP Lemma=seit NamedEntityTag=O]
-[Text=2005 CharacterOffsetBegin=23 CharacterOffsetEnd=27 PartOfSpeech=NUM Lemma=2005 NamedEntityTag=O]
-[Text=Bundeskanzlerin CharacterOffsetBegin=28 CharacterOffsetEnd=43 PartOfSpeech=NOUN Lemma=bundeskanzlerin NamedEntityTag=O]
-[Text=der CharacterOffsetBegin=44 CharacterOffsetEnd=47 PartOfSpeech=DET Lemma=der NamedEntityTag=O]
-[Text=Bundesrepublik CharacterOffsetBegin=48 CharacterOffsetEnd=62 PartOfSpeech=PROPN Lemma=bundesrepublik NamedEntityTag=LOCATION]
-[Text=Deutschland CharacterOffsetBegin=63 CharacterOffsetEnd=74 PartOfSpeech=PROPN Lemma=deutschland NamedEntityTag=LOCATION]
-[Text=. CharacterOffsetBegin=74 CharacterOffsetEnd=75 PartOfSpeech=PUNCT Lemma=. NamedEntityTag=O]
-
-Dependency Parse (enhanced plus plus dependencies):
-root(ROOT-0, Bundeskanzlerin-6)
-nsubj(Bundeskanzlerin-6, Angela-1)
-flat(Angela-1, Merkel-2)
-cop(Bundeskanzlerin-6, ist-3)
-case(2005-5, seit-4)
-nmod:seit(Bundeskanzlerin-6, 2005-5)
-det(Bundesrepublik-8, der-7)
-nmod(Bundeskanzlerin-6, Bundesrepublik-8)
-appos(Bundesrepublik-8, Deutschland-9)
-punct(Bundeskanzlerin-6, .-10)
-
-Extracted the following NER entity mentions:
-Angela Merkel              PERSON   PERSON:0.9999981583351504
-Bundesrepublik Deutschland LOCATION LOCATION:0.9682902289749544
-"""
-
-
-GERMAN_SMALL_PROPS = {'annotators': 'tokenize,ssplit,pos', 'tokenize.language': 'de',
-                      'pos.model': 'edu/stanford/nlp/models/pos-tagger/german-ud.tagger'}
-
-# results with custom Python dictionary set properties
-GERMAN_SMALL_PROPS_GOLD = """
-Sentence #1 (10 tokens):
-Angela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland.
-
-Tokens:
-[Text=Angela CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=PROPN]
-[Text=Merkel CharacterOffsetBegin=7 CharacterOffsetEnd=13 PartOfSpeech=PROPN]
-[Text=ist CharacterOffsetBegin=14 CharacterOffsetEnd=17 PartOfSpeech=AUX]
-[Text=seit CharacterOffsetBegin=18 CharacterOffsetEnd=22 PartOfSpeech=ADP]
-[Text=2005 CharacterOffsetBegin=23 CharacterOffsetEnd=27 PartOfSpeech=NUM]
-[Text=Bundeskanzlerin CharacterOffsetBegin=28 CharacterOffsetEnd=43 PartOfSpeech=NOUN]
-[Text=der CharacterOffsetBegin=44 CharacterOffsetEnd=47 PartOfSpeech=DET]
-[Text=Bundesrepublik CharacterOffsetBegin=48 CharacterOffsetEnd=62 PartOfSpeech=PROPN]
-[Text=Deutschland CharacterOffsetBegin=63 CharacterOffsetEnd=74 PartOfSpeech=PROPN]
-[Text=. CharacterOffsetBegin=74 CharacterOffsetEnd=75 PartOfSpeech=PUNCT]
-"""
-
-# results with custom Python dictionary set properties and annotators=tokenize,ssplit
-GERMAN_SMALL_PROPS_W_ANNOTATORS_GOLD = """
-Sentence #1 (10 tokens):
-Angela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland.
-
-Tokens:
-[Text=Angela CharacterOffsetBegin=0 CharacterOffsetEnd=6]
-[Text=Merkel CharacterOffsetBegin=7 CharacterOffsetEnd=13]
-[Text=ist CharacterOffsetBegin=14 CharacterOffsetEnd=17]
-[Text=seit CharacterOffsetBegin=18 CharacterOffsetEnd=22]
-[Text=2005 CharacterOffsetBegin=23 CharacterOffsetEnd=27]
-[Text=Bundeskanzlerin CharacterOffsetBegin=28 CharacterOffsetEnd=43]
-[Text=der CharacterOffsetBegin=44 CharacterOffsetEnd=47]
-[Text=Bundesrepublik CharacterOffsetBegin=48 CharacterOffsetEnd=62]
-[Text=Deutschland CharacterOffsetBegin=63 CharacterOffsetEnd=74]
-[Text=. CharacterOffsetBegin=74 CharacterOffsetEnd=75]
-"""
-
-# properties for username/password example
-USERNAME_PASS_PROPS = {'annotators': 'tokenize,ssplit,pos'}
-
-USERNAME_PASS_GOLD = """
-Sentence #1 (6 tokens):
-Joe Smith lives in California.
-
-Tokens:
-[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP]
-[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP]
-[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ]
-[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN]
-[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP]
-[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=.]
-"""
-
-
-def annotate_and_time(client, text, properties={}):
-    """ Submit an annotation request and return how long it took """
-    start = time.time()
-    ann = client.annotate(text, properties=properties, output_format="text")
-    end = time.time()
-    return {'annotation': ann, 'start_time': start, 'end_time': end}
-
-def test_preload():
-    """ Test that the default annotators load fully immediately upon server start """
-    with corenlp.CoreNLPClient(server_id='test_server_start_preload') as client:
-        # wait for annotators to load
-        time.sleep(140)
-        results = annotate_and_time(client, EN_DOC)
-        compare_ignoring_whitespace(results['annotation'], EN_PRELOAD_GOLD)
-        assert results['end_time'] - results['start_time'] < 3
-
-
-def test_props_file():
-    """ Test starting the server with a props file """
-    with corenlp.CoreNLPClient(properties=SERVER_TEST_PROPS, server_id='test_server_start_props_file') as client:
-        ann = client.annotate(EN_DOC, output_format="text")
-        assert ann.strip() == EN_PROPS_FILE_GOLD.strip()
-
-
-def test_lang_start():
-    """ Test starting the server with a Stanford CoreNLP language name """
-    with corenlp.CoreNLPClient(properties='german', server_id='test_server_start_lang_name') as client:
-        ann = client.annotate(GERMAN_DOC, output_format='text')
-        compare_ignoring_whitespace(ann, GERMAN_FULL_PROPS_GOLD)
-
-
-def test_python_dict():
-    """ Test starting the server with a Python dictionary as default properties """
-    with corenlp.CoreNLPClient(properties=GERMAN_SMALL_PROPS, server_id='test_server_start_python_dict') as client:
-        ann = client.annotate(GERMAN_DOC, output_format='text')
-        assert ann.strip() == GERMAN_SMALL_PROPS_GOLD.strip()
-
-
-def test_python_dict_w_annotators():
-    """ Test starting the server with a Python dictionary as default properties, override annotators """
-    with corenlp.CoreNLPClient(properties=GERMAN_SMALL_PROPS, annotators="tokenize,ssplit",
-                               server_id='test_server_start_python_dict_w_annotators') as client:
-        ann = client.annotate(GERMAN_DOC, output_format='text')
-        assert ann.strip() == GERMAN_SMALL_PROPS_W_ANNOTATORS_GOLD.strip()
-
-
-def test_username_password():
-    """ Test starting a server with a username and password """
-    with corenlp.CoreNLPClient(properties=USERNAME_PASS_PROPS, username='user-1234', password='1234',
-                               server_id="test_server_username_pass") as client:
-        # check with correct password
-        ann = client.annotate(EN_DOC, output_format='text', username='user-1234', password='1234')
-        assert ann.strip() == USERNAME_PASS_GOLD.strip()
-        # check with incorrect password, should throw AnnotationException
-        try:
-            ann = client.annotate(EN_DOC, output_format='text', username='user-1234', password='12345')
-            assert False
-        except AnnotationException as ae:
-            pass
-        except Exception as e:
-            assert False
-
-
+"""
+Tests for starting a server in Python code
+"""
+
+import pytest
+import classla.server as corenlp
+from classla.server.client import AnnotationException
+import time
+
+from tests import *
+
+pytestmark = pytest.mark.client
+
+EN_DOC = "Joe Smith lives in California."
+
+# results on EN_DOC with standard StanfordCoreNLP defaults
+EN_PRELOAD_GOLD = """
+Sentence #1 (6 tokens):
+Joe Smith lives in California.
+
+Tokens:
+[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP Lemma=Joe NamedEntityTag=PERSON]
+[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP Lemma=Smith NamedEntityTag=PERSON]
+[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ Lemma=live NamedEntityTag=O]
+[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN Lemma=in NamedEntityTag=O]
+[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP Lemma=California NamedEntityTag=STATE_OR_PROVINCE]
+[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=. Lemma=. NamedEntityTag=O]
+
+Dependency Parse (enhanced plus plus dependencies):
+root(ROOT-0, lives-3)
+compound(Smith-2, Joe-1)
+nsubj(lives-3, Smith-2)
+case(California-5, in-4)
+obl:in(lives-3, California-5)
+punct(lives-3, .-6)
+
+Extracted the following NER entity mentions:
+Joe Smith       PERSON              PERSON:0.9972202681743931
+California      STATE_OR_PROVINCE   LOCATION:0.9990868267559281
+
+Extracted the following KBP triples:
+1.0 Joe Smith per:statesorprovinces_of_residence California
+"""
+
+# results with an example properties file
+EN_PROPS_FILE_GOLD = """
+Sentence #1 (6 tokens):
+Joe Smith lives in California.
+
+Tokens:
+[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP]
+[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP]
+[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ]
+[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN]
+[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP]
+[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=.]
+"""
+
+GERMAN_DOC = "Angela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland."
+
+# results with standard German properties
+GERMAN_FULL_PROPS_GOLD = """
+Sentence #1 (10 tokens):
+Angela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland.
+
+Tokens:
+[Text=Angela CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=PROPN Lemma=angela NamedEntityTag=PERSON]
+[Text=Merkel CharacterOffsetBegin=7 CharacterOffsetEnd=13 PartOfSpeech=PROPN Lemma=merkel NamedEntityTag=PERSON]
+[Text=ist CharacterOffsetBegin=14 CharacterOffsetEnd=17 PartOfSpeech=AUX Lemma=ist NamedEntityTag=O]
+[Text=seit CharacterOffsetBegin=18 CharacterOffsetEnd=22 PartOfSpeech=ADP Lemma=seit NamedEntityTag=O]
+[Text=2005 CharacterOffsetBegin=23 CharacterOffsetEnd=27 PartOfSpeech=NUM Lemma=2005 NamedEntityTag=O]
+[Text=Bundeskanzlerin CharacterOffsetBegin=28 CharacterOffsetEnd=43 PartOfSpeech=NOUN Lemma=bundeskanzlerin NamedEntityTag=O]
+[Text=der CharacterOffsetBegin=44 CharacterOffsetEnd=47 PartOfSpeech=DET Lemma=der NamedEntityTag=O]
+[Text=Bundesrepublik CharacterOffsetBegin=48 CharacterOffsetEnd=62 PartOfSpeech=PROPN Lemma=bundesrepublik NamedEntityTag=LOCATION]
+[Text=Deutschland CharacterOffsetBegin=63 CharacterOffsetEnd=74 PartOfSpeech=PROPN Lemma=deutschland NamedEntityTag=LOCATION]
+[Text=. CharacterOffsetBegin=74 CharacterOffsetEnd=75 PartOfSpeech=PUNCT Lemma=. NamedEntityTag=O]
+
+Dependency Parse (enhanced plus plus dependencies):
+root(ROOT-0, Bundeskanzlerin-6)
+nsubj(Bundeskanzlerin-6, Angela-1)
+flat(Angela-1, Merkel-2)
+cop(Bundeskanzlerin-6, ist-3)
+case(2005-5, seit-4)
+nmod:seit(Bundeskanzlerin-6, 2005-5)
+det(Bundesrepublik-8, der-7)
+nmod(Bundeskanzlerin-6, Bundesrepublik-8)
+appos(Bundesrepublik-8, Deutschland-9)
+punct(Bundeskanzlerin-6, .-10)
+
+Extracted the following NER entity mentions:
+Angela Merkel              PERSON   PERSON:0.9999981583351504
+Bundesrepublik Deutschland LOCATION LOCATION:0.9682902289749544
+"""
+
+
+GERMAN_SMALL_PROPS = {'annotators': 'tokenize,ssplit,pos', 'tokenize.language': 'de',
+                      'pos.model': 'edu/stanford/nlp/models/pos-tagger/german-ud.tagger'}
+
+# results with custom Python dictionary set properties
+GERMAN_SMALL_PROPS_GOLD = """
+Sentence #1 (10 tokens):
+Angela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland.
+
+Tokens:
+[Text=Angela CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=PROPN]
+[Text=Merkel CharacterOffsetBegin=7 CharacterOffsetEnd=13 PartOfSpeech=PROPN]
+[Text=ist CharacterOffsetBegin=14 CharacterOffsetEnd=17 PartOfSpeech=AUX]
+[Text=seit CharacterOffsetBegin=18 CharacterOffsetEnd=22 PartOfSpeech=ADP]
+[Text=2005 CharacterOffsetBegin=23 CharacterOffsetEnd=27 PartOfSpeech=NUM]
+[Text=Bundeskanzlerin CharacterOffsetBegin=28 CharacterOffsetEnd=43 PartOfSpeech=NOUN]
+[Text=der CharacterOffsetBegin=44 CharacterOffsetEnd=47 PartOfSpeech=DET]
+[Text=Bundesrepublik CharacterOffsetBegin=48 CharacterOffsetEnd=62 PartOfSpeech=PROPN]
+[Text=Deutschland CharacterOffsetBegin=63 CharacterOffsetEnd=74 PartOfSpeech=PROPN]
+[Text=. CharacterOffsetBegin=74 CharacterOffsetEnd=75 PartOfSpeech=PUNCT]
+"""
+
+# results with custom Python dictionary set properties and annotators=tokenize,ssplit
+GERMAN_SMALL_PROPS_W_ANNOTATORS_GOLD = """
+Sentence #1 (10 tokens):
+Angela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland.
+
+Tokens:
+[Text=Angela CharacterOffsetBegin=0 CharacterOffsetEnd=6]
+[Text=Merkel CharacterOffsetBegin=7 CharacterOffsetEnd=13]
+[Text=ist CharacterOffsetBegin=14 CharacterOffsetEnd=17]
+[Text=seit CharacterOffsetBegin=18 CharacterOffsetEnd=22]
+[Text=2005 CharacterOffsetBegin=23 CharacterOffsetEnd=27]
+[Text=Bundeskanzlerin CharacterOffsetBegin=28 CharacterOffsetEnd=43]
+[Text=der CharacterOffsetBegin=44 CharacterOffsetEnd=47]
+[Text=Bundesrepublik CharacterOffsetBegin=48 CharacterOffsetEnd=62]
+[Text=Deutschland CharacterOffsetBegin=63 CharacterOffsetEnd=74]
+[Text=. CharacterOffsetBegin=74 CharacterOffsetEnd=75]
+"""
+
+# properties for username/password example
+USERNAME_PASS_PROPS = {'annotators': 'tokenize,ssplit,pos'}
+
+USERNAME_PASS_GOLD = """
+Sentence #1 (6 tokens):
+Joe Smith lives in California.
+
+Tokens:
+[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP]
+[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP]
+[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ]
+[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN]
+[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP]
+[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=.]
+"""
+
+
+def annotate_and_time(client, text, properties={}):
+    """ Submit an annotation request and return how long it took """
+    start = time.time()
+    ann = client.annotate(text, properties=properties, output_format="text")
+    end = time.time()
+    return {'annotation': ann, 'start_time': start, 'end_time': end}
+
+def test_preload():
+    """ Test that the default annotators load fully immediately upon server start """
+    with corenlp.CoreNLPClient(server_id='test_server_start_preload') as client:
+        # wait for annotators to load
+        time.sleep(140)
+        results = annotate_and_time(client, EN_DOC)
+        compare_ignoring_whitespace(results['annotation'], EN_PRELOAD_GOLD)
+        assert results['end_time'] - results['start_time'] < 3
+
+
+def test_props_file():
+    """ Test starting the server with a props file """
+    with corenlp.CoreNLPClient(properties=SERVER_TEST_PROPS, server_id='test_server_start_props_file') as client:
+        ann = client.annotate(EN_DOC, output_format="text")
+        assert ann.strip() == EN_PROPS_FILE_GOLD.strip()
+
+
+def test_lang_start():
+    """ Test starting the server with a Stanford CoreNLP language name """
+    with corenlp.CoreNLPClient(properties='german', server_id='test_server_start_lang_name') as client:
+        ann = client.annotate(GERMAN_DOC, output_format='text')
+        compare_ignoring_whitespace(ann, GERMAN_FULL_PROPS_GOLD)
+
+
+def test_python_dict():
+    """ Test starting the server with a Python dictionary as default properties """
+    with corenlp.CoreNLPClient(properties=GERMAN_SMALL_PROPS, server_id='test_server_start_python_dict') as client:
+        ann = client.annotate(GERMAN_DOC, output_format='text')
+        assert ann.strip() == GERMAN_SMALL_PROPS_GOLD.strip()
+
+
+def test_python_dict_w_annotators():
+    """ Test starting the server with a Python dictionary as default properties, override annotators """
+    with corenlp.CoreNLPClient(properties=GERMAN_SMALL_PROPS, annotators="tokenize,ssplit",
+                               server_id='test_server_start_python_dict_w_annotators') as client:
+        ann = client.annotate(GERMAN_DOC, output_format='text')
+        assert ann.strip() == GERMAN_SMALL_PROPS_W_ANNOTATORS_GOLD.strip()
+
+
+def test_username_password():
+    """ Test starting a server with a username and password """
+    with corenlp.CoreNLPClient(properties=USERNAME_PASS_PROPS, username='user-1234', password='1234',
+                               server_id="test_server_username_pass") as client:
+        # check with correct password
+        ann = client.annotate(EN_DOC, output_format='text', username='user-1234', password='1234')
+        assert ann.strip() == USERNAME_PASS_GOLD.strip()
+        # check with incorrect password, should throw AnnotationException
+        try:
+            ann = client.annotate(EN_DOC, output_format='text', username='user-1234', password='12345')
+            assert False
+        except AnnotationException as ae:
+            pass
+        except Exception as e:
+            assert False
+
+
```

### Comparing `classla-2.0/tests/test_tagger.py` & `classla-2.1/tests/test_tagger.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,28 +1,28 @@
-"""
-Basic testing of part of speech tagging
-"""
-
-import pytest
-import classla
-
-from tests import *
-
-pytestmark = pytest.mark.pipeline
-
-EN_DOC = "Joe Smith was born in California."
-
-EN_DOC_GOLD = """
-<Token id=1;words=[<Word id=1;text=Joe;upos=PROPN;xpos=NNP;feats=Number=Sing>]>
-<Token id=2;words=[<Word id=2;text=Smith;upos=PROPN;xpos=NNP;feats=Number=Sing>]>
-<Token id=3;words=[<Word id=3;text=was;upos=AUX;xpos=VBD;feats=Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin>]>
-<Token id=4;words=[<Word id=4;text=born;upos=VERB;xpos=VBN;feats=Tense=Past|VerbForm=Part|Voice=Pass>]>
-<Token id=5;words=[<Word id=5;text=in;upos=ADP;xpos=IN>]>
-<Token id=6;words=[<Word id=6;text=California;upos=PROPN;xpos=NNP;feats=Number=Sing>]>
-<Token id=7;words=[<Word id=7;text=.;upos=PUNCT;xpos=.>]>
-""".strip()
-
-
-def test_part_of_speech():
-    nlp = classla.Pipeline(**{'processors': 'tokenize,pos', 'dir': TEST_MODELS_DIR, 'lang': 'en'})
-    doc = nlp(EN_DOC)
-    assert EN_DOC_GOLD == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
+"""
+Basic testing of part of speech tagging
+"""
+
+import pytest
+import classla
+
+from tests import *
+
+pytestmark = pytest.mark.pipeline
+
+EN_DOC = "Joe Smith was born in California."
+
+EN_DOC_GOLD = """
+<Token id=1;words=[<Word id=1;text=Joe;upos=PROPN;xpos=NNP;feats=Number=Sing>]>
+<Token id=2;words=[<Word id=2;text=Smith;upos=PROPN;xpos=NNP;feats=Number=Sing>]>
+<Token id=3;words=[<Word id=3;text=was;upos=AUX;xpos=VBD;feats=Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin>]>
+<Token id=4;words=[<Word id=4;text=born;upos=VERB;xpos=VBN;feats=Tense=Past|VerbForm=Part|Voice=Pass>]>
+<Token id=5;words=[<Word id=5;text=in;upos=ADP;xpos=IN>]>
+<Token id=6;words=[<Word id=6;text=California;upos=PROPN;xpos=NNP;feats=Number=Sing>]>
+<Token id=7;words=[<Word id=7;text=.;upos=PUNCT;xpos=.>]>
+""".strip()
+
+
+def test_part_of_speech():
+    nlp = classla.Pipeline(**{'processors': 'tokenize,pos', 'dir': TEST_MODELS_DIR, 'lang': 'en'})
+    doc = nlp(EN_DOC)
+    assert EN_DOC_GOLD == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
```

### Comparing `classla-2.0/tests/test_tokenizer.py` & `classla-2.1/tests/test_tokenizer.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,176 +1,176 @@
-"""
-Basic testing of tokenization
-"""
-
-import pytest
-import classla
-
-from tests import *
-
-pytestmark = pytest.mark.pipeline
-
-EN_DOC = "Joe Smith lives in California. Joe's favorite food is pizza. He enjoys going to the beach."
-EN_DOC_WITH_EXTRA_WHITESPACE = "Joe   Smith \n lives in\n California.   Joe's    favorite food \tis pizza. \t\t\tHe enjoys \t\tgoing to the beach."
-EN_DOC_GOLD_TOKENS = """
-<Token id=1;words=[<Word id=1;text=Joe>]>
-<Token id=2;words=[<Word id=2;text=Smith>]>
-<Token id=3;words=[<Word id=3;text=lives>]>
-<Token id=4;words=[<Word id=4;text=in>]>
-<Token id=5;words=[<Word id=5;text=California>]>
-<Token id=6;words=[<Word id=6;text=.>]>
-
-<Token id=1;words=[<Word id=1;text=Joe>]>
-<Token id=2;words=[<Word id=2;text='s>]>
-<Token id=3;words=[<Word id=3;text=favorite>]>
-<Token id=4;words=[<Word id=4;text=food>]>
-<Token id=5;words=[<Word id=5;text=is>]>
-<Token id=6;words=[<Word id=6;text=pizza>]>
-<Token id=7;words=[<Word id=7;text=.>]>
-
-<Token id=1;words=[<Word id=1;text=He>]>
-<Token id=2;words=[<Word id=2;text=enjoys>]>
-<Token id=3;words=[<Word id=3;text=going>]>
-<Token id=4;words=[<Word id=4;text=to>]>
-<Token id=5;words=[<Word id=5;text=the>]>
-<Token id=6;words=[<Word id=6;text=beach>]>
-<Token id=7;words=[<Word id=7;text=.>]>
-""".strip()
-
-EN_DOC_PRETOKENIZED = \
-    "Joe Smith lives in California .\nJoe's favorite  food is  pizza .\n\nHe enjoys going to the beach.\n"
-EN_DOC_PRETOKENIZED_GOLD_TOKENS = """
-<Token id=1;words=[<Word id=1;text=Joe>]>
-<Token id=2;words=[<Word id=2;text=Smith>]>
-<Token id=3;words=[<Word id=3;text=lives>]>
-<Token id=4;words=[<Word id=4;text=in>]>
-<Token id=5;words=[<Word id=5;text=California>]>
-<Token id=6;words=[<Word id=6;text=.>]>
-
-<Token id=1;words=[<Word id=1;text=Joe's>]>
-<Token id=2;words=[<Word id=2;text=favorite>]>
-<Token id=3;words=[<Word id=3;text=food>]>
-<Token id=4;words=[<Word id=4;text=is>]>
-<Token id=5;words=[<Word id=5;text=pizza>]>
-<Token id=6;words=[<Word id=6;text=.>]>
-
-<Token id=1;words=[<Word id=1;text=He>]>
-<Token id=2;words=[<Word id=2;text=enjoys>]>
-<Token id=3;words=[<Word id=3;text=going>]>
-<Token id=4;words=[<Word id=4;text=to>]>
-<Token id=5;words=[<Word id=5;text=the>]>
-<Token id=6;words=[<Word id=6;text=beach.>]>
-""".strip()
-
-EN_DOC_PRETOKENIZED_LIST = [['Joe', 'Smith', 'lives', 'in', 'California', '.'], ['He', 'loves', 'pizza', '.']]
-EN_DOC_PRETOKENIZED_LIST_GOLD_TOKENS = """
-<Token id=1;words=[<Word id=1;text=Joe>]>
-<Token id=2;words=[<Word id=2;text=Smith>]>
-<Token id=3;words=[<Word id=3;text=lives>]>
-<Token id=4;words=[<Word id=4;text=in>]>
-<Token id=5;words=[<Word id=5;text=California>]>
-<Token id=6;words=[<Word id=6;text=.>]>
-
-<Token id=1;words=[<Word id=1;text=He>]>
-<Token id=2;words=[<Word id=2;text=loves>]>
-<Token id=3;words=[<Word id=3;text=pizza>]>
-<Token id=4;words=[<Word id=4;text=.>]>
-""".strip()
-
-EN_DOC_NO_SSPLIT = ["This is a sentence. This is another.", "This is a third."]
-EN_DOC_NO_SSPLIT_SENTENCES = [['This', 'is', 'a', 'sentence', '.', 'This', 'is', 'another', '.'], ['This', 'is', 'a', 'third', '.']]
-
-JA_DOC = "北京は中国の首都です。 北京の人口は2152万人です。\n" # add some random whitespaces that need to be skipped
-JA_DOC_GOLD_TOKENS = """
-<Token id=1;words=[<Word id=1;text=北京>]>
-<Token id=2;words=[<Word id=2;text=は>]>
-<Token id=3;words=[<Word id=3;text=中国>]>
-<Token id=4;words=[<Word id=4;text=の>]>
-<Token id=5;words=[<Word id=5;text=首都>]>
-<Token id=6;words=[<Word id=6;text=です>]>
-<Token id=7;words=[<Word id=7;text=。>]>
-
-<Token id=1;words=[<Word id=1;text=北京>]>
-<Token id=2;words=[<Word id=2;text=の>]>
-<Token id=3;words=[<Word id=3;text=人口>]>
-<Token id=4;words=[<Word id=4;text=は>]>
-<Token id=5;words=[<Word id=5;text=2152万>]>
-<Token id=6;words=[<Word id=6;text=人>]>
-<Token id=7;words=[<Word id=7;text=です>]>
-<Token id=8;words=[<Word id=8;text=。>]>
-""".strip()
-
-ZH_DOC = "北京是中国的首都。 北京有2100万人口，是一个直辖市。\n"
-ZH_DOC_GOLD_TOKENS = """
-<Token id=1;words=[<Word id=1;text=北京>]>
-<Token id=2;words=[<Word id=2;text=是>]>
-<Token id=3;words=[<Word id=3;text=中国>]>
-<Token id=4;words=[<Word id=4;text=的>]>
-<Token id=5;words=[<Word id=5;text=首都>]>
-<Token id=6;words=[<Word id=6;text=。>]>
-
-<Token id=1;words=[<Word id=1;text=北京>]>
-<Token id=2;words=[<Word id=2;text=有>]>
-<Token id=3;words=[<Word id=3;text=2100>]>
-<Token id=4;words=[<Word id=4;text=万>]>
-<Token id=5;words=[<Word id=5;text=人口>]>
-<Token id=6;words=[<Word id=6;text=，>]>
-<Token id=7;words=[<Word id=7;text=是>]>
-<Token id=8;words=[<Word id=8;text=一个>]>
-<Token id=9;words=[<Word id=9;text=直辖市>]>
-<Token id=10;words=[<Word id=10;text=。>]>
-""".strip()
-
-def test_tokenize():
-    nlp = classla.Pipeline(processors='tokenize', dir=TEST_MODELS_DIR, lang='en')
-    doc = nlp(EN_DOC)
-    assert EN_DOC_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
-    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
-
-def test_tokenize_ssplit_robustness():
-    nlp = classla.Pipeline(processors='tokenize', dir=TEST_MODELS_DIR, lang='en')
-    doc = nlp(EN_DOC_WITH_EXTRA_WHITESPACE)
-    assert EN_DOC_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
-    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
-
-def test_pretokenized():
-    nlp = classla.Pipeline(**{'processors': 'tokenize', 'dir': TEST_MODELS_DIR, 'lang': 'en',
-                                  'tokenize_pretokenized': True})
-    doc = nlp(EN_DOC_PRETOKENIZED)
-    assert EN_DOC_PRETOKENIZED_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
-    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
-    doc = nlp(EN_DOC_PRETOKENIZED_LIST)
-    assert EN_DOC_PRETOKENIZED_LIST_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
-    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
-
-def test_no_ssplit():
-    nlp = classla.Pipeline(**{'processors': 'tokenize', 'dir': TEST_MODELS_DIR, 'lang': 'en',
-                                  'tokenize_no_ssplit': True})
-
-    doc = nlp(EN_DOC_NO_SSPLIT)
-    assert EN_DOC_NO_SSPLIT_SENTENCES == [[w.text for w in s.words] for s in doc.sentences]
-    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
-
-def test_spacy():
-    nlp = classla.Pipeline(processors='tokenize', dir=TEST_MODELS_DIR, lang='en', tokenize_with_spacy=True)
-    doc = nlp(EN_DOC)
-
-    # make sure the loaded tokenizer is actually spacy
-    assert "SpacyTokenizer" == nlp.processors['tokenize']._variant.__class__.__name__
-    assert EN_DOC_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
-    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
-
-def test_sudachipy():
-    nlp = classla.Pipeline(lang='ja', dir=TEST_MODELS_DIR, processors={'tokenize': 'sudachipy'}, package=None)
-    doc = nlp(JA_DOC)
-
-    assert "SudachiPyTokenizer" == nlp.processors['tokenize']._variant.__class__.__name__
-    assert JA_DOC_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
-    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
-
-def test_jieba():
-    nlp = classla.Pipeline(lang='zh', dir=TEST_MODELS_DIR, processors={'tokenize': 'jieba'}, package=None)
-    doc = nlp(ZH_DOC)
-
-    assert "JiebaTokenizer" == nlp.processors['tokenize']._variant.__class__.__name__
-    assert ZH_DOC_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
-    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
+"""
+Basic testing of tokenization
+"""
+
+import pytest
+import classla
+
+from tests import *
+
+pytestmark = pytest.mark.pipeline
+
+EN_DOC = "Joe Smith lives in California. Joe's favorite food is pizza. He enjoys going to the beach."
+EN_DOC_WITH_EXTRA_WHITESPACE = "Joe   Smith \n lives in\n California.   Joe's    favorite food \tis pizza. \t\t\tHe enjoys \t\tgoing to the beach."
+EN_DOC_GOLD_TOKENS = """
+<Token id=1;words=[<Word id=1;text=Joe>]>
+<Token id=2;words=[<Word id=2;text=Smith>]>
+<Token id=3;words=[<Word id=3;text=lives>]>
+<Token id=4;words=[<Word id=4;text=in>]>
+<Token id=5;words=[<Word id=5;text=California>]>
+<Token id=6;words=[<Word id=6;text=.>]>
+
+<Token id=1;words=[<Word id=1;text=Joe>]>
+<Token id=2;words=[<Word id=2;text='s>]>
+<Token id=3;words=[<Word id=3;text=favorite>]>
+<Token id=4;words=[<Word id=4;text=food>]>
+<Token id=5;words=[<Word id=5;text=is>]>
+<Token id=6;words=[<Word id=6;text=pizza>]>
+<Token id=7;words=[<Word id=7;text=.>]>
+
+<Token id=1;words=[<Word id=1;text=He>]>
+<Token id=2;words=[<Word id=2;text=enjoys>]>
+<Token id=3;words=[<Word id=3;text=going>]>
+<Token id=4;words=[<Word id=4;text=to>]>
+<Token id=5;words=[<Word id=5;text=the>]>
+<Token id=6;words=[<Word id=6;text=beach>]>
+<Token id=7;words=[<Word id=7;text=.>]>
+""".strip()
+
+EN_DOC_PRETOKENIZED = \
+    "Joe Smith lives in California .\nJoe's favorite  food is  pizza .\n\nHe enjoys going to the beach.\n"
+EN_DOC_PRETOKENIZED_GOLD_TOKENS = """
+<Token id=1;words=[<Word id=1;text=Joe>]>
+<Token id=2;words=[<Word id=2;text=Smith>]>
+<Token id=3;words=[<Word id=3;text=lives>]>
+<Token id=4;words=[<Word id=4;text=in>]>
+<Token id=5;words=[<Word id=5;text=California>]>
+<Token id=6;words=[<Word id=6;text=.>]>
+
+<Token id=1;words=[<Word id=1;text=Joe's>]>
+<Token id=2;words=[<Word id=2;text=favorite>]>
+<Token id=3;words=[<Word id=3;text=food>]>
+<Token id=4;words=[<Word id=4;text=is>]>
+<Token id=5;words=[<Word id=5;text=pizza>]>
+<Token id=6;words=[<Word id=6;text=.>]>
+
+<Token id=1;words=[<Word id=1;text=He>]>
+<Token id=2;words=[<Word id=2;text=enjoys>]>
+<Token id=3;words=[<Word id=3;text=going>]>
+<Token id=4;words=[<Word id=4;text=to>]>
+<Token id=5;words=[<Word id=5;text=the>]>
+<Token id=6;words=[<Word id=6;text=beach.>]>
+""".strip()
+
+EN_DOC_PRETOKENIZED_LIST = [['Joe', 'Smith', 'lives', 'in', 'California', '.'], ['He', 'loves', 'pizza', '.']]
+EN_DOC_PRETOKENIZED_LIST_GOLD_TOKENS = """
+<Token id=1;words=[<Word id=1;text=Joe>]>
+<Token id=2;words=[<Word id=2;text=Smith>]>
+<Token id=3;words=[<Word id=3;text=lives>]>
+<Token id=4;words=[<Word id=4;text=in>]>
+<Token id=5;words=[<Word id=5;text=California>]>
+<Token id=6;words=[<Word id=6;text=.>]>
+
+<Token id=1;words=[<Word id=1;text=He>]>
+<Token id=2;words=[<Word id=2;text=loves>]>
+<Token id=3;words=[<Word id=3;text=pizza>]>
+<Token id=4;words=[<Word id=4;text=.>]>
+""".strip()
+
+EN_DOC_NO_SSPLIT = ["This is a sentence. This is another.", "This is a third."]
+EN_DOC_NO_SSPLIT_SENTENCES = [['This', 'is', 'a', 'sentence', '.', 'This', 'is', 'another', '.'], ['This', 'is', 'a', 'third', '.']]
+
+JA_DOC = "北京は中国の首都です。 北京の人口は2152万人です。\n" # add some random whitespaces that need to be skipped
+JA_DOC_GOLD_TOKENS = """
+<Token id=1;words=[<Word id=1;text=北京>]>
+<Token id=2;words=[<Word id=2;text=は>]>
+<Token id=3;words=[<Word id=3;text=中国>]>
+<Token id=4;words=[<Word id=4;text=の>]>
+<Token id=5;words=[<Word id=5;text=首都>]>
+<Token id=6;words=[<Word id=6;text=です>]>
+<Token id=7;words=[<Word id=7;text=。>]>
+
+<Token id=1;words=[<Word id=1;text=北京>]>
+<Token id=2;words=[<Word id=2;text=の>]>
+<Token id=3;words=[<Word id=3;text=人口>]>
+<Token id=4;words=[<Word id=4;text=は>]>
+<Token id=5;words=[<Word id=5;text=2152万>]>
+<Token id=6;words=[<Word id=6;text=人>]>
+<Token id=7;words=[<Word id=7;text=です>]>
+<Token id=8;words=[<Word id=8;text=。>]>
+""".strip()
+
+ZH_DOC = "北京是中国的首都。 北京有2100万人口，是一个直辖市。\n"
+ZH_DOC_GOLD_TOKENS = """
+<Token id=1;words=[<Word id=1;text=北京>]>
+<Token id=2;words=[<Word id=2;text=是>]>
+<Token id=3;words=[<Word id=3;text=中国>]>
+<Token id=4;words=[<Word id=4;text=的>]>
+<Token id=5;words=[<Word id=5;text=首都>]>
+<Token id=6;words=[<Word id=6;text=。>]>
+
+<Token id=1;words=[<Word id=1;text=北京>]>
+<Token id=2;words=[<Word id=2;text=有>]>
+<Token id=3;words=[<Word id=3;text=2100>]>
+<Token id=4;words=[<Word id=4;text=万>]>
+<Token id=5;words=[<Word id=5;text=人口>]>
+<Token id=6;words=[<Word id=6;text=，>]>
+<Token id=7;words=[<Word id=7;text=是>]>
+<Token id=8;words=[<Word id=8;text=一个>]>
+<Token id=9;words=[<Word id=9;text=直辖市>]>
+<Token id=10;words=[<Word id=10;text=。>]>
+""".strip()
+
+def test_tokenize():
+    nlp = classla.Pipeline(processors='tokenize', dir=TEST_MODELS_DIR, lang='en')
+    doc = nlp(EN_DOC)
+    assert EN_DOC_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
+    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
+
+def test_tokenize_ssplit_robustness():
+    nlp = classla.Pipeline(processors='tokenize', dir=TEST_MODELS_DIR, lang='en')
+    doc = nlp(EN_DOC_WITH_EXTRA_WHITESPACE)
+    assert EN_DOC_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
+    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
+
+def test_pretokenized():
+    nlp = classla.Pipeline(**{'processors': 'tokenize', 'dir': TEST_MODELS_DIR, 'lang': 'en',
+                                  'tokenize_pretokenized': True})
+    doc = nlp(EN_DOC_PRETOKENIZED)
+    assert EN_DOC_PRETOKENIZED_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
+    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
+    doc = nlp(EN_DOC_PRETOKENIZED_LIST)
+    assert EN_DOC_PRETOKENIZED_LIST_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
+    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
+
+def test_no_ssplit():
+    nlp = classla.Pipeline(**{'processors': 'tokenize', 'dir': TEST_MODELS_DIR, 'lang': 'en',
+                                  'tokenize_no_ssplit': True})
+
+    doc = nlp(EN_DOC_NO_SSPLIT)
+    assert EN_DOC_NO_SSPLIT_SENTENCES == [[w.text for w in s.words] for s in doc.sentences]
+    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
+
+def test_spacy():
+    nlp = classla.Pipeline(processors='tokenize', dir=TEST_MODELS_DIR, lang='en', tokenize_with_spacy=True)
+    doc = nlp(EN_DOC)
+
+    # make sure the loaded tokenizer is actually spacy
+    assert "SpacyTokenizer" == nlp.processors['tokenize']._variant.__class__.__name__
+    assert EN_DOC_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
+    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
+
+def test_sudachipy():
+    nlp = classla.Pipeline(lang='ja', dir=TEST_MODELS_DIR, processors={'tokenize': 'sudachipy'}, package=None)
+    doc = nlp(JA_DOC)
+
+    assert "SudachiPyTokenizer" == nlp.processors['tokenize']._variant.__class__.__name__
+    assert JA_DOC_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
+    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
+
+def test_jieba():
+    nlp = classla.Pipeline(lang='zh', dir=TEST_MODELS_DIR, processors={'tokenize': 'jieba'}, package=None)
+    doc = nlp(ZH_DOC)
+
+    assert "JiebaTokenizer" == nlp.processors['tokenize']._variant.__class__.__name__
+    assert ZH_DOC_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
+    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])
```

### Comparing `classla-2.0/tests_classla/__init__.py` & `classla-2.1/tests/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,119 +1,120 @@
-"""
-Utilities for testing
-"""
-
-import os
-import re
-
-os.environ['CLASSLA_TEST_HOME'] = 'classla_test'
-
-# Environment Variables
-# set this to specify working directory of tests
-TEST_HOME_VAR = 'CLASSLA_TEST_HOME'
-
-# Global Variables
-# test working directory base name must be classla_test
-TEST_DIR_BASE_NAME = 'classla_test'
-
-# check the working dir is set and compliant
-assert os.getenv(TEST_HOME_VAR) is not None, \
-    f'Please set {TEST_HOME_VAR} environment variable for test working dir, base name must be: {TEST_DIR_BASE_NAME}'
-TEST_WORKING_DIR = os.getenv(TEST_HOME_VAR)
-assert os.path.basename(TEST_WORKING_DIR) == TEST_DIR_BASE_NAME, \
-    f'Base name of test home dir must be: {TEST_DIR_BASE_NAME}'
-
-TEST_MODELS_DIR = f'{TEST_WORKING_DIR}/models'
-
-# server resources
-SERVER_TEST_PROPS = f'{TEST_WORKING_DIR}/scripts/external_server.properties'
-
-# langauge resources
-LANGUAGE_RESOURCES = {}
-
-TOKENIZE_MODEL = 'tokenizer.pt'
-MWT_MODEL = 'mwt_expander.pt'
-POS_MODEL = 'tagger.pt'
-POS_PRETRAIN = 'pretrain.pt'
-LEMMA_MODEL = 'lemmatizer.pt'
-DEPPARSE_MODEL = 'parser.pt'
-DEPPARSE_PRETRAIN = 'pretrain.pt'
-
-MODEL_FILES = [TOKENIZE_MODEL, MWT_MODEL, POS_MODEL, POS_PRETRAIN, LEMMA_MODEL, DEPPARSE_MODEL, DEPPARSE_PRETRAIN]
-
-# English resources
-EN_KEY = 'en'
-EN_SHORTHAND = 'en_ewt'
-# models
-EN_MODELS_DIR = f'{TEST_WORKING_DIR}/models/{EN_SHORTHAND}_models'
-EN_MODEL_FILES = [f'{EN_MODELS_DIR}/{EN_SHORTHAND}_{model_fname}' for model_fname in MODEL_FILES]
-
-# French resources
-FR_KEY = 'fr'
-FR_SHORTHAND = 'fr_gsd'
-# regression file paths
-FR_TEST_IN = f'{TEST_WORKING_DIR}/in/fr_gsd.test.txt'
-FR_TEST_OUT = f'{TEST_WORKING_DIR}/out/fr_gsd.test.txt.out'
-FR_TEST_GOLD_OUT = f'{TEST_WORKING_DIR}/out/fr_gsd.test.txt.out.gold'
-# models
-FR_MODELS_DIR = f'{TEST_WORKING_DIR}/models/{FR_SHORTHAND}_models'
-FR_MODEL_FILES = [f'{FR_MODELS_DIR}/{FR_SHORTHAND}_{model_fname}' for model_fname in MODEL_FILES]
-
-# Other language resources
-AR_SHORTHAND = 'ar_padt'
-DE_SHORTHAND = 'de_gsd'
-KK_SHORTHAND = 'kk_ktb'
-KO_SHORTHAND = 'ko_gsd'
-
-
-# Other language resources
-SL_SHORTHAND = 'ssj'
-SL_JOS_SHORTHAND = 'ssj_jos'
-SL_NS_SHORTHAND = 'nonstandard'
-
-# utils for clean up
-# only allow removal of dirs/files in this approved list
-REMOVABLE_PATHS = ['en_ewt_models', 'en_ewt_tokenizer.pt', 'en_ewt_mwt_expander.pt', 'en_ewt_tagger.pt',
-                   'en_ewt.pretrain.pt', 'en_ewt_lemmatizer.pt', 'en_ewt_parser.pt', 'fr_gsd_models',
-                   'fr_gsd_tokenizer.pt', 'fr_gsd_mwt_expander.pt', 'fr_gsd_tagger.pt', 'fr_gsd.pretrain.pt',
-                   'fr_gsd_lemmatizer.pt', 'fr_gsd_parser.pt', 'ar_padt_models', 'ar_padt_tokenizer.pt',
-                   'ar_padt_mwt_expander.pt', 'ar_padt_tagger.pt', 'ar_padt.pretrain.pt', 'ar_padt_lemmatizer.pt',
-                   'ar_padt_parser.pt', 'de_gsd_models', 'de_gsd_tokenizer.pt', 'de_gsd_mwt_expander.pt',
-                   'de_gsd_tagger.pt', 'de_gsd.pretrain.pt', 'de_gsd_lemmatizer.pt', 'de_gsd_parser.pt',
-                   'kk_ktb_models', 'kk_ktb_tokenizer.pt', 'kk_ktb_mwt_expander.pt', 'kk_ktb_tagger.pt',
-                   'kk_ktb.pretrain.pt', 'kk_ktb_lemmatizer.pt', 'kk_ktb_parser.pt', 'ko_gsd_models',
-                   'ko_gsd_tokenizer.pt', 'ko_gsd_mwt_expander.pt', 'ko_gsd_tagger.pt', 'ko_gsd.pretrain.pt',
-                   'ko_gsd_lemmatizer.pt', 'ko_gsd_parser.pt']
-
-
-def safe_rm(path_to_rm):
-    """
-    Safely remove a directory of files or a file
-    1.) check path exists, files are files, dirs are dirs
-    2.) only remove things on approved list REMOVABLE_PATHS
-    3.) assert no longer exists
-    """
-    # just return if path doesn't exist
-    if not os.path.exists(path_to_rm):
-        return
-    # handle directory
-    if os.path.isdir(path_to_rm):
-        files_to_rm = [f'{path_to_rm}/{fname}' for fname in os.listdir(path_to_rm)]
-        dir_to_rm = path_to_rm
-    else:
-        files_to_rm = [path_to_rm]
-        dir_to_rm = None
-    # clear out files
-    for file_to_rm in files_to_rm:
-        if os.path.isfile(file_to_rm) and os.path.basename(file_to_rm) in REMOVABLE_PATHS:
-            os.remove(file_to_rm)
-            assert not os.path.exists(file_to_rm), f'Error removing: {file_to_rm}'
-    # clear out directory
-    if dir_to_rm is not None and os.path.isdir(dir_to_rm):
-        os.rmdir(dir_to_rm)
-        assert not os.path.exists(dir_to_rm), f'Error removing: {dir_to_rm}'
-
-def compare_ignoring_whitespace(predicted, expected):
-    predicted = re.sub('[ \t]+', ' ', predicted.strip())
-    expected = re.sub('[ \t]+', ' ', expected.strip())
-    assert predicted == expected
-
+"""
+Utilities for testing
+"""
+
+import os
+import re
+from pathlib import Path
+
+# Environment Variables
+# set this to specify working directory of tests
+TEST_HOME_VAR = 'STANZA_TEST_HOME'
+
+# Global Variables
+# test working directory base name must be stanza_test
+TEST_DIR_BASE_NAME = 'stanza_test'
+
+# check the working dir is set and compliant
+assert os.getenv(TEST_HOME_VAR) is not None, \
+    f'Please set {TEST_HOME_VAR} environment variable for test working dir, base name must be: {TEST_DIR_BASE_NAME}'
+TEST_WORKING_DIR = os.getenv(TEST_HOME_VAR)
+assert os.path.basename(TEST_WORKING_DIR) == TEST_DIR_BASE_NAME, \
+    f'Base name of test home dir must be: {TEST_DIR_BASE_NAME}'
+
+HOME_DIR = str(Path.home())
+
+TEST_MODELS_DIR = os.path.join(HOME_DIR, f'{TEST_WORKING_DIR}/models')
+
+DEFAULT_MODEL_DIR = os.getenv(
+    'CLASSLA_RESOURCES_DIR',
+    os.path.join(HOME_DIR, 'classla_resources')
+)
+
+# server resources
+SERVER_TEST_PROPS = f'{TEST_WORKING_DIR}/scripts/external_server.properties'
+
+# langauge resources
+LANGUAGE_RESOURCES = {}
+
+TOKENIZE_MODEL = 'tokenizer.pt'
+MWT_MODEL = 'mwt_expander.pt'
+POS_MODEL = 'tagger.pt'
+POS_PRETRAIN = 'pretrain.pt'
+LEMMA_MODEL = 'lemmatizer.pt'
+DEPPARSE_MODEL = 'parser.pt'
+DEPPARSE_PRETRAIN = 'pretrain.pt'
+
+MODEL_FILES = [TOKENIZE_MODEL, MWT_MODEL, POS_MODEL, POS_PRETRAIN, LEMMA_MODEL, DEPPARSE_MODEL, DEPPARSE_PRETRAIN]
+
+# English resources
+EN_KEY = 'en'
+EN_SHORTHAND = 'en_ewt'
+# models
+EN_MODELS_DIR = f'{TEST_WORKING_DIR}/models/{EN_SHORTHAND}_models'
+EN_MODEL_FILES = [f'{EN_MODELS_DIR}/{EN_SHORTHAND}_{model_fname}' for model_fname in MODEL_FILES]
+
+# French resources
+FR_KEY = 'fr'
+FR_SHORTHAND = 'fr_gsd'
+# regression file paths
+FR_TEST_IN = f'{TEST_WORKING_DIR}/in/fr_gsd.test.txt'
+FR_TEST_OUT = f'{TEST_WORKING_DIR}/out/fr_gsd.test.txt.out'
+FR_TEST_GOLD_OUT = f'{TEST_WORKING_DIR}/out/fr_gsd.test.txt.out.gold'
+# models
+FR_MODELS_DIR = f'{TEST_WORKING_DIR}/models/{FR_SHORTHAND}_models'
+FR_MODEL_FILES = [f'{FR_MODELS_DIR}/{FR_SHORTHAND}_{model_fname}' for model_fname in MODEL_FILES]
+
+# Other language resources
+AR_SHORTHAND = 'ar_padt'
+DE_SHORTHAND = 'de_gsd'
+KK_SHORTHAND = 'kk_ktb'
+KO_SHORTHAND = 'ko_gsd'
+
+
+# utils for clean up
+# only allow removal of dirs/files in this approved list
+REMOVABLE_PATHS = ['en_ewt_models', 'en_ewt_tokenizer.pt', 'en_ewt_mwt_expander.pt', 'en_ewt_tagger.pt',
+                   'en_ewt.pretrain.pt', 'en_ewt_lemmatizer.pt', 'en_ewt_parser.pt', 'fr_gsd_models',
+                   'fr_gsd_tokenizer.pt', 'fr_gsd_mwt_expander.pt', 'fr_gsd_tagger.pt', 'fr_gsd.pretrain.pt',
+                   'fr_gsd_lemmatizer.pt', 'fr_gsd_parser.pt', 'ar_padt_models', 'ar_padt_tokenizer.pt',
+                   'ar_padt_mwt_expander.pt', 'ar_padt_tagger.pt', 'ar_padt.pretrain.pt', 'ar_padt_lemmatizer.pt',
+                   'ar_padt_parser.pt', 'de_gsd_models', 'de_gsd_tokenizer.pt', 'de_gsd_mwt_expander.pt',
+                   'de_gsd_tagger.pt', 'de_gsd.pretrain.pt', 'de_gsd_lemmatizer.pt', 'de_gsd_parser.pt',
+                   'kk_ktb_models', 'kk_ktb_tokenizer.pt', 'kk_ktb_mwt_expander.pt', 'kk_ktb_tagger.pt',
+                   'kk_ktb.pretrain.pt', 'kk_ktb_lemmatizer.pt', 'kk_ktb_parser.pt', 'ko_gsd_models',
+                   'ko_gsd_tokenizer.pt', 'ko_gsd_mwt_expander.pt', 'ko_gsd_tagger.pt', 'ko_gsd.pretrain.pt',
+                   'ko_gsd_lemmatizer.pt', 'ko_gsd_parser.pt']
+
+
+def safe_rm(path_to_rm):
+    """
+    Safely remove a directory of files or a file
+    1.) check path exists, files are files, dirs are dirs
+    2.) only remove things on approved list REMOVABLE_PATHS
+    3.) assert no longer exists
+    """
+    # just return if path doesn't exist
+    if not os.path.exists(path_to_rm):
+        return
+    # handle directory
+    if os.path.isdir(path_to_rm):
+        files_to_rm = [f'{path_to_rm}/{fname}' for fname in os.listdir(path_to_rm)]
+        dir_to_rm = path_to_rm
+    else:
+        files_to_rm = [path_to_rm]
+        dir_to_rm = None
+    # clear out files
+    for file_to_rm in files_to_rm:
+        if os.path.isfile(file_to_rm) and os.path.basename(file_to_rm) in REMOVABLE_PATHS:
+            os.remove(file_to_rm)
+            assert not os.path.exists(file_to_rm), f'Error removing: {file_to_rm}'
+    # clear out directory
+    if dir_to_rm is not None and os.path.isdir(dir_to_rm):
+        os.rmdir(dir_to_rm)
+        assert not os.path.exists(dir_to_rm), f'Error removing: {dir_to_rm}'
+
+def compare_ignoring_whitespace(predicted, expected):
+    predicted = re.sub('[ \t]+', ' ', predicted.strip())
+    expected = re.sub('[ \t]+', ' ', expected.strip())
+    assert predicted == expected
+
```

### Comparing `classla-2.0/tests_classla/test_lemmatizer.py` & `classla-2.1/tests_classla/test_lemmatizer.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,57 +1,57 @@
-"""
-Basic testing of lemmatization
-"""
-
-import classla
-import classla.models.lemmatizer as trainer
-
-from tests_classla import *
-
-with open('test_data/slovenian.raw') as f:
-    SL_DOC = f.read()
-
-with open('test_data/slovenian.lemmatizer') as f:
-    SL_DOC_LEMMATIZER_MODEL_GOLD = f.read()
-
-
-SL_DOC_SMALL = "France Prešeren se je rodil v vrbi."
-
-SL_DOC_IDENTITY_GOLD = """
-<Token id=1;words=[<Word id=1;text=France;lemma=France>]>
-<Token id=2;words=[<Word id=2;text=Prešeren;lemma=Prešeren>]>
-<Token id=3;words=[<Word id=3;text=se;lemma=se>]>
-<Token id=4;words=[<Word id=4;text=je;lemma=je>]>
-<Token id=5;words=[<Word id=5;text=rodil;lemma=rodil>]>
-<Token id=6;words=[<Word id=6;text=v;lemma=v>]>
-<Token id=7;words=[<Word id=7;text=vrbi;lemma=vrbi>]>
-<Token id=8;words=[<Word id=8;text=.;lemma=.>]>
-""".strip()
-
-
-def test_identity_lemmatizer():
-    nlp = classla.Pipeline(**{'processors': 'tokenize,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'sl',
-                                  'lemma_use_identity': True})
-    doc = nlp(SL_DOC_SMALL)
-    assert SL_DOC_IDENTITY_GOLD == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
-
-
-def test_full_lemmatizer():
-    nlp = classla.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'sl'})
-    doc = nlp(SL_DOC)
-    assert SL_DOC_LEMMATIZER_MODEL_GOLD == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
-
-
-def test_lemmatizer_sloleks_trainer():
-    trainer.main(args=['--model_dir', 'test_data/train/data', '--model_file', 'ssj500k+Sloleks.pt', '--train_file', 'test_data/train/tagger_lemmatizer_parser_example.conll',
-                       '--eval_file', 'test_data/train/tagger_lemmatizer_parser_example.conll', '--output_file', 'test_data/train/data/lemmatizer', '--gold_file', 'test_data/train/tagger_lemmatizer_parser_example.conll',
-                       '--mode', 'train', '--num_epoch', '1', '--decay_epoch', '20',  '--pos', '--pos_model_path', 'classla_test/models/sl/pos/standard.pt'])
-
-def test_lemmatizer_trainer():
-    trainer.main(args=['--model_dir', 'test_data/train/data', '--model_file', 'ssj500k+Sloleks.pt', '--train_file', 'test_data/train/tagger_lemmatizer_parser_example.conll',
-                       '--eval_file', 'test_data/train/tagger_lemmatizer_parser_example.conll', '--output_file', 'test_data/train/data/lemmatizer', '--gold_file', 'test_data/train/tagger_lemmatizer_parser_example.conll',
-                       '--mode', 'train', '--num_epoch', '1', '--decay_epoch', '20',  '--pos', '--external_dict', 'test_data/train/external_dict_example.tsv'])
-
-def test_lemmatizer_evaluation():
-    trainer.main(args=['--model_dir', 'test_data/train/data', '--model_file', 'ssj500k+Sloleks.pt',
-                       '--eval_file', 'test_data/train/tagger_lemmatizer_parser_example.conll', '--output_file', 'test_data/train/data/lemmatizer', '--gold_file', 'test_data/train/tagger_lemmatizer_parser_example.conll',
-                       '--mode', 'predict', '--pos_model_path', TEST_MODELS_DIR + '/sl/pos/standard.pt', '--pos_force_inf_lexicon'])
+"""
+Basic testing of lemmatization
+"""
+
+import classla
+import classla.models.lemmatizer as trainer
+
+from tests_classla import *
+
+with open('test_data/slovenian.raw') as f:
+    SL_DOC = f.read()
+
+with open('test_data/slovenian.lemmatizer') as f:
+    SL_DOC_LEMMATIZER_MODEL_GOLD = f.read()
+
+
+SL_DOC_SMALL = "France Prešeren se je rodil v vrbi."
+
+SL_DOC_IDENTITY_GOLD = """
+<Token id=1;words=[<Word id=1;text=France;lemma=France>]>
+<Token id=2;words=[<Word id=2;text=Prešeren;lemma=Prešeren>]>
+<Token id=3;words=[<Word id=3;text=se;lemma=se>]>
+<Token id=4;words=[<Word id=4;text=je;lemma=je>]>
+<Token id=5;words=[<Word id=5;text=rodil;lemma=rodil>]>
+<Token id=6;words=[<Word id=6;text=v;lemma=v>]>
+<Token id=7;words=[<Word id=7;text=vrbi;lemma=vrbi>]>
+<Token id=8;words=[<Word id=8;text=.;lemma=.>]>
+""".strip()
+
+
+def test_identity_lemmatizer():
+    nlp = classla.Pipeline(**{'processors': 'tokenize,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'sl',
+                                  'lemma_use_identity': True})
+    doc = nlp(SL_DOC_SMALL)
+    assert SL_DOC_IDENTITY_GOLD == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
+
+
+def test_full_lemmatizer():
+    nlp = classla.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'sl'})
+    doc = nlp(SL_DOC)
+    assert SL_DOC_LEMMATIZER_MODEL_GOLD == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
+
+
+def test_lemmatizer_sloleks_trainer():
+    trainer.main(args=['--model_dir', 'test_data/train/data', '--model_file', 'ssj500k+Sloleks.pt', '--train_file', 'test_data/train/tagger_lemmatizer_parser_example.conll',
+                       '--eval_file', 'test_data/train/tagger_lemmatizer_parser_example.conll', '--output_file', 'test_data/train/data/lemmatizer', '--gold_file', 'test_data/train/tagger_lemmatizer_parser_example.conll',
+                       '--mode', 'train', '--num_epoch', '1', '--decay_epoch', '20',  '--pos', '--pos_model_path', 'classla_test/models/sl/pos/standard.pt'])
+
+def test_lemmatizer_trainer():
+    trainer.main(args=['--model_dir', 'test_data/train/data', '--model_file', 'ssj500k+Sloleks.pt', '--train_file', 'test_data/train/tagger_lemmatizer_parser_example.conll',
+                       '--eval_file', 'test_data/train/tagger_lemmatizer_parser_example.conll', '--output_file', 'test_data/train/data/lemmatizer', '--gold_file', 'test_data/train/tagger_lemmatizer_parser_example.conll',
+                       '--mode', 'train', '--num_epoch', '1', '--decay_epoch', '20',  '--pos', '--external_dict', 'test_data/train/external_dict_example.tsv'])
+
+def test_lemmatizer_evaluation():
+    trainer.main(args=['--model_dir', 'test_data/train/data', '--model_file', 'ssj500k+Sloleks.pt',
+                       '--eval_file', 'test_data/train/tagger_lemmatizer_parser_example.conll', '--output_file', 'test_data/train/data/lemmatizer', '--gold_file', 'test_data/train/tagger_lemmatizer_parser_example.conll',
+                       '--mode', 'predict', '--pos_model_path', TEST_MODELS_DIR + '/sl/pos/standard.pt', '--pos_force_inf_lexicon'])
```

### Comparing `classla-2.0/tests_classla/test_ner.py` & `classla-2.1/tests_classla/test_ner.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-"""
-Basic testing of part of speech tagging
-"""
-
-import classla
-import classla.models.ner_tagger as trainer
-
-from tests_classla import *
-
-with open('test_data/slovenian.raw') as f:
-    SL_DOC = f.read()
-
-with open('test_data/slovenian.ner') as f:
-    SL_DOC_GOLD = f.read()
-
-
-def test_ner():
-    nlp = classla.Pipeline(**{'processors': 'tokenize,ner', 'dir': TEST_MODELS_DIR, 'lang': 'sl'})
-    doc = nlp(SL_DOC)
-    # with open('test_data/slovenian.ner', 'w') as f:
-    #     f.write(doc.to_conll())
-    assert SL_DOC_GOLD == doc.to_conll()
-
-
-def test_ner_trainer():
-    trainer.main(args=['--save_dir', 'test_data/train/data', '--save_name', 'ner.pt', '--train_file', 'test_data/train/ner_example.conll',
-                       '--eval_file', 'test_data/train/ner_example.conll', '--shorthand', 'sl_ssj',
-                       '--mode', 'train', '--max_steps', '500', '--scheme', 'bio', '--batch_size', '128',
-                       '--wordvec_file', 'test_data/train/embed.sl.small'])
+"""
+Basic testing of part of speech tagging
+"""
+
+import classla
+import classla.models.ner_tagger as trainer
+
+from tests_classla import *
+
+with open('test_data/slovenian.raw') as f:
+    SL_DOC = f.read()
+
+with open('test_data/slovenian.ner') as f:
+    SL_DOC_GOLD = f.read()
+
+
+def test_ner():
+    nlp = classla.Pipeline(**{'processors': 'tokenize,ner', 'dir': TEST_MODELS_DIR, 'lang': 'sl'})
+    doc = nlp(SL_DOC)
+    # with open('test_data/slovenian.ner', 'w') as f:
+    #     f.write(doc.to_conll())
+    assert SL_DOC_GOLD == doc.to_conll()
+
+
+def test_ner_trainer():
+    trainer.main(args=['--save_dir', 'test_data/train/data', '--save_name', 'ner.pt', '--train_file', 'test_data/train/ner_example.conll',
+                       '--eval_file', 'test_data/train/ner_example.conll', '--shorthand', 'sl_ssj',
+                       '--mode', 'train', '--max_steps', '500', '--scheme', 'bio', '--batch_size', '128',
+                       '--wordvec_file', 'test_data/train/embed.sl.small'])
```

### Comparing `classla-2.0/tests_classla/test_parser.py` & `classla-2.1/tests_classla/test_parser.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,30 +1,30 @@
-"""
-Basic testing of dependency parser
-"""
-
-import classla
-import classla.models.parser as trainer
-from classla.utils.conll import CoNLL
-
-from tests_classla import *
-
-with open('test_data/slovenian.raw') as f:
-    SL_DOC = f.read()
-
-with open('test_data/slovenian.parser') as f:
-    SL_DOC_GOLD = f.read()
-
-
-def test_parser():
-    nlp = classla.Pipeline(
-        **{'processors': 'tokenize,pos,lemma,depparse', 'dir': TEST_MODELS_DIR, 'lang': 'sl'})
-    doc = nlp(SL_DOC)
-    # with open('test_data/slovenian.parser', 'w') as f:
-    #     f.write(doc.to_conll())
-    assert SL_DOC_GOLD == doc.to_conll()
-
-def test_tagger_trainer():
-    trainer.main(args=['--save_dir', 'test_data/train/data', '--save_name', 'parser.pt', '--train_file', 'test_data/train/tagger_lemmatizer_parser_example.conll',
-                       '--eval_file', 'test_data/train/tagger_lemmatizer_parser_example.conll', '--output_file', 'test_data/train/data/parser', '--gold_file', 'test_data/train/tagger_lemmatizer_parser_example.conll', '--shorthand', 'sl_ssj',
-                       '--mode', 'train', '--pretrain_file', 'classla_test/models/sl/pretrain/standard.pt', '--max_steps', '100', '--wordvec_file', 'test_data/train/embed.sl-token.ft.sg.vec'])
-
+"""
+Basic testing of dependency parser
+"""
+
+import classla
+import classla.models.parser as trainer
+from classla.utils.conll import CoNLL
+
+from tests_classla import *
+
+with open('test_data/slovenian.raw') as f:
+    SL_DOC = f.read()
+
+with open('test_data/slovenian.parser') as f:
+    SL_DOC_GOLD = f.read()
+
+
+def test_parser():
+    nlp = classla.Pipeline(
+        **{'processors': 'tokenize,pos,lemma,depparse', 'dir': TEST_MODELS_DIR, 'lang': 'sl'})
+    doc = nlp(SL_DOC)
+    # with open('test_data/slovenian.parser', 'w') as f:
+    #     f.write(doc.to_conll())
+    assert SL_DOC_GOLD == doc.to_conll()
+
+def test_tagger_trainer():
+    trainer.main(args=['--save_dir', 'test_data/train/data', '--save_name', 'parser.pt', '--train_file', 'test_data/train/tagger_lemmatizer_parser_example.conll',
+                       '--eval_file', 'test_data/train/tagger_lemmatizer_parser_example.conll', '--output_file', 'test_data/train/data/parser', '--gold_file', 'test_data/train/tagger_lemmatizer_parser_example.conll', '--shorthand', 'sl_ssj',
+                       '--mode', 'train', '--pretrain_file', 'classla_test/models/sl/pretrain/standard.pt', '--max_steps', '100', '--wordvec_file', 'test_data/train/embed.sl-token.ft.sg.vec'])
+
```

### Comparing `classla-2.0/tests_classla/test_readme_examples.py` & `classla-2.1/tests_classla/test_readme_examples.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,262 +1,262 @@
-"""
-Basic testing of the English pipeline
-"""
-
-import pytest
-import classla
-from classla.utils.conll import CoNLL
-
-from tests_classla import *
-
-
-# data for testing
-SL_STANDARD = "France Prešeren je rojen v Vrbi."
-
-SL_STANDARD_CONLL = """
-# newpar id = 1
-# sent_id = 1.1
-# text = France Prešeren je rojen v Vrbi.
-1	France	France	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	4	nsubj	_	NER=B-PER
-2	Prešeren	Prešeren	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat:name	_	NER=I-PER
-3	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	cop	_	NER=O
-4	rojen	rojen	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part	0	root	_	NER=O
-5	v	v	ADP	Sl	Case=Loc	6	case	_	NER=O
-6	Vrbi	Vrba	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
-7	.	.	PUNCT	Z	_	4	punct	_	NER=O
-
-""".strip()
-
-SL_NONSTANDARD = "kva smo mi zurali zadnje leto v zagrebu..."
-
-SL_NONSTANDARD_CONLL = """
-# newpar id = 1
-# sent_id = 1.1
-# text = kva smo mi zurali zadnje leto v zagrebu...
-1	kva	kaj	PRON	Pq-nsn	Case=Nom|Gender=Neut|Number=Sing|PronType=Int	4	nsubj	_	NER=O
-2	smo	biti	AUX	Va-r1p-n	Mood=Ind|Number=Plur|Person=1|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
-3	mi	jaz	PRON	Pp3msa--y	Case=Acc|Gender=Masc|Number=Sing|Person=3|PronType=Prs|Variant=Short	4	obj	_	NER=O
-4	zurali	zurati	VERB	Vmep-pm	Aspect=Perf|Gender=Masc|Number=Plur|VerbForm=Part	0	root	_	NER=O
-5	zadnje	zadnji	ADJ	Agpnsa	Case=Acc|Degree=Pos|Gender=Neut|Number=Sing	6	amod	_	NER=O
-6	leto	leto	NOUN	Ncnsa	Case=Acc|Gender=Neut|Number=Sing	4	obl	_	NER=O
-7	v	v	ADP	Sl	Case=Loc	8	case	_	NER=O
-8	zagrebu	zagreb	NOUN	Ncmsl	Case=Loc|Gender=Masc|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
-9	...	...	PUNCT	Z	_	4	punct	_	NER=O
-
-""".strip()
-
-HR_STANDARD = "Ante Starčević rođen je u Velikom Žitniku."
-
-HR_STANDARD_CONLL = """
-# newpar id = 1
-# sent_id = 1.1
-# text = Ante Starčević rođen je u Velikom Žitniku.
-1	Ante	Ante	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj:pass	_	NER=B-PER
-2	Starčević	Starčević	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
-3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
-4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux:pass	_	NER=O
-5	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
-6	Velikom	velik	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	7	amod	_	NER=B-LOC
-7	Žitniku	Žitnik	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	3	obl	_	NER=I-LOC|SpaceAfter=No
-8	.	.	PUNCT	Z	_	3	punct	_	NER=O
-
-""".strip()
-
-HR_NONSTANDARD = "kaj sam ja tulumaril jucer u ljubljani..."
-
-HR_NONSTANDARD_CONLL = """
-# newpar id = 1
-# sent_id = 1.1
-# text = kaj sam ja tulumaril jucer u ljubljani...
-1	kaj	što	PRON	Pi3n-a	Case=Acc|Gender=Neut|PronType=Int,Rel	4	obj	_	NER=O
-2	sam	biti	AUX	Var1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
-3	ja	ja	PRON	Pp1-sn	Case=Nom|Number=Sing|Person=1|PronType=Prs	4	nsubj	_	NER=O
-4	tulumaril	tulumariti	VERB	Vmp-sm	Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	0	root	_	NER=O
-5	jucer	jučer	ADV	Rgp	Degree=Pos	4	advmod	_	NER=O
-6	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
-7	ljubljani	Ljubljana	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
-8	...	...	PUNCT	Z	_	4	punct	_	NER=O
-
-""".strip()
-
-SR_STANDARD = "Slobodan Jovanović rođen je u Novom Sadu."
-
-SR_STANDARD_CONLL = """
-# newpar id = 1
-# sent_id = 1.1
-# text = Slobodan Jovanović rođen je u Novom Sadu.
-1	Slobodan	Slobodan	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj	_	NER=B-PER
-2	Jovanović	Jovanović	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
-3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
-4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux	_	NER=O
-5	u	u	ADP	Sl	Case=Loc	6	case	_	NER=O
-6	Novom	nov	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	3	obl	_	NER=B-LOC
-7	Sadu	Sad	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	6	flat	_	NER=I-LOC|SpaceAfter=No
-8	.	.	PUNCT	Z	_	3	punct	_	NER=O
-
-""".strip()
-
-SR_NONSTANDARD = "ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu..."
-
-SR_NONSTANDARD_CONLL = """
-# newpar id = 1
-# sent_id = 1.1
-# text = ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu...
-1	ne	ne	PART	Qz	Polarity=Neg	2	advmod	_	NER=O
-2	mogu	moći	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	0	root	_	NER=O
-3	da	da	SCONJ	Cs	_	4	mark	_	NER=O
-4	verujem	verovati	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	2	xcomp	_	NER=O
-5	kakvo	kakav	DET	Pi-nsn	Case=Nom|Gender=Neut|Number=Sing|PronType=Int,Rel	4	ccomp	_	NER=O
-6	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	5	aux	_	NER=O
-7	zezanje	zezanje	NOUN	Ncnsn	Case=Nom|Gender=Neut|Number=Sing	5	nsubj	_	NER=O
-8	bilo	biti	AUX	Vap-sn	Gender=Neut|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	5	cop	_	NER=O
-9	prosle	prošli	ADJ	Agpfsgy	Case=Gen|Definite=Def|Degree=Pos|Gender=Fem|Number=Sing	10	amod	_	NER=O
-10	godine	godina	NOUN	Ncfsg	Case=Gen|Gender=Fem|Number=Sing	8	obl	_	NER=O
-11	u	u	ADP	Sl	Case=Loc	12	case	_	NER=O
-12	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	8	obl	_	NER=B-LOC|SpaceAfter=No
-13	...	...	PUNCT	Z	_	2	punct	_	NER=O
-
-""".strip()
-
-BG_STANDARD = "Алеко Константинов е роден в Свищов."
-
-BG_STANDARD_CONLL = """
-# newpar id = 1
-# sent_id = 1.1
-# text = Алеко Константинов е роден в Свищов.
-1	Алеко	алеко	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	nsubj:pass	_	NER=B-PER
-2	Константинов	константинов	PROPN	Hmsi	Definite=Ind|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
-3	е	съм	AUX	Vxitf-r3s	Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act	4	aux:pass	_	NER=O
-4	роден	родя-(се)	VERB	Vpptcv--smi	Aspect=Perf|Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
-5	в	в	ADP	R	_	6	case	_	NER=O
-6	Свищов	свищов	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	iobj	_	NER=B-LOC|SpaceAfter=No
-7	.	.	PUNCT	punct	_	4	punct	_	NER=O
-
-""".strip()
-
-MK_STANDARD = 'Крсте Петков Мисирков е роден во Постол.'
-
-MK_STANDARD_CONLL = """
-# newpar id = 1
-# sent_id = 1.1
-# text = Крсте Петков Мисирков е роден во Постол.
-1	Крсте	крсте	ADJ	Afpms-n	Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
-2	Петков	петков	NOUN	Ncmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
-3	Мисирков	мисирков	NOUN	Ncmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
-4	е	сум	AUX	Vapip3s-n	Aspect=Prog|Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres	_	_	_	_
-5	роден	роден	ADJ	Ap-ms-n	Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part	_	_	_	_
-6	во	во	ADP	Sps	AdpType=Prep	_	_	_	_
-7	Постол	постол	NOUN	Ncmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	SpaceAfter=No
-8	.	.	PUNCT	Z	_	_	_	_	_
-
-""".strip()
-
-SL_STANDARD_JOS = "France Prešeren je rojen v Vrbi."
-
-SL_STANDARD_JOS_CONLL = """
-# newpar id = 1
-# sent_id = 1.1
-# text = France Prešeren je rojen v Vrbi.
-1	France	France	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	ena	_	NER=B-PER|SRL=PAT
-2	Prešeren	Prešeren	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	dol	_	NER=I-PER
-3	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	0	modra	_	NER=O
-4	rojen	rojen	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part	3	dol	_	NER=O|SRL=RESLT
-5	v	v	ADP	Sl	Case=Loc	6	dol	_	NER=O
-6	Vrbi	Vrba	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	3	štiri	_	NER=B-LOC|SpaceAfter=No|SRL=LOC
-7	.	.	PUNCT	Z	_	0	modra	_	NER=O
-
-""".strip()
-
-
-def test_sl_standard():
-    classla.download('sl', dir=TEST_MODELS_DIR)
-    nlp = classla.Pipeline('sl', dir=TEST_MODELS_DIR)
-    doc = nlp(SL_STANDARD)
-    assert doc.to_conll().strip() == SL_STANDARD_CONLL
-
-
-def test_sl_nonstandard():
-    classla.download('sl', type='nonstandard', dir=TEST_MODELS_DIR)
-    nlp = classla.Pipeline('sl', type='nonstandard', dir=TEST_MODELS_DIR)
-    doc = nlp(SL_NONSTANDARD)
-    assert doc.to_conll().strip() == SL_NONSTANDARD_CONLL
-
-
-def test_hr_standard():
-    classla.download('hr', dir=TEST_MODELS_DIR)
-    nlp = classla.Pipeline('hr', dir=TEST_MODELS_DIR)
-    doc = nlp(HR_STANDARD)
-    assert doc.to_conll().strip() == HR_STANDARD_CONLL
-
-
-def test_hr_nonstandard():
-    classla.download('hr', type='nonstandard', dir=TEST_MODELS_DIR)
-    nlp = classla.Pipeline('hr', type='nonstandard', dir=TEST_MODELS_DIR)
-    doc = nlp(HR_NONSTANDARD)
-    assert doc.to_conll().strip() == HR_NONSTANDARD_CONLL
-
-
-def test_sr_standard():
-    classla.download('sr', dir=TEST_MODELS_DIR)
-    nlp = classla.Pipeline('sr', dir=TEST_MODELS_DIR)
-    doc = nlp(SR_STANDARD)
-    assert doc.to_conll().strip() == SR_STANDARD_CONLL
-
-
-def test_sr_nonstandard():
-    classla.download('sr', type='nonstandard', dir=TEST_MODELS_DIR)
-    nlp = classla.Pipeline('sr', type='nonstandard', dir=TEST_MODELS_DIR)
-    doc = nlp(SR_NONSTANDARD)
-    assert doc.to_conll().strip() == SR_NONSTANDARD_CONLL
-
-
-def test_bg_standard():
-    classla.download('bg', dir=TEST_MODELS_DIR)
-    nlp = classla.Pipeline('bg', dir=TEST_MODELS_DIR)
-    doc = nlp(BG_STANDARD)
-    assert doc.to_conll().strip() == BG_STANDARD_CONLL
-
-
-def test_mk_standard():
-    classla.download('mk', dir=TEST_MODELS_DIR)
-    nlp = classla.Pipeline('mk', dir=TEST_MODELS_DIR)
-    doc = nlp(MK_STANDARD)
-    assert doc.to_conll().strip() == MK_STANDARD_CONLL
-
-
-def test_sl_standard_jos():
-    classla.download('sl', type='standard_jos', dir=TEST_MODELS_DIR)
-    nlp = classla.Pipeline('sl', processors='tokenize,pos,lemma,depparse,ner,srl', type='standard_jos', dir=TEST_MODELS_DIR)
-    doc = nlp(SL_STANDARD_JOS)
-    assert doc.to_conll().strip() == SL_STANDARD_JOS_CONLL
-
-
-def test_sl_inflectional():
-    classla.download('sl', dir=TEST_MODELS_DIR)
-    nlp = classla.Pipeline('sl', pos_use_lexicon=True, dir=TEST_MODELS_DIR)
-    doc = nlp(SL_STANDARD)
-    assert doc.to_conll().strip() == SL_STANDARD_CONLL
-
-def test_sl_pos_lemma_pretag():
-    classla.download('sl', dir=TEST_MODELS_DIR)
-    nlp = classla.Pipeline('sl', pos_lemma_pretag=False, dir=TEST_MODELS_DIR)
-    doc = nlp(SL_STANDARD)
-    assert doc.to_conll().strip() == SL_STANDARD_CONLL
-
-def test_sl_pretokenized_conllu():
-    classla.download('sl', dir=TEST_MODELS_DIR)
-    nlp = classla.Pipeline('sl', tokenize_pretokenized='conllu', dir=TEST_MODELS_DIR)
-    conllu_pretokenized = """
-# newpar id = 1
-# sent_id = 1.1
-# text = France Prešeren je rojen v Vrbi.
-1	France	France	_	_	_	_	_	_	_
-2	Prešeren	Prešeren	_	_	_	_	_	_	_
-3	je	biti	_	_	_	_	_	_	_
-4	rojen	rojen	_	_	_	_	_	_	_
-5	v	v	_	_	_	_	_	_	_
-6	Vrbi	Vrba	_	_	_	_	_	_	SpaceAfter=No
-7	.	.	_	_	_	_	_	_	_
-
-"""
-    doc = nlp(conllu_pretokenized)
-    assert doc.to_conll().strip() == SL_STANDARD_CONLL
+"""
+Basic testing of the English pipeline
+"""
+
+import pytest
+import classla
+from classla.utils.conll import CoNLL
+
+from tests_classla import *
+
+
+# data for testing
+SL_STANDARD = "France Prešeren je rojen v Vrbi."
+
+SL_STANDARD_CONLL = """
+# newpar id = 1
+# sent_id = 1.1
+# text = France Prešeren je rojen v Vrbi.
+1	France	France	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	4	nsubj	_	NER=B-PER
+2	Prešeren	Prešeren	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat:name	_	NER=I-PER
+3	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	cop	_	NER=O
+4	rojen	rojen	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part	0	root	_	NER=O
+5	v	v	ADP	Sl	Case=Loc	6	case	_	NER=O
+6	Vrbi	Vrba	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
+7	.	.	PUNCT	Z	_	4	punct	_	NER=O
+
+""".strip()
+
+SL_NONSTANDARD = "kva smo mi zurali zadnje leto v zagrebu..."
+
+SL_NONSTANDARD_CONLL = """
+# newpar id = 1
+# sent_id = 1.1
+# text = kva smo mi zurali zadnje leto v zagrebu...
+1	kva	kaj	PRON	Pq-nsa	Case=Acc|Gender=Neut|Number=Sing|PronType=Int	4	obj	_	NER=O
+2	smo	biti	AUX	Va-r1p-n	Mood=Ind|Number=Plur|Person=1|Polarity=Pos|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
+3	mi	jaz	PRON	Pp1mpn	Case=Nom|Gender=Masc|Number=Plur|Person=1|PronType=Prs	4	nsubj	_	NER=O
+4	zurali	zurati	VERB	Vmpp-pm	Aspect=Imp|Gender=Masc|Number=Plur|VerbForm=Part	0	root	_	NER=O
+5	zadnje	zadnji	ADJ	Agpnsa	Case=Acc|Degree=Pos|Gender=Neut|Number=Sing	6	amod	_	NER=O
+6	leto	leto	NOUN	Ncnsa	Case=Acc|Gender=Neut|Number=Sing	4	obl	_	NER=O
+7	v	v	ADP	Sl	Case=Loc	8	case	_	NER=O
+8	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
+9	...	...	PUNCT	Z	_	4	punct	_	NER=O
+
+""".strip()
+
+HR_STANDARD = "Ante Starčević rođen je u Velikom Žitniku."
+
+HR_STANDARD_CONLL = """
+# newpar id = 1
+# sent_id = 1.1
+# text = Ante Starčević rođen je u Velikom Žitniku.
+1	Ante	Ante	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj	_	NER=B-PER
+2	Starčević	Starčević	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
+3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
+4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux	_	NER=O
+5	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
+6	Velikom	velik	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	7	amod	_	NER=B-LOC
+7	Žitniku	Žitnik	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	3	obl	_	NER=I-LOC|SpaceAfter=No
+8	.	.	PUNCT	Z	_	3	punct	_	NER=O
+
+""".strip()
+
+HR_NONSTANDARD = "kaj sam ja tulumaril jucer u ljubljani..."
+
+HR_NONSTANDARD_CONLL = """
+# newpar id = 1
+# sent_id = 1.1
+# text = kaj sam ja tulumaril jucer u ljubljani...
+1	kaj	što	PRON	Pq3n-a	Case=Acc|Gender=Neut|PronType=Int,Rel	4	obj	_	NER=O
+2	sam	biti	AUX	Var1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	4	aux	_	NER=O
+3	ja	ja	PRON	Pp1-sn	Case=Nom|Number=Sing|Person=1|PronType=Prs	4	nsubj	_	NER=O
+4	tulumaril	tulumariti	VERB	Vmp-sm	Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	0	root	_	NER=O
+5	jucer	jučer	ADV	Rgp	Degree=Pos	4	advmod	_	NER=O
+6	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
+7	ljubljani	Ljubljana	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	4	obl	_	NER=B-LOC|SpaceAfter=No
+8	...	...	PUNCT	Z	_	4	punct	_	NER=O
+
+""".strip()
+
+SR_STANDARD = "Slobodan Jovanović rođen je u Novom Sadu."
+
+SR_STANDARD_CONLL = """
+# newpar id = 1
+# sent_id = 1.1
+# text = Slobodan Jovanović rođen je u Novom Sadu.
+1	Slobodan	Slobodan	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	nsubj	_	NER=B-PER
+2	Jovanović	Jovanović	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
+3	rođen	roditi	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
+4	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	3	aux	_	NER=O
+5	u	u	ADP	Sl	Case=Loc	7	case	_	NER=O
+6	Novom	nov	ADJ	Agpmsly	Case=Loc|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	7	amod	_	NER=B-LOC
+7	Sadu	Sad	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	3	obl	_	NER=I-LOC|SpaceAfter=No
+8	.	.	PUNCT	Z	_	3	punct	_	NER=O
+
+""".strip()
+
+SR_NONSTANDARD = "ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu..."
+
+SR_NONSTANDARD_CONLL = """
+# newpar id = 1
+# sent_id = 1.1
+# text = ne mogu da verujem kakvo je zezanje bilo prosle godine u zagrebu...
+1	ne	ne	PART	Qz	Polarity=Neg	2	advmod	_	NER=O
+2	mogu	moći	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	0	root	_	NER=O
+3	da	da	SCONJ	Cs	_	4	mark	_	NER=O
+4	verujem	verovati	VERB	Vmr1s	Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin	2	xcomp	_	NER=O
+5	kakvo	kakav	DET	Pi-nsn	Case=Nom|Gender=Neut|Number=Sing|PronType=Int,Rel	4	ccomp	_	NER=O
+6	je	biti	AUX	Var3s	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin	5	aux	_	NER=O
+7	zezanje	zezanje	NOUN	Ncnsn	Case=Nom|Gender=Neut|Number=Sing	8	nsubj	_	NER=O
+8	bilo	biti	AUX	Vap-sn	Gender=Neut|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act	5	cop	_	NER=O
+9	prosle	prošli	ADJ	Agpfsgy	Case=Gen|Definite=Def|Degree=Pos|Gender=Fem|Number=Sing	10	amod	_	NER=O
+10	godine	godina	NOUN	Ncfsg	Case=Gen|Gender=Fem|Number=Sing	8	obl	_	NER=O
+11	u	u	ADP	Sl	Case=Loc	12	case	_	NER=O
+12	zagrebu	Zagreb	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	8	obl	_	NER=B-LOC|SpaceAfter=No
+13	...	...	PUNCT	Z	_	2	punct	_	NER=O
+
+""".strip()
+
+BG_STANDARD = "Алеко Константинов е роден в Свищов."
+
+BG_STANDARD_CONLL = """
+# newpar id = 1
+# sent_id = 1.1
+# text = Алеко Константинов е роден в Свищов.
+1	Алеко	алеко	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	nsubj:pass	_	NER=B-PER
+2	Константинов	константинов	PROPN	Hmsi	Definite=Ind|Gender=Masc|Number=Sing	1	flat	_	NER=I-PER
+3	е	съм	AUX	Vxitf-r3s	Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act	4	aux:pass	_	NER=O
+4	роден	родя-(се)	VERB	Vpptcv--smi	Aspect=Perf|Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part|Voice=Pass	0	root	_	NER=O
+5	в	в	ADP	R	_	6	case	_	NER=O
+6	Свищов	свищов	PROPN	Npmsi	Definite=Ind|Gender=Masc|Number=Sing	4	iobj	_	NER=B-LOC|SpaceAfter=No
+7	.	.	PUNCT	punct	_	4	punct	_	NER=O
+
+""".strip()
+
+MK_STANDARD = 'Крсте Петков Мисирков е роден во Постол.'
+
+MK_STANDARD_CONLL = """
+# newpar id = 1
+# sent_id = 1.1
+# text = Крсте Петков Мисирков е роден во Постол.
+1	Крсте	Крсте	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
+2	Петков	Петков	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
+3	Мисирков	Мисирков	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	_
+4	е	сум	AUX	Vapip3s-n	Aspect=Prog|Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres	_	_	_	_
+5	роден	роден	ADJ	Ap-ms-n	Definite=Ind|Gender=Masc|Number=Sing|VerbForm=Part	_	_	_	_
+6	во	во	ADP	Sps	AdpType=Prep	_	_	_	_
+7	Постол	Постол	PROPN	Npmsnn	Case=Nom|Definite=Ind|Gender=Masc|Number=Sing	_	_	_	SpaceAfter=No
+8	.	.	PUNCT	Z	_	_	_	_	_
+
+""".strip()
+
+SL_STANDARD_JOS = "France Prešeren je rojen v Vrbi."
+
+SL_STANDARD_JOS_CONLL = """
+# newpar id = 1
+# sent_id = 1.1
+# text = France Prešeren je rojen v Vrbi.
+1	France	France	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	3	ena	_	NER=B-PER|SRL=PAT
+2	Prešeren	Prešeren	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	dol	_	NER=I-PER
+3	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	0	modra	_	NER=O
+4	rojen	rojen	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part	3	dol	_	NER=O|SRL=RESLT
+5	v	v	ADP	Sl	Case=Loc	6	dol	_	NER=O
+6	Vrbi	Vrba	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	3	štiri	_	NER=B-LOC|SpaceAfter=No|SRL=LOC
+7	.	.	PUNCT	Z	_	0	modra	_	NER=O
+
+""".strip()
+
+
+def test_sl_standard():
+    classla.download('sl', dir=TEST_MODELS_DIR)
+    nlp = classla.Pipeline('sl', dir=TEST_MODELS_DIR)
+    doc = nlp(SL_STANDARD)
+    assert doc.to_conll().strip() == SL_STANDARD_CONLL
+
+
+def test_sl_nonstandard():
+    classla.download('sl', type='nonstandard', dir=TEST_MODELS_DIR)
+    nlp = classla.Pipeline('sl', type='nonstandard', dir=TEST_MODELS_DIR)
+    doc = nlp(SL_NONSTANDARD)
+    assert doc.to_conll().strip() == SL_NONSTANDARD_CONLL
+
+
+def test_hr_standard():
+    classla.download('hr', dir=TEST_MODELS_DIR)
+    nlp = classla.Pipeline('hr', dir=TEST_MODELS_DIR)
+    doc = nlp(HR_STANDARD)
+    assert doc.to_conll().strip() == HR_STANDARD_CONLL
+
+
+def test_hr_nonstandard():
+    classla.download('hr', type='nonstandard', dir=TEST_MODELS_DIR)
+    nlp = classla.Pipeline('hr', type='nonstandard', dir=TEST_MODELS_DIR)
+    doc = nlp(HR_NONSTANDARD)
+    assert doc.to_conll().strip() == HR_NONSTANDARD_CONLL
+
+
+def test_sr_standard():
+    classla.download('sr', dir=TEST_MODELS_DIR)
+    nlp = classla.Pipeline('sr', dir=TEST_MODELS_DIR)
+    doc = nlp(SR_STANDARD)
+    assert doc.to_conll().strip() == SR_STANDARD_CONLL
+
+
+def test_sr_nonstandard():
+    classla.download('sr', type='nonstandard', dir=TEST_MODELS_DIR)
+    nlp = classla.Pipeline('sr', type='nonstandard', dir=TEST_MODELS_DIR)
+    doc = nlp(SR_NONSTANDARD)
+    assert doc.to_conll().strip() == SR_NONSTANDARD_CONLL
+
+
+def test_bg_standard():
+    classla.download('bg', dir=TEST_MODELS_DIR)
+    nlp = classla.Pipeline('bg', dir=TEST_MODELS_DIR)
+    doc = nlp(BG_STANDARD)
+    assert doc.to_conll().strip() == BG_STANDARD_CONLL
+
+
+def test_mk_standard():
+    classla.download('mk', dir=TEST_MODELS_DIR)
+    nlp = classla.Pipeline('mk', dir=TEST_MODELS_DIR)
+    doc = nlp(MK_STANDARD)
+    assert doc.to_conll().strip() == MK_STANDARD_CONLL
+
+
+def test_sl_standard_jos():
+    classla.download('sl', type='standard_jos', dir=TEST_MODELS_DIR)
+    nlp = classla.Pipeline('sl', processors='tokenize,pos,lemma,depparse,ner,srl', type='standard_jos', dir=TEST_MODELS_DIR)
+    doc = nlp(SL_STANDARD_JOS)
+    assert doc.to_conll().strip() == SL_STANDARD_JOS_CONLL
+
+
+def test_sl_inflectional():
+    classla.download('sl', dir=TEST_MODELS_DIR)
+    nlp = classla.Pipeline('sl', pos_use_lexicon=True, dir=TEST_MODELS_DIR)
+    doc = nlp(SL_STANDARD)
+    assert doc.to_conll().strip() == SL_STANDARD_CONLL
+
+def test_sl_pos_lemma_pretag():
+    classla.download('sl', dir=TEST_MODELS_DIR)
+    nlp = classla.Pipeline('sl', pos_lemma_pretag=False, dir=TEST_MODELS_DIR)
+    doc = nlp(SL_STANDARD)
+    assert doc.to_conll().strip() == SL_STANDARD_CONLL
+
+def test_sl_pretokenized_conllu():
+    classla.download('sl', dir=TEST_MODELS_DIR)
+    nlp = classla.Pipeline('sl', tokenize_pretokenized='conllu', dir=TEST_MODELS_DIR)
+    conllu_pretokenized = """
+# newpar id = 1
+# sent_id = 1.1
+# text = France Prešeren je rojen v Vrbi.
+1	France	France	_	_	_	_	_	_	_
+2	Prešeren	Prešeren	_	_	_	_	_	_	_
+3	je	biti	_	_	_	_	_	_	_
+4	rojen	rojen	_	_	_	_	_	_	_
+5	v	v	_	_	_	_	_	_	_
+6	Vrbi	Vrba	_	_	_	_	_	_	SpaceAfter=No
+7	.	.	_	_	_	_	_	_	_
+
+"""
+    doc = nlp(conllu_pretokenized)
+    assert doc.to_conll().strip() == SL_STANDARD_CONLL
```

### Comparing `classla-2.0/tests_classla/test_slovenian_pipeline.py` & `classla-2.1/tests_classla/test_slovenian_pipeline.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,153 +1,153 @@
-"""
-Basic testing of the English pipeline
-"""
-
-import pytest
-import classla
-from classla.utils.conll import CoNLL
-
-from tests_classla import *
-
-
-# data for testing
-SL_DOC = "France Prešeren je bil rojen v Vrbi. Danes je poznan kot največji slovenski pesnik. Študiral je na Dunaju."
-
-SL_DOC_TOKENS_GOLD = """
-<Token id=1;words=[<Word id=1;text=France;lemma=France;upos=PROPN;xpos=Npmsn;feats=Case=Nom|Gender=Masc|Number=Sing;head=5;deprel=nsubj>]>
-<Token id=2;words=[<Word id=2;text=Prešeren;lemma=Prešeren;upos=PROPN;xpos=Npmsn;feats=Case=Nom|Gender=Masc|Number=Sing;head=1;deprel=flat:name>]>
-<Token id=3;words=[<Word id=3;text=je;lemma=biti;upos=AUX;xpos=Va-r3s-n;feats=Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin;head=5;deprel=aux>]>
-<Token id=4;words=[<Word id=4;text=bil;lemma=biti;upos=AUX;xpos=Va-p-sm;feats=Gender=Masc|Number=Sing|VerbForm=Part;head=5;deprel=cop>]>
-<Token id=5;words=[<Word id=5;text=rojen;lemma=rojen;upos=ADJ;xpos=Appmsnn;feats=Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part;head=0;deprel=root>]>
-<Token id=6;words=[<Word id=6;text=v;lemma=v;upos=ADP;xpos=Sl;feats=Case=Loc;head=7;deprel=case>]>
-<Token id=7;words=[<Word id=7;text=Vrbi;lemma=Vrba;upos=PROPN;xpos=Npfsl;feats=Case=Loc|Gender=Fem|Number=Sing;head=5;deprel=obl>]>
-<Token id=8;words=[<Word id=8;text=.;lemma=.;upos=PUNCT;xpos=Z;head=5;deprel=punct>]>
-
-<Token id=1;words=[<Word id=1;text=Danes;lemma=danes;upos=ADV;xpos=Rgp;feats=Degree=Pos;head=3;deprel=advmod>]>
-<Token id=2;words=[<Word id=2;text=je;lemma=biti;upos=AUX;xpos=Va-r3s-n;feats=Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin;head=3;deprel=cop>]>
-<Token id=3;words=[<Word id=3;text=poznan;lemma=poznan;upos=ADJ;xpos=Appmsnn;feats=Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part;head=0;deprel=root>]>
-<Token id=4;words=[<Word id=4;text=kot;lemma=kot;upos=SCONJ;xpos=Cs;head=7;deprel=case>]>
-<Token id=5;words=[<Word id=5;text=največji;lemma=velik;upos=ADJ;xpos=Agsmsny;feats=Case=Nom|Definite=Def|Degree=Sup|Gender=Masc|Number=Sing;head=7;deprel=amod>]>
-<Token id=6;words=[<Word id=6;text=slovenski;lemma=slovenski;upos=ADJ;xpos=Agpmsny;feats=Case=Nom|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing;head=7;deprel=amod>]>
-<Token id=7;words=[<Word id=7;text=pesnik;lemma=pesnik;upos=NOUN;xpos=Ncmsn;feats=Case=Nom|Gender=Masc|Number=Sing;head=3;deprel=obl>]>
-<Token id=8;words=[<Word id=8;text=.;lemma=.;upos=PUNCT;xpos=Z;head=3;deprel=punct>]>
-
-<Token id=1;words=[<Word id=1;text=Študiral;lemma=študirati;upos=VERB;xpos=Vmpp-sm;feats=Aspect=Imp|Gender=Masc|Number=Sing|VerbForm=Part;head=0;deprel=root>]>
-<Token id=2;words=[<Word id=2;text=je;lemma=biti;upos=AUX;xpos=Va-r3s-n;feats=Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin;head=1;deprel=aux>]>
-<Token id=3;words=[<Word id=3;text=na;lemma=na;upos=ADP;xpos=Sl;feats=Case=Loc;head=4;deprel=case>]>
-<Token id=4;words=[<Word id=4;text=Dunaju;lemma=Dunaj;upos=PROPN;xpos=Npmsl;feats=Case=Loc|Gender=Masc|Number=Sing;head=1;deprel=obl>]>
-<Token id=5;words=[<Word id=5;text=.;lemma=.;upos=PUNCT;xpos=Z;head=1;deprel=punct>]>
-""".strip()
-
-SL_DOC_WORDS_GOLD = """
-<Word id=1;text=France;lemma=France;upos=PROPN;xpos=Npmsn;feats=Case=Nom|Gender=Masc|Number=Sing;head=5;deprel=nsubj>
-<Word id=2;text=Prešeren;lemma=Prešeren;upos=PROPN;xpos=Npmsn;feats=Case=Nom|Gender=Masc|Number=Sing;head=1;deprel=flat:name>
-<Word id=3;text=je;lemma=biti;upos=AUX;xpos=Va-r3s-n;feats=Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin;head=5;deprel=aux>
-<Word id=4;text=bil;lemma=biti;upos=AUX;xpos=Va-p-sm;feats=Gender=Masc|Number=Sing|VerbForm=Part;head=5;deprel=cop>
-<Word id=5;text=rojen;lemma=rojen;upos=ADJ;xpos=Appmsnn;feats=Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part;head=0;deprel=root>
-<Word id=6;text=v;lemma=v;upos=ADP;xpos=Sl;feats=Case=Loc;head=7;deprel=case>
-<Word id=7;text=Vrbi;lemma=Vrba;upos=PROPN;xpos=Npfsl;feats=Case=Loc|Gender=Fem|Number=Sing;head=5;deprel=obl>
-<Word id=8;text=.;lemma=.;upos=PUNCT;xpos=Z;head=5;deprel=punct>
-
-<Word id=1;text=Danes;lemma=danes;upos=ADV;xpos=Rgp;feats=Degree=Pos;head=3;deprel=advmod>
-<Word id=2;text=je;lemma=biti;upos=AUX;xpos=Va-r3s-n;feats=Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin;head=3;deprel=cop>
-<Word id=3;text=poznan;lemma=poznan;upos=ADJ;xpos=Appmsnn;feats=Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part;head=0;deprel=root>
-<Word id=4;text=kot;lemma=kot;upos=SCONJ;xpos=Cs;head=7;deprel=case>
-<Word id=5;text=največji;lemma=velik;upos=ADJ;xpos=Agsmsny;feats=Case=Nom|Definite=Def|Degree=Sup|Gender=Masc|Number=Sing;head=7;deprel=amod>
-<Word id=6;text=slovenski;lemma=slovenski;upos=ADJ;xpos=Agpmsny;feats=Case=Nom|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing;head=7;deprel=amod>
-<Word id=7;text=pesnik;lemma=pesnik;upos=NOUN;xpos=Ncmsn;feats=Case=Nom|Gender=Masc|Number=Sing;head=3;deprel=obl>
-<Word id=8;text=.;lemma=.;upos=PUNCT;xpos=Z;head=3;deprel=punct>
-
-<Word id=1;text=Študiral;lemma=študirati;upos=VERB;xpos=Vmpp-sm;feats=Aspect=Imp|Gender=Masc|Number=Sing|VerbForm=Part;head=0;deprel=root>
-<Word id=2;text=je;lemma=biti;upos=AUX;xpos=Va-r3s-n;feats=Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin;head=1;deprel=aux>
-<Word id=3;text=na;lemma=na;upos=ADP;xpos=Sl;feats=Case=Loc;head=4;deprel=case>
-<Word id=4;text=Dunaju;lemma=Dunaj;upos=PROPN;xpos=Npmsl;feats=Case=Loc|Gender=Masc|Number=Sing;head=1;deprel=obl>
-<Word id=5;text=.;lemma=.;upos=PUNCT;xpos=Z;head=1;deprel=punct>
-""".strip()
-
-SL_DOC_DEPENDENCY_PARSES_GOLD = """
-('France', 5, 'nsubj')
-('Prešeren', 1, 'flat:name')
-('je', 5, 'aux')
-('bil', 5, 'cop')
-('rojen', 0, 'root')
-('v', 7, 'case')
-('Vrbi', 5, 'obl')
-('.', 5, 'punct')
-
-('Danes', 3, 'advmod')
-('je', 3, 'cop')
-('poznan', 0, 'root')
-('kot', 7, 'case')
-('največji', 7, 'amod')
-('slovenski', 7, 'amod')
-('pesnik', 3, 'obl')
-('.', 3, 'punct')
-
-('Študiral', 0, 'root')
-('je', 1, 'aux')
-('na', 4, 'case')
-('Dunaju', 1, 'obl')
-('.', 1, 'punct')
-""".strip()
-
-SL_DOC_CONLLU_GOLD = """
-# newpar id = 1
-# sent_id = 1.1
-# text = France Prešeren je bil rojen v Vrbi.
-1	France	France	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	5	nsubj	_	NER=B-PER
-2	Prešeren	Prešeren	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat:name	_	NER=I-PER
-3	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	5	aux	_	NER=O
-4	bil	biti	AUX	Va-p-sm	Gender=Masc|Number=Sing|VerbForm=Part	5	cop	_	NER=O
-5	rojen	rojen	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part	0	root	_	NER=O
-6	v	v	ADP	Sl	Case=Loc	7	case	_	NER=O
-7	Vrbi	Vrba	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	5	obl	_	NER=B-LOC|SpaceAfter=No
-8	.	.	PUNCT	Z	_	5	punct	_	NER=O
-
-# sent_id = 1.2
-# text = Danes je poznan kot največji slovenski pesnik.
-1	Danes	danes	ADV	Rgp	Degree=Pos	3	advmod	_	NER=O
-2	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	3	cop	_	NER=O
-3	poznan	poznan	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part	0	root	_	NER=O
-4	kot	kot	SCONJ	Cs	_	7	case	_	NER=O
-5	največji	velik	ADJ	Agsmsny	Case=Nom|Definite=Def|Degree=Sup|Gender=Masc|Number=Sing	7	amod	_	NER=O
-6	slovenski	slovenski	ADJ	Agpmsny	Case=Nom|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	7	amod	_	NER=O
-7	pesnik	pesnik	NOUN	Ncmsn	Case=Nom|Gender=Masc|Number=Sing	3	obl	_	NER=O|SpaceAfter=No
-8	.	.	PUNCT	Z	_	3	punct	_	NER=O
-
-# sent_id = 1.3
-# text = Študiral je na Dunaju.
-1	Študiral	študirati	VERB	Vmpp-sm	Aspect=Imp|Gender=Masc|Number=Sing|VerbForm=Part	0	root	_	NER=O
-2	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	1	aux	_	NER=O
-3	na	na	ADP	Sl	Case=Loc	4	case	_	NER=O
-4	Dunaju	Dunaj	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	1	obl	_	NER=B-LOC|SpaceAfter=No
-5	.	.	PUNCT	Z	_	1	punct	_	NER=O
-
-""".lstrip()
-
-
-@pytest.fixture(scope="module")
-def processed_doc():
-    """ Document created by running full Slovenian pipeline on a few sentences """
-    nlp = classla.Pipeline(dir=TEST_MODELS_DIR)
-    return nlp(SL_DOC)
-
-
-def test_text(processed_doc):
-    assert processed_doc.text == SL_DOC
-
-
-def test_conllu(processed_doc):
-    assert processed_doc.to_conll() == SL_DOC_CONLLU_GOLD
-
-
-def test_tokens(processed_doc):
-    assert "\n\n".join([sent.tokens_string() for sent in processed_doc.sentences]) == SL_DOC_TOKENS_GOLD
-
-
-def test_words(processed_doc):
-    assert "\n\n".join([sent.words_string() for sent in processed_doc.sentences]) == SL_DOC_WORDS_GOLD
-
-
-def test_dependency_parse(processed_doc):
-    assert "\n\n".join([sent.dependencies_string() for sent in processed_doc.sentences]) == \
-           SL_DOC_DEPENDENCY_PARSES_GOLD
+"""
+Basic testing of the English pipeline
+"""
+
+import pytest
+import classla
+from classla.utils.conll import CoNLL
+
+from tests_classla import *
+
+
+# data for testing
+SL_DOC = "France Prešeren je bil rojen v Vrbi. Danes je poznan kot največji slovenski pesnik. Študiral je na Dunaju."
+
+SL_DOC_TOKENS_GOLD = """
+<Token id=1;words=[<Word id=1;text=France;lemma=France;upos=PROPN;xpos=Npmsn;feats=Case=Nom|Gender=Masc|Number=Sing;head=5;deprel=nsubj>]>
+<Token id=2;words=[<Word id=2;text=Prešeren;lemma=Prešeren;upos=PROPN;xpos=Npmsn;feats=Case=Nom|Gender=Masc|Number=Sing;head=1;deprel=flat:name>]>
+<Token id=3;words=[<Word id=3;text=je;lemma=biti;upos=AUX;xpos=Va-r3s-n;feats=Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin;head=5;deprel=aux>]>
+<Token id=4;words=[<Word id=4;text=bil;lemma=biti;upos=AUX;xpos=Va-p-sm;feats=Gender=Masc|Number=Sing|VerbForm=Part;head=5;deprel=cop>]>
+<Token id=5;words=[<Word id=5;text=rojen;lemma=rojen;upos=ADJ;xpos=Appmsnn;feats=Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part;head=0;deprel=root>]>
+<Token id=6;words=[<Word id=6;text=v;lemma=v;upos=ADP;xpos=Sl;feats=Case=Loc;head=7;deprel=case>]>
+<Token id=7;words=[<Word id=7;text=Vrbi;lemma=Vrba;upos=PROPN;xpos=Npfsl;feats=Case=Loc|Gender=Fem|Number=Sing;head=5;deprel=obl>]>
+<Token id=8;words=[<Word id=8;text=.;lemma=.;upos=PUNCT;xpos=Z;head=5;deprel=punct>]>
+
+<Token id=1;words=[<Word id=1;text=Danes;lemma=danes;upos=ADV;xpos=Rgp;feats=Degree=Pos;head=3;deprel=advmod>]>
+<Token id=2;words=[<Word id=2;text=je;lemma=biti;upos=AUX;xpos=Va-r3s-n;feats=Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin;head=3;deprel=cop>]>
+<Token id=3;words=[<Word id=3;text=poznan;lemma=poznan;upos=ADJ;xpos=Appmsnn;feats=Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part;head=0;deprel=root>]>
+<Token id=4;words=[<Word id=4;text=kot;lemma=kot;upos=SCONJ;xpos=Cs;head=7;deprel=case>]>
+<Token id=5;words=[<Word id=5;text=največji;lemma=velik;upos=ADJ;xpos=Agsmsny;feats=Case=Nom|Definite=Def|Degree=Sup|Gender=Masc|Number=Sing;head=7;deprel=amod>]>
+<Token id=6;words=[<Word id=6;text=slovenski;lemma=slovenski;upos=ADJ;xpos=Agpmsny;feats=Case=Nom|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing;head=7;deprel=amod>]>
+<Token id=7;words=[<Word id=7;text=pesnik;lemma=pesnik;upos=NOUN;xpos=Ncmsn;feats=Case=Nom|Gender=Masc|Number=Sing;head=3;deprel=obl>]>
+<Token id=8;words=[<Word id=8;text=.;lemma=.;upos=PUNCT;xpos=Z;head=3;deprel=punct>]>
+
+<Token id=1;words=[<Word id=1;text=Študiral;lemma=študirati;upos=VERB;xpos=Vmpp-sm;feats=Aspect=Imp|Gender=Masc|Number=Sing|VerbForm=Part;head=0;deprel=root>]>
+<Token id=2;words=[<Word id=2;text=je;lemma=biti;upos=AUX;xpos=Va-r3s-n;feats=Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin;head=1;deprel=aux>]>
+<Token id=3;words=[<Word id=3;text=na;lemma=na;upos=ADP;xpos=Sl;feats=Case=Loc;head=4;deprel=case>]>
+<Token id=4;words=[<Word id=4;text=Dunaju;lemma=Dunaj;upos=PROPN;xpos=Npmsl;feats=Case=Loc|Gender=Masc|Number=Sing;head=1;deprel=obl>]>
+<Token id=5;words=[<Word id=5;text=.;lemma=.;upos=PUNCT;xpos=Z;head=1;deprel=punct>]>
+""".strip()
+
+SL_DOC_WORDS_GOLD = """
+<Word id=1;text=France;lemma=France;upos=PROPN;xpos=Npmsn;feats=Case=Nom|Gender=Masc|Number=Sing;head=5;deprel=nsubj>
+<Word id=2;text=Prešeren;lemma=Prešeren;upos=PROPN;xpos=Npmsn;feats=Case=Nom|Gender=Masc|Number=Sing;head=1;deprel=flat:name>
+<Word id=3;text=je;lemma=biti;upos=AUX;xpos=Va-r3s-n;feats=Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin;head=5;deprel=aux>
+<Word id=4;text=bil;lemma=biti;upos=AUX;xpos=Va-p-sm;feats=Gender=Masc|Number=Sing|VerbForm=Part;head=5;deprel=cop>
+<Word id=5;text=rojen;lemma=rojen;upos=ADJ;xpos=Appmsnn;feats=Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part;head=0;deprel=root>
+<Word id=6;text=v;lemma=v;upos=ADP;xpos=Sl;feats=Case=Loc;head=7;deprel=case>
+<Word id=7;text=Vrbi;lemma=Vrba;upos=PROPN;xpos=Npfsl;feats=Case=Loc|Gender=Fem|Number=Sing;head=5;deprel=obl>
+<Word id=8;text=.;lemma=.;upos=PUNCT;xpos=Z;head=5;deprel=punct>
+
+<Word id=1;text=Danes;lemma=danes;upos=ADV;xpos=Rgp;feats=Degree=Pos;head=3;deprel=advmod>
+<Word id=2;text=je;lemma=biti;upos=AUX;xpos=Va-r3s-n;feats=Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin;head=3;deprel=cop>
+<Word id=3;text=poznan;lemma=poznan;upos=ADJ;xpos=Appmsnn;feats=Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part;head=0;deprel=root>
+<Word id=4;text=kot;lemma=kot;upos=SCONJ;xpos=Cs;head=7;deprel=case>
+<Word id=5;text=največji;lemma=velik;upos=ADJ;xpos=Agsmsny;feats=Case=Nom|Definite=Def|Degree=Sup|Gender=Masc|Number=Sing;head=7;deprel=amod>
+<Word id=6;text=slovenski;lemma=slovenski;upos=ADJ;xpos=Agpmsny;feats=Case=Nom|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing;head=7;deprel=amod>
+<Word id=7;text=pesnik;lemma=pesnik;upos=NOUN;xpos=Ncmsn;feats=Case=Nom|Gender=Masc|Number=Sing;head=3;deprel=obl>
+<Word id=8;text=.;lemma=.;upos=PUNCT;xpos=Z;head=3;deprel=punct>
+
+<Word id=1;text=Študiral;lemma=študirati;upos=VERB;xpos=Vmpp-sm;feats=Aspect=Imp|Gender=Masc|Number=Sing|VerbForm=Part;head=0;deprel=root>
+<Word id=2;text=je;lemma=biti;upos=AUX;xpos=Va-r3s-n;feats=Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin;head=1;deprel=aux>
+<Word id=3;text=na;lemma=na;upos=ADP;xpos=Sl;feats=Case=Loc;head=4;deprel=case>
+<Word id=4;text=Dunaju;lemma=Dunaj;upos=PROPN;xpos=Npmsl;feats=Case=Loc|Gender=Masc|Number=Sing;head=1;deprel=obl>
+<Word id=5;text=.;lemma=.;upos=PUNCT;xpos=Z;head=1;deprel=punct>
+""".strip()
+
+SL_DOC_DEPENDENCY_PARSES_GOLD = """
+('France', 5, 'nsubj')
+('Prešeren', 1, 'flat:name')
+('je', 5, 'aux')
+('bil', 5, 'cop')
+('rojen', 0, 'root')
+('v', 7, 'case')
+('Vrbi', 5, 'obl')
+('.', 5, 'punct')
+
+('Danes', 3, 'advmod')
+('je', 3, 'cop')
+('poznan', 0, 'root')
+('kot', 7, 'case')
+('največji', 7, 'amod')
+('slovenski', 7, 'amod')
+('pesnik', 3, 'obl')
+('.', 3, 'punct')
+
+('Študiral', 0, 'root')
+('je', 1, 'aux')
+('na', 4, 'case')
+('Dunaju', 1, 'obl')
+('.', 1, 'punct')
+""".strip()
+
+SL_DOC_CONLLU_GOLD = """
+# newpar id = 1
+# sent_id = 1.1
+# text = France Prešeren je bil rojen v Vrbi.
+1	France	France	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	5	nsubj	_	NER=B-PER
+2	Prešeren	Prešeren	PROPN	Npmsn	Case=Nom|Gender=Masc|Number=Sing	1	flat:name	_	NER=I-PER
+3	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	5	aux	_	NER=O
+4	bil	biti	AUX	Va-p-sm	Gender=Masc|Number=Sing|VerbForm=Part	5	cop	_	NER=O
+5	rojen	rojen	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part	0	root	_	NER=O
+6	v	v	ADP	Sl	Case=Loc	7	case	_	NER=O
+7	Vrbi	Vrba	PROPN	Npfsl	Case=Loc|Gender=Fem|Number=Sing	5	obl	_	NER=B-LOC|SpaceAfter=No
+8	.	.	PUNCT	Z	_	5	punct	_	NER=O
+
+# sent_id = 1.2
+# text = Danes je poznan kot največji slovenski pesnik.
+1	Danes	danes	ADV	Rgp	Degree=Pos	3	advmod	_	NER=O
+2	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	3	cop	_	NER=O
+3	poznan	poznan	ADJ	Appmsnn	Case=Nom|Definite=Ind|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part	0	root	_	NER=O
+4	kot	kot	SCONJ	Cs	_	7	case	_	NER=O
+5	največji	velik	ADJ	Agsmsny	Case=Nom|Definite=Def|Degree=Sup|Gender=Masc|Number=Sing	7	amod	_	NER=O
+6	slovenski	slovenski	ADJ	Agpmsny	Case=Nom|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing	7	amod	_	NER=O
+7	pesnik	pesnik	NOUN	Ncmsn	Case=Nom|Gender=Masc|Number=Sing	3	obl	_	NER=O|SpaceAfter=No
+8	.	.	PUNCT	Z	_	3	punct	_	NER=O
+
+# sent_id = 1.3
+# text = Študiral je na Dunaju.
+1	Študiral	študirati	VERB	Vmpp-sm	Aspect=Imp|Gender=Masc|Number=Sing|VerbForm=Part	0	root	_	NER=O
+2	je	biti	AUX	Va-r3s-n	Mood=Ind|Number=Sing|Person=3|Polarity=Pos|Tense=Pres|VerbForm=Fin	1	aux	_	NER=O
+3	na	na	ADP	Sl	Case=Loc	4	case	_	NER=O
+4	Dunaju	Dunaj	PROPN	Npmsl	Case=Loc|Gender=Masc|Number=Sing	1	obl	_	NER=B-LOC|SpaceAfter=No
+5	.	.	PUNCT	Z	_	1	punct	_	NER=O
+
+""".lstrip()
+
+
+@pytest.fixture(scope="module")
+def processed_doc():
+    """ Document created by running full Slovenian pipeline on a few sentences """
+    nlp = classla.Pipeline(dir=TEST_MODELS_DIR)
+    return nlp(SL_DOC)
+
+
+def test_text(processed_doc):
+    assert processed_doc.text == SL_DOC
+
+
+def test_conllu(processed_doc):
+    assert processed_doc.to_conll() == SL_DOC_CONLLU_GOLD
+
+
+def test_tokens(processed_doc):
+    assert "\n\n".join([sent.tokens_string() for sent in processed_doc.sentences]) == SL_DOC_TOKENS_GOLD
+
+
+def test_words(processed_doc):
+    assert "\n\n".join([sent.words_string() for sent in processed_doc.sentences]) == SL_DOC_WORDS_GOLD
+
+
+def test_dependency_parse(processed_doc):
+    assert "\n\n".join([sent.dependencies_string() for sent in processed_doc.sentences]) == \
+           SL_DOC_DEPENDENCY_PARSES_GOLD
```

### Comparing `classla-2.0/tests_classla/test_srl.py` & `classla-2.1/tests_classla/test_tagger.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,28 +1,28 @@
-"""
-Basic testing of dependency parser
-"""
-
-import classla
-import classla.models.srl_tagger as trainer
-
-from tests_classla import *
-
-with open('test_data/slovenian.raw') as f:
-    SL_DOC = f.read()
-
-with open('test_data/slovenian.srl') as f:
-    SL_DOC_GOLD = f.read()
-
-
-def test_parser():
-    nlp = classla.Pipeline(
-        **{'processors': 'tokenize,pos,lemma,depparse,srl', 'dir': TEST_MODELS_DIR, 'lang': 'sl', 'type': 'standard_jos'})
-    doc = nlp(SL_DOC)
-    # with open('test_data/slovenian.srl', 'w') as f:
-    #     f.write(doc.to_conll())
-    assert SL_DOC_GOLD == doc.to_conll()
-
-def test_parser_trainer():
-    trainer.main(args=['--save_dir', 'test_data/train/data', '--save_name', 'srl.pt', '--train_file', 'test_data/train/srl_jos_example.conll',
-                       '--eval_file', 'test_data/train/srl_jos_example.conll', '--output_file', 'test_data/train/data/srl', '--gold_file', 'test_data/train/srl_jos_example.conll', '--shorthand', 'sl_ssj',
-                       '--mode', 'train', '--pretrain_file', 'classla_test/models/sl/pretrain/standard.pt', '--max_steps', '100'])
+"""
+Basic testing of part of speech tagging
+"""
+
+import classla
+import classla.models.tagger as trainer
+
+from tests_classla import *
+
+with open('test_data/slovenian.raw') as f:
+    SL_DOC = f.read()
+
+with open('test_data/slovenian.tagger') as f:
+    SL_DOC_GOLD = f.read()
+
+
+def test_part_of_speech():
+    nlp = classla.Pipeline(**{'processors': 'tokenize,pos', 'dir': TEST_MODELS_DIR, 'lang': 'sl'})
+    doc = nlp(SL_DOC)
+    # with open('test_data/slovenian.tagger', 'w') as f:
+    #     f.write('\n\n'.join([sent.tokens_string() for sent in doc.sentences]))
+    assert SL_DOC_GOLD == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
+
+
+def test_parser_trainer():
+    trainer.main(args=['--save_dir', 'test_data/train/data', '--save_name', 'tagger.pt', '--train_file', 'test_data/train/tagger_lemmatizer_parser_example.conll',
+                       '--eval_file', 'test_data/train/tagger_lemmatizer_parser_example.conll', '--output_file', 'test_data/train/data/tagger', '--gold_file', 'test_data/train/tagger_lemmatizer_parser_example.conll', '--shorthand', 'sl_ssj',
+                       '--mode', 'train', '--pretrain_file', 'classla_test/models/sl/pretrain/standard.pt', '--max_steps', '100', '--inflectional_lexicon_path', 'test_data/train/sloleks_example.tbl'])
```

### Comparing `classla-2.0/tests_classla/test_tokenizer.py` & `classla-2.1/tests_classla/test_tokenizer.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,81 +1,81 @@
-"""
-Basic testing of tokenization
-"""
-
-import classla
-
-from tests_classla import *
-
-with open('test_data/slovenian.raw') as f:
-    SL_DOC = f.read()
-
-
-with open('test_data/slovenian.tokenizer') as f:
-    SL_DOC_GOLD_TOKENS = f.read()
-
-
-SL_DOC_PRETOKENIZED = \
-    "France Prešeren je bil rojen v Vrbi .\nDanes je poznan kot največji slovenski pesnik .\n\nŠtudiral je na Dunaju.\n"
-
-SL_DOC_PRETOKENIZED_GOLD_TOKENS = """
-<Token id=1;words=[<Word id=1;text=France>]>
-<Token id=2;words=[<Word id=2;text=Prešeren>]>
-<Token id=3;words=[<Word id=3;text=je>]>
-<Token id=4;words=[<Word id=4;text=bil>]>
-<Token id=5;words=[<Word id=5;text=rojen>]>
-<Token id=6;words=[<Word id=6;text=v>]>
-<Token id=7;words=[<Word id=7;text=Vrbi>]>
-<Token id=8;words=[<Word id=8;text=.>]>
-
-<Token id=1;words=[<Word id=1;text=Danes>]>
-<Token id=2;words=[<Word id=2;text=je>]>
-<Token id=3;words=[<Word id=3;text=poznan>]>
-<Token id=4;words=[<Word id=4;text=kot>]>
-<Token id=5;words=[<Word id=5;text=največji>]>
-<Token id=6;words=[<Word id=6;text=slovenski>]>
-<Token id=7;words=[<Word id=7;text=pesnik>]>
-<Token id=8;words=[<Word id=8;text=.>]>
-
-<Token id=1;words=[<Word id=1;text=Študiral>]>
-<Token id=2;words=[<Word id=2;text=je>]>
-<Token id=3;words=[<Word id=3;text=na>]>
-<Token id=4;words=[<Word id=4;text=Dunaju.>]>
-""".strip()
-
-
-SL_DOC_PRETOKENIZED_LIST = [['France', 'Prešeren', 'je', 'bil', 'rojen', 'v', 'Vrbi', '.'], ['Danes', 'živi', 'v',
-                                                                                             'poeziji', '.']]
-
-SL_DOC_PRETOKENIZED_LIST_GOLD_TOKENS = """
-<Token id=1;words=[<Word id=1;text=France>]>
-<Token id=2;words=[<Word id=2;text=Prešeren>]>
-<Token id=3;words=[<Word id=3;text=je>]>
-<Token id=4;words=[<Word id=4;text=bil>]>
-<Token id=5;words=[<Word id=5;text=rojen>]>
-<Token id=6;words=[<Word id=6;text=v>]>
-<Token id=7;words=[<Word id=7;text=Vrbi>]>
-<Token id=8;words=[<Word id=8;text=.>]>
-
-<Token id=1;words=[<Word id=1;text=Danes>]>
-<Token id=2;words=[<Word id=2;text=živi>]>
-<Token id=3;words=[<Word id=3;text=v>]>
-<Token id=4;words=[<Word id=4;text=poeziji>]>
-<Token id=5;words=[<Word id=5;text=.>]>
-""".strip()
-
-
-def test_tokenize():
-    nlp = classla.Pipeline(processors='tokenize', dir=TEST_MODELS_DIR, lang='sl')
-    doc = nlp(SL_DOC)
-    # with open('test_data/slovenian.tokenizer', 'w') as f:
-    #     f.write('\n\n'.join([sent.tokens_string() for sent in doc.sentences]))
-    assert SL_DOC_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
-
-
-def test_pretokenized():
-    nlp = classla.Pipeline(**{'processors': 'tokenize', 'dir': TEST_MODELS_DIR, 'lang': 'sl',
-                                  'tokenize_pretokenized': True})
-    doc = nlp(SL_DOC_PRETOKENIZED)
-    assert SL_DOC_PRETOKENIZED_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
-    doc = nlp(SL_DOC_PRETOKENIZED_LIST)
-    assert SL_DOC_PRETOKENIZED_LIST_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
+"""
+Basic testing of tokenization
+"""
+
+import classla
+
+from tests_classla import *
+
+with open('test_data/slovenian.raw') as f:
+    SL_DOC = f.read()
+
+
+with open('test_data/slovenian.tokenizer') as f:
+    SL_DOC_GOLD_TOKENS = f.read()
+
+
+SL_DOC_PRETOKENIZED = \
+    "France Prešeren je bil rojen v Vrbi .\nDanes je poznan kot največji slovenski pesnik .\n\nŠtudiral je na Dunaju.\n"
+
+SL_DOC_PRETOKENIZED_GOLD_TOKENS = """
+<Token id=1;words=[<Word id=1;text=France>]>
+<Token id=2;words=[<Word id=2;text=Prešeren>]>
+<Token id=3;words=[<Word id=3;text=je>]>
+<Token id=4;words=[<Word id=4;text=bil>]>
+<Token id=5;words=[<Word id=5;text=rojen>]>
+<Token id=6;words=[<Word id=6;text=v>]>
+<Token id=7;words=[<Word id=7;text=Vrbi>]>
+<Token id=8;words=[<Word id=8;text=.>]>
+
+<Token id=1;words=[<Word id=1;text=Danes>]>
+<Token id=2;words=[<Word id=2;text=je>]>
+<Token id=3;words=[<Word id=3;text=poznan>]>
+<Token id=4;words=[<Word id=4;text=kot>]>
+<Token id=5;words=[<Word id=5;text=največji>]>
+<Token id=6;words=[<Word id=6;text=slovenski>]>
+<Token id=7;words=[<Word id=7;text=pesnik>]>
+<Token id=8;words=[<Word id=8;text=.>]>
+
+<Token id=1;words=[<Word id=1;text=Študiral>]>
+<Token id=2;words=[<Word id=2;text=je>]>
+<Token id=3;words=[<Word id=3;text=na>]>
+<Token id=4;words=[<Word id=4;text=Dunaju.>]>
+""".strip()
+
+
+SL_DOC_PRETOKENIZED_LIST = [['France', 'Prešeren', 'je', 'bil', 'rojen', 'v', 'Vrbi', '.'], ['Danes', 'živi', 'v',
+                                                                                             'poeziji', '.']]
+
+SL_DOC_PRETOKENIZED_LIST_GOLD_TOKENS = """
+<Token id=1;words=[<Word id=1;text=France>]>
+<Token id=2;words=[<Word id=2;text=Prešeren>]>
+<Token id=3;words=[<Word id=3;text=je>]>
+<Token id=4;words=[<Word id=4;text=bil>]>
+<Token id=5;words=[<Word id=5;text=rojen>]>
+<Token id=6;words=[<Word id=6;text=v>]>
+<Token id=7;words=[<Word id=7;text=Vrbi>]>
+<Token id=8;words=[<Word id=8;text=.>]>
+
+<Token id=1;words=[<Word id=1;text=Danes>]>
+<Token id=2;words=[<Word id=2;text=živi>]>
+<Token id=3;words=[<Word id=3;text=v>]>
+<Token id=4;words=[<Word id=4;text=poeziji>]>
+<Token id=5;words=[<Word id=5;text=.>]>
+""".strip()
+
+
+def test_tokenize():
+    nlp = classla.Pipeline(processors='tokenize', dir=TEST_MODELS_DIR, lang='sl')
+    doc = nlp(SL_DOC)
+    # with open('test_data/slovenian.tokenizer', 'w') as f:
+    #     f.write('\n\n'.join([sent.tokens_string() for sent in doc.sentences]))
+    assert SL_DOC_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
+
+
+def test_pretokenized():
+    nlp = classla.Pipeline(**{'processors': 'tokenize', 'dir': TEST_MODELS_DIR, 'lang': 'sl',
+                                  'tokenize_pretokenized': True})
+    doc = nlp(SL_DOC_PRETOKENIZED)
+    assert SL_DOC_PRETOKENIZED_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
+    doc = nlp(SL_DOC_PRETOKENIZED_LIST)
+    assert SL_DOC_PRETOKENIZED_LIST_GOLD_TOKENS == '\n\n'.join([sent.tokens_string() for sent in doc.sentences])
```

