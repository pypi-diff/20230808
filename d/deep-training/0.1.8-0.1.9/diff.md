# Comparing `tmp/deep_training-0.1.8-py3-none-any.whl.zip` & `tmp/deep_training-0.1.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,34 +1,33 @@
-Zip file size: 346017 bytes, number of entries: 185
--rw-rw-rw-  2.0 fat       47 b- defN 23-Mar-22 07:43 deep_training/__init__.py
--rw-rw-rw-  2.0 fat      897 b- defN 23-May-24 02:24 deep_training/setup.py
--rw-rw-rw-  2.0 fat       55 b- defN 22-Dec-09 05:30 deep_training/cv/__init__.py
--rw-rw-rw-  2.0 fat      195 b- defN 23-Jan-29 01:07 deep_training/data_helper/__init__.py
--rw-rw-rw-  2.0 fat    18145 b- defN 23-May-23 06:49 deep_training/data_helper/data_helper.py
--rw-rw-rw-  2.0 fat     3757 b- defN 23-May-22 00:45 deep_training/data_helper/data_module.py
--rw-rw-rw-  2.0 fat    12121 b- defN 23-Apr-27 00:33 deep_training/data_helper/training_args.py
+Zip file size: 396025 bytes, number of entries: 182
+-rw-rw-rw-  2.0 fat       47 b- defN 23-Mar-22 10:40 deep_training/__init__.py
+-rw-rw-rw-  2.0 fat      897 b- defN 23-May-25 14:13 deep_training/setup.py
+-rw-rw-rw-  2.0 fat       55 b- defN 23-Mar-19 08:44 deep_training/cv/__init__.py
+-rw-rw-rw-  2.0 fat      155 b- defN 23-May-25 10:51 deep_training/data_helper/__init__.py
+-rw-rw-rw-  2.0 fat    14675 b- defN 23-May-25 13:45 deep_training/data_helper/data_helper.py
+-rw-rw-rw-  2.0 fat    12572 b- defN 23-May-27 08:27 deep_training/data_helper/training_args.py
 -rw-rw-rw-  2.0 fat       70 b- defN 22-Dec-13 03:17 deep_training/nlp/__init__.py
 -rw-rw-rw-  2.0 fat       56 b- defN 22-Nov-10 08:28 deep_training/nlp/layers/__init__.py
 -rw-rw-rw-  2.0 fat      241 b- defN 23-Mar-13 05:48 deep_training/nlp/layers/activate.py
 -rw-rw-rw-  2.0 fat    13271 b- defN 22-Nov-14 00:17 deep_training/nlp/layers/crf.py
 -rw-rw-rw-  2.0 fat     4653 b- defN 22-Dec-12 00:12 deep_training/nlp/layers/handshakingkernel.py
 -rw-rw-rw-  2.0 fat      435 b- defN 22-Dec-02 00:22 deep_training/nlp/layers/mask.py
 -rw-rw-rw-  2.0 fat     1319 b- defN 22-Dec-12 00:12 deep_training/nlp/layers/mhslayer.py
 -rw-rw-rw-  2.0 fat     5911 b- defN 22-Dec-08 00:08 deep_training/nlp/layers/norm.py
 -rw-rw-rw-  2.0 fat     1406 b- defN 23-Apr-28 00:24 deep_training/nlp/layers/ppo.py
 -rw-rw-rw-  2.0 fat     1220 b- defN 22-Jul-21 00:57 deep_training/nlp/layers/prefix_encoder.py
 -rw-rw-rw-  2.0 fat     7259 b- defN 22-Dec-14 02:36 deep_training/nlp/layers/seq_pointer.py
 -rw-rw-rw-  2.0 fat     3550 b- defN 22-Dec-15 00:57 deep_training/nlp/layers/w2ner.py
 -rw-rw-rw-  2.0 fat       72 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v1/__init__.py
--rw-rw-rw-  2.0 fat    15095 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v1/layers.py
+-rw-rw-rw-  2.0 fat    15529 b- defN 23-May-26 00:28 deep_training/nlp/layers/lora_v1/layers.py
 -rw-rw-rw-  2.0 fat     1819 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v1/utils.py
 -rw-rw-rw-  2.0 fat       72 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v2/__init__.py
 -rw-rw-rw-  2.0 fat    15465 b- defN 23-Apr-12 00:34 deep_training/nlp/layers/lora_v2/adalora.py
--rw-rw-rw-  2.0 fat     8565 b- defN 23-Apr-12 00:34 deep_training/nlp/layers/lora_v2/layers.py
--rw-rw-rw-  2.0 fat     9287 b- defN 23-May-10 01:12 deep_training/nlp/layers/lora_v2/utils.py
+-rw-rw-rw-  2.0 fat    16258 b- defN 23-May-26 00:28 deep_training/nlp/layers/lora_v2/layers.py
+-rw-rw-rw-  2.0 fat     9083 b- defN 23-May-26 01:24 deep_training/nlp/layers/lora_v2/utils.py
 -rw-rw-rw-  2.0 fat       80 b- defN 23-May-04 00:28 deep_training/nlp/layers/prompt/__init__.py
 -rw-rw-rw-  2.0 fat    15541 b- defN 23-May-04 00:28 deep_training/nlp/layers/prompt/adaption_prompt.py
 -rw-rw-rw-  2.0 fat     5463 b- defN 23-May-04 00:28 deep_training/nlp/layers/prompt/p_tuning.py
 -rw-rw-rw-  2.0 fat     3053 b- defN 23-May-04 00:28 deep_training/nlp/layers/prompt/prefix_tuning.py
 -rw-rw-rw-  2.0 fat     3523 b- defN 23-May-04 00:28 deep_training/nlp/layers/prompt/prompt_tuning.py
 -rw-rw-rw-  2.0 fat     9285 b- defN 23-May-15 05:41 deep_training/nlp/layers/prompt/utils.py
 -rw-rw-rw-  2.0 fat     3662 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/BatchAllTripletLoss.py
@@ -90,63 +89,64 @@
 -rw-rw-rw-  2.0 fat     5149 b- defN 23-Apr-25 03:34 deep_training/nlp/models/pure_model.py
 -rw-rw-rw-  2.0 fat     3949 b- defN 23-Apr-25 03:34 deep_training/nlp/models/simcse.py
 -rw-rw-rw-  2.0 fat     6022 b- defN 23-Apr-25 03:34 deep_training/nlp/models/span_ner.py
 -rw-rw-rw-  2.0 fat    14454 b- defN 23-Apr-25 03:34 deep_training/nlp/models/spn4re.py
 -rw-rw-rw-  2.0 fat    11383 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tplinker.py
 -rw-rw-rw-  2.0 fat     8157 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tplinkerplus.py
 -rw-rw-rw-  2.0 fat     6624 b- defN 23-Apr-25 03:34 deep_training/nlp/models/transformer.py
--rw-rw-rw-  2.0 fat    26596 b- defN 23-May-24 00:37 deep_training/nlp/models/transformer_base.py
+-rw-rw-rw-  2.0 fat    26743 b- defN 23-May-27 17:51 deep_training/nlp/models/transformer_base.py
 -rw-rw-rw-  2.0 fat     7968 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tsdae_model.py
 -rw-rw-rw-  2.0 fat     9040 b- defN 23-Apr-25 03:34 deep_training/nlp/models/w2ner.py
 -rw-rw-rw-  2.0 fat    16524 b- defN 23-Apr-28 07:32 deep_training/nlp/models/LLaMA/__init__.py
 -rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-13 01:04 deep_training/nlp/models/LLaMA/configuration.py
 -rw-rw-rw-  2.0 fat    19207 b- defN 23-Apr-28 07:32 deep_training/nlp/models/LLaMA_parallel/__init__.py
 -rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-10 00:30 deep_training/nlp/models/LLaMA_parallel/configuration.py
 -rw-rw-rw-  2.0 fat    31627 b- defN 23-Mar-13 06:15 deep_training/nlp/models/PaLM/__init__.py
 -rw-rw-rw-  2.0 fat     5890 b- defN 23-Mar-13 06:15 deep_training/nlp/models/PaLM/configuration.py
 -rw-rw-rw-  2.0 fat    60510 b- defN 23-May-22 06:43 deep_training/nlp/models/chatglm/__init__.py
 -rw-rw-rw-  2.0 fat     4575 b- defN 23-Apr-10 00:42 deep_training/nlp/models/chatglm/configuration.py
 -rw-rw-rw-  2.0 fat    15169 b- defN 23-May-22 06:39 deep_training/nlp/models/chatglm/quantization.py
 -rw-rw-rw-  2.0 fat    17037 b- defN 23-May-12 07:25 deep_training/nlp/models/chatglm/tokenization.py
 -rw-rw-rw-  2.0 fat    34123 b- defN 23-Mar-13 06:18 deep_training/nlp/models/laMDA/__init__.py
 -rw-rw-rw-  2.0 fat     5981 b- defN 23-Mar-13 06:15 deep_training/nlp/models/laMDA/configuration.py
--rw-rw-rw-  2.0 fat      181 b- defN 23-Apr-11 07:19 deep_training/nlp/models/lora/__init__.py
+-rw-rw-rw-  2.0 fat      182 b- defN 23-May-26 13:52 deep_training/nlp/models/lora/__init__.py
 -rw-rw-rw-  2.0 fat      123 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v1/__init__.py
 -rw-rw-rw-  2.0 fat     7095 b- defN 23-May-19 07:59 deep_training/nlp/models/lora/v1/configuration.py
--rw-rw-rw-  2.0 fat    13688 b- defN 23-May-04 00:28 deep_training/nlp/models/lora/v1/lora_wrapper.py
--rw-rw-rw-  2.0 fat      206 b- defN 23-Apr-11 01:58 deep_training/nlp/models/lora/v2/__init__.py
--rw-rw-rw-  2.0 fat    13112 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v2/adalora_model.py
+-rw-rw-rw-  2.0 fat    13842 b- defN 23-May-25 09:17 deep_training/nlp/models/lora/v1/lora_wrapper.py
+-rw-rw-rw-  2.0 fat      217 b- defN 23-May-26 13:53 deep_training/nlp/models/lora/v2/__init__.py
+-rw-rw-rw-  2.0 fat    14096 b- defN 23-May-27 01:52 deep_training/nlp/models/lora/v2/adalora_model.py
 -rw-rw-rw-  2.0 fat    11325 b- defN 23-May-19 07:53 deep_training/nlp/models/lora/v2/configuration.py
--rw-rw-rw-  2.0 fat    11745 b- defN 23-Apr-18 01:24 deep_training/nlp/models/lora/v2/lora_model.py
--rw-rw-rw-  2.0 fat    10868 b- defN 23-May-24 00:30 deep_training/nlp/models/lora/v2/lora_wrapper.py
--rw-rw-rw-  2.0 fat     4889 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v2/save_and_load.py
+-rw-rw-rw-  2.0 fat    16383 b- defN 23-May-27 01:52 deep_training/nlp/models/lora/v2/lora_model.py
+-rw-rw-rw-  2.0 fat    11453 b- defN 23-May-27 09:04 deep_training/nlp/models/lora/v2/lora_wrapper.py
+-rw-rw-rw-  2.0 fat     4883 b- defN 23-May-26 00:28 deep_training/nlp/models/lora/v2/save_and_load.py
 -rw-rw-rw-  2.0 fat      467 b- defN 23-Apr-21 04:29 deep_training/nlp/models/moss/__init__.py
 -rw-rw-rw-  2.0 fat     5097 b- defN 23-Apr-23 01:11 deep_training/nlp/models/moss/configuration_moss.py
 -rw-rw-rw-  2.0 fat     6735 b- defN 23-Apr-23 01:05 deep_training/nlp/models/moss/custom_autotune.py
 -rw-rw-rw-  2.0 fat    31079 b- defN 23-Apr-23 02:08 deep_training/nlp/models/moss/modeling_moss.py
 -rw-rw-rw-  2.0 fat    18773 b- defN 23-May-15 06:21 deep_training/nlp/models/moss/quantization.py
 -rw-rw-rw-  2.0 fat    15939 b- defN 23-Apr-24 00:32 deep_training/nlp/models/moss/tokenization_moss.py
 -rw-rw-rw-  2.0 fat      203 b- defN 23-May-04 00:28 deep_training/nlp/models/prompt/__init__.py
 -rw-rw-rw-  2.0 fat    12103 b- defN 23-May-19 07:59 deep_training/nlp/models/prompt/configuration.py
--rw-rw-rw-  2.0 fat    52598 b- defN 23-May-24 01:46 deep_training/nlp/models/prompt/prompt_model.py
+-rw-rw-rw-  2.0 fat    52867 b- defN 23-May-27 10:10 deep_training/nlp/models/prompt/prompt_model.py
 -rw-rw-rw-  2.0 fat     3560 b- defN 23-May-24 00:30 deep_training/nlp/models/prompt/save_and_load.py
 -rw-rw-rw-  2.0 fat     1917 b- defN 23-May-04 00:28 deep_training/nlp/models/prompt/utils.py
 -rw-rw-rw-  2.0 fat       54 b- defN 23-May-11 01:00 deep_training/nlp/models/rl/__init__.py
 -rw-rw-rw-  2.0 fat      272 b- defN 23-May-15 00:35 deep_training/nlp/models/rl/modeling.py
 -rw-rw-rw-  2.0 fat    21581 b- defN 23-May-19 04:12 deep_training/nlp/models/rl/modeling_ilql.py
 -rw-rw-rw-  2.0 fat    42065 b- defN 23-May-19 04:12 deep_training/nlp/models/rl/modeling_ppo.py
 -rw-rw-rw-  2.0 fat     8163 b- defN 23-May-19 03:28 deep_training/nlp/models/rl/utils.py
 -rw-rw-rw-  2.0 fat      102 b- defN 22-Nov-22 08:00 deep_training/nlp/models/splinker/__init__.py
 -rw-rw-rw-  2.0 fat     2866 b- defN 23-Apr-25 03:34 deep_training/nlp/models/splinker/splinker.py
 -rw-rw-rw-  2.0 fat    14478 b- defN 23-Feb-11 09:07 deep_training/nlp/models/t5decoder/__init__.py
 -rw-rw-rw-  2.0 fat     6646 b- defN 23-Feb-09 00:28 deep_training/nlp/models/t5encoder/__init__.py
 -rw-rw-rw-  2.0 fat       56 b- defN 22-Dec-14 08:02 deep_training/nlp/optimizer/__init__.py
 -rw-rw-rw-  2.0 fat     5225 b- defN 23-Mar-08 00:14 deep_training/nlp/optimizer/lamb.py
+-rw-rw-rw-  2.0 fat     6549 b- defN 23-May-27 08:28 deep_training/nlp/optimizer/optimizer.py
 -rw-rw-rw-  2.0 fat       99 b- defN 23-Mar-02 05:27 deep_training/nlp/optimizer/lion/__init__.py
--rw-rw-rw-  2.0 fat     2516 b- defN 23-May-10 03:20 deep_training/nlp/optimizer/lion/lion.py
+-rw-rw-rw-  2.0 fat     2535 b- defN 23-May-27 08:15 deep_training/nlp/optimizer/lion/lion.py
 -rw-rw-rw-  2.0 fat     2499 b- defN 23-May-10 03:18 deep_training/nlp/optimizer/lion/triton.py
 -rw-rw-rw-  2.0 fat       79 b- defN 23-Apr-27 00:33 deep_training/nlp/rl/__init__.py
 -rw-rw-rw-  2.0 fat       88 b- defN 23-May-15 00:35 deep_training/nlp/rl/ilql/__init__.py
 -rw-rw-rw-  2.0 fat     2232 b- defN 23-May-16 05:01 deep_training/nlp/rl/ilql/configuration.py
 -rw-rw-rw-  2.0 fat     3691 b- defN 23-May-15 00:35 deep_training/nlp/rl/ilql/data_define.py
 -rw-rw-rw-  2.0 fat     7800 b- defN 23-May-15 02:51 deep_training/nlp/rl/ilql/ilql_dataset.py
 -rw-rw-rw-  2.0 fat     6244 b- defN 23-May-16 07:17 deep_training/nlp/rl/ilql/ilql_module.py
@@ -159,29 +159,26 @@
 -rw-rw-rw-  2.0 fat    47611 b- defN 23-May-19 04:52 deep_training/nlp/rl/ppo/ppo_trainer.py
 -rw-rw-rw-  2.0 fat       88 b- defN 23-May-15 00:35 deep_training/nlp/rl/rl_base/__init__.py
 -rw-rw-rw-  2.0 fat     3724 b- defN 23-May-15 00:35 deep_training/nlp/rl/rl_base/rl_dataset.py
 -rw-rw-rw-  2.0 fat    10700 b- defN 23-May-15 00:35 deep_training/nlp/rl/utils/__init__.py
 -rw-rw-rw-  2.0 fat     3529 b- defN 23-May-15 00:35 deep_training/nlp/rl/utils/configuration.py
 -rw-rw-rw-  2.0 fat     9844 b- defN 23-May-15 00:35 deep_training/nlp/rl/utils/logging.py
 -rw-rw-rw-  2.0 fat     2868 b- defN 22-Dec-14 08:00 deep_training/nlp/scheduler/__init__.py
--rw-rw-rw-  2.0 fat     7393 b- defN 23-May-19 07:03 deep_training/nlp/utils/__init__.py
+-rw-rw-rw-  2.0 fat     3725 b- defN 23-May-27 07:27 deep_training/nlp/scheduler/scheduler.py
+-rw-rw-rw-  2.0 fat     7500 b- defN 23-May-27 08:12 deep_training/nlp/utils/__init__.py
 -rw-rw-rw-  2.0 fat     6323 b- defN 23-Jan-29 01:07 deep_training/nlp/utils/adversarial.py
 -rw-rw-rw-  2.0 fat    15256 b- defN 23-Jan-03 01:54 deep_training/nlp/utils/nlputils.py
 -rw-rw-rw-  2.0 fat      795 b- defN 23-Jan-11 07:02 deep_training/nlp/utils/spearman.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/layers/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/losses/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/metrics/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/models/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/optimizer/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/scheduler/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:26 deep_training/tfnlp/utils/__init__.py
--rw-rw-rw-  2.0 fat       55 b- defN 23-Mar-07 01:20 deep_training/utils/__init__.py
--rw-rw-rw-  2.0 fat     1941 b- defN 23-Mar-07 01:20 deep_training/utils/distributed.py
--rw-rw-rw-  2.0 fat     1724 b- defN 23-Mar-07 01:22 deep_training/utils/func.py
--rw-rw-rw-  2.0 fat     5117 b- defN 23-Feb-21 09:01 deep_training/utils/maskedlm.py
--rw-rw-rw-  2.0 fat    14500 b- defN 23-May-11 00:39 deep_training/utils/trainer.py
--rw-rw-rw-  2.0 fat      602 b- defN 23-May-24 02:30 deep_training-0.1.8.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-May-24 02:30 deep_training-0.1.8.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       14 b- defN 23-May-24 02:30 deep_training-0.1.8.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    17940 b- defN 23-May-24 02:30 deep_training-0.1.8.dist-info/RECORD
-185 files, 1246309 bytes uncompressed, 316961 bytes compressed:  74.6%
+-rw-rw-rw-  2.0 fat       80 b- defN 23-May-27 08:35 deep_training/trainer/__init__.py
+-rw-rw-rw-  2.0 fat       80 b- defN 23-May-27 08:35 deep_training/trainer/hf/__init__.py
+-rw-rw-rw-  2.0 fat   193777 b- defN 23-May-27 09:15 deep_training/trainer/hf/trainer.py
+-rw-rw-rw-  2.0 fat    15669 b- defN 23-May-27 09:16 deep_training/trainer/hf/trainer_seq2seq.py
+-rw-rw-rw-  2.0 fat       55 b- defN 23-Mar-19 08:44 deep_training/utils/__init__.py
+-rw-rw-rw-  2.0 fat     1941 b- defN 23-Mar-19 08:44 deep_training/utils/distributed.py
+-rw-rw-rw-  2.0 fat     1724 b- defN 23-Mar-19 08:44 deep_training/utils/func.py
+-rw-rw-rw-  2.0 fat     5117 b- defN 23-Mar-19 08:44 deep_training/utils/maskedlm.py
+-rw-rw-rw-  2.0 fat    14500 b- defN 23-May-20 01:59 deep_training/utils/trainer.py
+-rw-rw-rw-  2.0 fat      603 b- defN 23-May-27 17:55 deep_training-0.1.9.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-May-27 17:55 deep_training-0.1.9.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       14 b- defN 23-May-27 17:55 deep_training-0.1.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    17669 b- defN 23-May-27 17:55 deep_training-0.1.9.dist-info/RECORD
+182 files, 1473511 bytes uncompressed, 367429 bytes compressed:  75.1%
```

## zipnote {}

```diff
@@ -9,17 +9,14 @@
 
 Filename: deep_training/data_helper/__init__.py
 Comment: 
 
 Filename: deep_training/data_helper/data_helper.py
 Comment: 
 
-Filename: deep_training/data_helper/data_module.py
-Comment: 
-
 Filename: deep_training/data_helper/training_args.py
 Comment: 
 
 Filename: deep_training/nlp/__init__.py
 Comment: 
 
 Filename: deep_training/nlp/layers/__init__.py
@@ -420,14 +417,17 @@
 
 Filename: deep_training/nlp/optimizer/__init__.py
 Comment: 
 
 Filename: deep_training/nlp/optimizer/lamb.py
 Comment: 
 
+Filename: deep_training/nlp/optimizer/optimizer.py
+Comment: 
+
 Filename: deep_training/nlp/optimizer/lion/__init__.py
 Comment: 
 
 Filename: deep_training/nlp/optimizer/lion/lion.py
 Comment: 
 
 Filename: deep_training/nlp/optimizer/lion/triton.py
@@ -486,48 +486,39 @@
 
 Filename: deep_training/nlp/rl/utils/logging.py
 Comment: 
 
 Filename: deep_training/nlp/scheduler/__init__.py
 Comment: 
 
+Filename: deep_training/nlp/scheduler/scheduler.py
+Comment: 
+
 Filename: deep_training/nlp/utils/__init__.py
 Comment: 
 
 Filename: deep_training/nlp/utils/adversarial.py
 Comment: 
 
 Filename: deep_training/nlp/utils/nlputils.py
 Comment: 
 
 Filename: deep_training/nlp/utils/spearman.py
 Comment: 
 
-Filename: deep_training/tfnlp/__init__.py
-Comment: 
-
-Filename: deep_training/tfnlp/layers/__init__.py
-Comment: 
-
-Filename: deep_training/tfnlp/losses/__init__.py
-Comment: 
-
-Filename: deep_training/tfnlp/metrics/__init__.py
-Comment: 
-
-Filename: deep_training/tfnlp/models/__init__.py
+Filename: deep_training/trainer/__init__.py
 Comment: 
 
-Filename: deep_training/tfnlp/optimizer/__init__.py
+Filename: deep_training/trainer/hf/__init__.py
 Comment: 
 
-Filename: deep_training/tfnlp/scheduler/__init__.py
+Filename: deep_training/trainer/hf/trainer.py
 Comment: 
 
-Filename: deep_training/tfnlp/utils/__init__.py
+Filename: deep_training/trainer/hf/trainer_seq2seq.py
 Comment: 
 
 Filename: deep_training/utils/__init__.py
 Comment: 
 
 Filename: deep_training/utils/distributed.py
 Comment: 
@@ -537,20 +528,20 @@
 
 Filename: deep_training/utils/maskedlm.py
 Comment: 
 
 Filename: deep_training/utils/trainer.py
 Comment: 
 
-Filename: deep_training-0.1.8.dist-info/METADATA
+Filename: deep_training-0.1.9.dist-info/METADATA
 Comment: 
 
-Filename: deep_training-0.1.8.dist-info/WHEEL
+Filename: deep_training-0.1.9.dist-info/WHEEL
 Comment: 
 
-Filename: deep_training-0.1.8.dist-info/top_level.txt
+Filename: deep_training-0.1.9.dist-info/top_level.txt
 Comment: 
 
-Filename: deep_training-0.1.8.dist-info/RECORD
+Filename: deep_training-0.1.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deep_training/setup.py

```diff
@@ -1,23 +1,23 @@
 #! -*- coding: utf-8 -*-
 
 from setuptools import setup, find_packages
 
 ignore = ['test','tests']
 setup(
     name='deep_training',
-    version='0.1.8',
+    version='0.1.9',
     description='an easy training architecture',
     long_description='torch_training: https://github.com/ssbuild/deep_training.git',
     license='Apache License 2.0',
     url='https://github.com/ssbuild/deep_training',
     author='ssbuild',
     author_email='9727464@qq.com',
     install_requires=['lightning>=2',
-                      'numpy-io>=0.0.2 , < 0.1.0',
+                      'numpy-io>=0.0.3 , < 0.1.0',
                       'sentencepiece',
                       'numpy',
                       'transformers >= 4.22',
                       'seqmetric',
                       'scipy',
                       'scikit-learn',
                       'tensorboard',
```

## deep_training/data_helper/__init__.py

```diff
@@ -1,6 +1,5 @@
 # -*- coding: utf-8 -*-
 # @Time    : 2022/11/9 11:04
 
-from .data_helper import make_dataset, DataHelper
-from .data_module import load_tokenizer, load_configure
+from .data_helper import DataHelper,load_tokenizer, load_configure
 from .training_args import *
```

## deep_training/data_helper/data_helper.py

```diff
@@ -6,151 +6,95 @@
 import typing
 import torch
 from fastdatasets import memory as MEMORY
 from fastdatasets.common.iterable_dataset import IterableDatasetBase
 from fastdatasets.common.random_dataset import RandomDatasetBase
 from fastdatasets.torch_dataset import IterableDataset as torch_IterableDataset, Dataset as torch_Dataset
 from torch.utils.data import DataLoader, IterableDataset
-from .data_module import load_tokenizer, load_configure
+from transformers import PreTrainedTokenizer, PretrainedConfig
 from .training_args import ModelArguments, DataArguments, TrainingArguments
 from ..utils.func import is_chinese_char
 from numpy_io.core.writer import DataWriteHelper
-from numpy_io.core.reader import load_numpy_dataset
-from numpy_io.pytorch_loader.dataloaders import load_distributed_random_sampler,load_random_sampler,load_sequential_sampler
+from numpy_io.pytorch_loader.data_helper import DataHelperBase,load_tokenizer, load_configure
 
 __all__ = [
     'DataHelper',
-    'make_dataset',
     'is_chinese_char',
     'get_filename_no_ext',
     'get_filename_replace_dir',
+    "load_tokenizer",
+    "load_configure"
 ]
 
 def get_filename_no_ext(filename):
     filename = os.path.basename(filename)
     pos = filename.rfind('.')
     if pos >= 0:
         filename = filename[:pos]
     return filename
 
 
 def get_filename_replace_dir(filename,new_path_dir,ext=None):
     return os.path.join(new_path_dir,get_filename_no_ext(filename) + '.' + ext)
 
 
-
-class DataPreprocessHelper(object):
-
-    def on_data_ready(self):...
-
-    def on_data_finalize(self):...
-
-    # 下游任务继承
-    def on_data_process(self, data: typing.Any, user_data: tuple):
-        raise NotImplemented
-
-    def on_task_specific_params(self) -> typing.Dict:
-        return {}
-
-    def on_get_labels(self, files: typing.List[str]):
-        if not files:
-            return None, None
-        label_fname = files[0]
-        is_json_file = label_fname.endswith('.json')
-        D = set()
-        with open(label_fname, 'r', encoding='utf-8') as f:
-            lines = f.readlines()
-            for line in lines:
-                line = line.replace('\r\n', '').replace('\n', '')
-                if not line: continue
-                if is_json_file:
-                    jd = json.loads(line)
-                    line = jd['label']
-                D.add(line)
-        label2id = {label: i for i, label in enumerate(D)}
-        id2label = {i: label for i, label in enumerate(D)}
-        return label2id, id2label
-
-
-    # 读取文件
-    def on_get_corpus(self, files: typing.List[str], mode: str):
-        D = []
-        for filename in files:
-            with open(filename, 'r', encoding='utf-8') as f:
-                lines = f.readlines()
-                for line in lines:
-                    line = line.replace('\r\n', '').replace('\n', '')
-                    if not line: continue
-                    D.append(line)
-        return D
-
-
-
-
-class DataHelper(DataPreprocessHelper):
+class DataHelper(DataHelperBase):
+    tokenizer: typing.Optional[PreTrainedTokenizer] = None
+    config: typing.Optional[PretrainedConfig] = None
+    model_args: typing.Optional[ModelArguments] = None
+    training_args: typing.Optional[TrainingArguments] = None
+    data_args: typing.Optional[DataArguments] = None
     def __init__(self,
                  model_args: ModelArguments,
                  training_args: typing.Optional[TrainingArguments] = None,
                  data_args: typing.Optional[DataArguments] = None,
                  **kwargs):
         super(DataHelper, self).__init__()
 
-        self.data_process_fn = self.on_data_process
 
         self.train_files = []
         self.eval_files = []
         self.test_files = []
 
-        self.tokenizer = None
-        self.config = None
+
         self.label2id = None
         self.id2label = None
-        self.model_args =None
-        self.training_args = None
-        self.data_args = None
         self.max_seq_length_dict = {}
-
-
         self._external_kwargs = kwargs
-
+        self.backend = data_args.data_backend if data_args else 'record'
         self.model_args = model_args
         self.training_args = training_args
-
-        self.backend = data_args.data_backend if data_args else 'record'
         self.data_args = data_args
 
         if data_args:
+            #训练
             label2id, id2label = self.on_get_labels(data_args.label_file)
             self.label2id = label2id
             self.id2label = id2label
 
             self.max_seq_length_dict['train'] = data_args.train_max_seq_length
             self.max_seq_length_dict['eval'] = data_args.eval_max_seq_length
             self.max_seq_length_dict['val'] = data_args.eval_max_seq_length
             self.max_seq_length_dict['test'] = data_args.test_max_seq_length
             self.max_seq_length_dict['predict'] = data_args.test_max_seq_length
         else:
+            #推理
             self.label2id = None
             self.id2label = None
 
-
     @property
     def external_kwargs(self):
         return self._external_kwargs
 
-
-
-
     def load_tokenizer(self,*args,**kwargs):
         tokenizer = load_tokenizer(*args,**kwargs)
         self.tokenizer = tokenizer
         return tokenizer
 
 
-
     def load_config(self,
                     config_name=None,
                     class_name=None,
                     model_name_or_path=None,
                     task_specific_params=None,
                     with_labels=True,
                     with_task_params=True,
@@ -296,42 +240,23 @@
                                 )
         self.config = config
         if with_print_config:
             print(config)
 
         if with_labels and self.label2id is not None and hasattr(config, 'num_labels'):
             if with_print_labels:
-                print('*' * 30, 'num_labels = ', config.num_labels)
+                print('==' * 30, 'num_labels = ', config.num_labels)
                 print(self.label2id)
                 print(self.id2label)
 
         if with_labels:
             return tokenizer, config, self.label2id, self.id2label
         return tokenizer, config
 
 
-
-
-    def load_distributed_random_sampler(self,*args,**kwargs):
-        if 'backend' not in kwargs:
-            kwargs.update({"backend": self.backend})
-        return load_distributed_random_sampler(*args,**kwargs)
-
-
-    def load_random_sampler(self,*args,**kwargs):
-        if 'backend' not in kwargs:
-            kwargs.update({"backend": self.backend})
-        return load_random_sampler(*args, **kwargs)
-
-    def load_sequential_sampler(self,*args,**kwargs):
-        if 'backend' not in kwargs:
-            kwargs.update({"backend": self.backend})
-        return load_sequential_sampler(*args, **kwargs)
-
-
     # 返回制作特征数据的中间文件
     def get_intermediate_file(self, intermediate_name, mode):
         data_args: DataArguments = self.data_args
         if data_args.data_backend.startswith('memory'):
             # 内存数据: list
             intermediate_output = []
             logging.info('make data {} {}...'.format(data_args.output_dir,
@@ -339,22 +264,15 @@
         else:
             # 本地文件数据: 文件名
             intermediate_output = os.path.join(data_args.output_dir,
                                                intermediate_name + '-' + mode + '.' + self.backend)
             logging.info('make data {}...'.format(intermediate_output))
         return intermediate_output
 
-    '''
-        mode: one of [ train , eval , test]
-        shuffle: whether shuffle data
-        num_process_worker: the number of mutiprocess
-        overwrite: whether overwrite data
-        mixed_data: Whether the mixed data
 
-    '''
     def make_dataset_with_args(self, input_files,
                                mode,
                                shuffle=False,
                                num_process_worker: int=0,
                                overwrite: bool=False,
                                mixed_data=True,
                                dupe_factor=1):
@@ -410,39 +328,7 @@
                         contain_objs.append(intermediate_output)
 
             else:
                 for input_item in input_files:
                     contain_objs.append(input_item)
 
 
-
-
-    def make_dataset(self,outfile: typing.Union[str,list],
-                     data,
-                     input_fn_args: typing.Any,
-                     num_process_worker: int = 0,
-                     shuffle: bool=True):
-
-        self.on_data_ready()
-        fw = DataWriteHelper(self.data_process_fn,
-                             input_fn_args,
-                             outfile,
-                             self.backend,
-                             num_process_worker=num_process_worker,
-                             shuffle=shuffle)
-        fw.save(data)
-        self.on_data_finalize()
-
-
-
-
-def make_dataset(data: typing.List,
-               input_fn:typing.Callable[[int,typing.Any,tuple],typing.Union[typing.Dict,typing.List,typing.Tuple]],
-               input_fn_args:typing.Tuple,
-               outfile:str,
-               backend: str,
-               overwrite = False,
-               num_process_worker:int = 8):
-
-    if not os.path.exists(outfile) or overwrite:
-        fw = DataWriteHelper(input_fn,input_fn_args,outfile,backend,num_process_worker)
-        fw.save(data)
```

## deep_training/data_helper/training_args.py

```diff
@@ -114,19 +114,21 @@
 
 
 
 @dataclass
 class TrainingArguments:
     optimizer: str = field(
         default='adamw',
-        metadata={"help": "one of [adamw,adam,lamb,lion]"},
+        metadata={"help": "one of [lamb,adamw_hf,adamw,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_apex_fused,adafactor,adamw_anyprecision,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_lion_32bit,paged_lion_8bit]"},
     )
+    optimizer_args: Optional[str] = field(default=None,metadata={"help": "sample a=100,b=10 "})
     scheduler_type: str = field(
         default='linear',
-        metadata={"help": "one of [linear,WarmupCosine,CAWR,CAL,Step,ReduceLROnPlateau]"},
+        metadata={"help": "one of [linear,WarmupCosine,CAWR,CAL,Step,ReduceLROnPlateau, "
+                          "cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau]"},
     )
 
     scheduler: dict = field(
         default=None,
         # {
         #     # StepLR
         #     "decay_rate": 0.999,
@@ -193,15 +195,15 @@
         metadata={"help": "max_grad_norm"},
     )
     weight_decay: float = field(
         default=0,
         metadata={"help": "weight_decay"},
     )
 
-    warmup_steps: int = field(
+    warmup_steps: float = field(
         default=0,
         metadata={"help": "warmup_steps"},
     )
 
     train_batch_size: int = field(
         default=16,
         metadata={"help": "train_batch_size"},
```

## deep_training/nlp/layers/lora_v1/layers.py

```diff
@@ -1,24 +1,43 @@
 #  ------------------------------------------------------------------------------------------
 #  Copyright (c) Microsoft Corporation. All rights reserved.
 #  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.
 #  ------------------------------------------------------------------------------------------
 import importlib
+import sys
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 import math
 from typing import Optional, List
 
+if sys.version_info[0] < 3.8:
+    _is_python_greater_3_8 = False
+else:
+    _is_python_greater_3_8 = True
+
+
 def is_bnb_available():
     return importlib.util.find_spec("bitsandbytes") is not None
 
 
+def is_bnb_4bit_available():
+    if _is_python_greater_3_8:
+        from importlib.metadata import version
+
+        bnb_version = version("bitsandbytes")
+    else:
+        from pkg_resources import get_distribution
+
+        bnb_version = get_distribution("bitsandbytes").version
+    return bnb_version >= "0.39.0"
+
+
 if is_bnb_available():
     import bitsandbytes as bnb
 
 class LoraLayer():
     def __init__(
         self, 
         r: int,
```

## deep_training/nlp/layers/lora_v2/layers.py

```diff
@@ -1,23 +1,42 @@
 # @Time    : 2023/4/10 23:58
 # @Author  : tk
 # @FileName: layers
 
 import importlib
 import math
+import sys
 import warnings
 import torch
 from torch import nn
 from torch.nn import functional as F
 from .utils import transpose
 
+if sys.version_info[0] < 3.8:
+    _is_python_greater_3_8 = False
+else:
+    _is_python_greater_3_8 = True
+
+
 def is_bnb_available():
     return importlib.util.find_spec("bitsandbytes") is not None
 
 
+def is_bnb_4bit_available():
+    if _is_python_greater_3_8:
+        from importlib.metadata import version
+
+        bnb_version = version("bitsandbytes")
+    else:
+        from pkg_resources import get_distribution
+
+        bnb_version = get_distribution("bitsandbytes").version
+    return bnb_version >= "0.39.0"
+
+
 if is_bnb_available():
     import bitsandbytes as bnb
 
 
 def mark_only_lora_as_trainable(model: nn.Module, bias: str = "none") -> None:
     for n, p in model.named_parameters():
         if "lora_" not in n:
@@ -44,14 +63,17 @@
     ):
         self.r = {}
         self.lora_alpha = {}
         self.scaling = {}
         self.lora_dropout = nn.ModuleDict({})
         self.lora_A = nn.ModuleDict({})
         self.lora_B = nn.ModuleDict({})
+        # For Embedding layer
+        self.lora_embedding_A = nn.ParameterDict({})
+        self.lora_embedding_B = nn.ParameterDict({})
         # Mark the weight as unmerged
         self.merged = False
         self.disable_adapters = False
         self.in_features = in_features
         self.out_features = out_features
 
     def update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights,dtype=None):
@@ -68,25 +90,46 @@
             self.lora_A.update(nn.ModuleDict({adapter_name: nn.Linear(self.in_features, r, bias=False,dtype=dtype)}))
             self.lora_B.update(nn.ModuleDict({adapter_name: nn.Linear(r, self.out_features, bias=False,dtype=dtype)}))
             self.scaling[adapter_name] = lora_alpha / r
         if init_lora_weights:
             self.reset_lora_parameters(adapter_name)
         self.to(self.weight.device)
 
+    def update_layer_embedding(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights,dtype=None):
+        self.r[adapter_name] = r
+        self.lora_alpha[adapter_name] = lora_alpha
+        if lora_dropout > 0.0:
+            lora_dropout_layer = nn.Dropout(p=lora_dropout)
+        else:
+            lora_dropout_layer = nn.Identity()
+
+        self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))
+        # Actual trainable parameters
+        if r > 0:
+            self.lora_embedding_A.update(
+                nn.ParameterDict({adapter_name: nn.Parameter(self.weight.new_zeros((r, self.in_features),dtype=dtype))})
+            )
+            self.lora_embedding_B.update(
+                nn.ParameterDict({adapter_name: nn.Parameter(self.weight.new_zeros((self.out_features, r),dtype=dtype))})
+            )
+            self.scaling[adapter_name] = lora_alpha / r
+        if init_lora_weights:
+            self.reset_lora_parameters(adapter_name)
+        self.to(self.weight.device)
+
     def reset_lora_parameters(self, adapter_name):
         if adapter_name in self.lora_A.keys():
             # initialize A the same way as the default for nn.Linear and B to zero
             nn.init.kaiming_uniform_(self.lora_A[adapter_name].weight, a=math.sqrt(5))
             nn.init.zeros_(self.lora_B[adapter_name].weight)
+        if adapter_name in self.lora_embedding_A.keys():
+            # initialize a the same way as the default for nn.linear and b to zero
+            nn.init.zeros_(self.lora_embedding_A[adapter_name])
+            nn.init.normal_(self.lora_embedding_B[adapter_name])
 
-    def merge(self):
-        raise NotImplementedError
-
-    def unmerge(self):
-        raise NotImplementedError
 
 class Linear(nn.Linear, LoraLayer):
     # Lora implemented in a dense layer
     def __init__(
         self,
         adapter_name: str,
         in_features: int,
@@ -141,33 +184,123 @@
                     self.fan_in_fan_out,
                 )
                 * self.scaling[self.active_adapter]
             )
             self.merged = False
 
     def forward(self, x: torch.Tensor):
+        previous_dtype = x.dtype
         if self.active_adapter not in self.lora_A.keys():
             return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
         if self.disable_adapters:
             if self.r[self.active_adapter] > 0 and self.merged:
                 self.unmerge()
             result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
         elif self.r[self.active_adapter] > 0 and not self.merged:
             result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
+
+            x = x.to(self.lora_A[self.active_adapter].weight.dtype)
+
             result += (
                 self.lora_B[self.active_adapter](
                     self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))
                 )
                 * self.scaling[self.active_adapter]
             )
         else:
             result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
+
+        result = result.to(previous_dtype)
+
         return result
 
 
+
+
+class Embedding(nn.Embedding, LoraLayer):
+    # LoRA implemented in a Embedding layer
+    def __init__(
+        self,
+        adapter_name: str,
+        num_embeddings: int,
+        embedding_dim: int,
+        r: int = 0,
+        lora_alpha: int = 1,
+        lora_dropout: float = 0.0,
+        **kwargs,
+    ):
+        init_lora_weights = kwargs.pop("init_lora_weights", True)
+
+        nn.Embedding.__init__(self, num_embeddings, embedding_dim, **kwargs)
+        LoraLayer.__init__(self, in_features=num_embeddings, out_features=embedding_dim)
+
+        self.weight.requires_grad = False
+
+        nn.Embedding.reset_parameters(self)
+        self.update_layer_embedding(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights,dtype=kwargs.get('dtype',None))
+        self.active_adapter = adapter_name
+
+    def unmerge(self, mode: bool = True):
+        if not self.merged:
+            warnings.warn("Already unmerged. Nothing to do.")
+            return
+        if self.r[self.active_adapter] > 0:
+            self.weight.data -= (
+                transpose(
+                    self.lora_embedding_B[self.active_adapter] @ self.lora_embedding_A[self.active_adapter], True
+                )
+                * self.scaling[self.active_adapter]
+            )
+            self.merged = False
+
+    def merge(self):
+        if self.merged:
+            warnings.warn("Already merged. Nothing to do.")
+            return
+        if self.r[self.active_adapter] > 0:
+            self.weight.data += (
+                transpose(
+                    self.lora_embedding_B[self.active_adapter] @ self.lora_embedding_A[self.active_adapter], True
+                )
+                * self.scaling[self.active_adapter]
+            )
+            self.merged = True
+
+    def forward(self, x: torch.Tensor):
+        if self.disable_adapters:
+            if self.r[self.active.adapter] > 0 and self.merged:
+                self.weight.data -= (
+                    transpose(
+                        self.lora_embedding_B[self.active_adapter].weight
+                        @ self.lora_embedding_A[self.active_adapter].weight,
+                        True,
+                    )
+                    * self.scaling[self.active_adapter]
+                )
+                self.merged = False
+            return nn.Embedding.forward(self, x)
+
+        elif self.r[self.active_adapter] > 0 and not self.merged:
+            result = nn.Embedding.forward(self, x)
+            if self.r[self.active_adapter] > 0:
+                after_A = F.embedding(
+                    x,
+                    self.lora_embedding_A[self.active_adapter].T,
+                    self.padding_idx,
+                    self.max_norm,
+                    self.norm_type,
+                    self.scale_grad_by_freq,
+                    self.sparse,
+                )
+                result += (after_A @ self.lora_embedding_B[self.active_adapter].T) * self.scaling[self.active_adapter]
+            return result
+        else:
+            return nn.Embedding.forward(self, x)
+
+
 if is_bnb_available():
 
     class Linear8bitLt(bnb.nn.Linear8bitLt, LoraLayer):
         # Lora implemented in a dense layer
         def __init__(
             self,
             adapter_name,
@@ -218,8 +351,67 @@
                     output = (
                         self.lora_B[self.active_adapter](
                             self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))
                         )
                         * self.scaling[self.active_adapter]
                     )
                 result += output
-            return result
+            return result
+
+
+    if is_bnb_4bit_available():
+
+        class Linear4bit(bnb.nn.Linear4bit, LoraLayer):
+            # Lora implemented in a dense layer
+            def __init__(
+                self,
+                adapter_name,
+                in_features,
+                out_features,
+                r: int = 0,
+                lora_alpha: int = 1,
+                lora_dropout: float = 0.0,
+                **kwargs,
+            ):
+                bnb.nn.Linear4bit.__init__(
+                    self,
+                    in_features,
+                    out_features,
+                    bias=kwargs.get("bias", True),
+                    compute_dtype=kwargs.get("compute_dtype", torch.float32),
+                    compress_statistics=kwargs.get("compress_statistics", True),
+                    quant_type=kwargs.get("quant_type", "nf4"),
+                )
+                LoraLayer.__init__(self, in_features=in_features, out_features=out_features)
+
+                # Freezing the pre-trained weight matrix
+                self.weight.requires_grad = False
+
+                init_lora_weights = kwargs.pop("init_lora_weights", True)
+                self.update_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights,dtype=kwargs.get('dtype',None))
+                self.active_adapter = adapter_name
+
+            def forward(self, x: torch.Tensor):
+                result = super().forward(x)
+
+                if self.disable_adapters or self.active_adapter not in self.lora_A.keys():
+                    return result
+                elif self.r[self.active_adapter] > 0:
+                    result = result.clone()
+                    if not torch.is_autocast_enabled():
+                        expected_dtype = result.dtype
+                        x = x.to(self.lora_A[self.active_adapter].weight.dtype)
+                        output = (
+                            self.lora_B[self.active_adapter](
+                                self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))
+                            ).to(expected_dtype)
+                            * self.scaling[self.active_adapter]
+                        )
+                    else:
+                        output = (
+                            self.lora_B[self.active_adapter](
+                                self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))
+                            )
+                            * self.scaling[self.active_adapter]
+                        )
+                    result += output
+                return result
```

## deep_training/nlp/layers/lora_v2/utils.py

```diff
@@ -10,14 +10,16 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import copy
+import warnings
+
 import torch
 
 
 # needed for prefix-tuning of bloom model
 def bloom_model_postprocess_past_key_value(past_key_values):
     past_key_values = torch.cat(past_key_values)
     total_layers, batch_size, num_attention_heads, num_virtual_tokens, head_dim = past_key_values.shape
@@ -27,70 +29,62 @@
     )
     values = past_key_values[total_layers // 2 :]
     values = values.reshape(total_layers // 2, batch_size * num_attention_heads, num_virtual_tokens, head_dim)
 
     return tuple(zip(keys, values))
 
 
-def prepare_model_for_int8_training(
-    model, output_embedding_layer_name="lm_head", use_gradient_checkpointing=True, layer_norm_names=["layer_norm"]
-):
+def prepare_model_for_kbit_training(model,
+                                    use_input_require_grads=True,
+                                    use_gradient_checkpointing=True):
     r"""
-    This method wraps the entire protocol for preparing a model before running a training. This includes:
-        1- Cast the layernorm in fp32 2- making output embedding layer require grads 3- Add the upcasting of the lm
-        head to fp32
-
-    Args:
-        model, (`transformers.PreTrainedModel`):
-            The loaded model from `transformers`
-    """
-    loaded_in_8bit = getattr(model, "is_loaded_in_8bit", False)
+       This method wraps the entire protocol for preparing a model before running a training. This includes:
+           1- Cast the layernorm in fp32 2- making output embedding layer require grads 3- Add the upcasting of the lm
+           head to fp32
+
+       Args:
+           model, (`transformers.PreTrainedModel`):
+               The loaded model from `transformers`
+       """
+    loaded_in_kbit = getattr(model, "is_loaded_in_8bit", False) or getattr(model, "is_loaded_in_4bit", False)
 
     for name, param in model.named_parameters():
         # freeze base model's layers
         param.requires_grad = False
 
-        if loaded_in_8bit:
-            # cast layer norm in fp32 for stability for 8bit models
-            if param.ndim == 1 and any(layer_norm_name in name for layer_norm_name in layer_norm_names):
-                param.data = param.data.to(torch.float32)
+    # cast all non INT8 parameters to fp32
+    for param in model.parameters():
+        if (param.dtype == torch.float16) or (param.dtype == torch.bfloat16):
+            param.data = param.data.to(torch.float32)
 
-    if loaded_in_8bit and use_gradient_checkpointing:
+    if loaded_in_kbit and use_input_require_grads:
         # For backward compatibility
         if hasattr(model, "enable_input_require_grads"):
             model.enable_input_require_grads()
         else:
-
             def make_inputs_require_grad(module, input, output):
                 output.requires_grad_(True)
 
             model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)
 
+    if loaded_in_kbit and use_gradient_checkpointing:
+
         # enable gradient checkpointing for memory efficiency
         model.gradient_checkpointing_enable()
 
-    if hasattr(model, output_embedding_layer_name):
-        output_embedding_layer = getattr(model, output_embedding_layer_name)
-        input_dtype = output_embedding_layer.weight.dtype
-
-        class CastOutputToFloat(torch.nn.Sequential):
-            r"""
-            Manually cast to the expected dtype of the lm_head as sometimes there is a final layer norm that is casted
-            in fp32
-
-            """
-
-            def forward(self, x):
-                return super().forward(x.to(input_dtype)).to(torch.float32)
-
-        setattr(model, output_embedding_layer_name, CastOutputToFloat(output_embedding_layer))
-
     return model
 
 
+def prepare_model_for_int8_training(*args, **kwargs):
+    warnings.warn(
+        "prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.",
+        FutureWarning,
+    )
+    return prepare_model_for_kbit_training(*args, **kwargs)
+
 # copied from transformers.models.bart.modeling_bart
 def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):
     """
     Shift input ids one token to the right.
 
     Args:
         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`): input ids
```

## deep_training/nlp/models/transformer_base.py

```diff
@@ -249,15 +249,15 @@
             if training_args.adv is not None and training_args.adv['mode'] != None:
                 assert training_args.adv['mode']  in AdversarialMethods.keys(), ValueError('no support adv mode {} , must be in {}'.format(training_args.adv['mode'],','.join(AdversarialMethods.keys())))
                 self.automatic_optimization = False
         print(training_args)
         print(model_args)
 
         try:
-            self.save_hyperparameters(ignore=['config'])
+            self.save_hyperparameters(ignore=['config','torch_dtype'])
         except:
             pass
         self.config = config
         self.model_args = model_args
         self.training_args = training_args
         self.__backbone : typing.Optional[TransformerBase] = None
         if hasattr(self,'__BACKBONE_CLASS__') and len(self.__BACKBONE_CLASS__) > 0:
@@ -401,15 +401,18 @@
         opt = []
         a: nn.Module
         for a, lr in model_attrs:
             opt += __get_named_parameters(a)
         return opt
 
     def configure_optimizers(self):
-        return configure_optimizers(self.get_named_parameters(), self.training_args,self.trainer.estimated_stepping_batches)
+        return configure_optimizers(self.get_named_parameters(),
+                                    self.training_args,
+                                    self.trainer.estimated_stepping_batches,
+                                    self.get_model_lr())
 
 
     def manual_backward(self,loss: Tensor, *args: Any, **kwargs: Any):
         if isinstance(loss,dict):
             loss = loss['loss']
         return super(TransformerLightningModule, self).manual_backward(loss)
```

## deep_training/nlp/models/lora/__init__.py

```diff
@@ -1,6 +1,6 @@
 # @Time    : 2023/3/2 20:55
 # @Author  : tk
 
 # 兼容旧版本
 from .v1 import LoraModel,LoraArguments
-from .v2 import LoraModel as LoraModelV2,LoraArguments as LoraArgumentsV2
+from .v2 import LoraModule as LoraModelV2,LoraArguments as LoraArgumentsV2
```

## deep_training/nlp/models/lora/v1/lora_wrapper.py

```diff
@@ -15,14 +15,15 @@
 from typing import Optional, Union, List
 
 import torch
 from transformers import Conv1D
 from transformers.utils import PushToHubMixin
 
 from .configuration import LoraArguments, WEIGHTS_NAME
+from ...transformer_base import TransformerBase
 from ....layers.lora_v1.layers import MergedLinear, is_bnb_available, LoraLayer, Linear
 from ....layers.lora_v1.utils import mark_only_lora_as_trainable
 
 __all__ = [
     'LoraArguments',
     'LoraModel',
     'LoraLayer'
@@ -93,25 +94,26 @@
         PushToHubMixin.__init__(self)
         self.lora_config = config
         self.model = model
         self._find_and_replace()
         mark_only_lora_as_trainable(self.model, self.lora_config.bias)
         self.forward = self.model.forward
 
-    def _find_and_replace(self):
-        loaded_in_8bit = getattr(self.model, "is_loaded_in_8bit", False)
-        if not loaded_in_8bit:
-            if hasattr(self.model, 'model'):
-                loaded_in_8bit = getattr(self.model.model, "is_loaded_in_8bit", False)
+    def get_transformer_model(self):
+        return self.model.model if isinstance(self.model, TransformerBase) else self.model
 
-        if loaded_in_8bit and not is_bnb_available():
+    def _find_and_replace(self):
+        loaded_in_4bit = getattr(self.get_transformer_model(), "is_loaded_in_4bit", False)
+        loaded_in_8bit = getattr(self.get_transformer_model(), "is_loaded_in_8bit", False)
+        if (loaded_in_4bit or loaded_in_8bit) and not is_bnb_available():
             raise ImportError(
-                "To use Lora with 8-bit quantization, please install the `bitsandbytes` package. "
+                "To use Lora with 8-bit or 4-bit quantization, please install the `bitsandbytes` package. "
                 "You can install it with `pip install bitsandbytes`."
             )
+
         is_target_modules_in_base_model = False
         kwargs = {
             "r": self.lora_config.r,
             "lora_alpha": self.lora_config.lora_alpha,
             "lora_dropout": self.lora_config.lora_dropout,
             "fan_in_fan_out": self.lora_config.fan_in_fan_out,
             "merge_weights": self.lora_config.merge_weights or self.lora_config.inference_mode,
```

## deep_training/nlp/models/lora/v2/__init__.py

```diff
@@ -1,6 +1,6 @@
 # @Time    : 2023/4/7 21:50
 # @Author  : tk
 # @FileName: __init__.py
 
 from ....layers.lora_v2.layers import is_bnb_available
-from .lora_wrapper import LoraModel,LoraConfig,AdaLoraConfig,LoraArguments
+from .lora_wrapper import LoraModel,LoraModule,LoraConfig,AdaLoraConfig,LoraArguments
```

## deep_training/nlp/models/lora/v2/adalora_model.py

```diff
@@ -7,30 +7,30 @@
 import torch
 from torch import nn
 from transformers import Conv1D
 
 from ....layers.lora_v2.layers import mark_only_lora_as_trainable, is_bnb_available, LoraLayer
 from ....layers.lora_v2.adalora import RankAllocator,SVDLinear
 from ....layers.lora_v2.utils import _freeze_adapter, _get_submodules, \
-    TRANSFORMERS_MODELS_TO_ADALORA_TARGET_MODULES_MAPPING
-from .lora_model import LoraModel
+    TRANSFORMERS_MODELS_TO_ADALORA_TARGET_MODULES_MAPPING, prepare_model_for_kbit_training
+from .lora_model import LoraModule
 
 __all__ = [
     'is_bnb_available',
-    'AdaLoraModel'
+    'AdaLoraModule'
 ]
 
 
 
 if is_bnb_available():
     import bitsandbytes as bnb
     from ....layers.lora_v2.adalora import SVDLinear8bitLt
 
 
-class AdaLoraModel(LoraModel):
+class AdaLoraModule(LoraModule):
     """
     Creates AdaLoRA (Adaptive LoRA) model from a pretrained transformers model. Paper:
     https://openreview.net/pdf?id=lq62uWRJjiY
 
     Args:
         model ([`transformers.PreTrainedModel`]): The model to be adapted.
         config ([`AdaLoraConfig`]): The configuration of the AdaLora model.
@@ -40,20 +40,32 @@
 
 
     **Attributes**:
         - **model** ([`transformers.PreTrainedModel`]) -- The model to be adapted.
         - **peft_config** ([`AdaLoraConfig`]): The configuration of the AdaLora model.
     """
 
-    def __init__(self, model, config, adapter_name):
+    def __init__(self, model, config, adapter_name,auto_prepare_kbit_training=True,
+                 use_input_require_grads=True,
+                 use_gradient_checkpointing=True):
         nn.Module.__init__(self)
         self.model = model
+        transformer_model = self.get_transformer_model()
+        loaded_in_4bit = getattr(transformer_model, "is_loaded_in_4bit", False)
+        loaded_in_8bit = getattr(transformer_model, "is_loaded_in_8bit", False)
+        if auto_prepare_kbit_training and (loaded_in_4bit or loaded_in_8bit):
+            prepare_model_for_kbit_training(transformer_model,
+                                            use_input_require_grads=use_input_require_grads,
+                                            use_gradient_checkpointing=use_gradient_checkpointing)
+
         self.peft_config = config
         self.add_adapter(adapter_name, self.peft_config[adapter_name])
 
+
+
     def add_adapter(self, adapter_name, config=None):
         if config is not None:
             config = self._prepare_adalora_config(config, self.model.config.to_dict())
             self.peft_config[adapter_name] = config
         self._find_and_replace(adapter_name)
         if len(self.peft_config) > 1 and self.peft_config[adapter_name].bias != "none":
             raise ValueError(
@@ -75,18 +87,19 @@
             _freeze_adapter(self.model, adapter_name)
         else:
             self.trainable_adapter_name = adapter_name
             self.rankallocator = RankAllocator(self.model, self.peft_config[adapter_name], self.trainable_adapter_name)
 
     def _find_and_replace(self, adapter_name):
         lora_config = self.peft_config[adapter_name]
-        loaded_in_8bit = getattr(self.model, "is_loaded_in_8bit", False)
-        if loaded_in_8bit and not is_bnb_available():
+        loaded_in_4bit = getattr(self.get_transformer_model(), "is_loaded_in_4bit", False)
+        loaded_in_8bit = getattr(self.get_transformer_model(), "is_loaded_in_8bit", False)
+        if (loaded_in_4bit or loaded_in_8bit) and not is_bnb_available():
             raise ImportError(
-                "To use Lora with 8-bit quantization, please install the `bitsandbytes` package. "
+                "To use Lora with 8-bit or 4-bit quantization, please install the `bitsandbytes` package. "
                 "You can install it with `pip install bitsandbytes`."
             )
         is_target_modules_in_base_model = False
         kwargs = {
             "r": lora_config.init_r,
             "lora_alpha": lora_config.lora_alpha,
             "lora_dropout": lora_config.lora_dropout,
@@ -159,15 +172,18 @@
             )
 
     def __getattr__(self, name: str):
         """Forward missing attributes to the wrapped module."""
         try:
             return super().__getattr__(name)  # defer to nn.Module's logic
         except AttributeError:
-            return getattr(self.model, name)
+            try:
+                return getattr(self.model, name)
+            except AttributeError:
+                return getattr(self.model.model, name)
 
     def forward(self, *args, **kwargs):
         outputs = self.model.forward(*args, **kwargs)
 
         # Calculate the orthogonal regularization
         orth_reg_weight = self.peft_config[self.trainable_adapter_name].orth_reg_weight
         assert orth_reg_weight > 0
```

## deep_training/nlp/models/lora/v2/lora_model.py

```diff
@@ -9,108 +9,122 @@
 import warnings
 from dataclasses import asdict
 from enum import Enum
 import torch
 from torch import nn
 from transformers import Conv1D
 
-from ....layers.lora_v2.layers import mark_only_lora_as_trainable, is_bnb_available, LoraLayer, Linear
-from ....layers.lora_v2.utils import _freeze_adapter, _get_submodules, ModulesToSaveWrapper
+from ...transformer_base import TransformerBase
+from ....layers.lora_v2.layers import mark_only_lora_as_trainable, is_bnb_available, LoraLayer, Linear, \
+    is_bnb_4bit_available,  Embedding
+from ....layers.lora_v2.utils import _freeze_adapter, _get_submodules, ModulesToSaveWrapper, \
+    prepare_model_for_kbit_training, TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING
 
 __all__ = [
     'is_bnb_available',
-    'LoraModel',
+    'LoraModule',
 ]
 
 
 if is_bnb_available():
     import bitsandbytes as bnb
     from ....layers.lora_v2.layers import Linear8bitLt
 
-class LoraModel(torch.nn.Module):
+if is_bnb_4bit_available():
+    from ....layers.lora_v2.layers import Linear4bit
+
+class LoraModule(torch.nn.Module):
     """
     Creates Low Rank Adapter (Lora) model from a pretrained transformers model.
 
     Args:
         model ([`~transformers.PreTrainedModel`]): The model to be adapted.
         config ([`LoraConfig`]): The configuration of the Lora model.
 
     Returns:
         `torch.nn.Module`: The Lora model.
 
 
     **Attributes**:
         - **model** ([`~transformers.PreTrainedModel`]) -- The model to be adapted.
-        - **peft_config** ([`LoraConfig`]): The configuration of the Lora model.
+        - **lora_config** ([`LoraConfig`]): The configuration of the Lora model.
     """
 
-    def __init__(self, model, config, adapter_name):
+    def __init__(self, model, config, adapter_name,auto_prepare_kbit_training=True,
+                 use_input_require_grads=True,
+                 use_gradient_checkpointing=True):
         super().__init__()
         self.model = model
+        transformer_model = self.get_transformer_model()
+        loaded_in_4bit = getattr(transformer_model, "is_loaded_in_4bit", False)
+        loaded_in_8bit = getattr(transformer_model, "is_loaded_in_8bit", False)
+
+
+        if auto_prepare_kbit_training and (loaded_in_4bit or loaded_in_8bit):
+            prepare_model_for_kbit_training(transformer_model,
+                                            use_input_require_grads=use_input_require_grads,
+                                            use_gradient_checkpointing=use_gradient_checkpointing)
+            # self.model.set_model(model,copy_attr=False)
+
+
         self.forward = self.model.forward
-        self.peft_config = config
-        self.add_adapter(adapter_name, self.peft_config[adapter_name])
+        self.lora_config = config
+        self.add_adapter(adapter_name, self.lora_config[adapter_name])
+
+
+
+    def get_transformer_model(self):
+        return self.model.model if isinstance(self.model, TransformerBase) else self.model
 
     def add_adapter(self, adapter_name, config=None):
         if config is not None:
-            config = self._prepare_lora_config(config, self.model.config.to_dict())
-            self.peft_config[adapter_name] = config
+            model = self.get_transformer_model()
+            model_config = model.config.to_dict() if hasattr(model.config, "to_dict") else model.config
+            config = self._prepare_lora_config(config, model_config)
+            self.lora_config[adapter_name] = config
         self._find_and_replace(adapter_name)
-        if len(self.peft_config) > 1 and self.peft_config[adapter_name].bias != "none":
+        if len(self.lora_config) > 1 and self.lora_config[adapter_name].bias != "none":
             raise ValueError(
                 "LoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to 'none' for all adapters."
             )
-        if self.peft_config[adapter_name].inference_mode:
+        if self.lora_config[adapter_name].inference_mode:
             _freeze_adapter(self.model, adapter_name)
         else:
-            mark_only_lora_as_trainable(self.model, self.peft_config[adapter_name].bias)
+            mark_only_lora_as_trainable(self.model, self.lora_config[adapter_name].bias)
 
     def _find_and_replace(self, adapter_name):
-        lora_config = self.peft_config[adapter_name]
-        loaded_in_8bit = getattr(self.model, "is_loaded_in_8bit", False)
-        if not loaded_in_8bit:
-            if hasattr(self.model,'model'):
-                loaded_in_8bit = getattr(self.model.model, "is_loaded_in_8bit", False)
-
-        if loaded_in_8bit and not is_bnb_available():
+        lora_config = self.lora_config[adapter_name]
+        loaded_in_4bit = getattr(self.get_transformer_model(), "is_loaded_in_4bit", False)
+        loaded_in_8bit = getattr(self.get_transformer_model(), "is_loaded_in_8bit", False)
+        if (loaded_in_4bit or loaded_in_8bit) and not is_bnb_available():
             raise ImportError(
-                "To use Lora with 8-bit quantization, please install the `bitsandbytes` package. "
+                "To use Lora with 8-bit or 4-bit quantization, please install the `bitsandbytes` package. "
                 "You can install it with `pip install bitsandbytes`."
             )
         is_target_modules_in_base_model = False
         kwargs = {
             "r": lora_config.r,
             "lora_alpha": lora_config.lora_alpha,
             "lora_dropout": lora_config.lora_dropout,
             "fan_in_fan_out": lora_config.fan_in_fan_out,
             "init_lora_weights": lora_config.init_lora_weights,
         }
-        if lora_config.target_dtype is not None and not loaded_in_8bit:
-            if lora_config.target_dtype == 16 or lora_config.target_dtype == '16':
-                kwargs['dtype'] = torch.float16
-            elif lora_config.target_dtype == 32 or lora_config.target_dtype == '32':
-                kwargs['dtype'] = torch.float32
-            elif lora_config.target_dtype == 64 or lora_config.target_dtype == '64':
-                kwargs['dtype'] = torch.float64
-            elif lora_config.target_dtype == 'bf16':
-                kwargs['dtype'] = torch.bfloat16
-            elif isinstance(lora_config.target_dtype, torch.dtype):
-                kwargs['dtype'] = lora_config.target_dtype
-
         key_list = [key for key, _ in self.model.named_modules()]
         for key in key_list:
             if isinstance(lora_config.target_modules, str):
                 target_module_found = re.fullmatch(lora_config.target_modules, key)
             else:
                 target_module_found = any(key.endswith(target_key) for target_key in lora_config.target_modules)
             if target_module_found:
                 if not is_target_modules_in_base_model:
                     is_target_modules_in_base_model = True
                 parent, target, target_name = _get_submodules(self.model, key)
-                bias = target.bias is not None
+                if hasattr(target, "bias"):
+                    bias = target.bias is not None
+
                 if isinstance(target, LoraLayer):
                     target.update_layer(
                         adapter_name,
                         lora_config.r,
                         lora_config.lora_alpha,
                         lora_config.lora_dropout,
                         lora_config.init_lora_weights,
@@ -126,14 +140,31 @@
                                 "threshold": target.state.threshold,
                                 "index": target.index,
                             }
                         )
                         new_module = Linear8bitLt(
                             adapter_name, target.in_features, target.out_features, bias=bias, **eightbit_kwargs
                         )
+                    elif loaded_in_4bit and is_bnb_4bit_available() and isinstance(target, bnb.nn.Linear4bit):
+                        fourbit_kwargs = kwargs.copy()
+                        fourbit_kwargs.update(
+                            {
+                                "compute_dtype": target.compute_dtype,
+                                "compress_statistics": target.weight.compress_statistics,
+                                "quant_type": target.weight.quant_type,
+                            }
+                        )
+                        new_module = Linear4bit(
+                            adapter_name, target.in_features, target.out_features, bias=bias, **fourbit_kwargs
+                        )
+                    elif isinstance(target, torch.nn.Embedding):
+                        embedding_kwargs = kwargs.copy()
+                        embedding_kwargs.pop("fan_in_fan_out", None)
+                        in_features, out_features = target.num_embeddings, target.embedding_dim
+                        new_module = Embedding(adapter_name, in_features, out_features, **embedding_kwargs)
                     else:
                         if isinstance(target, torch.nn.Linear):
                             in_features, out_features = target.in_features, target.out_features
                             if kwargs["fan_in_fan_out"]:
                                 warnings.warn(
                                     "fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. "
                                     "Setting fan_in_fan_out to False."
@@ -162,39 +193,42 @@
                 f"Target modules {lora_config.target_modules} not found in the base model. "
                 f"Please check the target modules and try again."
             )
 
     def _replace_module(self, parent_module, child_name, new_module, old_module):
         setattr(parent_module, child_name, new_module)
         new_module.weight = old_module.weight
-        if old_module.bias is not None:
-            new_module.bias = old_module.bias
+        if hasattr(old_module, "bias"):
+            if old_module.bias is not None:
+                new_module.bias = old_module.bias
+
         if getattr(old_module, "state", None) is not None:
             new_module.state = old_module.state
             new_module.to(old_module.weight.device)
 
         # dispatch to correct device
         for name, module in new_module.named_modules():
             if "lora_" in name:
                 module.to(old_module.weight.device)
 
     def __getattr__(self, name: str):
         """Forward missing attributes to the wrapped module."""
         try:
             return super().__getattr__(name)  # defer to nn.Module's logic
         except AttributeError:
-            return getattr(self.model, name)
+            try:
+                return getattr(self.model, name)
+            except AttributeError:
+                return getattr(self.model.model, name)
 
-    @property
-    def modules_to_save(self):
-        return None
 
-    def get_peft_config_as_dict(self, inference: bool = False):
+
+    def get_lora_config_as_dict(self, inference: bool = False):
         config_dict = {}
-        for key, value in self.peft_config.items():
+        for key, value in self.lora_config.items():
             config = {k: v.value if isinstance(v, Enum) else v for k, v in asdict(value).items()}
             if inference:
                 config["inference_mode"] = True
         config_dict[key] = config
         return config
 
     def _set_adapter_layers(self, enabled=True):
@@ -223,29 +257,35 @@
 
     def unmerge_adapter(self):
         for module in self.model.modules():
             if isinstance(module, LoraLayer):
                 module.unmerge()
 
     @staticmethod
-    def _prepare_lora_config(peft_config, model_config):
-        if peft_config.inference_mode:
-            peft_config.merge_weights = True
-        return peft_config
+    def _prepare_lora_config(lora_config, model_config):
+        if lora_config.target_modules is None:
+            if model_config["model_type"] not in TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING:
+                raise ValueError("Please specify `target_modules` in `peft_config`")
+            lora_config.target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[model_config["model_type"]]
+
+        if lora_config.inference_mode:
+            lora_config.merge_weights = True
+        return lora_config
 
     def merge_and_unload(self):
         r"""
         This method merges the LoRa layers into the base model. This is needed if someone wants to use the base model
         as a standalone model.
         """
         if self.config.model_type == "gpt2":
             raise ValueError("GPT2 models are not supported for merging LORA layers")
 
-        if getattr(self.model, "is_loaded_in_8bit", False):
-            raise ValueError("Cannot merge LORA layers when the model is loaded in 8-bit mode")
+
+        if getattr(self.get_transformer_model(), "is_loaded_in_8bit", False) or getattr(self.get_transformer_model(), "is_loaded_in_4bit", False):
+            raise ValueError("Cannot merge LORA layers when the model is loaded in 8-bit mode and is_loaded_in_4bit")
 
         key_list = [key for key, _ in self.model.named_modules() if "lora" not in key]
         for key in key_list:
             try:
                 parent, target, target_name = _get_submodules(self.model, key)
             except AttributeError:
                 continue
@@ -254,8 +294,42 @@
                 new_module = torch.nn.Linear(target.in_features, target.out_features, bias=bias)
                 target.merge()
                 self._replace_module(parent, target_name, new_module, target)
 
             # save any additional trainable modules part of `modules_to_save`
             if isinstance(target, ModulesToSaveWrapper):
                 setattr(parent, target_name, target.modules_to_save[target.active_adapter])
-        return self.model
+        return self.model
+
+    def add_weighted_adapter(self, adapters, weights, adapter_name):
+        if len({self.lora_config[adapter].r for adapter in adapters}) != 1:
+            raise ValueError("All adapters must have the same r value")
+        self.lora_config[adapter_name] = self.lora_config[adapters[0]]
+        self.lora_config[adapter_name].lora_alpha = self.lora_config[adapters[0]].r
+        self._find_and_replace(adapter_name)
+        mark_only_lora_as_trainable(self.model, self.lora_config[adapter_name].bias)
+        _freeze_adapter(self.model, adapter_name)
+        key_list = [key for key, _ in self.model.named_modules() if "lora" not in key]
+        for key in key_list:
+            _, target, _ = _get_submodules(self.model, key)
+            if isinstance(target, LoraLayer):
+                if adapter_name in target.lora_A:
+                    target.lora_A[adapter_name].weight.data = target.lora_A[adapter_name].weight.data * 0.0
+                    target.lora_B[adapter_name].weight.data = target.lora_B[adapter_name].weight.data * 0.0
+                    for adapter, weight in zip(adapters, weights):
+                        if adapter not in target.lora_A:
+                            continue
+                        target.lora_A[adapter_name].weight.data += (
+                                target.lora_A[adapter].weight.data * weight * target.scaling[adapter]
+                        )
+                        target.lora_B[adapter_name].weight.data += target.lora_B[adapter].weight.data * weight
+
+                elif adapter_name in target.lora_embedding_A:
+                    target.lora_embedding_A[adapter_name].data = target.lora_embedding_A[adapter_name].data * 0.0
+                    target.lora_embedding_B[adapter_name].data = target.lora_embedding_B[adapter_name].data * 0.0
+                    for adapter, weight in zip(adapters, weights):
+                        if adapter not in target.lora_embedding_A:
+                            continue
+                        target.lora_embedding_A[adapter_name].data += (
+                                target.lora_embedding_A[adapter].data * weight * target.scaling[adapter]
+                        )
+                        target.lora_embedding_B[adapter_name].data += target.lora_embedding_B[adapter].data * weight
```

## deep_training/nlp/models/lora/v2/lora_wrapper.py

```diff
@@ -1,71 +1,85 @@
 # -*- coding: utf-8 -*-
 # @Time:  10:42
 # @Author: tk
 # @File：lora_wrapper
 import os
+import typing
 from contextlib import contextmanager
 import torch
+from torch import nn
+from transformers import PreTrainedModel
 from transformers.utils import PushToHubMixin
+from ...transformer_base import TransformerBase
 from ....layers.lora_v2.utils import _set_trainable, _set_adapter
-from .adalora_model import AdaLoraModel
+from .adalora_model import AdaLoraModule
 from .configuration import WEIGHTS_NAME, LoraConfig, AdaLoraConfig,LoraArguments
-from .lora_model import LoraModel
+from .lora_model import LoraModule
 from .save_and_load import get_lora_model_state_dict, set_lora_model_state_dict
 
 
 LORA_TYPE_TO_MODEL_MAPPING = {
-    "lora": LoraModel,
-    "adalora": AdaLoraModel,
+    "lora": LoraModule,
+    "adalora": AdaLoraModule,
 }
 
 LORA_TYPE_TO_CONFIG_MAPPING = {
     "lora": LoraConfig,
     "adalora": AdaLoraConfig,
 }
 
 class LoraModel(PushToHubMixin, torch.nn.Module):
     """
-    Base model encompassing various Peft methods.
+    Base model encompassing various Lora methods.
 
     Args:
-        model ([`~transformers.PreTrainedModel`]): The base transformer model used for Peft.
-        lora_config_v2 ([`LoraConfig`]): The configuration of the Peft model.
+        model ([`~transformers.PreTrainedModel`]): The base transformer model used for Lora.
+        lora_config ([`LoraConfig`]): The configuration of the Lora model.
 
 
     **Attributes**:
-        - **base_model** ([`~transformers.PreTrainedModel`]) -- The base transformer model used for Peft.
-        - **lora_config** ([`LoraConfig`]) -- The configuration of the Peft model.
+        - **base_model** ([`~transformers.PreTrainedModel`]) -- The base transformer model used for Lora.
+        - **lora_config** ([`LoraConfig`]) -- The configuration of the Lora model.
         - **modules_to_save** (`list` of `str`) -- The list of sub-module names to save when
         saving the model.
-        - **prompt_encoder** ([`PromptEncoder`]) -- The prompt encoder used for Peft if
+        - **prompt_encoder** ([`PromptEncoder`]) -- The prompt encoder used for Lora if
         using [`PromptLearningConfig`].
-        - **prompt_tokens** (`torch.Tensor`) -- The virtual prompt tokens used for Peft if
+        - **prompt_tokens** (`torch.Tensor`) -- The virtual prompt tokens used for Lora if
         using [`PromptLearningConfig`].
         - **transformer_backbone_name** (`str`) -- The name of the transformer
         backbone in the base model if using [`PromptLearningConfig`].
         - **word_embeddings** (`torch.nn.Embedding`) -- The word embeddings of the transformer backbone
         in the base model if using [`PromptLearningConfig`].
     """
 
-    def __init__(self, model, lora_config_v2: LoraConfig, adapter_name="default"):
+    def __init__(self, model, lora_config: LoraConfig, adapter_name="default",
+                 auto_prepare_kbit_training=True,
+                 use_input_require_grads=True,
+                 use_gradient_checkpointing=True):
+        '''
+            model TransformerBase , model.model
+        '''
         super().__init__()
-        assert lora_config_v2.lora_type is not None
+        assert lora_config.lora_type is not None
         self.base_model = model
-        self.config = self.base_model.config
+        self.config = getattr(model.model if isinstance(model, TransformerBase) else model, "config", None)
         self.modules_to_save = None
-        self.lora_config_v2 = {}
+        self.lora_config = {}
         self.active_adapter = adapter_name
-        self.lora_type = lora_config_v2.lora_type
-        self.base_model_torch_dtype = getattr(model, "dtype", None)
-        self.lora_config_v2[adapter_name] = lora_config_v2
-        self.base_model = LORA_TYPE_TO_MODEL_MAPPING[lora_config_v2.lora_type](
-            self.base_model, self.lora_config_v2, adapter_name
+        self.lora_type = lora_config.lora_type
+        self.base_model_torch_dtype = getattr(model.model if isinstance(model, TransformerBase) else model, "dtype", None)
+        self.lora_config[adapter_name] = lora_config
+        self.base_model: LoraModule = LORA_TYPE_TO_MODEL_MAPPING[lora_config.lora_type](
+            self.base_model, self.lora_config, adapter_name,
+            auto_prepare_kbit_training=auto_prepare_kbit_training,
+            use_input_require_grads=use_input_require_grads,
+            use_gradient_checkpointing=use_gradient_checkpointing,
         )
-        self.set_additional_trainable_modules(lora_config_v2, adapter_name)
+        self.set_additional_trainable_modules(lora_config, adapter_name)
+
 
     def save_pretrained(self, save_directory, **kwargs):
         r"""
         This function saves the adapter model and the adapter configuration files to a directory, so that it can be
         reloaded using the [`LoraModel.from_pretrained`] class method, and also used by the [`LoraModel.push_to_hub`]
         method.
 
@@ -76,15 +90,15 @@
             kwargs (additional keyword arguments, *optional*):
                 Additional keyword arguments passed along to the `push_to_hub` method.
         """
         if os.path.isfile(save_directory):
             raise ValueError(f"Provided path ({save_directory}) should be a directory, not a file")
         os.makedirs(save_directory, exist_ok=True)
 
-        for adapter_name, lora_config in self.lora_config_v2.items():
+        for adapter_name, lora_config in self.lora_config.items():
             # save only the trainable weights
             output_state_dict = get_lora_model_state_dict(
                 self, state_dict=kwargs.get("state_dict", None), adapter_name=adapter_name
             )
             output_dir = os.path.join(save_directory, adapter_name) if adapter_name != "default" else save_directory
             os.makedirs(output_dir, exist_ok=True)
             torch.save(output_state_dict, os.path.join(output_dir, WEIGHTS_NAME))
@@ -152,18 +166,16 @@
         )
 
     def __getattr__(self, name: str):
         """Forward missing attributes to the wrapped module."""
         try:
             return super().__getattr__(name)  # defer to nn.Module's logic
         except AttributeError:
-            try:
-                return getattr(self.base_model, name) # defer to nn.Module's logic
-            except AttributeError:
-                return getattr(self.base_model.model, name)
+            return getattr(self.base_model, name)  # defer to nn.Module's logic
+
 
 
     def forward(self, *args, **kwargs):
         """
         Forward pass of the model.
         """
         return self.get_base_model()(*args, **kwargs)
@@ -174,41 +186,41 @@
         Disables the adapter module.
         """
 
         self.base_model.disable_adapter_layers()
         yield
         self.base_model.enable_adapter_layers()
 
-    def get_base_model(self):
+    def get_base_model(self)-> typing.Union[TransformerBase,PreTrainedModel,nn.Module]:
         """
         Returns the base model.
         """
         return self.base_model.model
 
     def add_adapter(self, adapter_name, lora_config):
         if lora_config.lora_type != self.lora_type:
             raise ValueError(
                 f"Cannot combine adapters with different peft types. "
                 f"Found {self.lora_type} and {lora_config.lora_type}."
             )
-        self.lora_config_v2[adapter_name] = lora_config
+        self.lora_config[adapter_name] = lora_config
         self.base_model.add_adapter(adapter_name, lora_config)
         self.set_additional_trainable_modules(lora_config, adapter_name)
 
 
     def set_additional_trainable_modules(self, lora_config, adapter_name):
         if getattr(lora_config, "modules_to_save", None) is not None:
             if self.modules_to_save is None:
                 self.modules_to_save = set(lora_config.modules_to_save)
             else:
                 self.modules_to_save = self.modules_to_save.update(lora_config.modules_to_save)
             _set_trainable(self, adapter_name)
 
     def load_adapter(self, model_id, adapter_name, is_trainable=False, **kwargs):
-        if adapter_name not in self.lora_config_v2:
+        if adapter_name not in self.lora_config:
             # load the config
             lora_config = LORA_TYPE_TO_CONFIG_MAPPING[
                 LoraConfig.from_pretrained(model_id, subfolder=kwargs.get("subfolder", None)).lora_type
             ].from_pretrained(model_id, subfolder=kwargs.get("subfolder", None))
 
             lora_config.inference_mode = not is_trainable
             self.add_adapter(adapter_name, lora_config)
@@ -233,16 +245,16 @@
 
 
 
     def set_adapter(self, adapter_name):
         """
         Sets the active adapter.
         """
-        if adapter_name not in self.lora_config_v2:
+        if adapter_name not in self.lora_config:
             raise ValueError(f"Adapter {adapter_name} not found.")
         self.active_adapter = adapter_name
         self.base_model.set_adapter(adapter_name)
         _set_adapter(self, adapter_name)
 
     @property
     def active_peft_config(self):
-        return self.lora_config_v2[self.active_adapter]
+        return self.lora_config[self.active_adapter]
```

## deep_training/nlp/models/lora/v2/save_and_load.py

```diff
@@ -23,15 +23,15 @@
     Args:
         model ([`PeftModel`]): The Peft model. When using torch.nn.DistributedDataParallel, DeepSpeed or FSDP,
         the model should be the underlying model/unwrapped model (i.e. model.module).
         state_dict (`dict`, *optional*, defaults to `None`):
             The state dict of the model. If not provided, the state dict of the model
         will be used.
     """
-    config = model.lora_config_v2[adapter_name]
+    config = model.lora_config[adapter_name]
     if state_dict is None:
         state_dict = model.state_dict()
     if config.lora_type in ('lora', 'adalora'):
         # to_return = lora_state_dict(model, bias=model.peft_config.bias)
         # adapted from `https://github.com/microsoft/LoRA/blob/main/loralib/utils.py`
         # to be used directly with the state dict which is necessary when using DeepSpeed or FSDP
         bias = config.bias
@@ -72,15 +72,15 @@
     """
     Set the state dict of the Peft model.
 
     Args:
         model ([`PeftModel`]): The Peft model.
         peft_model_state_dict (`dict`): The state dict of the Peft model.
     """
-    config = model.lora_config_v2[adapter_name]
+    config = model.lora_config[adapter_name]
     state_dict = {}
     if model.modules_to_save is not None:
         for key, value in peft_model_state_dict.items():
             if any(module_name in key for module_name in model.modules_to_save):
                 for module_name in model.modules_to_save:
                     if module_name in key:
                         key = key.replace(module_name, f"{module_name}.modules_to_save.{adapter_name}")
```

## deep_training/nlp/models/prompt/prompt_model.py

```diff
@@ -13,14 +13,15 @@
 from transformers.modeling_outputs import SequenceClassifierOutput, TokenClassifierOutput
 from transformers.utils import PushToHubMixin
 
 from .configuration import PromptLearningConfig, PromptType, PromptBaseArguments, PROMPT_TYPE_TO_CONFIG_MAPPING, \
     WEIGHTS_NAME, TaskType
 from .save_and_load import get_prompt_model_state_dict, set_prompt_model_state_dict
 from .utils import _prepare_prompt_learning_config
+from ..transformer_base import TransformerBase
 from ...layers.prompt.prefix_tuning import PrefixEncoder
 from ...layers.prompt.p_tuning import PromptEncoder
 from ...layers.prompt.prompt_tuning import PromptEmbedding
 from ...layers.prompt.utils import _set_trainable, _set_adapter, \
     TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING, shift_tokens_right
 
 
@@ -72,15 +73,17 @@
         self.active_adapter = adapter_name
         self.prompt_type = prompt_config.prompt_type
         self.base_model_torch_dtype = getattr(self.get_transformer_model(), "dtype", None)
 
         self.add_adapter(adapter_name, prompt_config)
 
     def get_transformer_model(self):
-        return self.base_model if isinstance(self.base_model,PreTrainedModel) else self.base_model.model
+        #return self.base_mode.model if isinstance(self.base_mode, TransformerBase) else self.base_mode
+
+        return self.base_model if isinstance(self.base_model, PreTrainedModel) else self.base_model.model
 
     def save_pretrained(self, save_directory, **kwargs):
         r"""
         This function saves the adapter model and the adapter configuration files to a directory, so that it can be
         reloaded using the [`LoraModel.from_pretrained`] class method, and also used by the [`LoraModel.push_to_hub`]
         method.
 
@@ -196,15 +199,18 @@
             config.num_virtual_tokens * config.num_transformer_submodules
         ).long()
 
     def get_prompt_embedding_to_save(self, adapter_name):
         """
         Returns the prompt embedding to save when saving the model.
         """
-        prompt_tokens = self.prompt_tokens[adapter_name].unsqueeze(0).expand(1, -1).to(self.device)
+        prompt_encoder = self.prompt_encoder[adapter_name]
+        prompt_tokens = (
+            self.prompt_tokens[adapter_name].unsqueeze(0).expand(1, -1).to(prompt_encoder.embedding.weight.device)
+        )
         if self.prompt_config[adapter_name].prompt_type == PromptType.PREFIX_TUNING:
             prompt_tokens = prompt_tokens[:, : self.prompt_config[adapter_name].num_virtual_tokens]
         prompt_embeddings = self.prompt_encoder[adapter_name](prompt_tokens)
         return prompt_embeddings[0].detach().cpu()
 
     def get_prompt(self, batch_size):
         """
```

## deep_training/nlp/optimizer/lion/lion.py

```diff
@@ -38,15 +38,16 @@
     def __init__(
         self,
         params,
         lr: float = 1e-4,
         betas: Tuple[float, float] = (0.9, 0.99),
         weight_decay: float = 0.0,
         use_triton: bool = False,
-        triton_block_size: int = 1024
+        triton_block_size: int = 1024,
+        **kwargs
     ):
         assert lr > 0.
         assert all([0. <= beta <= 1. for beta in betas])
 
         defaults = dict(
             lr=lr,
             betas=betas,
```

## deep_training/nlp/utils/__init__.py

```diff
@@ -1,110 +1,121 @@
 # -*- coding: utf-8 -*-
 # @Time    : 2022/11/15 13:33
+import math
 import random
 import typing
-
-from torch import optim, nn
-from transformers import get_linear_schedule_with_warmup
+import torch
+from torch import optim
+from ..optimizer.optimizer import get_optimizer_cls_and_kwargs
 from ..scheduler import WarmupCosineSchedule
+from ..scheduler.scheduler import get_scheduler, SchedulerType
 from ...data_helper import TrainingArguments
-from ..optimizer.lion import Lion
-from ..optimizer.lamb import Lamb
-from torch.optim import Adam
+
 try:
-    from transformers import AdamW
+    from transformers import AdamW as AdamWHF
 except:
-    from torch.optim import AdamW
+    AdamWHF = None
 
-def configure_optimizers(named_parameter: typing.Union[typing.List,typing.Tuple],
-                         training_args: TrainingArguments,
-                         estimated_stepping_batches: int):
-
-
-    optimizer_name = training_args.optimizer.lower()
-    if optimizer_name == 'adamw':
-        optimizer = AdamW(named_parameter, lr=training_args.learning_rate,
-                          eps=training_args.adam_epsilon,
-                          betas=training_args.optimizer_betas,
-                          weight_decay=training_args.weight_decay
-                          )
-    elif optimizer_name == 'adam':
-        optimizer = Adam(named_parameter, training_args.learning_rate,
-                         eps=training_args.adam_epsilon,
-                         betas=training_args.optimizer_betas,
-                         weight_decay=training_args.weight_decay
-                         )
-    elif optimizer_name == 'lamb':
-        optimizer = Lamb(named_parameter, training_args.learning_rate,
-                         eps=training_args.adam_epsilon,
-                         betas=training_args.optimizer_betas,
-                         weight_decay=training_args.weight_decay
-                         )
-    elif optimizer_name == 'lion':
-        optimizer = Lion(named_parameter, training_args.learning_rate,
-                         betas=training_args.optimizer_betas,
-                         weight_decay=training_args.weight_decay
-                         )
-    else:
-        raise ValueError('optimizer must one of adamw,adam,lion')
 
 
-    scheduler = None
-    if training_args.scheduler_type.lower() == 'linear'.lower():
-        scheduler = get_linear_schedule_with_warmup(
-            optimizer,
-            num_warmup_steps=training_args.warmup_steps,
-            num_training_steps=estimated_stepping_batches
-            # num_training_steps=self.trainer.estimated_stepping_batches,
-        )
-    elif training_args.scheduler_type.lower() in ['CAL'.lower(),'CosineAnnealingLR'.lower()]:
-        eta_min = training_args.scheduler.get('eta_min', 0.)
-        last_epoch = training_args.scheduler.get('last_epoch', -1)
-        verbose = training_args.scheduler.get('verbose', False)
-        if training_args.scheduler.get('T_0',None) is None:
-            rewarm_epoch_num = training_args.scheduler["rewarm_epoch_num"]
-            T_0 = int(estimated_stepping_batches * rewarm_epoch_num/ training_args.max_epochs)
-            T_0 = max(T_0,1)
-        else:
-            T_0 = int(training_args.scheduler["T_0"])
-            T_0 = max(T_0, 1)
-        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_0,
-                                                         eta_min=eta_min,
-                                                         last_epoch=last_epoch,
-                                                         verbose=verbose)
-    elif training_args.scheduler_type.lower() in ['CAWR'.lower(),'CosineAnnealingWarmRestarts'.lower()]:
-        T_mult = training_args.scheduler["T_mult"]
 
-        eta_min = training_args.scheduler.get('eta_min', 0.)
-        last_epoch = training_args.scheduler.get('last_epoch', -1)
-        verbose = training_args.scheduler.get('verbose', False)
-        if training_args.scheduler.get('T_0', None) is None:
-            rewarm_epoch_num = training_args.scheduler["rewarm_epoch_num"]
-            T_0 = int(estimated_stepping_batches * rewarm_epoch_num / training_args.max_epochs)
+def configure_optimizers(optimizer_grouped_parameters: typing.Union[typing.List,typing.Tuple],
+                         args: TrainingArguments,
+                         estimated_stepping_batches: int,
+                         model_attrs = None
+                         ):
+    num_training_steps = estimated_stepping_batches
+    optimizer_name = args.optimizer.lower()
+    optimizer_cls, optimizer_kwargs = get_optimizer_cls_and_kwargs(optimizer_name, args)
+
+    optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)
+    if optimizer_cls.__name__ == "Adam8bit":
+        import bitsandbytes
+
+        manager = bitsandbytes.optim.GlobalOptimManager.get_instance()
+        for (opt_model,lr) in model_attrs:
+            skipped = 0
+            for module in opt_model.modules():
+                if isinstance(module, torch.nn.Embedding):
+                    skipped += sum({p.data_ptr(): p.numel() for p in module.parameters()}.values())
+                    # logger.info(f"skipped {module}: {skipped / 2 ** 20}M params")
+                    manager.register_module_override(module, "weight", {"optim_bits": 32})
+                    # logger.debug(f"bitsandbytes: will optimize {module} in fp32")
+            # logger.info(f"skipped: {skipped / 2 ** 20}M params")
+
+    num_warmup_steps = args.warmup_steps if args.warmup_steps >= 1 else estimated_stepping_batches * args.warmup_steps
+    num_warmup_steps = int(num_warmup_steps)
+    lr_scheduler_type = args.scheduler_type.lower()
+
+
+    scheduler = None
+    if lr_scheduler_type == 'WarmupCosine'.lower():
+        lr_scheduler_type = SchedulerType.COSINE
+    elif lr_scheduler_type == 'CosineAnnealingWarmRestarts'.lower():
+        lr_scheduler_type = SchedulerType.COSINE_WITH_RESTARTS
+    elif lr_scheduler_type in ['CAL'.lower(), 'CosineAnnealingLR'.lower()]:
+        T_mult = args.scheduler["T_mult"]
+        eta_min = args.scheduler.get('eta_min', 0.)
+        last_epoch = args.scheduler.get('last_epoch', -1)
+        verbose = args.scheduler.get('verbose', False)
+        if args.scheduler.get('T_0', None) is None:
+            rewarm_epoch_num = args.scheduler["rewarm_epoch_num"]
+            T_0 = int(estimated_stepping_batches * rewarm_epoch_num / args.max_epochs)
             T_0 = max(T_0, 1)
         else:
-            T_0 = int(training_args.scheduler["T_0"])
+            T_0 = int(args.scheduler["T_0"])
             T_0 = max(T_0, 1)
-        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0 , T_mult,
+        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult,
                                                                    eta_min=eta_min,
                                                                    last_epoch=last_epoch,
                                                                    verbose=verbose)
-    elif training_args.scheduler_type.lower() == 'Step'.lower():
-        decay_rate = training_args.scheduler["decay_rate"]
-        decay_steps = training_args.scheduler["decay_steps"]
+    elif lr_scheduler_type in ['CAWR'.lower(), 'CosineAnnealingWarmRestarts'.lower()]:
+        # T_mult = args.scheduler["T_mult"]
+        eta_min = args.scheduler.get('eta_min', 0.)
+        last_epoch = args.scheduler.get('last_epoch', -1)
+        verbose = args.scheduler.get('verbose', False)
+        if args.scheduler.get('T_0', None) is None:
+            rewarm_epoch_num = args.scheduler["rewarm_epoch_num"]
+            T_0 = int(estimated_stepping_batches * rewarm_epoch_num / args.max_epochs)
+            T_0 = max(T_0, 1)
+        else:
+            T_0 = int(args.scheduler["T_0"])
+            T_0 = max(T_0, 1)
+
+        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_0,
+                                                         eta_min=eta_min,
+                                                         last_epoch=last_epoch,
+                                                         verbose=verbose)
+    elif args.scheduler_type.lower() == 'Step'.lower():
+        decay_rate = args.scheduler["decay_rate"]
+        decay_steps = args.scheduler["decay_steps"]
         scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=decay_steps, gamma=decay_rate)
-    elif training_args.scheduler_type.lower() == 'ReduceLROnPlateau'.lower():
+    elif args.scheduler_type.lower() == 'ReduceLROnPlateau'.lower():
         scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, "min", verbose=True, patience=6)
-    elif training_args.scheduler_type.lower() == 'WarmupCosine'.lower():
-        scheduler = WarmupCosineSchedule(optimizer, warmup_steps=training_args.warmup_steps, t_total=estimated_stepping_batches)
+    elif args.scheduler_type.lower() == 'WarmupCosine'.lower():
+        scheduler = WarmupCosineSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=estimated_stepping_batches)
 
+    if scheduler is None:
+        scheduler = get_scheduler(
+            lr_scheduler_type,
+            optimizer=optimizer,
+            num_warmup_steps=num_warmup_steps,
+            num_training_steps=num_training_steps,
+        )
 
     if scheduler:
-        scheduler = {"scheduler": scheduler, "interval": "step", "frequency": 1}
-        return [optimizer], [scheduler]
+        # scheduler = {"scheduler": scheduler, "interval": "step", "frequency": 1}
+        # return [optimizer], [scheduler]
+        return {
+            "optimizer": optimizer,
+            "lr_scheduler": {
+                "scheduler": scheduler,
+                "interval": "step", "frequency": 1
+            },
+        }
     return optimizer
 
 
 
 def generate_random_str(randomlength=16):
     """
     生成一个指定长度的随机字符串
```

## Comparing `deep_training-0.1.8.dist-info/METADATA` & `deep_training-0.1.9.dist-info/METADATA`

 * *Files 24% similar despite different names*

```diff
@@ -1,24 +1,21 @@
-Metadata-Version: 2.1
-Name: deep-training
-Version: 0.1.8
-Summary: an easy training architecture
-Home-page: https://github.com/ssbuild/deep_training
-Author: ssbuild
-Author-email: 9727464@qq.com
-License: Apache License 2.0
-Platform: UNKNOWN
-Requires-Dist: lightning (>=2)
-Requires-Dist: numpy-io (<0.1.0,>=0.0.2)
-Requires-Dist: sentencepiece
-Requires-Dist: numpy
-Requires-Dist: transformers (>=4.22)
-Requires-Dist: seqmetric
-Requires-Dist: scipy
-Requires-Dist: scikit-learn
-Requires-Dist: tensorboard
-Requires-Dist: tqdm
-Requires-Dist: six
-
-torch_training: https://github.com/ssbuild/deep_training.git
-
-
+Metadata-Version: 2.1
+Name: deep-training
+Version: 0.1.9
+Summary: an easy training architecture
+Home-page: https://github.com/ssbuild/deep_training
+Author: ssbuild
+Author-email: 9727464@qq.com
+License: Apache License 2.0
+Requires-Dist: lightning (>=2)
+Requires-Dist: numpy-io (<0.1.0,>=0.0.3)
+Requires-Dist: sentencepiece
+Requires-Dist: numpy
+Requires-Dist: transformers (>=4.22)
+Requires-Dist: seqmetric
+Requires-Dist: scipy
+Requires-Dist: scikit-learn
+Requires-Dist: tensorboard
+Requires-Dist: tqdm
+Requires-Dist: six
+
+torch_training: https://github.com/ssbuild/deep_training.git
```

## Comparing `deep_training-0.1.8.dist-info/RECORD` & `deep_training-0.1.9.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,33 +1,32 @@
 deep_training/__init__.py,sha256=bhATnUT4VEzwvA8_8IwxspnDRKf32ZgEeHYCN2E5Dd4,47
-deep_training/setup.py,sha256=ARYW0MUUpofCH0z83Xa18mUohfy01K3FivgojjIAQ0Q,897
+deep_training/setup.py,sha256=MxXrXV5kEGRbn0l4J2G2OEloyhIL3PenpCqqRh0k6AU,897
 deep_training/cv/__init__.py,sha256=J-zlKxMsAfAgoO0vSAzgYJXSuMSJcJ7NKAPKeaeC3TM,55
-deep_training/data_helper/__init__.py,sha256=P8rAMalR6xNepAf-9ldGoOSsEiUtur8Px6gUpTXQhd8,195
-deep_training/data_helper/data_helper.py,sha256=wjy3ziP894WqWrwJqQb5MFZzyh6aKAbSJ3Tz8yOyp1k,18145
-deep_training/data_helper/data_module.py,sha256=0V38xPpgHJK7gGgffBRyobUMgV2MqqtHKL5y5PQzUaA,3757
-deep_training/data_helper/training_args.py,sha256=XGUXdty0SE6n8xqk6J0lySFvaYSGMVo2zuq6paFQ8sM,12121
+deep_training/data_helper/__init__.py,sha256=VUCqZdaPd53eFzuMvZj3WhJat9L7qZRht8pRlCmvsqA,155
+deep_training/data_helper/data_helper.py,sha256=zDW9rr4Ol0YxbNqT8pKLfb32wHgIrijhvCt9jGEudnc,14675
+deep_training/data_helper/training_args.py,sha256=ttZX4mV3erDsvxORSXb_xtE-0MGjQ045mb6h2t3CzaE,12572
 deep_training/nlp/__init__.py,sha256=L4_ltrwpG8mrgN1hZRKimefLHgjhRYyXVtLMFzr1grw,70
 deep_training/nlp/layers/__init__.py,sha256=zbd9GfR02_YVgsTJSXjfyIcQwj8PmG4PscMdA0p6ONI,56
 deep_training/nlp/layers/activate.py,sha256=0q7htFl9Az2fdUjrjv-QMUCE5oenYPVTLZ3lRemIKzA,241
 deep_training/nlp/layers/crf.py,sha256=JTihPuJuBBp83I9UZzVg0wogwwpdJrs0VKtuLPBSCDM,13271
 deep_training/nlp/layers/handshakingkernel.py,sha256=BRJZbEjKM347q8zEMEtJXxXjmqhegmQgqebhqMy4UkI,4653
 deep_training/nlp/layers/mask.py,sha256=8SB_Hl9X48-yuJMCPjLDabDXvgWvH4VPqUOSVDmePFs,435
 deep_training/nlp/layers/mhslayer.py,sha256=Ky6xW3hNe0x4WWPPoWa8pZkCp_-MR5VE0yKHYPzzpd0,1319
 deep_training/nlp/layers/norm.py,sha256=r_yHSnDAv8RY1nb1VS1lMWttbJVyCO376OIuu0So8c8,5911
 deep_training/nlp/layers/ppo.py,sha256=wAz2eVJaXNYyepSHUOJSnAOUkJAdoG--bZ5EiRe_R_w,1406
 deep_training/nlp/layers/prefix_encoder.py,sha256=y_Y4wWLnEyZJf33pQJFjWfl-AX1VS8Aejj3JXOdiRXQ,1220
 deep_training/nlp/layers/seq_pointer.py,sha256=KmwZclK9gqdluy2o-h7nd3zL3EZjjjw1L6dz0isCFI8,7259
 deep_training/nlp/layers/w2ner.py,sha256=fP7hlMHp1NTH6elNMJA-wBOER76VTouSSsKCinLsCyM,3550
 deep_training/nlp/layers/lora_v1/__init__.py,sha256=gmwJqLmiKqPfh5_VGWWr38y8lLgdPWw6JrjNVrvsLEY,72
-deep_training/nlp/layers/lora_v1/layers.py,sha256=JWz9RtqA-hLsTxyhZPBlz82aN_VaPjNoDYRhssKV1H0,15095
+deep_training/nlp/layers/lora_v1/layers.py,sha256=a7vdUf2eSkgrc5-Mu8SdeD8_XkJCA1-svlXRmfHfme0,15529
 deep_training/nlp/layers/lora_v1/utils.py,sha256=1ouFUmTF9IXzum97eIlrTeT6J4OAnEwIaWkZdgXMjSc,1819
 deep_training/nlp/layers/lora_v2/__init__.py,sha256=dGpWUx0v7UoVgwZY5srCDCBvt_hlI77zA6mQO3CxMaE,72
 deep_training/nlp/layers/lora_v2/adalora.py,sha256=dOqXcfqFfs4tpiwqU52bEzO1WoX9wP2itSBysw9rQ90,15465
-deep_training/nlp/layers/lora_v2/layers.py,sha256=giRQXEolkWGhU0CsNoOMOZBXrgwBbp6_FvOf9_Cjdeg,8565
-deep_training/nlp/layers/lora_v2/utils.py,sha256=Ea6gcMZ276OWS3dk89Ai4YLlMje5HtadFhw5SziYsoM,9287
+deep_training/nlp/layers/lora_v2/layers.py,sha256=nBzlLlenuy6p0tewqiWERzsGfjD_pdDMEsCyrF3rjxM,16258
+deep_training/nlp/layers/lora_v2/utils.py,sha256=7SCr8pxMjcJ4849fqlUTKY0CjCWTHYTZ559di6qahRs,9083
 deep_training/nlp/layers/prompt/__init__.py,sha256=J8P_z5Lrd5h9c6mhMG1BY8ZP4Vl5l8Tc5yukwlzg9Ag,80
 deep_training/nlp/layers/prompt/adaption_prompt.py,sha256=J9iDQOQQSE98GN1Tmd9Yjo7v0QtCA6yXpyPsRGRXe1I,15541
 deep_training/nlp/layers/prompt/p_tuning.py,sha256=LTvyxWmSrMq6nIi8v8Ohx8knkvFYqvPUSuQ_hssGOpc,5463
 deep_training/nlp/layers/prompt/prefix_tuning.py,sha256=QwgiIoEPPjKy34gH7t1dHg4sh5RRdcbbMl_iMcYH-uA,3053
 deep_training/nlp/layers/prompt/prompt_tuning.py,sha256=STLP8rRqT1XEjcts3lwgjLWhd0_NZT6YvqYSN9nsMkI,3523
 deep_training/nlp/layers/prompt/utils.py,sha256=VrpX2NhGNtxQJJNRD0OTz54l0W0JXnPdMcx1tBiTwlg,9285
 deep_training/nlp/losses/BatchAllTripletLoss.py,sha256=_2Og7Hf3Bjd1GT55UFmbZq5QLxdcKUyv4T00loPrKUo,3662
@@ -89,63 +88,64 @@
 deep_training/nlp/models/pure_model.py,sha256=LD8cYvvRirnP8iMFCyRhsNXRHpZt93Kh2WGoUZoEnFw,5149
 deep_training/nlp/models/simcse.py,sha256=ubVGkeMatDeIUqySV8Tc2TJHvaRKb4p3JOvUTOOhaRo,3949
 deep_training/nlp/models/span_ner.py,sha256=rD0TY2K-zesfRFGfDguqkSAfxHGgbQHG3K5QZ6gc7Zg,6022
 deep_training/nlp/models/spn4re.py,sha256=g_pk3bNqpH41EnzJ77oWPsKvbUwzJfaBYMux5FiEc60,14454
 deep_training/nlp/models/tplinker.py,sha256=PJ9smipeIiA1CDi8xz1gIg2DBWzO5C1B2wITItEsd1A,11383
 deep_training/nlp/models/tplinkerplus.py,sha256=hP4KD3rf2hktfQzHnI7RA2j_2cjk_0G5v6CkbLt1gvQ,8157
 deep_training/nlp/models/transformer.py,sha256=ZuywgLt3HZhsR4sJ4SyZvrYgaVJW8lCn5JH1j_IceXE,6624
-deep_training/nlp/models/transformer_base.py,sha256=X2eni4yyI0p_Ui4nhzSae22IsZYsFgcy1Rq2GqgC-nA,26596
+deep_training/nlp/models/transformer_base.py,sha256=rMOCJg7egNOItFKVDXMmVtXGXP9naKdLCtJ5C2GD96k,26743
 deep_training/nlp/models/tsdae_model.py,sha256=lb04RIGkhHhilD-vdkfb8YK9hnTck1nN79WX1Pngbbk,7968
 deep_training/nlp/models/w2ner.py,sha256=z0BortOquZSzmma355wNLz1ofLku_hMb2CjL4KDf-PM,9040
 deep_training/nlp/models/LLaMA/__init__.py,sha256=asn9Wxkl4lG12dRVgxq7Lz4BGCNDaRFL155haVMDNso,16524
 deep_training/nlp/models/LLaMA/configuration.py,sha256=HNzzhIIdR9HBN9Y4Oavv6cGgIf0ExcphwsbVkltJ2ZM,5087
 deep_training/nlp/models/LLaMA_parallel/__init__.py,sha256=4fOhbq0tQOTSH5e3X6XN3PnI6athUR8tsTCn4AUg94Q,19207
 deep_training/nlp/models/LLaMA_parallel/configuration.py,sha256=HNzzhIIdR9HBN9Y4Oavv6cGgIf0ExcphwsbVkltJ2ZM,5087
 deep_training/nlp/models/PaLM/__init__.py,sha256=P1qwWPUycRmZ6I48tov6janJUNpp4L-iMoVN54ykcQw,31627
 deep_training/nlp/models/PaLM/configuration.py,sha256=kIb3nj-2pQB2wyNrYHSZqr_ta1F0Cg-VbGEbnM5icPc,5890
 deep_training/nlp/models/chatglm/__init__.py,sha256=osBnXXXUoHwREHqXRX6WIEY0OQ0K1Yg7MKjQf04FNIQ,60510
 deep_training/nlp/models/chatglm/configuration.py,sha256=4w-Kbp_FJ2crIQVyu6kie9lbMSuE3U4nnjwjVPos2E8,4575
 deep_training/nlp/models/chatglm/quantization.py,sha256=8B6OHPv0EH3sTRbKrYR5MCM0PmYIlLvAMPzNRkCKAjM,15169
 deep_training/nlp/models/chatglm/tokenization.py,sha256=IMyHa8uPOgE0ia1DYp8Lx-IZ0N4TKSh1HVlA5sDdw-s,17037
 deep_training/nlp/models/laMDA/__init__.py,sha256=fvxTQQ8jfU-msPRdC8KsGlCwzM6u8-WBmayu6gE-s0E,34123
 deep_training/nlp/models/laMDA/configuration.py,sha256=8ZvPEl1C1KUGYWw7a8XcgIgl3gWH9WXa_-ZNDqz34PE,5981
-deep_training/nlp/models/lora/__init__.py,sha256=YyYmxdwz0IdyE4QXMPWhjUNmvOccqO8kJzPblfPwGJI,181
+deep_training/nlp/models/lora/__init__.py,sha256=booP9mgMqsVCVN_vTtL9ktArSL-SdEFnyp-AH4OpqN0,182
 deep_training/nlp/models/lora/v1/__init__.py,sha256=zwGdNKqudVj7c8sMWbmZ9CnnncWXuEapAucWY-VEhLs,123
 deep_training/nlp/models/lora/v1/configuration.py,sha256=0Fw6hHmu0j1DwW-CdrAcT2InCyDCMHTa4SvVFJYizY8,7095
-deep_training/nlp/models/lora/v1/lora_wrapper.py,sha256=c7X2raQW6tEN1stIIBJxoHA87mHEsDJZjcPDl_8D9g0,13688
-deep_training/nlp/models/lora/v2/__init__.py,sha256=2XorjeFlyNuH6xTXiyNO1A8A3P5acBApOjxVv3YEon0,206
-deep_training/nlp/models/lora/v2/adalora_model.py,sha256=iKfKWnW--iY2gmXkMcBv6QWJr9vu-uSll57r1UNvRrY,13112
+deep_training/nlp/models/lora/v1/lora_wrapper.py,sha256=QgRnXm_sOdt3tRqVPDBDANz8mTHXJKoVrnS4bmhRA-M,13842
+deep_training/nlp/models/lora/v2/__init__.py,sha256=kLHwm8lgJQk3uhA9mm43lGPRHWYptGGQBEvCIt2qB_Q,217
+deep_training/nlp/models/lora/v2/adalora_model.py,sha256=SSdIiyWcQicsHTMjsPBDpMavsmdcMMOA3JPUD-m5CnI,14096
 deep_training/nlp/models/lora/v2/configuration.py,sha256=ttAveFarHqtPI3XvPcCpAD7OG1KJDNw33XjLuC8T6TQ,11325
-deep_training/nlp/models/lora/v2/lora_model.py,sha256=5k09kzj8bSvU-CQm5GJWtg5isXUEUYPmvKuxdLPc_V4,11745
-deep_training/nlp/models/lora/v2/lora_wrapper.py,sha256=v-MxTGDvAGDTIr3wG3xWarj9cGIP0n2EE0wMz3_Cu-4,10868
-deep_training/nlp/models/lora/v2/save_and_load.py,sha256=U7_ZaPm8gpg8gQhZei6UG5KvsJXDtSNZfZk1gWo6nWc,4889
+deep_training/nlp/models/lora/v2/lora_model.py,sha256=mSIwPtRS5sjShiDT_KjAK9xF3xbTfGIf7TuRatam8Ho,16383
+deep_training/nlp/models/lora/v2/lora_wrapper.py,sha256=TGfwE0fevvaShqDwCbxbPIpSy8KaB2ULRwcGKRZXf2o,11453
+deep_training/nlp/models/lora/v2/save_and_load.py,sha256=E_rbEoudrBAWMxzk9qM2FBeAG9jYZsFkvWqq3RvV9TA,4883
 deep_training/nlp/models/moss/__init__.py,sha256=_dQslDggRX8ZsR6RPTUuBzAHotQt8MPAqZ1-nGULPns,467
 deep_training/nlp/models/moss/configuration_moss.py,sha256=Qqp7anpWGnsotkqd5UOfc9e5zhxgx7j5xetSQUOmhaQ,5097
 deep_training/nlp/models/moss/custom_autotune.py,sha256=O-C9w-hZkcrUgDfK4B1iPtKjHQAZwELNefYhlLABHyc,6735
 deep_training/nlp/models/moss/modeling_moss.py,sha256=Trm6zkUFQo9SkgInIuusSFekRvyLa6x2W6WyRE9CswI,31079
 deep_training/nlp/models/moss/quantization.py,sha256=CwkgE9Qkb8HW9xxQUL_B8k-mV2lHrqJ7eS22pGumpB8,18773
 deep_training/nlp/models/moss/tokenization_moss.py,sha256=Ft7hwLBfYoAqn33anM0sbkvU7GuXJQW8NJ1Ddko_1hk,15939
 deep_training/nlp/models/prompt/__init__.py,sha256=D8B65xX2WyGoV4PBPmc1NujefRcgOz1DUq9zcYbBE2g,203
 deep_training/nlp/models/prompt/configuration.py,sha256=D5zvsx_OKEvN7cPQqCjGobY0t9gU30e87sgcxaXlD3c,12103
-deep_training/nlp/models/prompt/prompt_model.py,sha256=INAmygOdrmrxpvS4jeRsrm3E1OAjNdQ3lQIaM9wDBHw,52598
+deep_training/nlp/models/prompt/prompt_model.py,sha256=_I2jHCCQgKwKyDsA8oDJNydlWla4-mO31F9_oxfLV-Q,52867
 deep_training/nlp/models/prompt/save_and_load.py,sha256=kB0ha-h1FlyQfgmaczg7jYktaFEBYzRKTnoKJcXdKMU,3560
 deep_training/nlp/models/prompt/utils.py,sha256=nvNO26eHmgIuY2WrfX3IyyH6jDwEJmQv7ieZqUA-n-0,1917
 deep_training/nlp/models/rl/__init__.py,sha256=pg2jplYDS8gj_w4iUzDgKNFpklxdtOfw4xcTyjA-3xU,54
 deep_training/nlp/models/rl/modeling.py,sha256=3B_v9D-fwpkXH_5v3mBgE0P7MXEXvP-DT7ZlD9qBVis,272
 deep_training/nlp/models/rl/modeling_ilql.py,sha256=xBbwzzeo9ek3_XhUP387gObzdBxxYUVCyu5-WKzGreM,21581
 deep_training/nlp/models/rl/modeling_ppo.py,sha256=uvlUzg_6m_qELTYNXrOiDE3tXb7dEEfKVP1ABL9HLYg,42065
 deep_training/nlp/models/rl/utils.py,sha256=BgocGot9OAvgWug8Dd6DIg2ipnSva1vX9F_WuJpBqRA,8163
 deep_training/nlp/models/splinker/__init__.py,sha256=QtgnpJa78vAq9bzfjN67NmHU3dXU6WH84jeyZoD1sBs,102
 deep_training/nlp/models/splinker/splinker.py,sha256=AhIWyfUtNOLqwZn520J-mv8LJwIoDZpo8yNoc4V5Gss,2866
 deep_training/nlp/models/t5decoder/__init__.py,sha256=R9Op4Ysli9isootQQ2FcjhpbG13fNESlmUROu6cfGH0,14478
 deep_training/nlp/models/t5encoder/__init__.py,sha256=692ChfLf2sZWgzhBM37g1PdpmEmsU1R9RRl_uTHRET0,6646
 deep_training/nlp/optimizer/__init__.py,sha256=c4cmx9ebIdqwXBu3N9QbcNNHb32t2MV6fTK9aC2VBGQ,56
 deep_training/nlp/optimizer/lamb.py,sha256=htvZQHPWHG5GCDgo9xCaZikWwRyaD2PjDioIQvX7qXw,5225
+deep_training/nlp/optimizer/optimizer.py,sha256=frm7IfaQAYkBpEBEclbRX485f22M0a1THPGzJqD9ZOs,6549
 deep_training/nlp/optimizer/lion/__init__.py,sha256=AvYkLp7sOpRIC3a5ejuniUUKyQmmBA1TPJdt2RA7Nqg,99
-deep_training/nlp/optimizer/lion/lion.py,sha256=D_b3Z8cKI3d2cpEKeFiV5YCCZzC042mZqbjZAf-fY3o,2516
+deep_training/nlp/optimizer/lion/lion.py,sha256=Dv7yYvYxBiez0HZ6ZXVlhfI1VV7q4Fguo0j8wbWYoM0,2535
 deep_training/nlp/optimizer/lion/triton.py,sha256=W7aZkc5SSgsiyrQFazpeqEkc_UyW0g-EZiALBpT5a8k,2499
 deep_training/nlp/rl/__init__.py,sha256=lXJsb8d-9R7DshCEdcx3iPyndlf1t5FNXiJsh1SUr0s,79
 deep_training/nlp/rl/ilql/__init__.py,sha256=tW9NHjvG7VvDbFBU9pVD1xDFONGu8-RJkKfx1lK1BIQ,88
 deep_training/nlp/rl/ilql/configuration.py,sha256=FFGppxPxHDktvMxVPl-_a9InRmVB8PqqRRKRnc1e5xc,2232
 deep_training/nlp/rl/ilql/data_define.py,sha256=B-yDx2t1gs2947Vn-g7lv6evUyQwwbtjlqsw9lYQ0No,3691
 deep_training/nlp/rl/ilql/ilql_dataset.py,sha256=lBWBv5-OwalK-qmTbS_LQoHMP3CFuvdc-nsh0sSATqQ,7800
 deep_training/nlp/rl/ilql/ilql_module.py,sha256=ZeAKFmlLKbHHF045JyiPFqYv7DKu8Yoqn3bTSdBdDLE,6244
@@ -158,28 +158,25 @@
 deep_training/nlp/rl/ppo/ppo_trainer.py,sha256=SO5nUGHLJLoITzNQOHv-wRIX1Zq0e5zKV3wc9-udN1I,47611
 deep_training/nlp/rl/rl_base/__init__.py,sha256=6pBQ9y-xnuMFThlwlzpT1oCVLZJG0rDUvWvFwu0ox3Y,88
 deep_training/nlp/rl/rl_base/rl_dataset.py,sha256=dSFnBt8u1SddRYX3DThJx2tRxISpd4xQSlJFQ80YPlA,3724
 deep_training/nlp/rl/utils/__init__.py,sha256=RzPjehgDE4v_PEokLSdhIxD2yObNtjXx8i_ZdAvfjUY,10700
 deep_training/nlp/rl/utils/configuration.py,sha256=X-ACQ-UJ5nL24ivY0yyKfnWCxU0Qng_GTZzjHuzK7GU,3529
 deep_training/nlp/rl/utils/logging.py,sha256=mF0eKsv9BqBNymZt_wnkMP04AF8N9B_Lhg5Wpb9iRmc,9844
 deep_training/nlp/scheduler/__init__.py,sha256=-zaiinwJzOBWypkNodSZO12kqbswVsPy5JCsYpvLbbY,2868
-deep_training/nlp/utils/__init__.py,sha256=4KXIbHGRwYv5iWH1ZjZ6vjY4fWDtCWurQVki2_baOIU,7393
+deep_training/nlp/scheduler/scheduler.py,sha256=aMwlQBaFAkiqGjRHkceE2Yz7E6eLQfM-vfi7uvZJ9f8,3725
+deep_training/nlp/utils/__init__.py,sha256=Gc5x8lkOREeJlQGqdDrsXpdbhwDkFFahl1YdeEDx2DM,7500
 deep_training/nlp/utils/adversarial.py,sha256=FNZlg8mV23YXRu7aDcu1JZBUGBV01hi_bwRzfFyzEzM,6323
 deep_training/nlp/utils/nlputils.py,sha256=KEmFliU1IqJHy3INNDvOriEMlBkP8GNwe8Y8_c_imZQ,15256
 deep_training/nlp/utils/spearman.py,sha256=tOpaah5bt_65ferL_uI6FMfKvNexi7CQztSYLj-k3yo,795
-deep_training/tfnlp/__init__.py,sha256=TRm9uMMO340jiD1ZGdDhtoxQ5BxI-au-RnOwAV13mbM,53
-deep_training/tfnlp/layers/__init__.py,sha256=TRm9uMMO340jiD1ZGdDhtoxQ5BxI-au-RnOwAV13mbM,53
-deep_training/tfnlp/losses/__init__.py,sha256=TRm9uMMO340jiD1ZGdDhtoxQ5BxI-au-RnOwAV13mbM,53
-deep_training/tfnlp/metrics/__init__.py,sha256=69flKnae4cQQyWUDwuYE0w0iaPonvH0P_WjBd_t-IqU,53
-deep_training/tfnlp/models/__init__.py,sha256=69flKnae4cQQyWUDwuYE0w0iaPonvH0P_WjBd_t-IqU,53
-deep_training/tfnlp/optimizer/__init__.py,sha256=69flKnae4cQQyWUDwuYE0w0iaPonvH0P_WjBd_t-IqU,53
-deep_training/tfnlp/scheduler/__init__.py,sha256=69flKnae4cQQyWUDwuYE0w0iaPonvH0P_WjBd_t-IqU,53
-deep_training/tfnlp/utils/__init__.py,sha256=kAmlOWNSpQCHbtT-mAsKGQzQFoWKp2jQf3neCJ0cCRY,53
+deep_training/trainer/__init__.py,sha256=1C0ve6rdJmwneNfUBd0h3VuT2sx2Q0BR41auTsNt5e8,80
+deep_training/trainer/hf/__init__.py,sha256=1C0ve6rdJmwneNfUBd0h3VuT2sx2Q0BR41auTsNt5e8,80
+deep_training/trainer/hf/trainer.py,sha256=Awm8XHw0tLXLI4ffH7DSdORVmMuu7x8wmv-FAyJhuf0,193777
+deep_training/trainer/hf/trainer_seq2seq.py,sha256=awtYeOXL-w3zXp7l1dXJEoxlvtsyXuUxPWWirZcxfTA,15669
 deep_training/utils/__init__.py,sha256=JFm7m_LPsS9Oavyxn9rbWqllCmV_zBho19rISlHNX4c,55
 deep_training/utils/distributed.py,sha256=-dhvJ6YHpRxvtZ1_on50IE33fUFW3zKXBKqqK-L1HGM,1941
 deep_training/utils/func.py,sha256=1p8hiQDCyk_gQGKrF7y6Dt66k3jLXSAt2IQeJuHQEl8,1724
 deep_training/utils/maskedlm.py,sha256=o8EB2BbDdh7wdgqz9Oi6SsVr1uBWxV15qfTk2VPjWsU,5117
 deep_training/utils/trainer.py,sha256=F1usofzi1lBVHeieDJ7WWdfd1d0Q7tftktwdJgczlg8,14500
-deep_training-0.1.8.dist-info/METADATA,sha256=W7h-fsmr0Ni3cWIwwEY2AZF36DQpcjOpAzQF9zB0Cm0,602
-deep_training-0.1.8.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-deep_training-0.1.8.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
-deep_training-0.1.8.dist-info/RECORD,,
+deep_training-0.1.9.dist-info/METADATA,sha256=5wxv6f1WRKNrObHCLzbNsNLhqNrkO1zUIj0qUB-rZUQ,603
+deep_training-0.1.9.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+deep_training-0.1.9.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
+deep_training-0.1.9.dist-info/RECORD,,
```

