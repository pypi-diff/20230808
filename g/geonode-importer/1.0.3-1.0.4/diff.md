# Comparing `tmp/geonode_importer-1.0.3-py3-none-any.whl.zip` & `tmp/geonode_importer-1.0.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,79 +1,79 @@
-Zip file size: 89258 bytes, number of entries: 77
--rw-r--r--  2.0 unx     1132 b- defN 23-May-30 11:50 importer/__init__.py
--rw-r--r--  2.0 unx      891 b- defN 23-May-16 14:04 importer/apps.py
--rw-r--r--  2.0 unx      301 b- defN 23-May-16 14:04 importer/celery_app.py
--rw-r--r--  2.0 unx    25673 b- defN 23-May-16 14:04 importer/celery_tasks.py
--rw-r--r--  2.0 unx     1142 b- defN 23-May-16 14:04 importer/datastore.py
--rw-r--r--  2.0 unx     2137 b- defN 23-May-16 14:04 importer/db_router.py
--rw-r--r--  2.0 unx     1533 b- defN 23-May-16 14:04 importer/models.py
--rw-r--r--  2.0 unx    13953 b- defN 23-May-16 14:04 importer/orchestrator.py
--rw-r--r--  2.0 unx     3689 b- defN 23-May-16 14:04 importer/publisher.py
--rw-r--r--  2.0 unx      399 b- defN 23-May-16 14:04 importer/settings.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/urls.py
--rw-r--r--  2.0 unx     1958 b- defN 23-May-16 14:04 importer/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/views.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/api/__init__.py
--rw-r--r--  2.0 unx     1675 b- defN 23-May-16 14:04 importer/api/exception.py
--rw-r--r--  2.0 unx      921 b- defN 23-May-16 14:04 importer/api/serializer.py
--rw-r--r--  2.0 unx     6264 b- defN 23-May-16 14:04 importer/api/tests.py
--rw-r--r--  2.0 unx      479 b- defN 23-May-16 14:04 importer/api/urls.py
--rw-r--r--  2.0 unx    10650 b- defN 23-May-16 14:04 importer/api/views.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/handlers/__init__.py
--rw-r--r--  2.0 unx     2627 b- defN 23-May-16 14:04 importer/handlers/apps.py
--rw-r--r--  2.0 unx     5126 b- defN 23-May-16 14:04 importer/handlers/base.py
--rw-r--r--  2.0 unx     2786 b- defN 23-May-16 14:04 importer/handlers/tests.py
--rw-r--r--  2.0 unx     5291 b- defN 23-May-16 14:04 importer/handlers/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/handlers/common/__init__.py
--rw-r--r--  2.0 unx    20407 b- defN 23-May-30 11:50 importer/handlers/common/raster.py
--rw-r--r--  2.0 unx     3184 b- defN 23-May-16 14:04 importer/handlers/common/tests_raster.py
--rw-r--r--  2.0 unx    10000 b- defN 23-May-16 14:04 importer/handlers/common/tests_vector.py
--rw-r--r--  2.0 unx    34026 b- defN 23-May-17 09:15 importer/handlers/common/vector.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/handlers/csv/__init__.py
--rw-r--r--  2.0 unx      284 b- defN 23-May-16 14:04 importer/handlers/csv/exceptions.py
--rw-r--r--  2.0 unx     8993 b- defN 23-May-16 14:04 importer/handlers/csv/handler.py
--rw-r--r--  2.0 unx     7043 b- defN 23-May-16 14:04 importer/handlers/csv/tests.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/handlers/geojson/__init__.py
--rw-r--r--  2.0 unx      296 b- defN 23-May-16 14:04 importer/handlers/geojson/exceptions.py
--rw-r--r--  2.0 unx     3555 b- defN 23-May-16 14:04 importer/handlers/geojson/handler.py
--rw-r--r--  2.0 unx     5430 b- defN 23-May-16 14:04 importer/handlers/geojson/tests.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/handlers/geotiff/__init__.py
--rw-r--r--  2.0 unx      290 b- defN 23-May-16 14:04 importer/handlers/geotiff/exceptions.py
--rw-r--r--  2.0 unx     2728 b- defN 23-May-25 08:30 importer/handlers/geotiff/handler.py
--rw-r--r--  2.0 unx     4136 b- defN 23-May-16 14:04 importer/handlers/geotiff/tests.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/handlers/gpkg/__init__.py
--rw-r--r--  2.0 unx      299 b- defN 23-May-16 14:04 importer/handlers/gpkg/exceptions.py
--rw-r--r--  2.0 unx     4699 b- defN 23-May-16 14:04 importer/handlers/gpkg/handler.py
--rw-r--r--  2.0 unx      566 b- defN 23-May-16 14:04 importer/handlers/gpkg/tasks.py
--rw-r--r--  2.0 unx     5606 b- defN 23-May-16 14:04 importer/handlers/gpkg/tests.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/handlers/kml/__init__.py
--rw-r--r--  2.0 unx      284 b- defN 23-May-16 14:04 importer/handlers/kml/exceptions.py
--rw-r--r--  2.0 unx     4508 b- defN 23-May-16 14:04 importer/handlers/kml/handler.py
--rw-r--r--  2.0 unx     4273 b- defN 23-May-16 14:04 importer/handlers/kml/tests.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/handlers/shapefile/__init__.py
--rw-r--r--  2.0 unx      302 b- defN 23-May-16 14:04 importer/handlers/shapefile/exceptions.py
--rw-r--r--  2.0 unx     5562 b- defN 23-May-16 14:04 importer/handlers/shapefile/handler.py
--rw-r--r--  2.0 unx     1039 b- defN 23-May-16 14:04 importer/handlers/shapefile/serializer.py
--rw-r--r--  2.0 unx     5650 b- defN 23-May-16 14:04 importer/handlers/shapefile/tests.py
--rw-r--r--  2.0 unx     1031 b- defN 23-May-16 14:04 importer/migrations/0001_initial.py
--rw-r--r--  2.0 unx      455 b- defN 23-May-16 14:04 importer/migrations/0002_resourcehandlerinfo_kwargs.py
--rw-r--r--  2.0 unx      595 b- defN 23-May-16 14:04 importer/migrations/0003_resourcehandlerinfo_execution_id.py
--rw-r--r--  2.0 unx      409 b- defN 23-May-16 14:04 importer/migrations/0004_rename_execution_id_resourcehandlerinfo_execution_request.py
--rw-r--r--  2.0 unx     1106 b- defN 23-May-16 14:04 importer/migrations/0005_fixup_dynamic_shema_table_names.py
--rw-r--r--  2.0 unx     1584 b- defN 23-May-30 11:50 importer/migrations/0006_dataset_migration.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/migrations/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/tests/__init__.py
--rw-r--r--  2.0 unx     1815 b- defN 23-May-16 14:04 importer/tests/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/tests/end2end/__init__.py
--rw-r--r--  2.0 unx     6095 b- defN 23-May-16 14:04 importer/tests/end2end/test_end2end.py
--rw-r--r--  2.0 unx     7707 b- defN 23-May-16 14:04 importer/tests/end2end/test_end2end_copy.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 14:04 importer/tests/unit/__init__.py
--rw-r--r--  2.0 unx     1234 b- defN 23-May-16 14:04 importer/tests/unit/test_models.py
--rw-r--r--  2.0 unx    13739 b- defN 23-May-16 14:04 importer/tests/unit/test_orchestrator.py
--rw-r--r--  2.0 unx     3606 b- defN 23-May-16 14:04 importer/tests/unit/test_publisher.py
--rw-r--r--  2.0 unx    23904 b- defN 23-May-16 14:04 importer/tests/unit/test_task.py
--rw-r--r--  2.0 unx     1069 b- defN 23-May-30 11:51 geonode_importer-1.0.3.dist-info/LICENSE
--rw-r--r--  2.0 unx      712 b- defN 23-May-30 11:51 geonode_importer-1.0.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-30 11:51 geonode_importer-1.0.3.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 23-May-30 11:51 geonode_importer-1.0.3.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6809 b- defN 23-May-30 11:51 geonode_importer-1.0.3.dist-info/RECORD
-77 files, 293758 bytes uncompressed, 78320 bytes compressed:  73.3%
+Zip file size: 89920 bytes, number of entries: 77
+-rw-r--r--  2.0 unx     1132 b- defN 23-Aug-08 11:21 importer/__init__.py
+-rw-r--r--  2.0 unx      891 b- defN 23-Jan-27 09:05 importer/apps.py
+-rw-r--r--  2.0 unx      301 b- defN 22-Oct-06 08:47 importer/celery_app.py
+-rw-r--r--  2.0 unx    25927 b- defN 23-Jul-06 14:04 importer/celery_tasks.py
+-rw-r--r--  2.0 unx     1137 b- defN 23-Jul-06 14:04 importer/datastore.py
+-rw-r--r--  2.0 unx     2137 b- defN 22-Oct-06 08:47 importer/db_router.py
+-rw-r--r--  2.0 unx     1575 b- defN 23-Jul-06 14:04 importer/models.py
+-rw-r--r--  2.0 unx    14078 b- defN 23-Jul-06 14:04 importer/orchestrator.py
+-rw-r--r--  2.0 unx     3718 b- defN 23-Jul-06 14:04 importer/publisher.py
+-rw-r--r--  2.0 unx      399 b- defN 22-Oct-06 08:47 importer/settings.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Oct-06 08:47 importer/urls.py
+-rw-r--r--  2.0 unx     1993 b- defN 23-Jul-06 14:04 importer/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Oct-06 08:47 importer/views.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Oct-06 08:47 importer/api/__init__.py
+-rw-r--r--  2.0 unx     1675 b- defN 22-Oct-06 08:47 importer/api/exception.py
+-rw-r--r--  2.0 unx      920 b- defN 23-Jul-06 14:04 importer/api/serializer.py
+-rw-r--r--  2.0 unx     6262 b- defN 23-Jul-06 14:04 importer/api/tests.py
+-rw-r--r--  2.0 unx      479 b- defN 22-Oct-06 08:47 importer/api/urls.py
+-rw-r--r--  2.0 unx    10648 b- defN 23-Jul-06 14:04 importer/api/views.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Oct-06 08:47 importer/handlers/__init__.py
+-rw-r--r--  2.0 unx     2693 b- defN 23-Jul-06 14:04 importer/handlers/apps.py
+-rw-r--r--  2.0 unx     5823 b- defN 23-Aug-04 15:03 importer/handlers/base.py
+-rw-r--r--  2.0 unx     2786 b- defN 22-Oct-06 08:47 importer/handlers/tests.py
+-rw-r--r--  2.0 unx     5275 b- defN 23-Jul-06 14:04 importer/handlers/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Oct-06 08:47 importer/handlers/common/__init__.py
+-rw-r--r--  2.0 unx    21447 b- defN 23-Jul-06 14:04 importer/handlers/common/raster.py
+-rw-r--r--  2.0 unx     3162 b- defN 23-Jul-06 14:04 importer/handlers/common/tests_raster.py
+-rw-r--r--  2.0 unx    10076 b- defN 23-Jul-06 14:04 importer/handlers/common/tests_vector.py
+-rw-r--r--  2.0 unx    36000 b- defN 23-Jul-06 14:04 importer/handlers/common/vector.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-08 09:31 importer/handlers/csv/__init__.py
+-rw-r--r--  2.0 unx      284 b- defN 23-Mar-08 09:31 importer/handlers/csv/exceptions.py
+-rw-r--r--  2.0 unx     9585 b- defN 23-Jul-06 14:04 importer/handlers/csv/handler.py
+-rw-r--r--  2.0 unx     7042 b- defN 23-Jul-06 14:04 importer/handlers/csv/tests.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Oct-06 08:47 importer/handlers/geojson/__init__.py
+-rw-r--r--  2.0 unx      296 b- defN 22-Oct-06 08:47 importer/handlers/geojson/exceptions.py
+-rw-r--r--  2.0 unx     3553 b- defN 23-Jul-06 14:04 importer/handlers/geojson/handler.py
+-rw-r--r--  2.0 unx     5483 b- defN 23-Jul-06 14:04 importer/handlers/geojson/tests.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Oct-18 10:08 importer/handlers/geotiff/__init__.py
+-rw-r--r--  2.0 unx      290 b- defN 22-Oct-18 10:08 importer/handlers/geotiff/exceptions.py
+-rw-r--r--  2.0 unx     2729 b- defN 23-Jul-06 14:04 importer/handlers/geotiff/handler.py
+-rw-r--r--  2.0 unx     4105 b- defN 23-Jul-06 14:04 importer/handlers/geotiff/tests.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Oct-06 08:47 importer/handlers/gpkg/__init__.py
+-rw-r--r--  2.0 unx      299 b- defN 22-Oct-06 08:47 importer/handlers/gpkg/exceptions.py
+-rw-r--r--  2.0 unx     4994 b- defN 23-Jul-06 14:04 importer/handlers/gpkg/handler.py
+-rw-r--r--  2.0 unx      565 b- defN 23-Jul-06 14:04 importer/handlers/gpkg/tasks.py
+-rw-r--r--  2.0 unx     5605 b- defN 23-Jul-06 14:04 importer/handlers/gpkg/tests.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Oct-06 08:47 importer/handlers/kml/__init__.py
+-rw-r--r--  2.0 unx      284 b- defN 22-Oct-06 08:47 importer/handlers/kml/exceptions.py
+-rw-r--r--  2.0 unx     4566 b- defN 23-Jul-06 14:04 importer/handlers/kml/handler.py
+-rw-r--r--  2.0 unx     4251 b- defN 23-Jul-06 14:04 importer/handlers/kml/tests.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Oct-06 08:47 importer/handlers/shapefile/__init__.py
+-rw-r--r--  2.0 unx      302 b- defN 22-Oct-06 08:47 importer/handlers/shapefile/exceptions.py
+-rw-r--r--  2.0 unx     5770 b- defN 23-Jul-06 14:04 importer/handlers/shapefile/handler.py
+-rw-r--r--  2.0 unx     1111 b- defN 23-Jul-06 14:04 importer/handlers/shapefile/serializer.py
+-rw-r--r--  2.0 unx     5755 b- defN 23-Jul-06 14:04 importer/handlers/shapefile/tests.py
+-rw-r--r--  2.0 unx     1030 b- defN 23-Jul-06 14:04 importer/migrations/0001_initial.py
+-rw-r--r--  2.0 unx      501 b- defN 23-Jul-06 14:04 importer/migrations/0002_resourcehandlerinfo_kwargs.py
+-rw-r--r--  2.0 unx      673 b- defN 23-Jul-06 14:04 importer/migrations/0003_resourcehandlerinfo_execution_id.py
+-rw-r--r--  2.0 unx      408 b- defN 23-Jul-06 14:04 importer/migrations/0004_rename_execution_id_resourcehandlerinfo_execution_request.py
+-rw-r--r--  2.0 unx     1097 b- defN 23-Jul-06 14:04 importer/migrations/0005_fixup_dynamic_shema_table_names.py
+-rw-r--r--  2.0 unx     1571 b- defN 23-Jul-06 14:04 importer/migrations/0006_dataset_migration.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Oct-06 08:47 importer/migrations/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Oct-06 08:47 importer/tests/__init__.py
+-rw-r--r--  2.0 unx     1814 b- defN 23-Jul-06 14:04 importer/tests/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Oct-06 08:47 importer/tests/end2end/__init__.py
+-rw-r--r--  2.0 unx     6313 b- defN 23-Jul-06 14:04 importer/tests/end2end/test_end2end.py
+-rw-r--r--  2.0 unx     7742 b- defN 23-Jul-06 14:04 importer/tests/end2end/test_end2end_copy.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Oct-06 08:47 importer/tests/unit/__init__.py
+-rw-r--r--  2.0 unx     1269 b- defN 23-Jul-06 14:04 importer/tests/unit/test_models.py
+-rw-r--r--  2.0 unx    13739 b- defN 23-Jan-27 09:05 importer/tests/unit/test_orchestrator.py
+-rw-r--r--  2.0 unx     3606 b- defN 22-Oct-06 08:47 importer/tests/unit/test_publisher.py
+-rw-r--r--  2.0 unx    24454 b- defN 23-Jul-06 14:04 importer/tests/unit/test_task.py
+-rw-r--r--  2.0 unx     1069 b- defN 23-Aug-08 12:55 geonode_importer-1.0.4.dist-info/LICENSE
+-rw-r--r--  2.0 unx      712 b- defN 23-Aug-08 12:55 geonode_importer-1.0.4.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Aug-08 12:55 geonode_importer-1.0.4.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 23-Aug-08 12:55 geonode_importer-1.0.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6809 b- defN 23-Aug-08 12:55 geonode_importer-1.0.4.dist-info/RECORD
+77 files, 300311 bytes uncompressed, 78982 bytes compressed:  73.7%
```

## zipnote {}

```diff
@@ -210,23 +210,23 @@
 
 Filename: importer/tests/unit/test_publisher.py
 Comment: 
 
 Filename: importer/tests/unit/test_task.py
 Comment: 
 
-Filename: geonode_importer-1.0.3.dist-info/LICENSE
+Filename: geonode_importer-1.0.4.dist-info/LICENSE
 Comment: 
 
-Filename: geonode_importer-1.0.3.dist-info/METADATA
+Filename: geonode_importer-1.0.4.dist-info/METADATA
 Comment: 
 
-Filename: geonode_importer-1.0.3.dist-info/WHEEL
+Filename: geonode_importer-1.0.4.dist-info/WHEEL
 Comment: 
 
-Filename: geonode_importer-1.0.3.dist-info/top_level.txt
+Filename: geonode_importer-1.0.4.dist-info/top_level.txt
 Comment: 
 
-Filename: geonode_importer-1.0.3.dist-info/RECORD
+Filename: geonode_importer-1.0.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## importer/__init__.py

```diff
@@ -16,13 +16,13 @@
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 #########################################################################
 import os
 
 project_dir = os.path.dirname(os.path.abspath(__file__))
 
-VERSION = (1, 0, 3)
+VERSION = (1, 0, 4)
 __version__ = ".".join([str(i) for i in VERSION])
 __author__ = "geosolutions-it"
 __email__ = "info@geosolutionsgroup.com"
 __url__ = "https://github.com/GeoNode/geonode-importer"
 default_app_config = "importer.apps.ImporterConfig"
```

## importer/celery_tasks.py

```diff
@@ -17,15 +17,20 @@
     PublishResourceException,
     ResourceCreationException,
     StartImportException,
 )
 from importer.celery_app import importer_app
 from importer.datastore import DataStoreManager
 from importer.handlers.gpkg.tasks import SingleMessageErrorHandler
-from importer.handlers.utils import create_alternate, drop_dynamic_model_schema, evaluate_error, get_uuid
+from importer.handlers.utils import (
+    create_alternate,
+    drop_dynamic_model_schema,
+    evaluate_error,
+    get_uuid,
+)
 from importer.orchestrator import orchestrator
 from importer.publisher import DataPublisher
 from importer.settings import (
     IMPORTER_GLOBAL_RATE_LIMIT,
     IMPORTER_PUBLISHING_RATE_LIMIT,
     IMPORTER_RESOURCE_CREATION_RATE_LIMIT,
 )
@@ -35,14 +40,15 @@
 
 
 class ErrorBaseTaskClass(Task):
     """
     Basic Error task class. Is common to all the base tasks of the import pahse
     it defines a on_failure method which set the task as "failed" with some extra information
     """
+
     max_retries = 3
     track_started = True
 
     def on_failure(self, exc, task_id, args, kwargs, einfo):
         # exc (Exception) - The exception raised by the task.
         # args (Tuple) - Original arguments for the task that failed.
         # kwargs (Dict) - Original keyword arguments for the task that failed.
@@ -65,15 +71,14 @@
     handler=None,
     step="start_import",
     layer_name=None,
     alternate=None,
     action=exa.IMPORT.value,
     **kwargs,
 ):
-
     """
     Base task. Is the task responsible to call the orchestrator and redirect the upload to the next step
     mainly is a wrapper for the Orchestrator object.
 
             Parameters:
                     user (UserModel): user that is performing the request
                     execution_id (UUID): unique ID used to keep track of the execution request
@@ -98,15 +103,15 @@
 
     except Exception as e:
         raise StartImportException(detail=error_handler(e, execution_id))
 
 
 @importer_app.task(
     bind=True,
-    #base=ErrorBaseTaskClass,
+    # base=ErrorBaseTaskClass,
     name="importer.import_resource",
     queue="importer.import_resource",
     max_retries=1,
     rate_limit=IMPORTER_GLOBAL_RATE_LIMIT,
     ignore_result=False,
     task_track_started=True,
 )
@@ -158,15 +163,15 @@
         call_rollback_function(
             execution_id,
             handlers_module_path=handler_module_path,
             prev_action=exa.IMPORT.value,
             layer=None,
             alternate=None,
             error=e,
-            **kwargs      
+            **kwargs,
         )
         raise InvalidInputFileException(detail=error_handler(e, execution_id))
 
 
 @importer_app.task(
     bind=True,
     base=ErrorBaseTaskClass,
@@ -221,27 +226,31 @@
 
         # extracting the crs and the resource name, are needed for publish the resource
         data = _publisher.extract_resource_to_publish(
             _files, action, layer_name, alternate, **kwargs
         )
         if data:
             # we should not publish resource without a crs
-            if not _overwrite or (_overwrite and not _publisher.get_resource(alternate)):
+            if not _overwrite or (
+                _overwrite and not _publisher.get_resource(alternate)
+            ):
                 _publisher.publish_resources(data)
             else:
                 _publisher.overwrite_resources(data)
 
             # updating the execution request status
             orchestrator.update_execution_request_status(
                 execution_id=execution_id,
                 last_updated=timezone.now(),
                 celery_task_request=self.request,
             )
         else:
-            logger.error(f"Layer: {alternate} raised: Only resources with a CRS provided can be published for execution_id: {execution_id}")
+            logger.error(
+                f"Layer: {alternate} raised: Only resources with a CRS provided can be published for execution_id: {execution_id}"
+            )
             raise PublishResourceException(
                 "Only resources with a CRS provided can be published"
             )
 
         # at the end recall the import_orchestrator for the next step
 
         task_params = (
@@ -264,15 +273,15 @@
         call_rollback_function(
             execution_id,
             handlers_module_path=handler_module_path,
             prev_action=action,
             layer=layer_name,
             alternate=alternate,
             error=e,
-            **kwargs      
+            **kwargs,
         )
         raise PublishResourceException(detail=error_handler(e, execution_id))
 
 
 @importer_app.task(
     bind=True,
     base=ErrorBaseTaskClass,
@@ -322,25 +331,35 @@
         _files = _exec.input_params.get("files")
 
         handler = import_string(handler_module_path)()
         _overwrite = _exec.input_params.get("overwrite_existing_layer")
 
         if _overwrite:
             resource = handler.overwrite_geonode_resource(
-                layer_name=layer_name, alternate=alternate, execution_id=execution_id, files=_files
+                layer_name=layer_name,
+                alternate=alternate,
+                execution_id=execution_id,
+                files=_files,
             )
         else:
             resource = handler.create_geonode_resource(
-                layer_name=layer_name, alternate=alternate, execution_id=execution_id, files=_files
+                layer_name=layer_name,
+                alternate=alternate,
+                execution_id=execution_id,
+                files=_files,
             )
 
         if _overwrite:
-            handler.overwrite_resourcehandlerinfo(handler_module_path, resource, _exec, **kwargs)
+            handler.overwrite_resourcehandlerinfo(
+                handler_module_path, resource, _exec, **kwargs
+            )
         else:
-            handler.create_resourcehandlerinfo(handler_module_path, resource, _exec, **kwargs)
+            handler.create_resourcehandlerinfo(
+                handler_module_path, resource, _exec, **kwargs
+            )
 
         # at the end recall the import_orchestrator for the next step
         import_orchestrator.apply_async(
             (
                 _files,
                 execution_id,
                 handler_module_path,
@@ -356,15 +375,15 @@
         call_rollback_function(
             execution_id,
             handlers_module_path=handler_module_path,
             prev_action=action,
             layer=layer_name,
             alternate=alternate,
             error=e,
-            **kwargs      
+            **kwargs,
         )
         raise ResourceCreationException(detail=error_handler(e))
 
 
 @importer_app.task(
     base=ErrorBaseTaskClass,
     name="importer.copy_geonode_resource",
@@ -413,18 +432,22 @@
 
         new_resource = handler.copy_geonode_resource(
             alternate=alternate,
             resource=resource,
             _exec=_exec,
             data_to_update=data_to_update,
             new_alternate=new_alternate,
-            **kwargs
+            **kwargs,
         )
 
-        handler.create_resourcehandlerinfo(resource=new_resource, handler_module_path=handler_module_path, execution_id=_exec)
+        handler.create_resourcehandlerinfo(
+            resource=new_resource,
+            handler_module_path=handler_module_path,
+            execution_id=_exec,
+        )
 
         assert f"{workspace}:{new_alternate}" == new_resource.alternate
 
         orchestrator.update_execution_request_status(
             execution_id=str(_exec.exec_id),
             input_params={**_exec.input_params, **{"instance": resource.pk}},
             output_params={"output": {"uuid": str(new_resource.uuid)}},
@@ -449,15 +472,15 @@
         call_rollback_function(
             exec_id,
             handlers_module_path=handler_module_path,
             prev_action=action,
             layer=layer_name,
             alternate=alternate,
             error=e,
-            **kwargs      
+            **kwargs,
         )
         raise CopyResourceException(detail=e)
     return exec_id, new_alternate
 
 
 @importer_app.task(
     base=SingleMessageErrorHandler,
@@ -506,17 +529,17 @@
                 f"Error during the field creation. The field or class_name is None {field} for {layer_name} for execution {execution_id}"
             )
 
         _kwargs = {"null": field.get("null", True)}
         if field["class_name"].endswith("CharField"):
             _kwargs = {**_kwargs, **{"max_length": 255}}
 
-        if field.get('dim', None) is not None:
+        if field.get("dim", None) is not None:
             # setting the dimension for the gemetry. So that we can handle also 3d geometries
-            _kwargs = {**_kwargs, **{"dim": field.get('dim')}}
+            _kwargs = {**_kwargs, **{"dim": field.get("dim")}}
 
         # if is a new creation we generate the field model from scratch
         if not overwrite:
             row_to_insert.append(_create_field(dynamic_model_schema, field, _kwargs))
         else:
             # otherwise if is an overwrite, we update the existing one and create the one that does not exists
             _field_exists = FieldSchema.objects.filter(
@@ -580,15 +603,15 @@
                 name=new_dataset_alternate
             )
 
             if dynamic_schema.exists() and not alternative_dynamic_schema.exists():
                 # Creating the dynamic schema object
                 new_schema = dynamic_schema.first()
                 new_schema.name = new_dataset_alternate
-                new_schema.db_table_name = new_dataset_alternate            
+                new_schema.db_table_name = new_dataset_alternate
                 new_schema.pk = None
                 new_schema.save()
                 # create the field_schema object
                 fields = []
                 for field in dynamic_schema.first().fields.all():
                     obj = field
                     obj.model_schema = new_schema
@@ -618,15 +641,15 @@
         call_rollback_function(
             exec_id,
             handlers_module_path=handler_module_path,
             prev_action=action,
             layer=layer_name,
             alternate=alternate,
             error=e,
-            **{**kwargs, **additional_kwargs}
+            **{**kwargs, **additional_kwargs},
         )
         raise CopyResourceException(detail=e)
     return exec_id, kwargs
 
 
 @importer_app.task(
     base=ErrorBaseTaskClass,
@@ -635,17 +658,16 @@
     task_track_started=True,
 )
 def copy_geonode_data_table(
     exec_id, actual_step, layer_name, alternate, handlers_module_path, action, **kwargs
 ):
     """
     Once the base resource is copied, is time to copy also the dynamic model
-    """ 
+    """
     try:
-
         orchestrator.update_execution_request_status(
             execution_id=exec_id,
             last_updated=timezone.now(),
             func_name="copy_geonode_data_table",
             step=ugettext("importer.copy_geonode_data_table"),
         )
 
@@ -655,15 +677,17 @@
 
         new_dataset_alternate = kwargs.get("kwargs").get("new_dataset_alternate")
 
         from importer.celery_tasks import import_orchestrator
 
         db_name = os.getenv("DEFAULT_BACKEND_DATASTORE", "datastore")
         if os.getenv("IMPORTER_ENABLE_DYN_MODELS", False):
-            schema_exists = ModelSchema.objects.filter(name=new_dataset_alternate).first()
+            schema_exists = ModelSchema.objects.filter(
+                name=new_dataset_alternate
+            ).first()
             if schema_exists:
                 db_name = schema_exists.db_name
 
         with transaction.atomic():
             with connections[db_name].cursor() as cursor:
                 cursor.execute(
                     f'CREATE TABLE {new_dataset_alternate} AS TABLE "{original_dataset_alternate}";'
@@ -687,15 +711,15 @@
         call_rollback_function(
             exec_id,
             handlers_module_path=handlers_module_path,
             prev_action=action,
             layer=layer_name,
             alternate=alternate,
             error=e,
-            **kwargs      
+            **kwargs,
         )
         raise CopyResourceException(detail=e)
     return exec_id, kwargs
 
 
 @importer_app.task(
     bind=True,
@@ -706,15 +730,15 @@
 )
 def rollback(self, *args, **kwargs):
     """
     Task used to rollback the partially imported resource
     The handler must implement the code to rollback each step that
     is declared
     """
-    
+
     exec_id = get_uuid(args)
 
     logger.info(f"Calling rollback for execution_id {exec_id} in progress")
 
     exec_object = orchestrator.get_execution_object(exec_id)
     rollback_from_step = exec_object.step
     action_to_rollback = exec_object.action
@@ -725,22 +749,19 @@
         last_updated=timezone.now(),
         func_name="rollback",
         step=ugettext("importer.rollback"),
         celery_task_request=self.request,
     )
 
     handler = import_string(handler_module_path)()
-    handler.rollback(
-        exec_id,
-        rollback_from_step,
-        action_to_rollback,
-        *args,
-        **kwargs
+    handler.rollback(exec_id, rollback_from_step, action_to_rollback, *args, **kwargs)
+    error = (
+        find_key_recursively(kwargs, "error")
+        or "Some issue has occured, please check the logs"
     )
-    error = find_key_recursively(kwargs, "error") or "Some issue has occured, please check the logs"
     orchestrator.set_as_failed(exec_id, reason=error)
     return exec_id, kwargs
 
 
 @importer_app.task(name="dynamic_model_error_callback")
 def dynamic_model_error_callback(*args, **kwargs):
     # revert eventually the import in ogr2ogr or the creation of the model in case of failing
```

## importer/datastore.py

```diff
@@ -32,9 +32,8 @@
         """
         return self.handler().prepare_import(self.files, self.execution_id, **kwargs)
 
     def start_import(self, execution_id, **kwargs):
         """
         call the resource handler object to perform the import phase
         """
-        return self.handler().import_resource(self.files, execution_id,  **kwargs)
-    
+        return self.handler().import_resource(self.files, execution_id, **kwargs)
```

## importer/models.py

```diff
@@ -31,11 +31,17 @@
 
 class ResourceHandlerInfo(models.Model):
 
     """
     Here we save the relation between the geonode resource created and the handler that created that resource
     """
 
-    resource = models.ForeignKey(ResourceBase, blank=False, null=False, on_delete=models.CASCADE)
+    resource = models.ForeignKey(
+        ResourceBase, blank=False, null=False, on_delete=models.CASCADE
+    )
     handler_module_path = models.CharField(max_length=250, blank=False, null=False)
-    execution_request = models.ForeignKey(ExecutionRequest, null=True, default=None, on_delete=models.SET_NULL)
-    kwargs = models.JSONField(verbose_name="Storing strictly related information of the handler", default=dict)
+    execution_request = models.ForeignKey(
+        ExecutionRequest, null=True, default=None, on_delete=models.SET_NULL
+    )
+    kwargs = models.JSONField(
+        verbose_name="Storing strictly related information of the handler", default=dict
+    )
```

## importer/orchestrator.py

```diff
@@ -6,16 +6,15 @@
 from celery import states
 from django.contrib.auth import get_user_model
 from django.db.models import Q
 from django.db.transaction import rollback
 from django.utils import timezone
 from django.utils.module_loading import import_string
 from django_celery_results.models import TaskResult
-from geonode.base.enumerations import (STATE_INVALID, STATE_PROCESSED,
-                                       STATE_RUNNING)
+from geonode.base.enumerations import STATE_INVALID, STATE_PROCESSED, STATE_RUNNING
 from geonode.resource.models import ExecutionRequest
 from geonode.upload.models import Upload
 from rest_framework import serializers
 
 from importer.api.exception import ImportException
 from importer.api.serializer import ImporterSerializer
 from importer.celery_app import importer_app
@@ -109,15 +108,17 @@
                     execution_id=str(_exec_obj.exec_id),
                     status=ExecutionRequest.STATUS_RUNNING,
                 )
             # finding in the task_list the last step done
             remaining_tasks = tasks[_index:] if not _index >= len(tasks) else []
             if not remaining_tasks:
                 # The list of task is empty, it means that the process is finished
-                self.evaluate_execution_progress(execution_id, handler_module_path=handler_module_path)
+                self.evaluate_execution_progress(
+                    execution_id, handler_module_path=handler_module_path
+                )
                 return
             # getting the next step to perform
             next_step = next(iter(remaining_tasks))
             # calling the next step for the resource
 
             # defining the tasks parameter for the step
             task_params = (str(execution_id), handler_module_path, action)
@@ -190,77 +191,87 @@
             execution_id=str(execution_id),
             status=ExecutionRequest.STATUS_FINISHED,
             finished=timezone.now(),
             last_updated=timezone.now(),
             legacy_status=STATE_PROCESSED,
         )
 
-    def evaluate_execution_progress(self, execution_id, _log=None, handler_module_path=None):
+    def evaluate_execution_progress(
+        self, execution_id, _log=None, handler_module_path=None
+    ):
         from importer.models import ResourceHandlerInfo
 
         """
         The execution id is a mandatory argument for the task
         We use that to filter out all the task execution that are still in progress.
         if any is failed, we raise it.
         """
 
         _exec = self.get_execution_object(execution_id)
         expected_dataset = _exec.input_params.get("total_layers", 0)
-        actual_dataset = ResourceHandlerInfo.objects.filter(execution_request=_exec).count()
+        actual_dataset = ResourceHandlerInfo.objects.filter(
+            execution_request=_exec
+        ).count()
         is_last_dataset = actual_dataset >= expected_dataset
         execution_id = str(execution_id)  # force it as string to be sure
         lower_exec_id = execution_id.replace("-", "_").lower()
         exec_result = TaskResult.objects.filter(
             Q(task_args__icontains=lower_exec_id)
             | Q(task_kwargs__icontains=lower_exec_id)
             | Q(result__icontains=lower_exec_id)
             | Q(task_args__icontains=execution_id)
             | Q(task_kwargs__icontains=execution_id)
             | Q(result__icontains=execution_id)
         )
-        _has_data = ResourceHandlerInfo.objects.filter(execution_request__exec_id=execution_id).exists()
+        _has_data = ResourceHandlerInfo.objects.filter(
+            execution_request__exec_id=execution_id
+        ).exists()
 
         # .all() is needed since we want to have the last status on the DB without take in consideration the cache
         if (
             exec_result.all()
             .exclude(Q(status=states.SUCCESS) | Q(status=states.FAILURE))
             .exists()
         ):
-            self._evaluate_last_dataset(is_last_dataset, _log, execution_id, handler_module_path)
+            self._evaluate_last_dataset(
+                is_last_dataset, _log, execution_id, handler_module_path
+            )
         elif exec_result.all().filter(status=states.FAILURE).exists():
             """
             Should set it fail if all the execution are done and at least 1 is failed
             """
             # failed = [x.task_id for x in exec_result.filter(status=states.FAILURE)]
             # _log_message = f"For the execution ID {execution_id} The following celery task are failed: {failed}"
             if _has_data:
-                log = list(set(self.get_execution_object(execution_id).output_params.get("failed_layers", ['Unknown'])))
-                logger.error(log)
-                self.set_as_partially_failed(
-                    execution_id=execution_id, reason=log
+                log = list(
+                    set(
+                        self.get_execution_object(execution_id).output_params.get(
+                            "failed_layers", ["Unknown"]
+                        )
+                    )
                 )
+                logger.error(log)
+                self.set_as_partially_failed(execution_id=execution_id, reason=log)
                 self._last_step(execution_id, handler_module_path)
 
             elif is_last_dataset:
-                self.set_as_failed(
-                    execution_id=execution_id, reason=_log
-                )
+                self.set_as_failed(execution_id=execution_id, reason=_log)
             elif expected_dataset == 1 and not _has_data:
-                self.set_as_failed(
-                    execution_id=execution_id, reason=_log
-                )
+                self.set_as_failed(execution_id=execution_id, reason=_log)
         else:
-            self._evaluate_last_dataset(is_last_dataset, _log, execution_id, handler_module_path)
+            self._evaluate_last_dataset(
+                is_last_dataset, _log, execution_id, handler_module_path
+            )
 
-    def _evaluate_last_dataset(self, is_last_dataset, _log, execution_id, handler_module_path):
+    def _evaluate_last_dataset(
+        self, is_last_dataset, _log, execution_id, handler_module_path
+    ):
         if is_last_dataset:
-            if _log and 'ErrorDetail' in _log:
-                self.set_as_failed(
-                execution_id=execution_id, reason=_log
-            )
+            if _log and "ErrorDetail" in _log:
+                self.set_as_failed(execution_id=execution_id, reason=_log)
             else:
                 logger.info(
                     f"Execution with ID {execution_id} is completed. All tasks are done"
                 )
                 self._last_step(execution_id, handler_module_path)
                 self.set_as_completed(execution_id)
         else:
@@ -275,28 +286,28 @@
         func_name: str,
         step: str,
         input_params: dict = {},
         resource=None,
         legacy_upload_name="",
         action=None,
         name=None,
-        source=None
+        source=None,
     ) -> UUID:
         """
         Create an execution request for the user. Return the UUID of the request
         """
         execution = ExecutionRequest.objects.create(
             user=user,
             geonode_resource=resource,
             func_name=func_name,
             step=step,
             input_params=input_params,
             action=action,
             name=name,
-            source=source
+            source=source,
         )
         if self.enable_legacy_upload_status:
             # getting the package name from the base_filename
             Upload.objects.create(
                 name=legacy_upload_name
                 or os.path.basename(input_params.get("files", {}).get("base_file")),
                 state=STATE_RUNNING,
@@ -337,17 +348,17 @@
             )
         if celery_task_request:
             TaskResult.objects.filter(task_id=celery_task_request.id).update(
                 task_args=celery_task_request.args
             )
 
     def _last_step(self, execution_id, handler_module_path):
-        '''
+        """
         Last hookable step for each handler before mark the execution as completed
         To overwrite this, please hook the method perform_last_step from the Handler
-        '''
+        """
         if not handler_module_path:
             return
         return self.load_handler(handler_module_path).perform_last_step(execution_id)
 
 
 orchestrator = ImportOrchestrator(enable_legacy_upload_status=False)
```

## importer/publisher.py

```diff
@@ -46,65 +46,69 @@
 
         return self.handler.extract_resource_to_publish(
             files, action, layer_name, alternate, **kwargs
         )
 
     def get_resource(self, resource_name) -> bool:
         self.get_or_create_store()
-        _res = self.cat.get_resource(resource_name, store=self.store, workspace=self.workspace)
+        _res = self.cat.get_resource(
+            resource_name, store=self.store, workspace=self.workspace
+        )
         return True if _res else False
 
     def publish_resources(self, resources: List[str]):
         """
         Given a list of strings (which rappresent the table on geoserver)
         Will publish the resorces on geoserver
         """
         self.get_or_create_store()
         return self.handler.publish_resources(
             resources=resources,
             catalog=self.cat,
             store=self.store,
             workspace=self.workspace,
         )
-    
+
     def delete_resource(self, resource_name):
         layer = self.get_resource(resource_name)
         if layer:
-            self.cat.delete(layer.resource, purge='all', recurse=True)
-    
+            self.cat.delete(layer.resource, purge="all", recurse=True)
+
     def get_resource(self, dataset_name):
         return self.cat.get_layer(dataset_name)
 
     def overwrite_resources(self, resources: List[str]):
-        '''
+        """
         Not available for now, waiting geoserver 2.20/2.21 available with Geonode
-        '''
+        """
         pass
 
     def get_or_create_store(self):
         """
         Evaluate if the store exists. if not is created
         """
         geodatabase = os.environ.get("GEONODE_GEODATABASE", "geonode_data")
         self.store = self.cat.get_store(name=geodatabase, workspace=self.workspace)
         if not self.store:
             logger.warning(f"The store does not exists: {geodatabase} creating...")
             self.store = create_geoserver_db_featurestore(
                 store_name=geodatabase, workspace=self.workspace.name
             )
 
-    def publish_geoserver_view(self, layer_name, crs, view_name, sql=None, geometry=None):
+    def publish_geoserver_view(
+        self, layer_name, crs, view_name, sql=None, geometry=None
+    ):
         """
         Let the handler create a geoserver view given the input parameters
         """
         self.get_or_create_store()
 
         return self.handler.publish_geoserver_view(
             catalog=self.cat,
             workspace=self.workspace,
             datastore=self.store,
             layer_name=layer_name,
             crs=crs,
             view_name=view_name,
             sql=sql,
-            geometry=geometry
+            geometry=geometry,
         )
```

## importer/utils.py

```diff
@@ -26,35 +26,44 @@
     def update(self, uuid, **kwargs) -> ResourceBase:
         return ResourceBase.objects.get(uuid=uuid)
 
 
 custom_resource_manager = ResourceManager(concrete_manager=ImporterConcreteManager())
 
 
-def call_rollback_function(execution_id, handlers_module_path, prev_action, layer=None, alternate=None, error=None, **kwargs):
+def call_rollback_function(
+    execution_id,
+    handlers_module_path,
+    prev_action,
+    layer=None,
+    alternate=None,
+    error=None,
+    **kwargs,
+):
     from importer.celery_tasks import import_orchestrator
-    
+
     task_params = (
         {},
         execution_id,
         handlers_module_path,
         "start_rollback",
         layer,
         alternate,
         ImporterRequestAction.ROLLBACK.value,
     )
-    kwargs['previous_action'] = prev_action
+    kwargs["previous_action"] = prev_action
     kwargs["error"] = error_handler(error, exec_id=execution_id)
     import_orchestrator.apply_async(task_params, kwargs)
 
 
 def find_key_recursively(obj, key):
-    '''
+    """
     Celery (unluckly) append the kwargs for each task
     under a new kwargs key, so sometimes is faster
     to look into the key recursively instead of
     parsing the dict
-    '''
-    if key in obj: return obj.get(key, None)
+    """
+    if key in obj:
+        return obj.get(key, None)
     for _, v in obj.items():
         if isinstance(v, dict):
             return find_key_recursively(v, key)
```

## importer/api/serializer.py

```diff
@@ -1,15 +1,14 @@
 from rest_framework import serializers
 from dynamic_rest.serializers import DynamicModelSerializer
 from geonode.upload.models import Upload
 
 
 class ImporterSerializer(DynamicModelSerializer):
     class Meta:
-
         ref_name = "ImporterSerializer"
         model = Upload
         view_name = "importer_upload"
         fields = (
             "base_file",
             "xml_file",
             "sld_file",
```

## importer/api/tests.py

```diff
@@ -10,15 +10,14 @@
 from django.http import HttpResponse, QueryDict
 
 from importer.models import ResourceHandlerInfo
 from importer.tests.utils import ImporterBaseTestSupport
 
 
 class TestImporterViewSet(ImporterBaseTestSupport):
-
     @classmethod
     def setUpClass(cls):
         super().setUpClass()
         cls.url = reverse("importer_upload")
 
     def setUp(self):
         self.dataset = create_single_dataset(name="test_dataset_copy")
@@ -41,29 +40,29 @@
 
     @patch("importer.api.views.UploadViewSet")
     def test_redirect_to_old_upload_if_file_is_not_a_gpkg(self, patch_upload):
         upload = MagicMock()
         upload.upload.return_value = HttpResponse()
         patch_upload.return_value = upload
 
-        self.client.force_login(get_user_model().objects.get(username='admin'))
+        self.client.force_login(get_user_model().objects.get(username="admin"))
         payload = {
             "base_file": SimpleUploadedFile(name="file.invalid", content=b"abc"),
         }
         response = self.client.post(self.url, data=payload)
         self.assertEqual(200, response.status_code)
         upload.upload.assert_called_once()
 
     @patch("importer.api.views.UploadViewSet")
     def test_gpkg_raise_error_with_invalid_payload(self, patch_upload):
         upload = MagicMock()
         upload.upload.return_value = HttpResponse()
         patch_upload.return_value = upload
 
-        self.client.force_login(get_user_model().objects.get(username='admin'))
+        self.client.force_login(get_user_model().objects.get(username="admin"))
         payload = {
             "base_file": SimpleUploadedFile(name="test.gpkg", content=b"some-content"),
             "store_spatial_files": "invalid",
         }
         expected = {
             "success": False,
             "errors": ["Must be a valid boolean."],
@@ -75,29 +74,29 @@
         self.assertEqual(400, response.status_code)
         self.assertEqual(expected, response.json())
 
     @patch("importer.api.views.import_orchestrator")
     def test_gpkg_task_is_called(self, patch_upload):
         patch_upload.apply_async.side_effect = MagicMock()
 
-        self.client.force_login(get_user_model().objects.get(username='admin'))
+        self.client.force_login(get_user_model().objects.get(username="admin"))
         payload = {
             "base_file": SimpleUploadedFile(name="test.gpkg", content=b"some-content"),
             "store_spatial_files": True,
         }
 
         response = self.client.post(self.url, data=payload)
 
         self.assertEqual(201, response.status_code)
 
     @patch("importer.api.views.import_orchestrator")
     def test_geojson_task_is_called(self, patch_upload):
         patch_upload.apply_async.side_effect = MagicMock()
 
-        self.client.force_login(get_user_model().objects.get(username='admin'))
+        self.client.force_login(get_user_model().objects.get(username="admin"))
         payload = {
             "base_file": SimpleUploadedFile(
                 name="test.geojson", content=b"some-content"
             ),
             "store_spatial_files": True,
         }
 
@@ -107,56 +106,55 @@
 
         self.assertTrue(201, response.status_code)
 
     @patch("importer.api.views.import_orchestrator")
     def test_zip_file_is_unzip_and_the_handler_is_found(self, patch_upload):
         patch_upload.apply_async.side_effect = MagicMock()
 
-        self.client.force_login(get_user_model().objects.get(username='admin'))
+        self.client.force_login(get_user_model().objects.get(username="admin"))
         payload = {
             "base_file": open(f"{project_dir}/tests/fixture/valid.zip", "rb"),
             "zip_file": open(f"{project_dir}/tests/fixture/valid.zip", "rb"),
             "store_spatial_files": True,
         }
 
         response = self.client.post(self.url, data=payload)
 
         self.assertEqual(201, response.status_code)
 
     def test_copy_method_not_allowed(self):
-
-        self.client.force_login(get_user_model().objects.get(username='admin'))
+        self.client.force_login(get_user_model().objects.get(username="admin"))
 
         response = self.client.get(self.copy_url)
         self.assertEqual(405, response.status_code)
 
         response = self.client.post(self.copy_url)
         self.assertEqual(405, response.status_code)
 
         response = self.client.patch(self.copy_url)
         self.assertEqual(405, response.status_code)
 
     @patch("importer.api.views.import_orchestrator")
     @patch("importer.api.views.ResourceBaseViewSet.resource_service_copy")
     def test_redirect_to_old_upload_if_file_handler_is_not_set(self, copy_view, _orc):
         copy_view.return_value = HttpResponse()
-        self.client.force_login(get_user_model().objects.get(username='admin'))
+        self.client.force_login(get_user_model().objects.get(username="admin"))
 
         response = self.client.put(self.copy_url)
 
         self.assertEqual(200, response.status_code)
         _orc.assert_not_called()
         copy_view.assert_called_once()
 
     @patch("importer.api.views.import_orchestrator")
     def test_copy_ther_resource_if_file_handler_is_set(self, _orc):
-        user = get_user_model().objects.get(username='admin')
+        user = get_user_model().objects.get(username="admin")
         user.is_superuser = True
         user.save()
-        self.client.force_login(get_user_model().objects.get(username='admin'))
+        self.client.force_login(get_user_model().objects.get(username="admin"))
         ResourceHandlerInfo.objects.create(
             resource=self.dataset,
             handler_module_path="importer.handlers.gpkg.handler.GPKGFileHandler",
         )
         payload = QueryDict("", mutable=True)
         payload.update({"defaults": '{"title":"stili_di_vita_4scenari"}'})
         response = self.client.put(
```

## importer/api/views.py

```diff
@@ -79,15 +79,14 @@
     http_method_names = ["get", "post"]
 
     def get_serializer_class(self):
         specific_serializer = orchestrator.get_serializer(self.request.data)
         return specific_serializer or ImporterSerializer
 
     def create(self, request, *args, **kwargs):
-
         """
         Main function called by the new import flow.
         It received the file via the front end
         if is a gpkg (in future it will support all the vector file)
         the new import flow is follow, else the normal upload api is used.
         It clone on the local repo the file that the user want to upload
         """
@@ -116,15 +115,14 @@
             storage_manager.clone_remote_files()
             # update the payload with the unziped paths
             _data.update(storage_manager.get_retrieved_paths())
 
         handler = orchestrator.get_handler(_data)
 
         if _file and handler:
-
             try:
                 # cloning data into a local folder
                 extracted_params, _data = handler.extract_params_from_data(_data)
                 if storage_manager is None:
                     # means that the storage manager is not initialized yet, so
                     # the file is not a zip
                     storage_manager = StorageManager(remote_files=_data)
@@ -147,15 +145,15 @@
                     input_params={
                         **{"files": files, "handler_module_path": str(handler)},
                         **extracted_params,
                     },
                     legacy_upload_name=_file.name,
                     action=action,
                     name=_file.name,
-                    source='upload'
+                    source="upload",
                 )
 
                 sig = import_orchestrator.s(
                     files, str(execution_id), handler=str(handler), action=action
                 )
                 sig.apply_async()
                 return Response(data={"execution_id": execution_id}, status=201)
@@ -171,15 +169,14 @@
 
         # if is a geopackage we just use the new import flow
         request.GET._mutable = True
         return UploadViewSet().upload(request)
 
 
 class ResourceImporter(DynamicModelViewSet):
-
     authentication_classes = [
         SessionAuthentication,
         BasicAuthentication,
         OAuth2Authentication,
     ]
     permission_classes = [
         IsAuthenticatedOrReadOnly,
@@ -208,15 +205,14 @@
     queryset = ResourceBase.objects.all().order_by("-last_updated")
     serializer_class = ResourceBaseSerializer
     pagination_class = GeoNodeApiPagination
 
     def copy(self, request, *args, **kwargs):
         resource = self.get_object()
         if resource.resourcehandlerinfo_set.exists():
-
             handler_module_path = (
                 resource.resourcehandlerinfo_set.first().handler_module_path
             )
 
             action = ExecutionRequestAction.COPY.value
 
             handler = orchestrator.load_handler(handler_module_path)
@@ -237,15 +233,15 @@
                 func_name=step,
                 step=step,
                 action=action,
                 input_params={
                     **{"handler_module_path": handler_module_path},
                     **extracted_params,
                 },
-                source="importer_copy"
+                source="importer_copy",
             )
 
             sig = import_orchestrator.s(
                 {},
                 str(execution_id),
                 step=step,
                 handler=str(handler_module_path),
```

## importer/handlers/apps.py

```diff
@@ -15,19 +15,23 @@
     def ready(self):
         """Finalize setup"""
         run_setup_hooks()
         super(HandlersConfig, self).ready()
 
 
 def run_setup_hooks(*args, **kwargs):
-    if getattr(settings, 'IMPORTER_HANDLERS', []):
-        _handlers = [import_string(module_path) for module_path in settings.IMPORTER_HANDLERS]
+    if getattr(settings, "IMPORTER_HANDLERS", []):
+        _handlers = [
+            import_string(module_path) for module_path in settings.IMPORTER_HANDLERS
+        ]
         for item in _handlers:
             item.register()
-        logger.info(f"The following handlers have been registered: {', '.join(settings.IMPORTER_HANDLERS)}")
+        logger.info(
+            f"The following handlers have been registered: {', '.join(settings.IMPORTER_HANDLERS)}"
+        )
 
         _available_settings = [
             import_string(module_path)().supported_file_extension_config
             for module_path in settings.IMPORTER_HANDLERS
         ]
         # injecting the new config required for FE
         supported_type = [
@@ -67,14 +71,16 @@
                     "geojson",
                 ],
             },
         ]
         supported_type.extend(_available_settings)
         if not getattr(settings, "ADDITIONAL_DATASET_FILE_TYPES", None):
             setattr(settings, "ADDITIONAL_DATASET_FILE_TYPES", supported_type)
-        elif "gpkg" not in [x.get("id") for x in settings.ADDITIONAL_DATASET_FILE_TYPES]:
+        elif "gpkg" not in [
+            x.get("id") for x in settings.ADDITIONAL_DATASET_FILE_TYPES
+        ]:
             settings.ADDITIONAL_DATASET_FILE_TYPES.extend(supported_type)
             setattr(
                 settings,
                 "ADDITIONAL_DATASET_FILE_TYPES",
                 settings.ADDITIONAL_DATASET_FILE_TYPES,
             )
```

## importer/handlers/base.py

```diff
@@ -71,18 +71,18 @@
         This endpoint will return True or False if with the info provided
         the handler is able to handle the file or not
         """
         return False
 
     @staticmethod
     def has_serializer(_data) -> bool:
-        '''
+        """
         This endpoint should return (if set) the custom serializer used in the API
         to validate the input resource
-        '''
+        """
         return None
 
     @staticmethod
     def can_do(action) -> bool:
         """
         Evaluate if the handler can take care of a specific action.
         Each action (import/copy/etc...) can define different step so
@@ -96,18 +96,35 @@
         """
         Remove from the _data the params that needs to save into the executionRequest object
         all the other are returned
         """
         return []
 
     def fixup_name(self, name):
-        return name.lower().replace("-", "_")\
-            .replace(" ", "_").replace(")", "")\
-            .replace("(", "").replace(",", "")\
-            .replace("&", "").replace(".", "")
+        '''
+        Emulate the LAUNDER option in ogr2ogr which will normalize the string.
+        This is enriched with additional transformation for parentesis.
+        The basic normalized function can be found here
+        https://github.com/OSGeo/gdal/blob/0fc262675051b63f96c91ca920d27503655dfb7b/ogr/ogrsf_frmts/pgdump/ogrpgdumpdatasource.cpp#L130  # noqa
+        
+        We use replace because it looks to be one of the fasted options:
+        https://stackoverflow.com/questions/3411771/best-way-to-replace-multiple-characters-in-a-string
+        '''
+        return (
+            name.lower()
+            .replace("-", "_")
+            .replace(" ", "_")
+            .replace("#", "_")
+            .replace("\\", "_")
+            .replace(".", "")
+            .replace(")", "")
+            .replace("(", "")
+            .replace(",", "")
+            .replace("&", "")
+        )
 
     def extract_resource_to_publish(self, files, layer_name, alternate, **kwargs):
         """
         Function to extract the layer name and the CRS from needed in the
         publishing phase
         [
             {'name': 'alternate or layer_name', 'crs': 'EPSG:25832'}
```

## importer/handlers/utils.py

```diff
@@ -92,17 +92,17 @@
             UUID(el)
             return el
         except Exception:
             continue
 
 
 def evaluate_error(celery_task, exc, task_id, args, kwargs, einfo):
-    '''
+    """
     Main error function used by the task for the "on_failure" function
-    '''
+    """
     from importer.celery_tasks import orchestrator
 
     exec_id = orchestrator.get_execution_object(exec_id=get_uuid(args))
     output_params = exec_id.output_params.copy()
 
     if exec_id.status == ExecutionRequest.STATUS_FAILED:
         logger.info("Execution is already in status FAILED")
@@ -115,25 +115,23 @@
     # creting the log message
     _log = handler.create_error_log(exc, celery_task.name, *args)
 
     if output_params.get("errors"):
         output_params.get("errors").append(_log)
         output_params.get("failed_layers", []).append(args[-1] if args else [])
         failed = list(set(output_params.get("failed_layers", [])))
-        output_params['failed_layers'] = failed
+        output_params["failed_layers"] = failed
     else:
         output_params = {"errors": [_log], "failed_layers": [args[-1]]}
 
     celery_task.update_state(
         task_id=task_id,
         state="FAILURE",
         meta={"exec_id": str(exec_id.exec_id), "reason": _log},
     )
     orchestrator.update_execution_request_status(
-        execution_id=str(exec_id.exec_id),
-        output_params=output_params
+        execution_id=str(exec_id.exec_id), output_params=output_params
     )
 
     orchestrator.evaluate_execution_progress(
-        get_uuid(args),
-        _log=str(exc.detail if hasattr(exc, "detail") else exc.args[0])
+        get_uuid(args), _log=str(exc.detail if hasattr(exc, "detail") else exc.args[0])
     )
```

## importer/handlers/common/raster.py

```diff
@@ -12,16 +12,15 @@
 from django.db.models import Q
 from django_celery_results.models import TaskResult
 from geonode.base.models import ResourceBase
 from geonode.layers.models import Dataset
 from geonode.resource.enumerator import ExecutionRequestAction as exa
 from geonode.resource.manager import resource_manager
 from geonode.resource.models import ExecutionRequest
-from geonode.services.serviceprocessors.base import \
-    get_geoserver_cascading_workspace
+from geonode.services.serviceprocessors.base import get_geoserver_cascading_workspace
 from importer.api.exception import ImportException
 from importer.celery_tasks import ErrorBaseTaskClass, import_orchestrator
 from importer.handlers.base import BaseHandler
 from importer.handlers.geotiff.exceptions import InvalidGeoTiffException
 from importer.handlers.utils import create_alternate, should_be_imported
 from importer.models import ResourceHandlerInfo
 from importer.orchestrator import orchestrator
@@ -65,18 +64,18 @@
         This endpoint will return True or False if with the info provided
         the handler is able to handle the file or not
         """
         return False
 
     @staticmethod
     def has_serializer(_data) -> bool:
-        '''
+        """
         This endpoint will return True or False if with the info provided
         the handler is able to handle the file or not
-        '''
+        """
         return False
 
     @staticmethod
     def can_do(action) -> bool:
         """
         This endpoint will return True or False if with the info provided
         the handler is able to handle the file or not
@@ -117,15 +116,15 @@
             try:
                 catalog.create_coveragestore(
                     _resource.get("name"),
                     path=_resource.get("raster_path"),
                     layer_name=_resource.get("name"),
                     workspace=workspace,
                     overwrite=True,
-                    upload_data=False
+                    upload_data=False,
                 )
             except Exception as e:
                 if (
                     f"Resource named {_resource.get('name')} already exists in store:"
                     in str(e)
                 ):
                     continue
@@ -137,20 +136,20 @@
         # it should delete the image from the geoserver data dir
         # for now we can rely on the geonode delete behaviour
         # since the file is stored on local
         pass
 
     @staticmethod
     def perform_last_step(execution_id):
-        '''
+        """
         Override this method if there is some extra step to perform
         before considering the execution as completed.
         For example can be used to trigger an email-send to notify
         that the execution is completed
-        '''
+        """
         # as last step, we delete the celery task to keep the number of rows under control
         lower_exec_id = execution_id.replace("-", "_").lower()
         TaskResult.objects.filter(
             Q(task_args__icontains=lower_exec_id)
             | Q(task_kwargs__icontains=lower_exec_id)
             | Q(result__icontains=lower_exec_id)
             | Q(task_args__icontains=execution_id)
@@ -158,86 +157,106 @@
             | Q(result__icontains=execution_id)
         ).delete()
 
         _exec = orchestrator.get_execution_object(execution_id)
 
         _exec.save()
 
-        _exec.output_params.update(**{
-            "detail_url": [
-                x.resource.detail_url
-                for x in ResourceHandlerInfo.objects.filter(execution_request=_exec)
-            ]
-        })
-
+        _exec.output_params.update(
+            **{
+                "detail_url": [
+                    x.resource.detail_url
+                    for x in ResourceHandlerInfo.objects.filter(execution_request=_exec)
+                ]
+            }
+        )
 
-    def extract_resource_to_publish(self, files, action, layer_name, alternate, **kwargs):
+    def extract_resource_to_publish(
+        self, files, action, layer_name, alternate, **kwargs
+    ):
         if action == exa.COPY.value:
             return [
                 {
                     "name": alternate,
-                    "crs": ResourceBase.objects.filter(Q(alternate__icontains=layer_name) | Q(title__icontains=layer_name))
+                    "crs": ResourceBase.objects.filter(
+                        Q(alternate__icontains=layer_name)
+                        | Q(title__icontains=layer_name)
+                    )
                     .first()
                     .srid,
-                    "raster_path": kwargs['kwargs'].get("new_file_location").get("files")[0]
+                    "raster_path": kwargs["kwargs"]
+                    .get("new_file_location")
+                    .get("files")[0],
                 }
             ]
 
         layers = gdal.Open(files.get("base_file"))
         if not layers:
             return []
-        return [{
+        return [
+            {
                 "name": alternate or layer_name,
-                "crs": self.identify_authority(layers) if layers.GetSpatialRef() else None,
-                "raster_path": files.get("base_file")
-            }]
+                "crs": self.identify_authority(layers)
+                if layers.GetSpatialRef()
+                else None,
+                "raster_path": files.get("base_file"),
+            }
+        ]
 
     def identify_authority(self, layer):
         try:
             layer_wkt = layer.GetSpatialRef().ExportToWkt()
-            x = pyproj.CRS(layer_wkt)
             _name = "EPSG"
-            _code = x.to_epsg(min_confidence=20)
+            _code = pyproj.CRS(layer_wkt).to_epsg(min_confidence=20)
             if _code is None:
-                raise Exception("authority code not found, fallback to default behaviour")
+                layer_proj4 = layer.GetSpatialRef().ExportToProj4()
+                _code = pyproj.CRS(layer_proj4).to_epsg(min_confidence=20)
+                if _code is None:
+                    raise Exception(
+                        "CRS authority code not found, fallback to default behaviour"
+                    )
         except:
             spatial_ref = layer.GetSpatialRef()
             spatial_ref.AutoIdentifyEPSG()
-            _name = spatial_ref.GetAuthorityName(None) or spatial_ref.GetAttrValue('AUTHORITY', 0)
-            _code = spatial_ref.GetAuthorityCode('PROJCS') or spatial_ref.GetAuthorityCode('GEOGCS') or spatial_ref.GetAttrValue('AUTHORITY', 1)
+            _name = spatial_ref.GetAuthorityName(None) or spatial_ref.GetAttrValue(
+                "AUTHORITY", 0
+            )
+            _code = (
+                spatial_ref.GetAuthorityCode("PROJCS")
+                or spatial_ref.GetAuthorityCode("GEOGCS")
+                or spatial_ref.GetAttrValue("AUTHORITY", 1)
+            )
         return f"{_name}:{_code}"
 
     def import_resource(self, files: dict, execution_id: str, **kwargs) -> str:
         """
         Main function to import the resource.
         Internally will call the steps required to import the
         data inside the geonode_data database
         """
         # for the moment we skip the dyanamic model creation
         logger.info("Total number of layers available: 1")
         _exec = self._get_execution_request_object(execution_id)
         _input = {**_exec.input_params, **{"total_layers": 1}}
-        orchestrator.update_execution_request_status(execution_id=str(execution_id), input_params=_input)
+        orchestrator.update_execution_request_status(
+            execution_id=str(execution_id), input_params=_input
+        )
 
         try:
             filename = Path(files.get("base_file")).stem
             # start looping on the layers available
             layer_name = self.fixup_name(filename)
 
             should_be_overwritten = _exec.input_params.get("overwrite_existing_layer")
             # should_be_imported check if the user+layername already exists or not
-            if (
-                should_be_imported(
-                    layer_name,
-                    _exec.user,
-                    skip_existing_layer=_exec.input_params.get(
-                        "skip_existing_layer"
-                    ),
-                    overwrite_existing_layer=should_be_overwritten,
-                )
+            if should_be_imported(
+                layer_name,
+                _exec.user,
+                skip_existing_layer=_exec.input_params.get("skip_existing_layer"),
+                overwrite_existing_layer=should_be_overwritten,
             ):
                 workspace = get_geoserver_cascading_workspace(create=False)
                 user_datasets = Dataset.objects.filter(
                     owner=_exec.user, alternate=f"{workspace.name}:{layer_name}"
                 )
 
                 dataset_exists = user_datasets.exists()
@@ -262,15 +281,20 @@
 
         except Exception as e:
             logger.error(e)
             raise e
         return
 
     def create_geonode_resource(
-        self, layer_name: str, alternate: str, execution_id: str, resource_type: Dataset = Dataset, files=None
+        self,
+        layer_name: str,
+        alternate: str,
+        execution_id: str,
+        resource_type: Dataset = Dataset,
+        files=None,
     ):
         """
         Base function to create the resource into geonode. Each handler can specify
         and handle the resource in a different way
         """
         saved_dataset = resource_type.objects.filter(alternate__icontains=alternate)
 
@@ -296,15 +320,20 @@
                 name=alternate,
                 workspace=workspace,
                 subtype="raster",
                 alternate=f"{workspace}:{alternate}",
                 dirty_state=True,
                 title=layer_name,
                 owner=_exec.user,
-                files=list(set(list(_exec.input_params.get("files", {}).values()) or list(files))),
+                files=list(
+                    set(
+                        list(_exec.input_params.get("files", {}).values())
+                        or list(files)
+                    )
+                ),
             ),
         )
 
         saved_dataset.refresh_from_db()
 
         self.handle_xml_file(saved_dataset, _exec)
         self.handle_sld_file(saved_dataset, _exec)
@@ -313,17 +342,21 @@
 
         ResourceBase.objects.filter(alternate=alternate).update(dirty_state=False)
 
         saved_dataset.refresh_from_db()
         return saved_dataset
 
     def overwrite_geonode_resource(
-        self, layer_name: str, alternate: str, execution_id: str, resource_type: Dataset = Dataset, files=None
+        self,
+        layer_name: str,
+        alternate: str,
+        execution_id: str,
+        resource_type: Dataset = Dataset,
+        files=None,
     ):
-
         dataset = resource_type.objects.filter(alternate__icontains=alternate)
 
         _exec = self._get_execution_request_object(execution_id)
 
         _overwrite = _exec.input_params.get("overwrite_existing_layer", False)
         # if the layer exists, we just update the information of the dataset by
         # let it recreate the catalogue
@@ -331,22 +364,26 @@
             dataset = dataset.first()
 
             dataset = resource_manager.update(dataset.uuid, instance=dataset)
 
             self.handle_xml_file(dataset, _exec)
             self.handle_sld_file(dataset, _exec)
 
-            resource_manager.set_thumbnail(self.object.uuid, instance=self.object, overwrite=False)
+            resource_manager.set_thumbnail(
+                self.object.uuid, instance=self.object, overwrite=False
+            )
             dataset.refresh_from_db()
             return dataset
         elif not dataset.exists() and _overwrite:
             logger.warning(
                 f"The dataset required {alternate} does not exists, but an overwrite is required, the resource will be created"
             )
-            return self.create_geonode_resource(layer_name, alternate, execution_id, resource_type, files)
+            return self.create_geonode_resource(
+                layer_name, alternate, execution_id, resource_type, files
+            )
         elif not dataset.exists() and not _overwrite:
             logger.warning(
                 "The resource does not exists, please use 'create_geonode_resource' to create one"
             )
         return
 
     def handle_xml_file(self, saved_dataset: Dataset, _exec: ExecutionRequest):
@@ -366,159 +403,195 @@
             None,
             instance=saved_dataset,
             sld_file=_exec.input_params.get("files", {}).get("sld_file", ""),
             sld_uploaded=True if _path else False,
             vals={"dirty_state": True},
         )
 
-    def create_resourcehandlerinfo(self, handler_module_path: str, resource: Dataset, execution_id: ExecutionRequest, **kwargs):
+    def create_resourcehandlerinfo(
+        self,
+        handler_module_path: str,
+        resource: Dataset,
+        execution_id: ExecutionRequest,
+        **kwargs,
+    ):
         """
         Create relation between the GeonodeResource and the handler used
         to create/copy it
         """
         ResourceHandlerInfo.objects.create(
             handler_module_path=str(handler_module_path),
             resource=resource,
             execution_request=execution_id,
-            kwargs=kwargs.get('kwargs', {})
+            kwargs=kwargs.get("kwargs", {}),
         )
 
-    def overwrite_resourcehandlerinfo(self, handler_module_path: str, resource: Dataset, execution_id: ExecutionRequest, **kwargs):
+    def overwrite_resourcehandlerinfo(
+        self,
+        handler_module_path: str,
+        resource: Dataset,
+        execution_id: ExecutionRequest,
+        **kwargs,
+    ):
         """
         Overwrite the ResourceHandlerInfo
         """
         if resource.resourcehandlerinfo_set.exists():
             resource.resourcehandlerinfo_set.update(
                 handler_module_path=handler_module_path,
                 resource=resource,
                 execution_request=execution_id,
-                kwargs=kwargs.get('kwargs', {}) or kwargs
+                kwargs=kwargs.get("kwargs", {}) or kwargs,
             )
             return
-        return self.create_resourcehandlerinfo(handler_module_path, resource, execution_id, **kwargs)
+        return self.create_resourcehandlerinfo(
+            handler_module_path, resource, execution_id, **kwargs
+        )
 
     def copy_geonode_resource(
-        self, alternate: str, resource: Dataset, _exec: ExecutionRequest, data_to_update: dict, new_alternate: str, **kwargs
+        self,
+        alternate: str,
+        resource: Dataset,
+        _exec: ExecutionRequest,
+        data_to_update: dict,
+        new_alternate: str,
+        **kwargs,
     ):
         resource = self.create_geonode_resource(
             layer_name=data_to_update.get("title"),
             alternate=new_alternate,
             execution_id=str(_exec.exec_id),
-            files=kwargs.get("kwargs", {}).get("new_file_location", {}).get("files", [])
+            files=kwargs.get("kwargs", {})
+            .get("new_file_location", {})
+            .get("files", []),
         )
         resource.refresh_from_db()
         return resource
 
     def _get_execution_request_object(self, execution_id: str):
         return ExecutionRequest.objects.filter(exec_id=execution_id).first()
 
     @staticmethod
     def copy_original_file(dataset):
-        '''
+        """
         Copy the original file into a new location
-        '''
+        """
         return storage_manager.copy(dataset)
 
-    def rollback(self, exec_id, rollback_from_step, action_to_rollback, *args, **kwargs):
+    def rollback(
+        self, exec_id, rollback_from_step, action_to_rollback, *args, **kwargs
+    ):
         steps = self.ACTIONS.get(action_to_rollback)
         step_index = steps.index(rollback_from_step)
         # the start_import, start_copy etc.. dont do anything as step, is just the start
         # so there is nothing to rollback
-        steps_to_rollback = steps[1:step_index+1]
+        steps_to_rollback = steps[1 : step_index + 1]
         if not steps_to_rollback:
             return
         # reversing the tuple to going backwards with the rollback
         reversed_steps = steps_to_rollback[::-1]
         istance_name = None
         try:
-            istance_name = find_key_recursively(kwargs, "new_dataset_alternate") or args[3]
+            istance_name = (
+                find_key_recursively(kwargs, "new_dataset_alternate") or args[3]
+            )
         except:
             pass
-        
-        logger.warning(f"Starting rollback for execid: {exec_id} resource published was: {istance_name}")
+
+        logger.warning(
+            f"Starting rollback for execid: {exec_id} resource published was: {istance_name}"
+        )
 
         for step in reversed_steps:
             normalized_step_name = step.split(".")[-1]
             if getattr(self, f"_{normalized_step_name}_rollback", None):
                 function = getattr(self, f"_{normalized_step_name}_rollback")
                 function(exec_id, istance_name, *args, **kwargs)
 
-        logger.warning(f"Rollback for execid: {exec_id} resource published was: {istance_name} completed")
+        logger.warning(
+            f"Rollback for execid: {exec_id} resource published was: {istance_name} completed"
+        )
 
     def _import_resource_rollback(self, exec_id, istance_name=None, *args, **kwargs):
-        '''
+        """
         In the raster, this step just generate the alternate, no real action
         are done on the database
-        '''
+        """
         pass
 
-    def _publish_resource_rollback(self, exec_id, istance_name=None, *args, **kwargs):       
-        '''
+    def _publish_resource_rollback(self, exec_id, istance_name=None, *args, **kwargs):
+        """
         We delete the resource from geoserver
-        '''
-        logger.info(f"Rollback publishing step in progress for execid: {exec_id} resource published was: {istance_name}")
+        """
+        logger.info(
+            f"Rollback publishing step in progress for execid: {exec_id} resource published was: {istance_name}"
+        )
         exec_object = orchestrator.get_execution_object(exec_id)
         handler_module_path = exec_object.input_params.get("handler_module_path")
         publisher = DataPublisher(handler_module_path=handler_module_path)
         publisher.delete_resource(istance_name)
-    
-    def _create_geonode_resource_rollback(self, exec_id, istance_name=None, *args, **kwargs):
-        '''
+
+    def _create_geonode_resource_rollback(
+        self, exec_id, istance_name=None, *args, **kwargs
+    ):
+        """
         The handler will remove the resource from geonode
-        '''
-        logger.info(f"Rollback geonode step in progress for execid: {exec_id} resource created was: {istance_name}")
+        """
+        logger.info(
+            f"Rollback geonode step in progress for execid: {exec_id} resource created was: {istance_name}"
+        )
         resource = ResourceBase.objects.filter(alternate__icontains=istance_name)
         if resource.exists():
             resource.delete()
-    
+
     def _copy_dynamic_model_rollback(self, exec_id, istance_name=None, *args, **kwargs):
         self._import_resource_rollback(exec_id, istance_name=istance_name)
-    
-    def _copy_geonode_resource_rollback(self, exec_id, istance_name=None, *args, **kwargs):
+
+    def _copy_geonode_resource_rollback(
+        self, exec_id, istance_name=None, *args, **kwargs
+    ):
         self._create_geonode_resource_rollback(exec_id, istance_name=istance_name)
 
 
 @importer_app.task(
     base=ErrorBaseTaskClass,
     name="importer.copy_raster_file",
     queue="importer.copy_raster_file",
     max_retries=1,
     acks_late=False,
     ignore_result=False,
     task_track_started=True,
 )
 def copy_raster_file(
-    exec_id,
-    actual_step,
-    layer_name,
-    alternate,
-    handler_module_path,
-    action,
-    **kwargs
+    exec_id, actual_step, layer_name, alternate, handler_module_path, action, **kwargs
 ):
     """
-    Perform a copy of the original raster file    """
+    Perform a copy of the original raster file"""
 
     original_dataset = ResourceBase.objects.filter(alternate=alternate)
     if not original_dataset.exists():
         raise InvalidGeoTiffException("Dataset required does not exists")
 
     original_dataset = original_dataset.first()
 
     if not original_dataset.files:
-        raise InvalidGeoTiffException("The original file of the dataset is not available, Is not possible to copy the dataset")
+        raise InvalidGeoTiffException(
+            "The original file of the dataset is not available, Is not possible to copy the dataset"
+        )
 
-    new_file_location = orchestrator.load_handler(handler_module_path).copy_original_file(original_dataset)
+    new_file_location = orchestrator.load_handler(
+        handler_module_path
+    ).copy_original_file(original_dataset)
 
     new_dataset_alternate = create_alternate(original_dataset.title, exec_id)
 
     additional_kwargs = {
         "original_dataset_alternate": original_dataset.alternate,
         "new_dataset_alternate": new_dataset_alternate,
-        "new_file_location": new_file_location
+        "new_file_location": new_file_location,
     }
 
     task_params = (
         {},
         exec_id,
         handler_module_path,
         actual_step,
```

## importer/handlers/common/tests_raster.py

```diff
@@ -15,17 +15,15 @@
     def setUpClass(cls):
         super().setUpClass()
         cls.handler = BaseRasterFileHandler()
         cls.valid_raster = f"{project_dir}/tests/fixture/test_grid.tif"
         cls.user, _ = get_user_model().objects.get_or_create(username="admin")
         cls.valid_files = {"base_file": cls.valid_raster}
         cls.owner = get_user_model().objects.first()
-        cls.layer = create_single_dataset(
-            name="test_grid", owner=cls.owner
-        )
+        cls.layer = create_single_dataset(name="test_grid", owner=cls.owner)
 
     def test_create_error_log(self):
         """
         Should return the formatted way for the log of the handler
         """
         actual = self.handler.create_error_log(
             Exception("my exception"),
```

## importer/handlers/common/tests_vector.py

```diff
@@ -221,15 +221,18 @@
 
         self.assertEqual("ogr2ogr", _task)
         self.assertEqual(alternate, "alternate")
         self.assertEqual(str(_uuid), execution_id)
 
         _open.assert_called_once()
         _open.assert_called_with(
-            f'/usr/bin/ogr2ogr --config PG_USE_COPY YES -f PostgreSQL PG:" dbname=\'geonode_data\' host=localhost port=5434 user=\'geonode\' password=\'geonode\' " "{self.valid_files.get("base_file")}" -lco DIM=2 -nln alternate "dataset"', stdout=-1, stderr=-1, shell=True # noqa
+            f'/usr/bin/ogr2ogr --config PG_USE_COPY YES -f PostgreSQL PG:" dbname=\'geonode_data\' host=localhost port=5434 user=\'geonode\' password=\'geonode\' " "{self.valid_files.get("base_file")}" -lco DIM=2 -nln alternate "dataset"',
+            stdout=-1,
+            stderr=-1,
+            shell=True,  # noqa
         )
 
     @patch("importer.handlers.common.vector.Popen")
     def test_import_with_ogr2ogr_with_errors_should_raise_exception(self, _open):
         _uuid = uuid.uuid4()
 
         comm = MagicMock()
@@ -244,9 +247,12 @@
                 handler_module_path=str(self.handler),
                 ovverwrite_layer=False,
                 alternate="alternate",
             )
 
         _open.assert_called_once()
         _open.assert_called_with(
-            f'/usr/bin/ogr2ogr --config PG_USE_COPY YES -f PostgreSQL PG:" dbname=\'geonode_data\' host=localhost port=5434 user=\'geonode\' password=\'geonode\' " "{self.valid_files.get("base_file")}" -lco DIM=2 -nln alternate "dataset"', stdout=-1, stderr=-1, shell=True # noqa
+            f'/usr/bin/ogr2ogr --config PG_USE_COPY YES -f PostgreSQL PG:" dbname=\'geonode_data\' host=localhost port=5434 user=\'geonode\' password=\'geonode\' " "{self.valid_files.get("base_file")}" -lco DIM=2 -nln alternate "dataset"',
+            stdout=-1,
+            stderr=-1,
+            shell=True,  # noqa
         )
```

## importer/handlers/common/vector.py

```diff
@@ -72,18 +72,18 @@
         This endpoint will return True or False if with the info provided
         the handler is able to handle the file or not
         """
         return False
 
     @staticmethod
     def has_serializer(_data) -> bool:
-        '''
+        """
         This endpoint will return True or False if with the info provided
         the handler is able to handle the file or not
-        '''
+        """
         return False
 
     @staticmethod
     def can_do(action) -> bool:
         """
         This endpoint will return True or False if with the info provided
         the handler is able to handle the file or not
@@ -153,15 +153,15 @@
 
         options = "--config PG_USE_COPY YES "
         options += (
             "-f PostgreSQL PG:\" dbname='%s' host=%s port=%s user='%s' password='%s' \" "
             % (db_name, db_host, db_port, db_user, db_password)
         )
         options += f'"{files.get("base_file")}"' + " "
-#        options += "-lco DIM=2 "
+        #        options += "-lco DIM=2 "
         options += f'-nln {alternate} "{original_name}"'
 
         if ovverwrite_layer:
             options += " -overwrite"
 
         return options
 
@@ -172,94 +172,114 @@
         """
         try:
             name = instance.alternate.split(":")[1]
             schema = None
             if os.getenv("IMPORTER_ENABLE_DYN_MODELS", False):
                 schema = ModelSchema.objects.filter(name=name).first()
             if schema:
-                '''
+                """
                 We use the schema editor directly, because the model itself is not managed
                 on creation, but for the delete since we are going to handle, we can use it
-                '''
-                _model_editor = ModelSchemaEditor(initial_model=name, db_name=schema.db_name)
+                """
+                _model_editor = ModelSchemaEditor(
+                    initial_model=name, db_name=schema.db_name
+                )
                 _model_editor.drop_table(schema.as_model())
                 ModelSchema.objects.filter(name=name).delete()
         except Exception as e:
             logger.error(f"Error during deletion of Dynamic Model schema: {e.args[0]}")
 
     @staticmethod
     def perform_last_step(execution_id):
-        '''
+        """
         Override this method if there is some extra step to perform
         before considering the execution as completed.
         For example can be used to trigger an email-send to notify
         that the execution is completed
-        '''
+        """
         # as last step, we delete the celery task to keep the number of rows under control
         lower_exec_id = execution_id.replace("-", "_").lower()
         TaskResult.objects.filter(
             Q(task_args__icontains=lower_exec_id)
             | Q(task_kwargs__icontains=lower_exec_id)
             | Q(result__icontains=lower_exec_id)
             | Q(task_args__icontains=execution_id)
             | Q(task_kwargs__icontains=execution_id)
             | Q(result__icontains=execution_id)
         ).delete()
 
         _exec = orchestrator.get_execution_object(execution_id)
 
-        _exec.output_params.update(**{
-            "detail_url": [
-                x.resource.detail_url
-                for x in ResourceHandlerInfo.objects.filter(execution_request=_exec)
-            ]
-        })
+        _exec.output_params.update(
+            **{
+                "detail_url": [
+                    x.resource.detail_url
+                    for x in ResourceHandlerInfo.objects.filter(execution_request=_exec)
+                ]
+            }
+        )
         _exec.save()
         if _exec and not _exec.input_params.get("store_spatial_file", False):
             resources = ResourceHandlerInfo.objects.filter(execution_request=_exec)
             # getting all files list
             resources_files = list(set(chain(*[x.resource.files for x in resources])))
             # better to delete each single file since it can be a remove storage service
             list(map(storage_manager.delete, resources_files))
 
-    def extract_resource_to_publish(self, files, action, layer_name, alternate, **kwargs):
+    def extract_resource_to_publish(
+        self, files, action, layer_name, alternate, **kwargs
+    ):
         if action == exa.COPY.value:
             return [
                 {
                     "name": alternate,
-                    "crs": ResourceBase.objects.filter(Q(alternate__icontains=layer_name) | Q(title__icontains=layer_name))
+                    "crs": ResourceBase.objects.filter(
+                        Q(alternate__icontains=layer_name)
+                        | Q(title__icontains=layer_name)
+                    )
                     .first()
                     .srid,
                 }
             ]
 
         layers = self.get_ogr2ogr_driver().Open(files.get("base_file"))
         if not layers:
             return []
         return [
             {
                 "name": alternate or layer_name,
-                "crs": self.identify_authority(_l) if _l.GetSpatialRef() else None
+                "crs": self.identify_authority(_l) if _l.GetSpatialRef() else None,
             }
             for _l in layers
             if self.fixup_name(_l.GetName()) == layer_name
         ]
 
     def identify_authority(self, layer):
         try:
             layer_wkt = layer.GetSpatialRef().ExportToWkt()
             _name = "EPSG"
             _code = pyproj.CRS(layer_wkt).to_epsg(min_confidence=20)
             if _code is None:
-                raise Exception("authority code not found, fallback to default behaviour")
+                layer_proj4 = layer.GetSpatialRef().ExportToProj4()
+                _code = pyproj.CRS(layer_proj4).to_epsg(min_confidence=20)
+                if _code is None:
+                    raise Exception(
+                        "CRS authority code not found, fallback to default behaviour"
+                    )
         except:
             spatial_ref = layer.GetSpatialRef()
             spatial_ref.AutoIdentifyEPSG()
-            _name = spatial_ref.GetAuthorityName(None) or spatial_ref.GetAttrValue('AUTHORITY', 0)
-            _code = spatial_ref.GetAuthorityCode('PROJCS') or spatial_ref.GetAuthorityCode('GEOGCS') or spatial_ref.GetAttrValue('AUTHORITY', 1)
+            _name = spatial_ref.GetAuthorityName(None) or spatial_ref.GetAttrValue(
+                "AUTHORITY", 0
+            )
+            _code = (
+                spatial_ref.GetAuthorityCode("PROJCS")
+                or spatial_ref.GetAuthorityCode("GEOGCS")
+                or spatial_ref.GetAttrValue("AUTHORITY", 1)
+            )
         return f"{_name}:{_code}"
 
     def get_ogr2ogr_driver(self):
         """
         Should return the Driver object that is used to open the layers via OGR2OGR
         """
         return None
@@ -272,46 +292,58 @@
         """
         layers = self.get_ogr2ogr_driver().Open(files.get("base_file"))
         # for the moment we skip the dyanamic model creation
         layer_count = len(layers)
         logger.info(f"Total number of layers available: {layer_count}")
         _exec = self._get_execution_request_object(execution_id)
         _input = {**_exec.input_params, **{"total_layers": layer_count}}
-        orchestrator.update_execution_request_status(execution_id=str(execution_id), input_params=_input)
+        orchestrator.update_execution_request_status(
+            execution_id=str(execution_id), input_params=_input
+        )
         dynamic_model = None
         celery_group = None
         try:
             # start looping on the layers available
             for index, layer in enumerate(layers, start=1):
-
                 layer_name = self.fixup_name(layer.GetName())
 
-                should_be_overwritten = _exec.input_params.get("overwrite_existing_layer")
+                should_be_overwritten = _exec.input_params.get(
+                    "overwrite_existing_layer"
+                )
                 # should_be_imported check if the user+layername already exists or not
                 if (
                     should_be_imported(
                         layer_name,
                         _exec.user,
                         skip_existing_layer=_exec.input_params.get(
                             "skip_existing_layer"
                         ),
                         overwrite_existing_layer=should_be_overwritten,
                     )
-                    #and layer.GetGeometryColumn() is not None
+                    # and layer.GetGeometryColumn() is not None
                 ):
                     # update the execution request object
                     # setup dynamic model and retrieve the group task needed for tun the async workflow
-                    # create the async task for create the resource into geonode_data with ogr2ogr                    
+                    # create the async task for create the resource into geonode_data with ogr2ogr
                     if os.getenv("IMPORTER_ENABLE_DYN_MODELS", False):
-                        dynamic_model, alternate, celery_group = self.setup_dynamic_model(
-                            layer, execution_id, should_be_overwritten, username=_exec.user
+                        (
+                            dynamic_model,
+                            alternate,
+                            celery_group,
+                        ) = self.setup_dynamic_model(
+                            layer,
+                            execution_id,
+                            should_be_overwritten,
+                            username=_exec.user,
                         )
                     else:
-                        alternate = self.find_alternate_by_dataset(_exec, layer_name, should_be_overwritten)
-                    
+                        alternate = self.find_alternate_by_dataset(
+                            _exec, layer_name, should_be_overwritten
+                        )
+
                     ogr_res = self.get_ogr2ogr_task_group(
                         execution_id,
                         files,
                         layer.GetName().lower(),
                         should_be_overwritten,
                         alternate,
                     )
@@ -332,15 +364,15 @@
                     workflow = chord(group_to_call)(
                         import_next_step.s(
                             execution_id,
                             str(self),  # passing the handler module path
                             "importer.import_resource",
                             layer_name,
                             alternate,
-                            **kwargs
+                            **kwargs,
                         )
                     )
         except Exception as e:
             logger.error(e)
             if dynamic_model:
                 """
                 In case of fail, we want to delete the dynamic_model schema and his field
@@ -348,27 +380,29 @@
                 """
                 drop_dynamic_model_schema(dynamic_model)
             raise e
         return
 
     def find_alternate_by_dataset(self, _exec_obj, layer_name, should_be_overwritten):
         workspace = get_geoserver_cascading_workspace(create=False)
-        dataset_available = Dataset.objects.filter(alternate__iexact=f"{workspace.name}:{layer_name}")
+        dataset_available = Dataset.objects.filter(
+            alternate__iexact=f"{workspace.name}:{layer_name}"
+        )
 
         dataset_exists = dataset_available.exists()
 
         if dataset_exists and should_be_overwritten:
             alternate = dataset_available.first().alternate
         elif not dataset_exists:
             alternate = layer_name
         else:
             alternate = create_alternate(layer_name, str(_exec_obj.exec_id))
 
         return alternate
-        
+
     def setup_dynamic_model(
         self,
         layer: ogr.Layer,
         execution_id: str,
         should_be_overwritten: bool,
         username: str,
     ):
@@ -394,29 +428,29 @@
         if dataset_exists and dynamic_schema_exists and should_be_overwritten:
             """
             If the user have a dataset, the dynamic model has already been created and is in overwrite mode,
             we just take the dynamic_model to overwrite the existing one
             """
             dynamic_schema = dynamic_schema.get()
         elif not dataset_exists and not dynamic_schema_exists:
-            '''
+            """
             cames here when is a new brand upload or when (for any reasons) the dataset exists but the
             dynamic model has not been created before
-            '''
+            """
             #  layer_name = create_alternate(layer_name, execution_id)
             dynamic_schema = ModelSchema.objects.create(
                 name=layer_name,
                 db_name="datastore",
                 managed=False,
                 db_table_name=layer_name,
             )
-        elif (not dataset_exists and dynamic_schema_exists) or (
-            dataset_exists and dynamic_schema_exists and not should_be_overwritten
-        ) or (
-            dataset_exists and not dynamic_schema_exists
+        elif (
+            (not dataset_exists and dynamic_schema_exists)
+            or (dataset_exists and dynamic_schema_exists and not should_be_overwritten)
+            or (dataset_exists and not dynamic_schema_exists)
         ):
             """
             it comes here when the layer should not be overrided so we append the UUID
             to the layer to let it proceed to the next steps
             """
             layer_name = create_alternate(layer_name, execution_id)
             dynamic_schema, _ = ModelSchema.objects.get_or_create(
@@ -449,51 +483,70 @@
         layer_name: str,
     ):
         # retrieving the field schema from ogr2ogr and converting the type to Django Types
         layer_schema = [
             {"name": x.name.lower(), "class_name": self._get_type(x), "null": True}
             for x in layer.schema
         ]
-        if layer.GetGeometryColumn() or self.default_geometry_column_name and ogr.GeometryTypeToName(layer.GetGeomType()) not in ['Geometry Collection', 'Unknown (any)', 'None']:
+        if (
+            layer.GetGeometryColumn()
+            or self.default_geometry_column_name
+            and ogr.GeometryTypeToName(layer.GetGeomType())
+            not in ["Geometry Collection", "Unknown (any)", "None"]
+        ):
             # the geometry colum is not returned rom the layer.schema, so we need to extract it manually
             layer_schema += [
                 {
-                    "name": layer.GetGeometryColumn() or self.default_geometry_column_name,
-                    "class_name": GEOM_TYPE_MAPPING.get(self.promote_to_multi(ogr.GeometryTypeToName(layer.GetGeomType()))),
-                    "dim": 2 if not ogr.GeometryTypeToName(layer.GetGeomType()).lower().startswith('3d') else 3
+                    "name": layer.GetGeometryColumn()
+                    or self.default_geometry_column_name,
+                    "class_name": GEOM_TYPE_MAPPING.get(
+                        self.promote_to_multi(
+                            ogr.GeometryTypeToName(layer.GetGeomType())
+                        )
+                    ),
+                    "dim": 2
+                    if not ogr.GeometryTypeToName(layer.GetGeomType())
+                    .lower()
+                    .startswith("3d")
+                    else 3,
                 }
             ]
 
         # ones we have the schema, here we create a list of chunked value
         # so the async task will handle max of 30 field per task
         list_chunked = [
-            layer_schema[i: i + 30] for i in range(0, len(layer_schema), 30)
+            layer_schema[i : i + 30] for i in range(0, len(layer_schema), 30)
         ]
 
         # definition of the celery group needed to run the async workflow.
         # in this way each task of the group will handle only 30 field
         celery_group = group(
             create_dynamic_structure.s(
                 execution_id, schema, dynamic_model_schema.id, overwrite, layer_name
             )
             for schema in list_chunked
         )
 
         return dynamic_model_schema, celery_group
 
     def promote_to_multi(self, geometry_name: str):
-        '''
+        """
         If needed change the name of the geometry, by promoting it to Multi
         example if is Point -> MultiPoint
         Needed for the shapefiles
-        '''
+        """
         return geometry_name
 
     def create_geonode_resource(
-        self, layer_name: str, alternate: str, execution_id: str, resource_type: Dataset = Dataset, files=None
+        self,
+        layer_name: str,
+        alternate: str,
+        execution_id: str,
+        resource_type: Dataset = Dataset,
+        files=None,
     ):
         """
         Base function to create the resource into geonode. Each handler can specify
         and handle the resource in a different way
         """
         saved_dataset = resource_type.objects.filter(alternate__icontains=alternate)
 
@@ -520,15 +573,20 @@
                 workspace=workspace,
                 store=os.environ.get("GEONODE_GEODATABASE", "geonode_data"),
                 subtype="vector",
                 alternate=f"{workspace}:{alternate}",
                 dirty_state=True,
                 title=layer_name,
                 owner=_exec.user,
-                files=list(set(list(_exec.input_params.get("files", {}).values()) or list(files))),
+                files=list(
+                    set(
+                        list(_exec.input_params.get("files", {}).values())
+                        or list(files)
+                    )
+                ),
             ),
         )
 
         saved_dataset.refresh_from_db()
 
         self.handle_xml_file(saved_dataset, _exec)
         self.handle_sld_file(saved_dataset, _exec)
@@ -537,40 +595,50 @@
 
         ResourceBase.objects.filter(alternate=alternate).update(dirty_state=False)
 
         saved_dataset.refresh_from_db()
         return saved_dataset
 
     def overwrite_geonode_resource(
-        self, layer_name: str, alternate: str, execution_id: str, resource_type: Dataset = Dataset, files=None
+        self,
+        layer_name: str,
+        alternate: str,
+        execution_id: str,
+        resource_type: Dataset = Dataset,
+        files=None,
     ):
-
         dataset = resource_type.objects.filter(alternate__icontains=alternate)
 
         _exec = self._get_execution_request_object(execution_id)
 
         _overwrite = _exec.input_params.get("overwrite_existing_layer", False)
         # if the layer exists, we just update the information of the dataset by
         # let it recreate the catalogue
         if dataset.exists() and _overwrite:
             dataset = dataset.first()
 
-            dataset = resource_manager.update(dataset.uuid, instance=dataset, files=files)
+            dataset = resource_manager.update(
+                dataset.uuid, instance=dataset, files=files
+            )
 
             self.handle_xml_file(dataset, _exec)
             self.handle_sld_file(dataset, _exec)
 
-            resource_manager.set_thumbnail(dataset.uuid, instance=dataset, overwrite=False)
+            resource_manager.set_thumbnail(
+                dataset.uuid, instance=dataset, overwrite=False
+            )
             dataset.refresh_from_db()
             return dataset
         elif not dataset.exists() and _overwrite:
             logger.warning(
                 f"The dataset required {alternate} does not exists, but an overwrite is required, the resource will be created"
             )
-            return self.create_geonode_resource(layer_name, alternate, execution_id, resource_type, files)
+            return self.create_geonode_resource(
+                layer_name, alternate, execution_id, resource_type, files
+            )
         elif not dataset.exists() and not _overwrite:
             logger.warning(
                 "The resource does not exists, please use 'create_geonode_resource' to create one"
             )
         return
 
     def handle_xml_file(self, saved_dataset: Dataset, _exec: ExecutionRequest):
@@ -590,54 +658,79 @@
             None,
             instance=saved_dataset,
             sld_file=_exec.input_params.get("files", {}).get("sld_file", ""),
             sld_uploaded=True if _path else False,
             vals={"dirty_state": True},
         )
 
-    def create_resourcehandlerinfo(self, handler_module_path: str, resource: Dataset, execution_id: ExecutionRequest, **kwargs):
+    def create_resourcehandlerinfo(
+        self,
+        handler_module_path: str,
+        resource: Dataset,
+        execution_id: ExecutionRequest,
+        **kwargs,
+    ):
         """
         Create relation between the GeonodeResource and the handler used
         to create/copy it
         """
         ResourceHandlerInfo.objects.create(
             handler_module_path=handler_module_path,
             resource=resource,
             execution_request=execution_id,
-            kwargs=kwargs.get('kwargs', {}) or kwargs
+            kwargs=kwargs.get("kwargs", {}) or kwargs,
         )
 
-    def overwrite_resourcehandlerinfo(self, handler_module_path: str, resource: Dataset, execution_id: ExecutionRequest, **kwargs):
+    def overwrite_resourcehandlerinfo(
+        self,
+        handler_module_path: str,
+        resource: Dataset,
+        execution_id: ExecutionRequest,
+        **kwargs,
+    ):
         """
         Overwrite the ResourceHandlerInfo
         """
         if resource.resourcehandlerinfo_set.exists():
             resource.resourcehandlerinfo_set.update(
                 handler_module_path=handler_module_path,
                 resource=resource,
                 execution_request=execution_id,
-                kwargs=kwargs.get('kwargs', {}) or kwargs
+                kwargs=kwargs.get("kwargs", {}) or kwargs,
             )
             return
-        return self.create_resourcehandlerinfo(handler_module_path, resource, execution_id, **kwargs)
+        return self.create_resourcehandlerinfo(
+            handler_module_path, resource, execution_id, **kwargs
+        )
 
     def copy_geonode_resource(
-        self, alternate: str, resource: Dataset, _exec: ExecutionRequest, data_to_update: dict, new_alternate: str, **kwargs
+        self,
+        alternate: str,
+        resource: Dataset,
+        _exec: ExecutionRequest,
+        data_to_update: dict,
+        new_alternate: str,
+        **kwargs,
     ):
         resource = self.create_geonode_resource(
             layer_name=data_to_update.get("title"),
             alternate=new_alternate,
             execution_id=str(_exec.exec_id),
-            files=resource.files
+            files=resource.files,
         )
         resource.refresh_from_db()
         return resource
 
     def get_ogr2ogr_task_group(
-        self, execution_id: str, files: dict, layer, should_be_overwritten: bool, alternate: str
+        self,
+        execution_id: str,
+        files: dict,
+        layer,
+        should_be_overwritten: bool,
+        alternate: str,
     ):
         """
         In case the OGR2OGR is different from the default one, is enough to ovverride this method
         and return the celery task object needed
         """
         handler_module_path = str(self)
         return import_with_ogr2ogr.s(
@@ -654,87 +747,113 @@
 
     def _get_type(self, _type: str):
         """
         Used to get the standard field type in the dynamic_model_field definition
         """
         return STANDARD_TYPE_MAPPING.get(ogr.FieldDefn.GetTypeName(_type))
 
-    def rollback(self, exec_id, rollback_from_step, action_to_rollback, *args, **kwargs):
-        
+    def rollback(
+        self, exec_id, rollback_from_step, action_to_rollback, *args, **kwargs
+    ):
         steps = self.ACTIONS.get(action_to_rollback)
         step_index = steps.index(rollback_from_step)
         # the start_import, start_copy etc.. dont do anything as step, is just the start
         # so there is nothing to rollback
-        steps_to_rollback = steps[1:step_index+1]
+        steps_to_rollback = steps[1 : step_index + 1]
         if not steps_to_rollback:
             return
         # reversing the tuple to going backwards with the rollback
         reversed_steps = steps_to_rollback[::-1]
         instance_name = None
         try:
-            instance_name = find_key_recursively(kwargs, "new_dataset_alternate") or args[3]
+            instance_name = (
+                find_key_recursively(kwargs, "new_dataset_alternate") or args[3]
+            )
         except:
             pass
-        
-        logger.warning(f"Starting rollback for execid: {exec_id} resource published was: {instance_name}")
+
+        logger.warning(
+            f"Starting rollback for execid: {exec_id} resource published was: {instance_name}"
+        )
 
         for step in reversed_steps:
             normalized_step_name = step.split(".")[-1]
             if getattr(self, f"_{normalized_step_name}_rollback", None):
                 function = getattr(self, f"_{normalized_step_name}_rollback")
                 function(exec_id, instance_name, *args, **kwargs)
 
-        logger.warning(f"Rollback for execid: {exec_id} resource published was: {instance_name} completed")
+        logger.warning(
+            f"Rollback for execid: {exec_id} resource published was: {instance_name} completed"
+        )
 
     def _import_resource_rollback(self, exec_id, instance_name=None, *args, **kwargs):
-        '''
+        """
         We use the schema editor directly, because the model itself is not managed
         on creation, but for the delete since we are going to handle, we can use it
-        '''
-        logger.info(f"Rollback dynamic model & ogr2ogr step in progress for execid: {exec_id} resource published was: {instance_name}")
+        """
+        logger.info(
+            f"Rollback dynamic model & ogr2ogr step in progress for execid: {exec_id} resource published was: {instance_name}"
+        )
         schema = None
         if os.getenv("IMPORTER_ENABLE_DYN_MODELS", False):
             schema = ModelSchema.objects.filter(name=instance_name).first()
         if schema is not None:
-            _model_editor = ModelSchemaEditor(initial_model=instance_name, db_name=schema.db_name)
+            _model_editor = ModelSchemaEditor(
+                initial_model=instance_name, db_name=schema.db_name
+            )
             _model_editor.drop_table(schema.as_model())
             ModelSchema.objects.filter(name=instance_name).delete()
         elif schema is None:
             try:
-                logger.info("Dynamic model does not exists, removing ogr2ogr table in progress")
+                logger.info(
+                    "Dynamic model does not exists, removing ogr2ogr table in progress"
+                )
+                if instance_name is None:
+                    logger.info("No table created, skipping...")
+                    return
                 db_name = os.getenv("DEFAULT_BACKEND_DATASTORE", "datastore")
                 with connections[db_name].cursor() as cursor:
                     cursor.execute(f"DROP TABLE {instance_name}")
             except Exception as e:
                 logger.info(e)
                 pass
 
-    def _publish_resource_rollback(self, exec_id, instance_name=None, *args, **kwargs):       
-        '''
+    def _publish_resource_rollback(self, exec_id, instance_name=None, *args, **kwargs):
+        """
         We delete the resource from geoserver
-        '''
-        logger.info(f"Rollback publishing step in progress for execid: {exec_id} resource published was: {instance_name}")
+        """
+        logger.info(
+            f"Rollback publishing step in progress for execid: {exec_id} resource published was: {instance_name}"
+        )
         exec_object = orchestrator.get_execution_object(exec_id)
         handler_module_path = exec_object.input_params.get("handler_module_path")
         publisher = DataPublisher(handler_module_path=handler_module_path)
         publisher.delete_resource(instance_name)
-    
-    def _create_geonode_resource_rollback(self, exec_id, instance_name=None, *args, **kwargs):
-        '''
+
+    def _create_geonode_resource_rollback(
+        self, exec_id, instance_name=None, *args, **kwargs
+    ):
+        """
         The handler will remove the resource from geonode
-        '''
-        logger.info(f"Rollback geonode step in progress for execid: {exec_id} resource created was: {instance_name}")
+        """
+        logger.info(
+            f"Rollback geonode step in progress for execid: {exec_id} resource created was: {instance_name}"
+        )
         resource = ResourceBase.objects.filter(alternate__icontains=instance_name)
         if resource.exists():
             resource.delete()
-    
-    def _copy_dynamic_model_rollback(self, exec_id, instance_name=None, *args, **kwargs):
+
+    def _copy_dynamic_model_rollback(
+        self, exec_id, instance_name=None, *args, **kwargs
+    ):
         self._import_resource_rollback(exec_id, instance_name=instance_name)
-    
-    def _copy_geonode_resource_rollback(self, exec_id, instance_name=None, *args, **kwargs):
+
+    def _copy_geonode_resource_rollback(
+        self, exec_id, instance_name=None, *args, **kwargs
+    ):
         self._create_geonode_resource_rollback(exec_id, instance_name=instance_name)
 
 
 @importer_app.task(
     base=ErrorBaseTaskClass,
     name="importer.import_next_step",
     queue="importer.import_next_step",
@@ -743,20 +862,21 @@
 def import_next_step(
     _,
     execution_id: str,
     handlers_module_path: str,
     actual_step: str,
     layer_name: str,
     alternate: str,
-    **kwargs: dict
+    **kwargs: dict,
 ):
     """
     If the ingestion of the resource is successfuly, the next step for the layer is called
     """
     from importer.celery_tasks import import_orchestrator
+
     try:
         _exec = orchestrator.get_execution_object(execution_id)
 
         _files = _exec.input_params.get("files")
         # at the end recall the import_orchestrator for the next step
 
         task_params = (
@@ -774,22 +894,21 @@
         call_rollback_function(
             execution_id,
             handlers_module_path=handlers_module_path,
             prev_action=exa.IMPORT.value,
             layer=layer_name,
             alternate=alternate,
             error=e,
-            **kwargs      
+            **kwargs,
         )
 
     finally:
         return "import_next_step", alternate, execution_id
 
 
-
 @importer_app.task(
     base=SingleMessageErrorHandler,
     name="importer.import_with_ogr2ogr",
     queue="importer.import_with_ogr2ogr",
     max_retries=1,
     acks_late=False,
     ignore_result=False,
@@ -814,15 +933,20 @@
             files, original_name, ovverwrite_layer, alternate
         )
 
         commands = [ogr_exe] + options.split(" ")
 
         process = Popen(" ".join(commands), stdout=PIPE, stderr=PIPE, shell=True)
         stdout, stderr = process.communicate()
-        if stderr is not None and stderr != b"" and b"ERROR" in stderr or b'Syntax error' in stderr:
+        if (
+            stderr is not None
+            and stderr != b""
+            and b"ERROR" in stderr
+            or b"Syntax error" in stderr
+        ):
             try:
                 err = stderr.decode()
             except Exception:
                 err = stderr.decode("latin1")
             logger.error(f"Original error returned: {err}")
             message = normalize_ogr2ogr_error(err, original_name)
             raise Exception(f"{message} for layer {alternate}")
@@ -831,15 +955,17 @@
         call_rollback_function(
             execution_id,
             handlers_module_path=handler_module_path,
             prev_action=exa.IMPORT.value,
             layer=original_name,
             alternate=alternate,
             error=e,
-            **{}      
+            **{},
         )
         raise Exception(e)
 
 
 def normalize_ogr2ogr_error(err, original_name):
-    getting_errors = [y for y in err.split('\n') if 'ERROR ' in y]
-    return ', '.join([x.split(original_name)[0] for x in getting_errors if 'ERROR' in x])
+    getting_errors = [y for y in err.split("\n") if "ERROR " in y]
+    return ", ".join(
+        [x.split(original_name)[0] for x in getting_errors if "ERROR" in x]
+    )
```

## importer/handlers/csv/handler.py

```diff
@@ -38,22 +38,29 @@
         ),
         ira.ROLLBACK.value: (
             "start_rollback",
             "importer.rollback",
         ),
     }
 
-    possible_geometry_column_name = ['geom', 'geometry', 'wkt_geom', 'the_geom']
-    possible_lat_column = ['latitude', 'lat', 'y']
-    possible_long_column = ['longitude', 'long', 'x']
+    possible_geometry_column_name = ["geom", "geometry", "wkt_geom", "the_geom"]
+    possible_lat_column = ["latitude", "lat", "y"]
+    possible_long_column = ["longitude", "long", "x"]
     possible_latlong_column = possible_lat_column + possible_long_column
 
     @property
     def supported_file_extension_config(self):
-        return {"id": "csv", "label": "CSV", "format": "vector", "mimeType": ["text/csv"], "ext": ["csv"], "optional": ["sld", "xml"]}
+        return {
+            "id": "csv",
+            "label": "CSV",
+            "format": "vector",
+            "mimeType": ["text/csv"],
+            "ext": ["csv"],
+            "optional": ["sld", "xml"],
+        }
 
     @staticmethod
     def can_handle(_data) -> bool:
         """
         This endpoint will return True or False if with the info provided
         the handler is able to handle the file or not
         """
@@ -89,121 +96,155 @@
                 f"please upload a smaller file"
             )
         elif layers_count + actual_upload >= max_upload:
             raise UploadParallelismLimitException(
                 detail=f"With the provided CSV, the number of max parallel upload will exceed the limit of {max_upload}"
             )
 
-        schema_keys = [
-            x.name.lower()
-            for layer in layers
-            for x in layer.schema
-        ]
-        geom_is_in_schema = any(x in schema_keys for x in CSVFileHandler().possible_geometry_column_name)
+        schema_keys = [x.name.lower() for layer in layers for x in layer.schema]
+        geom_is_in_schema = any(
+            x in schema_keys for x in CSVFileHandler().possible_geometry_column_name
+        )
         has_lat = any(x in CSVFileHandler().possible_lat_column for x in schema_keys)
         has_long = any(x in CSVFileHandler().possible_long_column for x in schema_keys)
 
-        fields = CSVFileHandler().possible_geometry_column_name + CSVFileHandler().possible_latlong_column
+        fields = (
+            CSVFileHandler().possible_geometry_column_name
+            + CSVFileHandler().possible_latlong_column
+        )
         if has_lat and not has_long:
-            raise InvalidCSVException(f"Longitude is missing. Supported names: {', '.join(CSVFileHandler().possible_long_column)}")
+            raise InvalidCSVException(
+                f"Longitude is missing. Supported names: {', '.join(CSVFileHandler().possible_long_column)}"
+            )
 
         if not has_lat and has_long:
-            raise InvalidCSVException(f"Latitude is missing. Supported names: {', '.join(CSVFileHandler().possible_lat_column)}")
+            raise InvalidCSVException(
+                f"Latitude is missing. Supported names: {', '.join(CSVFileHandler().possible_lat_column)}"
+            )
 
         if not geom_is_in_schema and not has_lat and not has_long:
-            raise InvalidCSVException(f"Not enough geometry field are set. The possibilities are: {','.join(fields)}")
+            raise InvalidCSVException(
+                f"Not enough geometry field are set. The possibilities are: {','.join(fields)}"
+            )
 
         return True
 
     def get_ogr2ogr_driver(self):
         return ogr.GetDriverByName("CSV")
 
     @staticmethod
     def create_ogr2ogr_command(files, original_name, ovverwrite_layer, alternate):
-        '''
+        """
         Define the ogr2ogr command to be executed.
         This is a default command that is needed to import a vector file
-        '''
-        base_command = BaseVectorFileHandler.create_ogr2ogr_command(files, original_name, ovverwrite_layer, alternate)
+        """
+        base_command = BaseVectorFileHandler.create_ogr2ogr_command(
+            files, original_name, ovverwrite_layer, alternate
+        )
         additional_option = ' -oo "GEOM_POSSIBLE_NAMES=geom*,the_geom*,wkt_geom" -oo "X_POSSIBLE_NAMES=x,long*" -oo "Y_POSSIBLE_NAMES=y,lat*"'
-        return f"{base_command } -oo KEEP_GEOM_COLUMNS=NO -lco GEOMETRY_NAME={BaseVectorFileHandler().default_geometry_column_name} " + additional_option
+        return (
+            f"{base_command } -oo KEEP_GEOM_COLUMNS=NO -lco GEOMETRY_NAME={BaseVectorFileHandler().default_geometry_column_name} "
+            + additional_option
+        )
 
     def create_dynamic_model_fields(
         self,
         layer: str,
         dynamic_model_schema: ModelSchema,
         overwrite: bool,
         execution_id: str,
         layer_name: str,
     ):
         # retrieving the field schema from ogr2ogr and converting the type to Django Types
         layer_schema = [
             {"name": x.name.lower(), "class_name": self._get_type(x), "null": True}
             for x in layer.schema
         ]
-        if layer.GetGeometryColumn() or self.default_geometry_column_name and ogr.GeometryTypeToName(layer.GetGeomType()) not in ['Geometry Collection', 'Unknown (any)']:
+        if (
+            layer.GetGeometryColumn()
+            or self.default_geometry_column_name
+            and ogr.GeometryTypeToName(layer.GetGeomType())
+            not in ["Geometry Collection", "Unknown (any)"]
+        ):
             # the geometry colum is not returned rom the layer.schema, so we need to extract it manually
             # checking if the geometry has been wrogly read as string
-            schema_keys = [x['name'] for x in layer_schema]
-            geom_is_in_schema = (x in schema_keys for x in self.possible_geometry_column_name)
-            if any(geom_is_in_schema) and layer.GetGeomType() == 100:  # 100 means None so Geometry not found
-                field_name = [x for x in self.possible_geometry_column_name if x in schema_keys][0]
+            schema_keys = [x["name"] for x in layer_schema]
+            geom_is_in_schema = (
+                x in schema_keys for x in self.possible_geometry_column_name
+            )
+            if (
+                any(geom_is_in_schema) and layer.GetGeomType() == 100
+            ):  # 100 means None so Geometry not found
+                field_name = [
+                    x for x in self.possible_geometry_column_name if x in schema_keys
+                ][0]
                 index = layer.GetFeature(1).keys().index(field_name)
                 geom = [x for x in layer.GetFeature(1)][index]
                 class_name = GEOM_TYPE_MAPPING.get(
-                    self.promote_to_multi(
-                        geom.split("(")[0].replace(" ", "").title()
-                    )
+                    self.promote_to_multi(geom.split("(")[0].replace(" ", "").title())
                 )
-                layer_schema = [x for x in layer_schema if field_name not in x['name']]
+                layer_schema = [x for x in layer_schema if field_name not in x["name"]]
             elif any(x in self.possible_latlong_column for x in schema_keys):
-                class_name = GEOM_TYPE_MAPPING.get(self.promote_to_multi('Point'))
+                class_name = GEOM_TYPE_MAPPING.get(self.promote_to_multi("Point"))
             else:
-                class_name = GEOM_TYPE_MAPPING.get(self.promote_to_multi(ogr.GeometryTypeToName(layer.GetGeomType())))
+                class_name = GEOM_TYPE_MAPPING.get(
+                    self.promote_to_multi(ogr.GeometryTypeToName(layer.GetGeomType()))
+                )
 
             layer_schema += [
                 {
-                    "name": layer.GetGeometryColumn() or self.default_geometry_column_name,
+                    "name": layer.GetGeometryColumn()
+                    or self.default_geometry_column_name,
                     "class_name": class_name,
-                    "dim": 2 if not ogr.GeometryTypeToName(layer.GetGeomType()).lower().startswith('3d') else 3
+                    "dim": 2
+                    if not ogr.GeometryTypeToName(layer.GetGeomType())
+                    .lower()
+                    .startswith("3d")
+                    else 3,
                 }
             ]
 
         # ones we have the schema, here we create a list of chunked value
         # so the async task will handle max of 30 field per task
         list_chunked = [
-            layer_schema[i: i + 30] for i in range(0, len(layer_schema), 30)
+            layer_schema[i : i + 30] for i in range(0, len(layer_schema), 30)
         ]
 
         # definition of the celery group needed to run the async workflow.
         # in this way each task of the group will handle only 30 field
         celery_group = group(
             create_dynamic_structure.s(
                 execution_id, schema, dynamic_model_schema.id, overwrite, layer_name
             )
             for schema in list_chunked
         )
 
         return dynamic_model_schema, celery_group
 
-    def extract_resource_to_publish(self, files, action, layer_name, alternate, **kwargs):
+    def extract_resource_to_publish(
+        self, files, action, layer_name, alternate, **kwargs
+    ):
         if action == exa.COPY.value:
             return [
                 {
                     "name": alternate,
-                    "crs": ResourceBase.objects.filter(alternate__istartswith=layer_name)
+                    "crs": ResourceBase.objects.filter(
+                        alternate__istartswith=layer_name
+                    )
                     .first()
                     .srid,
                 }
             ]
 
         layers = self.get_ogr2ogr_driver().Open(files.get("base_file"), 0)
         if not layers:
             return []
         return [
             {
                 "name": alternate or layer_name,
-                "crs": self.identify_authority(_l) if _l.GetSpatialRef() else 'EPSG:4326'
+                "crs": self.identify_authority(_l)
+                if _l.GetSpatialRef()
+                else "EPSG:4326",
             }
             for _l in layers
             if self.fixup_name(_l.GetName()) == layer_name
         ]
```

## importer/handlers/csv/tests.py

```diff
@@ -25,17 +25,15 @@
         cls.missing_lat = f"{project_dir}/tests/fixture/missing_lat.csv"
         cls.missing_long = f"{project_dir}/tests/fixture/missing_long.csv"
         cls.missing_geom = f"{project_dir}/tests/fixture/missing_geom.csv"
         cls.user, _ = get_user_model().objects.get_or_create(username="admin")
         cls.invalid_files = {"base_file": cls.invalid_csv}
         cls.valid_files = {"base_file": cls.valid_csv}
         cls.owner = get_user_model().objects.first()
-        cls.layer = create_single_dataset(
-            name="test", owner=cls.owner
-        )
+        cls.layer = create_single_dataset(name="test", owner=cls.owner)
 
     def test_task_list_is_the_expected_one(self):
         expected = (
             "start_import",
             "importer.import_resource",
             "importer.publish_resource",
             "importer.create_geonode_resource",
@@ -56,47 +54,43 @@
 
     def test_is_valid_should_raise_exception_if_the_csv_is_invalid(self):
         with self.assertRaises(InvalidCSVException) as _exc:
             self.handler.is_valid(files=self.invalid_files, user=self.user)
 
         self.assertIsNotNone(_exc)
         self.assertTrue(
-            "The CSV provided is invalid, no layers found"
-            in str(_exc.exception.detail)
+            "The CSV provided is invalid, no layers found" in str(_exc.exception.detail)
         )
 
     def test_is_valid_should_raise_exception_if_the_csv_missing_geom(self):
         with self.assertRaises(InvalidCSVException) as _exc:
-            self.handler.is_valid(files={"base_file": self.missing_geom}, user=self.user)
+            self.handler.is_valid(
+                files={"base_file": self.missing_geom}, user=self.user
+            )
 
         self.assertIsNotNone(_exc)
         self.assertTrue(
-            "Not enough geometry field are set"
-            in str(_exc.exception.detail)
+            "Not enough geometry field are set" in str(_exc.exception.detail)
         )
 
     def test_is_valid_should_raise_exception_if_the_csv_missing_lat(self):
         with self.assertRaises(InvalidCSVException) as _exc:
             self.handler.is_valid(files={"base_file": self.missing_lat}, user=self.user)
 
         self.assertIsNotNone(_exc)
-        self.assertTrue(
-            "Latitude is missing"
-            in str(_exc.exception.detail)
-        )
+        self.assertTrue("Latitude is missing" in str(_exc.exception.detail))
 
     def test_is_valid_should_raise_exception_if_the_csv_missing_long(self):
         with self.assertRaises(InvalidCSVException) as _exc:
-            self.handler.is_valid(files={"base_file": self.missing_long}, user=self.user)
+            self.handler.is_valid(
+                files={"base_file": self.missing_long}, user=self.user
+            )
 
         self.assertIsNotNone(_exc)
-        self.assertTrue(
-            "Longitude is missing"
-            in str(_exc.exception.detail)
-        )
+        self.assertTrue("Longitude is missing" in str(_exc.exception.detail))
 
     def test_is_valid_should_raise_exception_if_the_parallelism_is_met(self):
         parallelism, created = UploadParallelismLimit.objects.get_or_create(
             slug="default_max_parallel_uploads"
         )
         old_value = parallelism.max_number
         try:
@@ -142,32 +136,37 @@
         actual = self.handler.can_handle(self.valid_files)
         self.assertTrue(actual)
 
     def test_can_handle_should_return_false_for_other_files(self):
         actual = self.handler.can_handle({"base_file": "random.file"})
         self.assertFalse(actual)
 
-    @patch('importer.handlers.common.vector.Popen')
-    def test_import_with_ogr2ogr_without_errors_should_call_the_right_command(self, _open):
+    @patch("importer.handlers.common.vector.Popen")
+    def test_import_with_ogr2ogr_without_errors_should_call_the_right_command(
+        self, _open
+    ):
         _uuid = uuid.uuid4()
 
         comm = MagicMock()
         comm.communicate.return_value = b"", b""
         _open.return_value = comm
 
         _task, alternate, execution_id = import_with_ogr2ogr(
             execution_id=str(_uuid),
             files=self.valid_files,
             original_name="dataset",
             handler_module_path=str(self.handler),
             ovverwrite_layer=False,
-            alternate="alternate"
+            alternate="alternate",
         )
 
-        self.assertEqual('ogr2ogr', _task)
+        self.assertEqual("ogr2ogr", _task)
         self.assertEqual(alternate, "alternate")
         self.assertEqual(str(_uuid), execution_id)
 
         _open.assert_called_once()
         _open.assert_called_with(
-            f'/usr/bin/ogr2ogr --config PG_USE_COPY YES -f PostgreSQL PG:" dbname=\'geonode_data\' host=localhost port=5434 user=\'geonode\' password=\'geonode\' " "{self.valid_csv}" -lco DIM=2 -nln alternate "dataset" -oo KEEP_GEOM_COLUMNS=NO -lco GEOMETRY_NAME=geometry  -oo "GEOM_POSSIBLE_NAMES=geom*,the_geom*,wkt_geom" -oo "X_POSSIBLE_NAMES=x,long*" -oo "Y_POSSIBLE_NAMES=y,lat*"', stdout=-1, stderr=-1, shell=True # noqa
+            f'/usr/bin/ogr2ogr --config PG_USE_COPY YES -f PostgreSQL PG:" dbname=\'geonode_data\' host=localhost port=5434 user=\'geonode\' password=\'geonode\' " "{self.valid_csv}" -lco DIM=2 -nln alternate "dataset" -oo KEEP_GEOM_COLUMNS=NO -lco GEOMETRY_NAME=geometry  -oo "GEOM_POSSIBLE_NAMES=geom*,the_geom*,wkt_geom" -oo "X_POSSIBLE_NAMES=x,long*" -oo "Y_POSSIBLE_NAMES=y,lat*"',
+            stdout=-1,
+            stderr=-1,
+            shell=True,  # noqa
         )
```

## importer/handlers/geojson/handler.py

```diff
@@ -38,16 +38,16 @@
         ),
     }
 
     @property
     def supported_file_extension_config(self):
         return {
             "id": "geojson",
-            "label": "GeoJson",
-            "format": "metadata",
+            "label": "GeoJSON",
+            "format": "vector",
             "ext": ["json", "geojson"],
             "optional": ["xml", "sld"],
         }
 
     @staticmethod
     def can_handle(_data) -> bool:
         """
```

## importer/handlers/geojson/tests.py

```diff
@@ -1,8 +1,7 @@
-
 import uuid
 from django.test import TestCase
 from mock import MagicMock, patch
 from importer.handlers.common.vector import import_with_ogr2ogr
 from importer.handlers.geojson.exceptions import InvalidGeoJsonException
 from importer.handlers.geojson.handler import GeoJsonFileHandler
 from django.contrib.auth import get_user_model
@@ -31,15 +30,15 @@
         )
 
     def test_task_list_is_the_expected_one(self):
         expected = (
             "start_import",
             "importer.import_resource",
             "importer.publish_resource",
-            "importer.create_geonode_resource"
+            "importer.create_geonode_resource",
         )
         self.assertEqual(len(self.handler.ACTIONS["import"]), 4)
         self.assertTupleEqual(expected, self.handler.ACTIONS["import"])
 
     def test_task_list_is_the_expected_one_copy(self):
         expected = (
             "start_copy",
@@ -102,32 +101,37 @@
         actual = self.handler.can_handle(self.valid_files)
         self.assertTrue(actual)
 
     def test_can_handle_should_return_false_for_other_files(self):
         actual = self.handler.can_handle({"base_file": "random.gpkg"})
         self.assertFalse(actual)
 
-    @patch('importer.handlers.common.vector.Popen')
-    def test_import_with_ogr2ogr_without_errors_should_call_the_right_command(self, _open):
+    @patch("importer.handlers.common.vector.Popen")
+    def test_import_with_ogr2ogr_without_errors_should_call_the_right_command(
+        self, _open
+    ):
         _uuid = uuid.uuid4()
 
         comm = MagicMock()
         comm.communicate.return_value = b"", b""
         _open.return_value = comm
 
         _task, alternate, execution_id = import_with_ogr2ogr(
             execution_id=str(_uuid),
             files=self.valid_files,
             original_name="dataset",
             handler_module_path=str(self.handler),
             ovverwrite_layer=False,
-            alternate="alternate"
+            alternate="alternate",
         )
 
-        self.assertEqual('ogr2ogr', _task)
+        self.assertEqual("ogr2ogr", _task)
         self.assertEqual(alternate, "alternate")
         self.assertEqual(str(_uuid), execution_id)
 
         _open.assert_called_once()
         _open.assert_called_with(
-            f'/usr/bin/ogr2ogr --config PG_USE_COPY YES -f PostgreSQL PG:" dbname=\'geonode_data\' host=localhost port=5434 user=\'geonode\' password=\'geonode\' " "{self.valid_files.get("base_file")}" -lco DIM=2 -nln alternate "dataset" -lco GEOMETRY_NAME=geometry', stdout=-1, stderr=-1, shell=True # noqa
+            f'/usr/bin/ogr2ogr --config PG_USE_COPY YES -f PostgreSQL PG:" dbname=\'geonode_data\' host=localhost port=5434 user=\'geonode\' password=\'geonode\' " "{self.valid_files.get("base_file")}" -lco DIM=2 -nln alternate "dataset" -lco GEOMETRY_NAME=geometry',
+            stdout=-1,
+            stderr=-1,
+            shell=True,  # noqa
         )
```

## importer/handlers/geotiff/handler.py

```diff
@@ -34,33 +34,33 @@
             "importer.rollback",
         ),
     }
 
     @property
     def supported_file_extension_config(self):
         return {
-            "id": 'tiff',
-            "label": 'GeoTIFF',
-            "format": 'raster',
-            "ext": ['tiff', 'tif', 'geotiff', 'geotif'],
-            "mimeType": ['image/tiff'],
-            "optional": ['xml', 'sld']
+            "id": "tiff",
+            "label": "GeoTIFF",
+            "format": "raster",
+            "ext": ["tiff", "tif", "geotiff", "geotif"],
+            "mimeType": ["image/tiff"],
+            "optional": ["xml", "sld"],
         }
 
     @staticmethod
     def can_handle(_data) -> bool:
         """
         This endpoint will return True or False if with the info provided
         the handler is able to handle the file or not
         """
         base = _data.get("base_file")
         if not base:
             return False
         ext = base.split(".")[-1] if isinstance(base, str) else base.name.split(".")[-1]
-        return ext in ["tiff", "geotiff", "tif", 'geotif']
+        return ext in ["tiff", "geotiff", "tif", "geotif"]
 
     @staticmethod
     def is_valid(files, user):
         """
         Define basic validation steps:
         """
         # calling base validation checks
```

## importer/handlers/geotiff/tests.py

```diff
@@ -1,8 +1,7 @@
-
 from django.test import TestCase
 from importer.handlers.geotiff.exceptions import InvalidGeoTiffException
 from django.contrib.auth import get_user_model
 from importer import project_dir
 from geonode.upload.models import UploadParallelismLimit
 from geonode.upload.api.exceptions import UploadParallelismLimitException
 from geonode.base.populate_test_data import create_single_dataset
@@ -18,34 +17,32 @@
         super().setUpClass()
         cls.handler = GeoTiffFileHandler()
         cls.valid_tiff = f"{project_dir}/tests/fixture/test_grid.tif"
         cls.valid_files = {"base_file": cls.valid_tiff}
         cls.user, _ = get_user_model().objects.get_or_create(username="admin")
         cls.invalid_tiff = {"base_file": "invalid.file.foo"}
         cls.owner = get_user_model().objects.first()
-        cls.layer = create_single_dataset(
-            name="test_grid", owner=cls.owner
-        )
+        cls.layer = create_single_dataset(name="test_grid", owner=cls.owner)
 
     def test_task_list_is_the_expected_one(self):
         expected = (
             "start_import",
             "importer.import_resource",
             "importer.publish_resource",
-            "importer.create_geonode_resource"
+            "importer.create_geonode_resource",
         )
         self.assertEqual(len(self.handler.ACTIONS["import"]), 4)
         self.assertTupleEqual(expected, self.handler.ACTIONS["import"])
 
     def test_task_list_is_the_expected_one_copy(self):
         expected = (
             "start_copy",
             "importer.copy_raster_file",
             "importer.publish_resource",
-            "importer.copy_geonode_resource"
+            "importer.copy_geonode_resource",
         )
         self.assertEqual(len(self.handler.ACTIONS["copy"]), 4)
         self.assertTupleEqual(expected, self.handler.ACTIONS["copy"])
 
     def test_is_valid_should_raise_exception_if_the_parallelism_is_met(self):
         parallelism, created = UploadParallelismLimit.objects.get_or_create(
             slug="default_max_parallel_uploads"
@@ -81,25 +78,24 @@
 
     def test_is_valid_should_raise_exception_if_the_tif_is_invalid_format(self):
         with self.assertRaises(InvalidGeoTiffException) as _exc:
             self.handler.is_valid(files=self.invalid_tiff, user=self.user)
 
         self.assertIsNotNone(_exc)
         self.assertTrue(
-            "Please remove the additional dots in the filename" in str(_exc.exception.detail)
+            "Please remove the additional dots in the filename"
+            in str(_exc.exception.detail)
         )
 
     def test_is_valid_should_raise_exception_if_the_tif_not_provided(self):
         with self.assertRaises(InvalidGeoTiffException) as _exc:
             self.handler.is_valid(files={"foo": "bar"}, user=self.user)
 
         self.assertIsNotNone(_exc)
-        self.assertTrue(
-            "base file is not provided" in str(_exc.exception.detail)
-        )
+        self.assertTrue("base file is not provided" in str(_exc.exception.detail))
 
     def test_can_handle_should_return_true_for_tif(self):
         actual = self.handler.can_handle(self.valid_files)
         self.assertTrue(actual)
 
     def test_can_handle_should_return_false_for_other_files(self):
         actual = self.handler.can_handle({"base_file": "random.gpkg"})
```

## importer/handlers/gpkg/handler.py

```diff
@@ -37,15 +37,20 @@
             "start_rollback",
             "importer.rollback",
         ),
     }
 
     @property
     def supported_file_extension_config(self):
-        return {"id": "gpkg", "label": "GeoPackage", "format": "archive", "ext": ["gpkg"]}
+        return {
+            "id": "gpkg",
+            "label": "GeoPackage",
+            "format": "vector",
+            "ext": ["gpkg"],
+        }
 
     @staticmethod
     def can_handle(_data) -> bool:
         """
         This endpoint will return True or False if with the info provided
         the handler is able to handle the file or not
         """
@@ -102,17 +107,23 @@
             )
 
         validator = validate(
             gpkg_path=files.get("base_file"),
             validations="RQ1, RQ2, RQ13, RQ14, RQ15, RC18",
         )
         if not validator[-1]:
-            raise InvalidGeopackageException(
-                '. '.join([x.get("validation_description") for x in validator[0]])
-            )
+            error_to_raise = []
+            for error in validator[0]:
+                logger.error(error)
+                if "locations" in error:
+                    error_to_raise.extend(error["locations"])
+                else:
+                    error_to_raise.append(error["validation_description"])
+
+            raise InvalidGeopackageException(". ".join(error_to_raise))
 
         return True
 
     def get_ogr2ogr_driver(self):
         return ogr.GetDriverByName("GPKG")
 
     def handle_xml_file(self, saved_dataset, _exec):
```

## importer/handlers/gpkg/tasks.py

```diff
@@ -4,19 +4,18 @@
 
 from importer.handlers.utils import evaluate_error
 
 logger = logging.getLogger(__name__)
 
 
 class SingleMessageErrorHandler(Task):
-
     max_retries = 1
     track_started = True
 
     def on_failure(self, exc, task_id, args, kwargs, einfo):
-        '''
+        """
         THis is separated because for gpkg we have a side effect
         (we rollback dynamic models and ogr2ogr)
         based on this failure step which is not meant for the other
         handlers
-        '''
+        """
         evaluate_error(self, exc, task_id, args, kwargs, einfo)
```

## importer/handlers/gpkg/tests.py

```diff
@@ -110,15 +110,14 @@
         self.assertTrue(actual)
 
     def test_can_handle_should_return_false_for_other_files(self):
         actual = self.handler.can_handle({"base_file": "random.file"})
         self.assertFalse(actual)
 
     def test_single_message_error_handler(self):
-
         exec_id = orchestrator.create_execution_request(
             user=get_user_model().objects.first(),
             func_name="funct1",
             step="step",
             input_params={
                 "files": self.valid_files,
                 "skip_existing_layer": True,
```

## importer/handlers/kml/handler.py

```diff
@@ -37,15 +37,20 @@
             "start_rollback",
             "importer.rollback",
         ),
     }
 
     @property
     def supported_file_extension_config(self):
-        return {"id": "kml", "label": "KML/KMZ", "format": "archive", "ext": ["kml", "kmz"]}
+        return {
+            "id": "kml",
+            "label": "KML/KMZ",
+            "format": "vector",
+            "ext": ["kml", "kmz"],
+        }
 
     @staticmethod
     def can_handle(_data) -> bool:
         """
         This endpoint will return True or False if with the info provided
         the handler is able to handle the file or not
         """
```

## importer/handlers/kml/tests.py

```diff
@@ -18,17 +18,15 @@
         cls.handler = KMLFileHandler()
         cls.valid_kml = f"{project_dir}/tests/fixture/valid.kml"
         cls.invalid_kml = f"{project_dir}/tests/fixture/inva.lid.kml"
         cls.user, _ = get_user_model().objects.get_or_create(username="admin")
         cls.invalid_files = {"base_file": cls.invalid_kml}
         cls.valid_files = {"base_file": cls.valid_kml}
         cls.owner = get_user_model().objects.first()
-        cls.layer = create_single_dataset(
-            name="extruded_polygon", owner=cls.owner
-        )
+        cls.layer = create_single_dataset(name="extruded_polygon", owner=cls.owner)
 
     def test_task_list_is_the_expected_one(self):
         expected = (
             "start_import",
             "importer.import_resource",
             "importer.publish_resource",
             "importer.create_geonode_resource",
```

## importer/handlers/shapefile/handler.py

```diff
@@ -28,31 +28,31 @@
             "importer.create_geonode_resource",
         ),
         exa.COPY.value: (
             "start_copy",
             "importer.copy_dynamic_model",
             "importer.copy_geonode_data_table",
             "importer.publish_resource",
-            "importer.copy_geonode_resource"
+            "importer.copy_geonode_resource",
         ),
         ira.ROLLBACK.value: (
             "start_rollback",
             "importer.rollback",
         ),
     }
 
     @property
     def supported_file_extension_config(self):
         return {
-            "id": 'shp',
-            "label": 'ESRI Shapefile',
-            "format": 'vector',
-            "ext": ['shp'],
-            "requires": ['shp', 'prj', 'dbf', 'shx'],
-            "optional": ['xml', 'sld']
+            "id": "shp",
+            "label": "ESRI Shapefile",
+            "format": "vector",
+            "ext": ["shp"],
+            "requires": ["shp", "prj", "dbf", "shx"],
+            "optional": ["xml", "sld"],
         }
 
     @staticmethod
     def can_handle(_data) -> bool:
         """
         This endpoint will return True or False if with the info provided
         the handler is able to handle the file or not
@@ -64,15 +64,19 @@
         return ext in ["shp"]
 
     @staticmethod
     def has_serializer(data) -> bool:
         _base = data.get("base_file")
         if not _base:
             return False
-        if _base.endswith("shp") if isinstance(_base, str) else _base.name.endswith("shp"):
+        if (
+            _base.endswith("shp")
+            if isinstance(_base, str)
+            else _base.name.endswith("shp")
+        ):
             return ShapeFileSerializer
         return False
 
     @staticmethod
     def extract_params_from_data(_data, action=None):
         """
         Remove from the _data the params that needs to save into the executionRequest object
@@ -107,22 +111,24 @@
 
         _shp_ext_needed = [
             x["requires"]
             for x in get_supported_datasets_file_types()
             if x["id"] == "shp"
         ][0]
 
-        '''
+        """
         Check if the ext required for the shape file are available in the files uploaded
         by the user
-        '''
+        """
         is_valid = all(
             map(
                 lambda x: any(
-                    _ext.endswith(f"{_filename}.{x}") if isinstance(_ext, str) else _ext.name.endswith(f"{_filename}.{x}")
+                    _ext.endswith(f"{_filename}.{x}")
+                    if isinstance(_ext, str)
+                    else _ext.name.endswith(f"{_filename}.{x}")
                     for _ext in files.values()
                 ),
                 _shp_ext_needed,
             )
         )
         if not is_valid:
             raise InvalidShapeFileException(
@@ -132,26 +138,36 @@
         return True
 
     def get_ogr2ogr_driver(self):
         return ogr.GetDriverByName("ESRI Shapefile")
 
     @staticmethod
     def create_ogr2ogr_command(files, original_name, ovverwrite_layer, alternate):
-        '''
+        """
         Define the ogr2ogr command to be executed.
         This is a default command that is needed to import a vector file
-        '''
-        base_command = BaseVectorFileHandler.create_ogr2ogr_command(files, original_name, ovverwrite_layer, alternate)
+        """
+        base_command = BaseVectorFileHandler.create_ogr2ogr_command(
+            files, original_name, ovverwrite_layer, alternate
+        )
         layers = ogr.Open(files.get("base_file"))
         layer = layers.GetLayer(original_name)
-        additional_option = " -nlt PROMOTE_TO_MULTI" if layer is not None and 'Point' not in ogr.GeometryTypeToName(layer.GetGeomType()) else " "
-        return f"{base_command } -lco precision=no -lco DIM=2 -lco GEOMETRY_NAME={BaseVectorFileHandler().default_geometry_column_name}" + additional_option
+        additional_option = (
+            " -nlt PROMOTE_TO_MULTI"
+            if layer is not None
+            and "Point" not in ogr.GeometryTypeToName(layer.GetGeomType())
+            else " "
+        )
+        return (
+            f"{base_command } -lco precision=no -lco DIM=2 -lco GEOMETRY_NAME={BaseVectorFileHandler().default_geometry_column_name}"
+            + additional_option
+        )
 
     def promote_to_multi(self, geometry_name):
-        '''
+        """
         If needed change the name of the geometry, by promoting it to Multi
         example if is Point -> MultiPoint
         Needed for the shapefiles
-        '''
-        if 'Multi' not in geometry_name and 'Point' not in geometry_name:
+        """
+        if "Multi" not in geometry_name and "Point" not in geometry_name:
             return f"Multi {geometry_name.title()}"
         return geometry_name
```

## importer/handlers/shapefile/serializer.py

```diff
@@ -1,22 +1,27 @@
-
 from rest_framework import serializers
 from dynamic_rest.serializers import DynamicModelSerializer
 from geonode.upload.models import Upload
 
 
 class ShapeFileSerializer(DynamicModelSerializer):
     class Meta:
-        ref_name = 'ShapeFileSerializer'
+        ref_name = "ShapeFileSerializer"
         model = Upload
         view_name = "importer_upload"
         fields = (
-            "base_file", "dbf_file", "shx_file", "prj_file", "xml_file",
-            "sld_file", "store_spatial_files", "overwrite_existing_layer",
-            "skip_existing_layers"
+            "base_file",
+            "dbf_file",
+            "shx_file",
+            "prj_file",
+            "xml_file",
+            "sld_file",
+            "store_spatial_files",
+            "overwrite_existing_layer",
+            "skip_existing_layers",
         )
 
     base_file = serializers.FileField()
     dbf_file = serializers.FileField()
     shx_file = serializers.FileField()
     prj_file = serializers.FileField()
     xml_file = serializers.FileField(required=False)
```

## importer/handlers/shapefile/tests.py

```diff
@@ -1,8 +1,7 @@
-
 import os
 import uuid
 
 import gisdata
 from django.contrib.auth import get_user_model
 from django.test import TestCase
 from geonode.upload.api.exceptions import UploadParallelismLimitException
@@ -37,56 +36,60 @@
         cls.owner = get_user_model().objects.first()
 
     def test_task_list_is_the_expected_one(self):
         expected = (
             "start_import",
             "importer.import_resource",
             "importer.publish_resource",
-            "importer.create_geonode_resource"
+            "importer.create_geonode_resource",
         )
-        self.assertEqual(len(self.handler.ACTIONS['import']), 4)
-        self.assertTupleEqual(expected, self.handler.ACTIONS['import'])
+        self.assertEqual(len(self.handler.ACTIONS["import"]), 4)
+        self.assertTupleEqual(expected, self.handler.ACTIONS["import"])
 
     def test_copy_task_list_is_the_expected_one(self):
         expected = (
             "start_copy",
             "importer.copy_dynamic_model",
             "importer.copy_geonode_data_table",
             "importer.publish_resource",
             "importer.copy_geonode_resource",
         )
-        self.assertEqual(len(self.handler.ACTIONS['copy']), 5)
-        self.assertTupleEqual(expected, self.handler.ACTIONS['copy'])
+        self.assertEqual(len(self.handler.ACTIONS["copy"]), 5)
+        self.assertTupleEqual(expected, self.handler.ACTIONS["copy"])
 
     def test_is_valid_should_raise_exception_if_the_parallelism_is_met(self):
-        parallelism, created = UploadParallelismLimit.objects.get_or_create(slug="default_max_parallel_uploads")
+        parallelism, created = UploadParallelismLimit.objects.get_or_create(
+            slug="default_max_parallel_uploads"
+        )
         old_value = parallelism.max_number
         try:
-            UploadParallelismLimit.objects.filter(slug="default_max_parallel_uploads").update(max_number=0)
+            UploadParallelismLimit.objects.filter(
+                slug="default_max_parallel_uploads"
+            ).update(max_number=0)
 
             with self.assertRaises(UploadParallelismLimitException):
                 self.handler.is_valid(files=self.valid_shp, user=self.user)
         finally:
             parallelism.max_number = old_value
             parallelism.save()
 
     def test_promote_to_multi(self):
         # point should be keep as point
-        actual = self.handler.promote_to_multi('Point')
+        actual = self.handler.promote_to_multi("Point")
         self.assertEqual("Point", actual)
         # polygon should be changed into multipolygon
-        actual = self.handler.promote_to_multi('Polygon')
+        actual = self.handler.promote_to_multi("Polygon")
         self.assertEqual("Multi Polygon", actual)
 
         # linestring should be changed into multilinestring
-        actual = self.handler.promote_to_multi('Linestring')
+        actual = self.handler.promote_to_multi("Linestring")
         self.assertEqual("Multi Linestring", actual)
 
         # if is already multi should be kept
-        actual = self.handler.promote_to_multi('Multi Point')
+        actual = self.handler.promote_to_multi("Multi Point")
         self.assertEqual("Multi Point", actual)
 
     def test_is_valid_should_pass_with_valid_shp(self):
         self.handler.is_valid(files=self.valid_shp, user=self.user)
 
     def test_get_ogr2ogr_driver_should_return_the_expected_driver(self):
         expected = ogr.GetDriverByName("ESRI Shapefile")
@@ -105,32 +108,37 @@
         actual = self.handler.has_serializer(self.valid_shp)
         self.assertEqual(type(actual), type(ShapeFileSerializer))
 
     def test_should_NOT_get_the_specific_serializer(self):
         actual = self.handler.has_serializer(self.invalid_files)
         self.assertFalse(actual)
 
-    @patch('importer.handlers.common.vector.Popen')
-    def test_import_with_ogr2ogr_without_errors_should_call_the_right_command(self, _open):
+    @patch("importer.handlers.common.vector.Popen")
+    def test_import_with_ogr2ogr_without_errors_should_call_the_right_command(
+        self, _open
+    ):
         _uuid = uuid.uuid4()
 
         comm = MagicMock()
         comm.communicate.return_value = b"", b""
         _open.return_value = comm
 
         _task, alternate, execution_id = import_with_ogr2ogr(
             execution_id=str(_uuid),
             files=self.valid_shp,
             original_name="dataset",
             handler_module_path=str(self.handler),
             ovverwrite_layer=False,
-            alternate="alternate"
+            alternate="alternate",
         )
 
-        self.assertEqual('ogr2ogr', _task)
+        self.assertEqual("ogr2ogr", _task)
         self.assertEqual(alternate, "alternate")
         self.assertEqual(str(_uuid), execution_id)
 
         _open.assert_called_once()
         _open.assert_called_with(
-            f'/usr/bin/ogr2ogr --config PG_USE_COPY YES -f PostgreSQL PG:" dbname=\'geonode_data\' host=localhost port=5434 user=\'geonode\' password=\'geonode\' " "{self.valid_shp.get("base_file")}" -lco DIM=2 -nln alternate "dataset" -lco precision=no -lco GEOMETRY_NAME=geometry ', stdout=-1, stderr=-1, shell=True # noqa
+            f'/usr/bin/ogr2ogr --config PG_USE_COPY YES -f PostgreSQL PG:" dbname=\'geonode_data\' host=localhost port=5434 user=\'geonode\' password=\'geonode\' " "{self.valid_shp.get("base_file")}" -lco DIM=2 -nln alternate "dataset" -lco precision=no -lco GEOMETRY_NAME=geometry ',
+            stdout=-1,
+            stderr=-1,
+            shell=True,  # noqa
         )
```

## importer/migrations/0001_initial.py

```diff
@@ -1,15 +1,14 @@
 # Generated by Django 3.2.13 on 2022-07-19 07:43
 
 from django.db import migrations, models
 import django.db.models.deletion
 
 
 class Migration(migrations.Migration):
-
     initial = True
 
     dependencies = [
         ("base", "0081_alter_resourcebase_alternate"),
     ]
 
     operations = [
```

## importer/migrations/0002_resourcehandlerinfo_kwargs.py

```diff
@@ -1,18 +1,20 @@
 # Generated by Django 3.2.15 on 2022-09-29 15:27
 
 from django.db import migrations, models
 
 
 class Migration(migrations.Migration):
-
     dependencies = [
-        ('importer', '0001_initial'),
+        ("importer", "0001_initial"),
     ]
 
     operations = [
         migrations.AddField(
-            model_name='resourcehandlerinfo',
-            name='kwargs',
-            field=models.JSONField(default=dict, verbose_name='Storing strictly related information of the handler'),
+            model_name="resourcehandlerinfo",
+            name="kwargs",
+            field=models.JSONField(
+                default=dict,
+                verbose_name="Storing strictly related information of the handler",
+            ),
         ),
     ]
```

## importer/migrations/0003_resourcehandlerinfo_execution_id.py

```diff
@@ -1,20 +1,24 @@
 # Generated by Django 3.2.15 on 2022-10-04 12:40
 
 from django.db import migrations, models
 import django.db.models.deletion
 
 
 class Migration(migrations.Migration):
-
     dependencies = [
-        ('resource', '0007_alter_executionrequest_action'),
-        ('importer', '0002_resourcehandlerinfo_kwargs'),
+        ("resource", "0007_alter_executionrequest_action"),
+        ("importer", "0002_resourcehandlerinfo_kwargs"),
     ]
 
     operations = [
         migrations.AddField(
-            model_name='resourcehandlerinfo',
-            name='execution_id',
-            field=models.ForeignKey(default=None, null=True, on_delete=django.db.models.deletion.SET_NULL, to='resource.executionrequest'),
+            model_name="resourcehandlerinfo",
+            name="execution_id",
+            field=models.ForeignKey(
+                default=None,
+                null=True,
+                on_delete=django.db.models.deletion.SET_NULL,
+                to="resource.executionrequest",
+            ),
         ),
     ]
```

## importer/migrations/0004_rename_execution_id_resourcehandlerinfo_execution_request.py

```diff
@@ -1,18 +1,17 @@
 # Generated by Django 3.2.15 on 2022-10-04 13:03
 
 from django.db import migrations
 
 
 class Migration(migrations.Migration):
-
     dependencies = [
-        ('importer', '0003_resourcehandlerinfo_execution_id'),
+        ("importer", "0003_resourcehandlerinfo_execution_id"),
     ]
 
     operations = [
         migrations.RenameField(
-            model_name='resourcehandlerinfo',
-            old_name='execution_id',
-            new_name='execution_request',
+            model_name="resourcehandlerinfo",
+            old_name="execution_id",
+            new_name="execution_request",
         ),
     ]
```

## importer/migrations/0005_fixup_dynamic_shema_table_names.py

```diff
@@ -1,37 +1,36 @@
 from django.db import migrations
 import logging
 from django.db import ProgrammingError
+
 logger = logging.getLogger(__name__)
 
 
 def fixup_table_name(apps, schema_editor):
     try:
-        schema = apps.get_model('dynamic_models', 'ModelSchema')
+        schema = apps.get_model("dynamic_models", "ModelSchema")
         for val in schema.objects.all():
             if val.name != val.db_table_name:
                 val.db_table_name = val.name
                 val.save()
     except ProgrammingError as e:
-        '''
+        """
         The dynamic model should exists to apply the above migration.
         In case it does not exists we can skip it
-        '''
+        """
         if 'relation "dynamic_models_modelschema" does not exist' in e.args[0]:
             logging.debug("Dynamic model does not exists yet, skipping")
             return
         raise e
     except Exception as e:
         raise e
-        
 
 
 class Migration(migrations.Migration):
-
     dependencies = [
-        ('importer', '0004_rename_execution_id_resourcehandlerinfo_execution_request'),
-        ('dynamic_models', '0005_auto_20220621_0718'),
+        ("importer", "0004_rename_execution_id_resourcehandlerinfo_execution_request"),
+        ("dynamic_models", "0005_auto_20220621_0718"),
     ]
 
     operations = [
         migrations.RunPython(fixup_table_name),
     ]
```

## importer/migrations/0006_dataset_migration.py

```diff
@@ -1,18 +1,19 @@
 # Generated by Django 3.2.15 on 2022-10-04 13:03
 
 from django.db import migrations
 from importer.orchestrator import orchestrator
 from geonode.layers.models import Dataset
 
+
 def dataset_migration(apps, _):
-    NewResources = apps.get_model('importer', 'ResourceHandlerInfo')
-    for old_resource in Dataset.objects\
-        .exclude(pk__in=NewResources.objects.values_list('resource_id', flat=True))\
-        .exclude(subtype__in=['remote', None]):        
+    NewResources = apps.get_model("importer", "ResourceHandlerInfo")
+    for old_resource in Dataset.objects.exclude(
+        pk__in=NewResources.objects.values_list("resource_id", flat=True)
+    ).exclude(subtype__in=["remote", None]):
         # generating orchestrator expected data file
         if not old_resource.files:
             if old_resource.is_vector():
                 converted_files = [{"base_file": "placeholder.shp"}]
             else:
                 converted_files = [{"base_file": "placeholder.tiff"}]
         else:
@@ -25,20 +26,19 @@
             if handler is not None:
                 handler_to_use = handler
                 break
         handler_to_use.create_resourcehandlerinfo(
             handler_module_path=str(handler_to_use),
             resource=old_resource,
             execution_id=None,
-            kwargs={"is_legacy": True}
+            kwargs={"is_legacy": True},
         )
 
 
 class Migration(migrations.Migration):
-
     dependencies = [
-        ('importer', '0005_fixup_dynamic_shema_table_names'),
+        ("importer", "0005_fixup_dynamic_shema_table_names"),
     ]
 
     operations = [
         migrations.RunPython(dataset_migration),
     ]
```

## importer/tests/utils.py

```diff
@@ -1,38 +1,36 @@
 from django.core.management import call_command
 from django.test import TestCase, TransactionTestCase
 
 
 class ImporterBaseTestSupport(TestCase):
-
-    databases = ('default', 'datastore')
+    databases = ("default", "datastore")
     multi_db = True
 
     @classmethod
     def setUpClass(cls) -> None:
         super().setUpClass()
-        '''
+        """
         Why manually load the fixture after the setupClass?
         Django in the setUpClass method, load the fixture in all the databases
         that are defined in the databases attribute. The problem is that the
         datastore database will contain only the dyanmic models infrastructure
         and not the whole geonode structure. So that, having the fixture as a
         attribute will raise and error
-        '''
+        """
         fixture = [
-            'initial_data.json',
-            'group_test_data.json',
-            'default_oauth_apps.json'
+            "initial_data.json",
+            "group_test_data.json",
+            "default_oauth_apps.json",
         ]
 
-        call_command('loaddata', *fixture, **{'verbosity': 0, 'database': "default"})
+        call_command("loaddata", *fixture, **{"verbosity": 0, "database": "default"})
 
 
 class TransactionImporterBaseTestSupport(TransactionTestCase):
-
     databases = ("default", "datastore")
     multi_db = True
 
     @classmethod
     def setUpClass(cls) -> None:
         super().setUpClass()
         """
```

## importer/tests/end2end/test_end2end.py

```diff
@@ -11,19 +11,19 @@
 from geonode.layers.models import Dataset
 from geonode.resource.models import ExecutionRequest
 from geonode.utils import OGC_Servers_Handler
 from geoserver.catalog import Catalog
 from importer import project_dir
 from importer.tests.utils import ImporterBaseTestSupport
 import gisdata
+
 geourl = settings.GEODATABASE_URL
 
 
 class BaseImporterEndToEndTest(ImporterBaseTestSupport):
-
     @classmethod
     def setUpClass(cls) -> None:
         super().setUpClass()
         cls.valid_gkpg = f"{project_dir}/tests/fixture/valid.gpkg"
         cls.valid_geojson = f"{project_dir}/tests/fixture/valid.geojson"
         file_path = gisdata.VECTOR_DATA
         filename = os.path.join(file_path, "san_andres_y_providencia_highway.shp")
@@ -31,23 +31,21 @@
             "base_file": filename,
             "dbf_file": f"{file_path}/san_andres_y_providencia_highway.dbf",
             "prj_file": f"{file_path}/san_andres_y_providencia_highway.prj",
             "shx_file": f"{file_path}/san_andres_y_providencia_highway.shx",
         }
         cls.valid_kml = f"{project_dir}/tests/fixture/valid.kml"
 
-        cls.url = reverse('importer_upload')
-        ogc_server_settings = OGC_Servers_Handler(settings.OGC_SERVER)['default']
+        cls.url = reverse("importer_upload")
+        ogc_server_settings = OGC_Servers_Handler(settings.OGC_SERVER)["default"]
 
         _user, _password = ogc_server_settings.credentials
 
         cls.cat = Catalog(
-            service_url=ogc_server_settings.rest,
-            username=_user,
-            password=_password
+            service_url=ogc_server_settings.rest, username=_user, password=_password
         )
 
     def setUp(self) -> None:
         self.admin = get_user_model().objects.get(username="admin")
 
     def tearDown(self) -> None:
         return super().tearDown()
@@ -57,18 +55,29 @@
 
         response = self.client.post(self.url, data=payload)
         self.assertEqual(201, response.status_code)
 
         # if is async, we must wait. It will wait for 1 min before raise exception
         if ast.literal_eval(os.getenv("ASYNC_SIGNALS", "False")):
             tentative = 1
-            while ExecutionRequest.objects.get(exec_id=response.json().get("execution_id")) != ExecutionRequest.STATUS_FINISHED and tentative <= 6:
+            while (
+                ExecutionRequest.objects.get(
+                    exec_id=response.json().get("execution_id")
+                )
+                != ExecutionRequest.STATUS_FINISHED
+                and tentative <= 6
+            ):
                 time.sleep(10)
                 tentative += 1
-        if ExecutionRequest.objects.get(exec_id=response.json().get("execution_id")).status != ExecutionRequest.STATUS_FINISHED:
+        if (
+            ExecutionRequest.objects.get(
+                exec_id=response.json().get("execution_id")
+            ).status
+            != ExecutionRequest.STATUS_FINISHED
+        ):
             raise Exception("Async still in progress after 1 min of waiting")
 
         # check if the dynamic model is created
         _schema_id = ModelSchema.objects.filter(name__icontains=initial_name)
         self.assertTrue(_schema_id.exists())
         schema_entity = _schema_id.first()
         self.assertTrue(FieldSchema.objects.filter(model_schema=schema_entity).exists())
@@ -83,67 +92,73 @@
 
         # check if the geonode resource exists
         dataset = Dataset.objects.filter(alternate=f"geonode:{schema_entity.name}")
         self.assertTrue(dataset.exists())
 
 
 class ImporterGeoPackageImportTest(BaseImporterEndToEndTest):
-
     @mock.patch.dict(os.environ, {"GEONODE_GEODATABASE": "test_geonode_data"})
-    @override_settings(GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data")
+    @override_settings(
+        GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data"
+    )
     def test_import_geopackage(self):
         layer = self.cat.get_layer("geonode:stazioni_metropolitana")
         self.cat.delete(layer)
         payload = {
-            "base_file": open(self.valid_gkpg, 'rb'),
+            "base_file": open(self.valid_gkpg, "rb"),
         }
         initial_name = "stazioni_metropolitana"
         self._assertimport(payload, initial_name)
         layer = self.cat.get_layer("geonode:stazioni_metropolitana")
         self.cat.delete(layer)
 
 
 class ImporterGeoJsonImportTest(BaseImporterEndToEndTest):
-
     @mock.patch.dict(os.environ, {"GEONODE_GEODATABASE": "test_geonode_data"})
-    @override_settings(GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data")
+    @override_settings(
+        GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data"
+    )
     def test_import_geojson(self):
         layer = self.cat.get_layer("geonode:valid")
         self.cat.delete(layer)
 
         payload = {
-            "base_file": open(self.valid_geojson, 'rb'),
+            "base_file": open(self.valid_geojson, "rb"),
         }
         initial_name = "valid"
         self._assertimport(payload, initial_name)
         layer = self.cat.get_layer("geonode:valid")
         self.cat.delete(layer)
 
 
 class ImporterKMLImportTest(BaseImporterEndToEndTest):
-
     @mock.patch.dict(os.environ, {"GEONODE_GEODATABASE": "test_geonode_data"})
-    @override_settings(GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data")
+    @override_settings(
+        GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data"
+    )
     def test_import_kml(self):
         layer = self.cat.get_layer("geonode:extruded_polygon")
         self.cat.delete(layer)
         payload = {
-            "base_file": open(self.valid_kml, 'rb'),
+            "base_file": open(self.valid_kml, "rb"),
         }
         initial_name = "extruded_polygon"
         self._assertimport(payload, initial_name)
         layer = self.cat.get_layer("geonode:extruded_polygon")
         self.cat.delete(layer)
 
 
 class ImporterShapefileImportTest(BaseImporterEndToEndTest):
-
     @mock.patch.dict(os.environ, {"GEONODE_GEODATABASE": "test_geonode_data"})
-    @override_settings(GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data")
+    @override_settings(
+        GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data"
+    )
     def test_import_shapefile(self):
         layer = self.cat.get_layer("geonode:san_andres_y_providencia_highway")
         self.cat.delete(layer)
-        payload = {_filename: open(_file, 'rb') for _filename, _file in self.valid_shp.items()}
+        payload = {
+            _filename: open(_file, "rb") for _filename, _file in self.valid_shp.items()
+        }
         initial_name = "san_andres_y_providencia_highway"
         self._assertimport(payload, initial_name)
         layer = self.cat.get_layer("geonode:san_andres_y_providencia_highway")
         self.cat.delete(layer)
```

## importer/tests/end2end/test_end2end_copy.py

```diff
@@ -30,16 +30,16 @@
         filename = os.path.join(file_path, "san_andres_y_providencia_highway.shp")
         cls.valid_shp = {
             "base_file": filename,
             "dbf_file": f"{file_path}/san_andres_y_providencia_highway.dbf",
             "prj_file": f"{file_path}/san_andres_y_providencia_highway.prj",
             "shx_file": f"{file_path}/san_andres_y_providencia_highway.shx",
         }
-        cls.url_create = reverse('importer_upload')
-        ogc_server_settings = OGC_Servers_Handler(settings.OGC_SERVER)['default']
+        cls.url_create = reverse("importer_upload")
+        ogc_server_settings = OGC_Servers_Handler(settings.OGC_SERVER)["default"]
         cls.valid_kml = f"{project_dir}/tests/fixture/valid.kml"
         cls.url_create = reverse("importer_upload")
         ogc_server_settings = OGC_Servers_Handler(settings.OGC_SERVER)["default"]
 
         _user, _password = ogc_server_settings.credentials
 
         cls.cat = Catalog(
@@ -63,15 +63,15 @@
                 "stazioni_metropolitana",
                 "valid",
             ]
         ).delete()
 
     def _assertCloning(self, initial_name):
         # getting the geonode resource
-        dataset = Dataset.objects.get(alternate__icontains=f'geonode:{initial_name}')
+        dataset = Dataset.objects.get(alternate__icontains=f"geonode:{initial_name}")
         prev_dataset_count = Dataset.objects.count()
         self.client.force_login(get_user_model().objects.get(username="admin"))
         # creating the url and login
         _url = reverse("importer_resource_copy", args=[dataset.id])
 
         # defining the payload
         payload = QueryDict("", mutable=True)
@@ -161,19 +161,22 @@
         # first we need to import a resource
         with transaction.atomic():
             self._import_resource(payload, initial_name)
             self._assertCloning(initial_name)
 
 
 class ImporterCopyEnd2EndShapeFileTest(BaseClassEnd2End):
-
     @mock.patch.dict(os.environ, {"GEONODE_GEODATABASE": "test_geonode_data"})
-    @override_settings(GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data")
+    @override_settings(
+        GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data"
+    )
     def test_copy_dataset_from_shapefile(self):
-        payload = {_filename: open(_file, 'rb') for _filename, _file in self.valid_shp.items()}
+        payload = {
+            _filename: open(_file, "rb") for _filename, _file in self.valid_shp.items()
+        }
         initial_name = "san_andres_y_providencia_highway"
         # first we need to import a resource
         with transaction.atomic():
             self._import_resource(payload, initial_name)
             self._assertCloning(initial_name)
```

## importer/tests/unit/test_models.py

```diff
@@ -5,15 +5,18 @@
 
 
 class TestModelSchemaSignal(TestCase):
     databases = ("default", "datastore")
 
     def setUp(self):
         self.resource = create_single_dataset(name="test_dataset")
-        ResourceHandlerInfo.objects.create(resource=self.resource, handler_module_path="importer.handlers.shapefile.handler.ShapeFileHandler")
+        ResourceHandlerInfo.objects.create(
+            resource=self.resource,
+            handler_module_path="importer.handlers.shapefile.handler.ShapeFileHandler",
+        )
         self.dynamic_model = ModelSchema.objects.create(
             name=self.resource.name, db_name="datastore"
         )
         self.dynamic_model_field = FieldSchema.objects.create(
             name="field",
             class_name="django.db.models.IntegerField",
             model_schema=self.dynamic_model,
```

## importer/tests/unit/test_task.py

```diff
@@ -9,31 +9,33 @@
     copy_geonode_resource,
     create_dynamic_structure,
     create_geonode_resource,
     import_orchestrator,
     import_resource,
     orchestrator,
     publish_resource,
-    rollback
+    rollback,
 )
 from geonode.resource.models import ExecutionRequest
 from geonode.layers.models import Dataset
 from geonode.resource.enumerator import ExecutionRequestAction
 from geonode.base.models import ResourceBase
 from geonode.base.populate_test_data import create_single_dataset
 from dynamic_models.models import ModelSchema, FieldSchema
 from dynamic_models.exceptions import DynamicModelError, InvalidFieldNameError
 
-from importer.tests.utils import ImporterBaseTestSupport, TransactionImporterBaseTestSupport
+from importer.tests.utils import (
+    ImporterBaseTestSupport,
+    TransactionImporterBaseTestSupport,
+)
 
 # Create your tests here.
 
 
 class TestCeleryTasks(ImporterBaseTestSupport):
-
     def setUp(self):
         self.user = get_user_model().objects.first()
         self.exec_id = orchestrator.create_execution_request(
             user=get_user_model().objects.get(username=self.user),
             func_name="dummy_func",
             step="dummy_step",
             legacy_upload_name="dummy",
@@ -163,18 +165,18 @@
     @patch("importer.celery_tasks.DataPublisher.publish_resources")
     def test_publish_resource_if_overwrite_should_call_the_publishing(
         self,
         publish_resources,
         extract_resource_to_publish,
         importer,
     ):
-        '''
+        """
         Publish resource should be called since the resource does not exists in geoserver
         even if an overwrite is required
-        '''
+        """
         try:
             publish_resources.return_value = True
             extract_resource_to_publish.return_value = [
                 {"crs": 12345, "name": "dataset3"}
             ]
             exec_id = orchestrator.create_execution_request(
                 user=get_user_model().objects.get(username=self.user),
@@ -215,18 +217,18 @@
     def test_publish_resource_if_overwrite_should_not_call_the_publishing(
         self,
         get_resource,
         publish_resources,
         extract_resource_to_publish,
         importer,
     ):
-        '''
+        """
         Publish resource should be called since the resource does not exists in geoserver
         even if an overwrite is required
-        '''
+        """
         try:
             get_resource.return_falue = True
             publish_resources.return_value = True
             extract_resource_to_publish.return_value = [
                 {"crs": 12345, "name": "dataset3"}
             ]
             exec_id = orchestrator.create_execution_request(
@@ -260,15 +262,14 @@
             # cleanup
             if exec_id:
                 ExecutionRequest.objects.filter(exec_id=str(exec_id)).delete()
 
     @patch("importer.celery_tasks.import_orchestrator.apply_async")
     def test_create_geonode_resource(self, import_orchestrator):
         try:
-
             alternate = "geonode:alternate_foo_dataset"
             self.assertFalse(Dataset.objects.filter(alternate=alternate).exists())
 
             create_geonode_resource(
                 str(self.exec_id),
                 resource_type="gpkg",
                 step_name="create_geonode_resource",
@@ -291,15 +292,14 @@
             if Dataset.objects.filter(alternate=alternate).exists():
                 Dataset.objects.filter(alternate=alternate).delete()
 
     @patch("importer.celery_tasks.import_orchestrator.apply_async")
     def test_copy_geonode_resource_should_raise_exeption_if_the_alternate_not_exists(
         self, async_call
     ):
-
         with self.assertRaises(Exception):
             copy_geonode_resource(
                 str(self.exec_id),
                 "importer.copy_geonode_resource",
                 "cloning",
                 "invalid_alternate",
                 "importer.handlers.gpkg.handler.GPKGFileHandler",
@@ -341,95 +341,122 @@
             if Dataset.objects.filter(alternate=alternate).exists():
                 Dataset.objects.filter(alternate=alternate).delete()
             if new_alternate:
                 Dataset.objects.filter(alternate=new_alternate).delete()
 
     @patch("importer.handlers.gpkg.handler.GPKGFileHandler._import_resource_rollback")
     @patch("importer.handlers.gpkg.handler.GPKGFileHandler._publish_resource_rollback")
-    @patch("importer.handlers.gpkg.handler.GPKGFileHandler._create_geonode_resource_rollback")
+    @patch(
+        "importer.handlers.gpkg.handler.GPKGFileHandler._create_geonode_resource_rollback"
+    )
     def test_rollback_works_as_expected_vector_step(
         self,
         _create_geonode_resource_rollback,
         _publish_resource_rollback,
-        _import_resource_rollback
+        _import_resource_rollback,
     ):
-        '''
+        """
         rollback should remove the resource based on the step it has reached
-        '''
+        """
         test_config = [
             ("importer.import_resource", [_import_resource_rollback]),
-            ("importer.publish_resource", [_import_resource_rollback, _publish_resource_rollback]),
-            ("importer.create_geonode_resource", [_import_resource_rollback, _publish_resource_rollback, _create_geonode_resource_rollback])
+            (
+                "importer.publish_resource",
+                [_import_resource_rollback, _publish_resource_rollback],
+            ),
+            (
+                "importer.create_geonode_resource",
+                [
+                    _import_resource_rollback,
+                    _publish_resource_rollback,
+                    _create_geonode_resource_rollback,
+                ],
+            ),
         ]
         for conf in test_config:
             try:
                 exec_id = orchestrator.create_execution_request(
                     user=get_user_model().objects.get(username=self.user),
                     func_name="dummy_func",
                     step=conf[0],  # step name
-                    action='import',
+                    action="import",
                     input_params={
                         "files": {"base_file": "/filepath"},
                         "overwrite_existing_layer": True,
                         "store_spatial_files": True,
-                        "handler_module_path": "importer.handlers.gpkg.handler.GPKGFileHandler"
+                        "handler_module_path": "importer.handlers.gpkg.handler.GPKGFileHandler",
                     },
                 )
                 rollback(str(exec_id))
 
                 # Evaluation
                 req = ExecutionRequest.objects.get(exec_id=str(exec_id))
                 self.assertEqual("importer.rollback", req.step)
-                self.assertTrue(req.status=='failed')
+                self.assertTrue(req.status == "failed")
                 for expected_function in conf[1]:
                     expected_function.assert_called_once()
                     expected_function.reset_mock()
             finally:
                 # cleanup
                 if exec_id:
                     ExecutionRequest.objects.filter(exec_id=str(exec_id)).delete()
 
-
-    @patch("importer.handlers.geotiff.handler.GeoTiffFileHandler._import_resource_rollback")
-    @patch("importer.handlers.geotiff.handler.GeoTiffFileHandler._publish_resource_rollback")
-    @patch("importer.handlers.geotiff.handler.GeoTiffFileHandler._create_geonode_resource_rollback")
+    @patch(
+        "importer.handlers.geotiff.handler.GeoTiffFileHandler._import_resource_rollback"
+    )
+    @patch(
+        "importer.handlers.geotiff.handler.GeoTiffFileHandler._publish_resource_rollback"
+    )
+    @patch(
+        "importer.handlers.geotiff.handler.GeoTiffFileHandler._create_geonode_resource_rollback"
+    )
     def test_rollback_works_as_expected_raster(
         self,
         _create_geonode_resource_rollback,
         _publish_resource_rollback,
-        _import_resource_rollback
+        _import_resource_rollback,
     ):
-        '''
+        """
         rollback should remove the resource based on the step it has reached
-        '''
+        """
         test_config = [
             ("importer.import_resource", [_import_resource_rollback]),
-            ("importer.publish_resource", [_import_resource_rollback, _publish_resource_rollback]),
-            ("importer.create_geonode_resource", [_import_resource_rollback, _publish_resource_rollback, _create_geonode_resource_rollback])
+            (
+                "importer.publish_resource",
+                [_import_resource_rollback, _publish_resource_rollback],
+            ),
+            (
+                "importer.create_geonode_resource",
+                [
+                    _import_resource_rollback,
+                    _publish_resource_rollback,
+                    _create_geonode_resource_rollback,
+                ],
+            ),
         ]
         for conf in test_config:
             try:
                 exec_id = orchestrator.create_execution_request(
                     user=get_user_model().objects.get(username=self.user),
                     func_name="dummy_func",
                     step=conf[0],  # step name
-                    action='import',
+                    action="import",
                     input_params={
                         "files": {"base_file": "/filepath"},
                         "overwrite_existing_layer": True,
                         "store_spatial_files": True,
-                        "handler_module_path": "importer.handlers.geotiff.handler.GeoTiffFileHandler"
+                        "handler_module_path": "importer.handlers.geotiff.handler.GeoTiffFileHandler",
                     },
                 )
                 rollback(str(exec_id))
 
                 # Evaluation
                 req = ExecutionRequest.objects.get(exec_id=str(exec_id))
                 self.assertEqual("importer.rollback", req.step)
-                self.assertTrue(req.status=='failed')
+                self.assertTrue(req.status == "failed")
                 for expected_function in conf[1]:
                     expected_function.assert_called_once()
                     expected_function.reset_mock()
             finally:
                 # cleanup
                 if exec_id:
                     ExecutionRequest.objects.filter(exec_id=str(exec_id)).delete()
@@ -482,15 +509,18 @@
                     execution_id=str(self.exec_id),
                     fields=dynamic_fields,
                     dynamic_model_schema_id=schema.pk,
                     overwrite=False,
                     layer_name="test_layer",
                 )
 
-            expected_msg = "Error during the field creation. The field or class_name is None {'name': 'field1', 'class_name': None, 'null': True} for test_layer " + f"for execution {name}"
+            expected_msg = (
+                "Error during the field creation. The field or class_name is None {'name': 'field1', 'class_name': None, 'null': True} for test_layer "
+                + f"for execution {name}"
+            )
             self.assertEqual(expected_msg, str(_exc.exception))
         finally:
             ModelSchema.objects.filter(name=f"schema_{name}").delete()
 
     def test_create_dynamic_structure_should_work(self):
         try:
             name = str(self.exec_id)
@@ -522,46 +552,52 @@
 
     @patch("importer.celery_tasks.import_orchestrator.apply_async")
     def test_copy_dynamic_model_should_work(self, async_call):
         try:
             name = str(self.exec_id)
             # setup model schema to be copied
             schema = ModelSchema.objects.create(
-                name=f"schema_{name}", db_name="datastore", db_table_name=f"schema_{name}"
+                name=f"schema_{name}",
+                db_name="datastore",
+                db_table_name=f"schema_{name}",
             )
             FieldSchema.objects.create(
                 name=f"field_{name}",
                 class_name="django.contrib.gis.db.models.fields.LineStringField",
                 model_schema=schema,
             )
 
             layer = create_single_dataset(f"schema_{name}")
             layer.alternate = f"geonode:schema_{name}"
             layer.save()
 
-            self.assertTrue(ModelSchema.objects.filter(name__icontains=f"schema_").count() == 1)
-            
+            self.assertTrue(
+                ModelSchema.objects.filter(name__icontains=f"schema_").count() == 1
+            )
+
             copy_dynamic_model(
                 exec_id=str(self.exec_id),
                 actual_step="copy",
                 layer_name=f"schema_{name}",
                 alternate=f"geonode:schema_{name}",
                 handler_module_path="importer.handlers.gpkg.handler.GPKGFileHandler",
                 action=ExecutionRequestAction.COPY.value,
                 kwargs={
                     "original_dataset_alternate": f"geonode:schema_{name}",
                 },
             )
             # the alternate is generated internally
             self.assertTrue(ModelSchema.objects.filter(name=f"schema_{name}").exists())
-            self.assertTrue(ModelSchema.objects.filter(name__icontains=f"schema_").count() == 2)
+            self.assertTrue(
+                ModelSchema.objects.filter(name__icontains=f"schema_").count() == 2
+            )
 
             schema = ModelSchema.objects.all()
             for val in schema:
-                self.assertEqual(val.name , val.db_table_name)
+                self.assertEqual(val.name, val.db_table_name)
 
             async_call.assert_called_once()
 
         finally:
             ModelSchema.objects.filter(name=f"schema_").delete()
             FieldSchema.objects.filter(name=f"field_").delete()
```

## Comparing `geonode_importer-1.0.3.dist-info/LICENSE` & `geonode_importer-1.0.4.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `geonode_importer-1.0.3.dist-info/METADATA` & `geonode_importer-1.0.4.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: geonode-importer
-Version: 1.0.3
+Version: 1.0.4
 Home-page: https://github.com/GeoNode/geonode-importer
 Author: geosolutions-it
 Author-email: info@geosolutionsgroup.com
 Project-URL: Bug Tracker, https://github.com/geosolutions-it/geonode-importer/issues
 Platform: any
 Classifier: Environment :: Web Environment
 Classifier: Framework :: Django :: 3.0
```

## Comparing `geonode_importer-1.0.3.dist-info/RECORD` & `geonode_importer-1.0.4.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,77 +1,77 @@
-importer/__init__.py,sha256=mSMvUe1vcXbgioIg-h-G46y0ftjWGZkBHoOCOK0t5Vg,1132
+importer/__init__.py,sha256=fAf3d5YclDI61sqEhP4blLFTt2gcMYHGKsZXghI_XJQ,1132
 importer/apps.py,sha256=nym_44W069KBuqrWbDw64y7CetrA8xFPVgEBcP7F1_A,891
 importer/celery_app.py,sha256=vXVNHq_WntdNstgbKbVVJsOhTllf__ZWr-skmDohJvE,301
-importer/celery_tasks.py,sha256=rbx_r7w9K5MPJh_gIkdLF4lgDMgAh_D9Vcv7_0LoG_A,25673
-importer/datastore.py,sha256=OejV7R8G5fmvSGW3fJPGoE5DE3C_GZylYUIYOz2kbFg,1142
+importer/celery_tasks.py,sha256=Uq6kmnKu1QJS5vYDQYTYAefoKTys_jbRyZHlui7edCw,25927
+importer/datastore.py,sha256=m2jAY2_sgewRyJ5_E1RpVHAK2vcOKb2AZJng4bwixRc,1137
 importer/db_router.py,sha256=zY65wik8Z6ptJ3MggYxp4isFt11ohmk_5t2pIS9MBeA,2137
-importer/models.py,sha256=MuLxhMp8MnFT3J00dPX1gigk1K-ol6fmcpIJ1D6QvMg,1533
-importer/orchestrator.py,sha256=Lsljy5kA0VTeg1oS-0ok55CtFUNUt_L8hOLWxtvam00,13953
-importer/publisher.py,sha256=VpCunxt5gioLnKG53iugRqc4GY9a-THi1LnDgXvoJ6E,3689
+importer/models.py,sha256=JrI3mDIx6on3JZrwcXLSg2HZRxI5VkKFQYrb9gS_i-A,1575
+importer/orchestrator.py,sha256=VVjb_AyAPxTPVkk96QitoyfgLJ34v6LJhtHGnV_Ul-Y,14078
+importer/publisher.py,sha256=w8AA-DviaHhSVm8BHuk4wOGesMG0e7J0mkawGIc_F-Q,3718
 importer/settings.py,sha256=LNbTsJn01-yf162a9F3LSVWefKlbEgXpRiSyrpo34j0,399
 importer/urls.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-importer/utils.py,sha256=1oyiy1cLImmsfxu9bfzp0hcfHv8QItiTkxuDWheUZTY,1958
+importer/utils.py,sha256=9X4Fm17i07UgtlBBzh7ULgkiYKdjOWNJ3jWUCuh_kqk,1993
 importer/views.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/api/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/api/exception.py,sha256=cvkC0bnGK0JFzHg8yLhmoniB3Vr3XU65TcU_zLrPZn4,1675
-importer/api/serializer.py,sha256=tWiPoJscyaiqPfb06VwsACoFbrtJ0qD_o9wZajHTTHg,921
-importer/api/tests.py,sha256=Eq05iJ6nDXXjCt9-k0kr56fJpxB83JCz7EwD9-q7zf0,6264
+importer/api/serializer.py,sha256=-C_FpjP-mkfbTkOSzt7hHXZcUW0ZJlgeJRANzVaxkpw,920
+importer/api/tests.py,sha256=zrMeDGxLYDz79Sa_Qw8AU09kZ8pxmxuG2crdBTZZAVE,6262
 importer/api/urls.py,sha256=MlBuCZBa1tfHAMhJbx5zaUz6cvC-5V5y8cJ3lKx2cNE,479
-importer/api/views.py,sha256=IJFZdDzKmKduxYp3CWHS0agaTUoapXTd71eML1JmI-g,10650
+importer/api/views.py,sha256=Yj-sAgVno03lwHn0YhqGUzVue8Df_9S6y0JjD1Xs2u0,10648
 importer/handlers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-importer/handlers/apps.py,sha256=1sr3cX-SOq0pIk_MCpv9Gzvncruwg7m4fWc-QjwUOMk,2627
-importer/handlers/base.py,sha256=7acaVWIe-qBawmk09DciqE-HIXdlvOMZE7UM6LWTFCE,5126
+importer/handlers/apps.py,sha256=-Jjxu4-_Dqq4e6FWES5hmISZFlBCjiaFe66Ad6R4Ez0,2693
+importer/handlers/base.py,sha256=qanh-hpkSpWkwsKKnAuXuRinW4K_WzLqUNknoN09CuY,5823
 importer/handlers/tests.py,sha256=ov4UXqVw7r1FzKn356-fXqukLWRNI1Eya9fPpKGGmsE,2786
-importer/handlers/utils.py,sha256=_jPg0oZBwE9o0dmQRfwsDYBYjapdrLARMo2nuMZQxX4,5291
+importer/handlers/utils.py,sha256=W8Rb2iToOh-MRh9EwtqS6KRWR2adUAysnOFz3qFDz7w,5275
 importer/handlers/common/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-importer/handlers/common/raster.py,sha256=g5VokU0SGZxknRA3aiU4Rs-jbsf-zwBGFGYslNerXUs,20407
-importer/handlers/common/tests_raster.py,sha256=utZSrN3f_cJmoOQwhBpnsC6exslTcO-p6QdLUnUK2BU,3184
-importer/handlers/common/tests_vector.py,sha256=y8fUy_qXAjWpMl5yUwAUc_umNjSkb4JP7KzhA6YaOCA,10000
-importer/handlers/common/vector.py,sha256=qyLZkdWHD_pXKuFxu2vT5WUNzBd8Os3wJYP7vcCM9rg,34026
+importer/handlers/common/raster.py,sha256=Qv_d9GVaRrgJxS1FxKygrE951BXu80MzZ9CCorQLcic,21447
+importer/handlers/common/tests_raster.py,sha256=a6d-cWz0_SYT0l2-51xUus7GJ-nPeavQbQXHq_Jv98U,3162
+importer/handlers/common/tests_vector.py,sha256=LhRuzHR8WY_uu0b6tjF40bmRi8SbSrfK9ft9-Uki_0o,10076
+importer/handlers/common/vector.py,sha256=ru_btLxgeq4C1BPD2Hp-U1aSa2-DySkjzPFLZTEI86g,36000
 importer/handlers/csv/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/handlers/csv/exceptions.py,sha256=WKpt-dN1O1OroyUJx7jztuJPxe0FRks5qGpoZlKVucM,284
-importer/handlers/csv/handler.py,sha256=ch0YgWx1DH-tpb1UIwRRgZewxdR8R9-_Jhp9TaGbonw,8993
-importer/handlers/csv/tests.py,sha256=oe6g9YS12owzHHxDyGu0PjJswV1uW_vNuqX0XNnFh2A,7043
+importer/handlers/csv/handler.py,sha256=Ru1zFwLezNtv-UDgnIZZL6woclKtevINGd7pwOZmQ4s,9585
+importer/handlers/csv/tests.py,sha256=4WgjGk2Rr5Yjtpf8E1wiKYfB6Ud6nWJZL_pfx8BEX-0,7042
 importer/handlers/geojson/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/handlers/geojson/exceptions.py,sha256=M4JAZguS2u-75i3TAcTyA_biGJW3O6BSW782mAQcI8k,296
-importer/handlers/geojson/handler.py,sha256=nJsD3SzfgXtaTUsRTaE8EROu7q_0zWrCKStTSJxQ8ds,3555
-importer/handlers/geojson/tests.py,sha256=MwkWfsX-s14xGtOvua_Ec9c-pwDGGRdChUgCy5NaAeE,5430
+importer/handlers/geojson/handler.py,sha256=Okawp5qS-IaN1qPsNCcsV5IhFroBbkeRHt49S9IdXdg,3553
+importer/handlers/geojson/tests.py,sha256=r1lUt0QYF7cIr21rE8FPrc_EPDtHN3WTtT47MD-HfK8,5483
 importer/handlers/geotiff/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/handlers/geotiff/exceptions.py,sha256=2KGd-1rIJxDNcD4vkRoUdwk6QyA0ML__TivrGQWr8oA,290
-importer/handlers/geotiff/handler.py,sha256=QQBXcGax1-XVDvsLD70_4R__cR68huS_tDcUzRRwJa0,2728
-importer/handlers/geotiff/tests.py,sha256=dO69s9xAqrknf4ONiMQg49dUNYADwjhbJbhVSEbv0T8,4136
+importer/handlers/geotiff/handler.py,sha256=V3Rz0eYZ8YrahK1ubrO_akffBdnHmCWq2LfhuA-XAik,2729
+importer/handlers/geotiff/tests.py,sha256=vFeFlfz-YxT5HHJGIB04dHXgY1dLgKPTUfhdIaa_jEs,4105
 importer/handlers/gpkg/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/handlers/gpkg/exceptions.py,sha256=AqVpMTPSZCvwSgd9klCAxZlbYUeBPz-sjCfrsE3QyTw,299
-importer/handlers/gpkg/handler.py,sha256=_1kXcaIFAmRs4Xb3Ojonm54rpugZKI69V5PPGo8zA8w,4699
-importer/handlers/gpkg/tasks.py,sha256=qZbKy1fQn-39GXan7BMvz1ac1MK8F2QfMz6D5NzR6sM,566
-importer/handlers/gpkg/tests.py,sha256=WWdCpdoejz2l79cUtUEXqmtYWvwqqzZerEBj5BZyDCg,5606
+importer/handlers/gpkg/handler.py,sha256=BfeI8S-pI8T9Xk8S1jixrjkZUbm9c8DHBpHRrJUSKXs,4994
+importer/handlers/gpkg/tasks.py,sha256=UMm_jEJL3FsGl8dW0WV5j8nCYHAxdUL11ef07OTFlYE,565
+importer/handlers/gpkg/tests.py,sha256=ar3hfeFrYyr8Y4goyaOUTEL54qRooeL_Ms8OuIGfoMI,5605
 importer/handlers/kml/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/handlers/kml/exceptions.py,sha256=eCF-oSy3Xk-zKveLpBQtoLQfsHk5gG_9vIufMdDU234,284
-importer/handlers/kml/handler.py,sha256=zry-MyqANyvKPNRlbIQgON20ry7AgNkiGZjCEGG9d-w,4508
-importer/handlers/kml/tests.py,sha256=x5onmP1BPC9Af9aP5L_z-RfuFzr46BpvfFSeqBxrnGc,4273
+importer/handlers/kml/handler.py,sha256=2TJwsChp2rPw_W-Js0ftNRLCRQRxIXazCMJ6YNAAmtM,4566
+importer/handlers/kml/tests.py,sha256=opipzkoRsWbjHl7GZKNd1AcZ09nuki3ZFXoUCK5fZ5c,4251
 importer/handlers/shapefile/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/handlers/shapefile/exceptions.py,sha256=3zOLlarTh9_xS8cOdf_HdA3mHzeopKSmsSG8AAwJpOg,302
-importer/handlers/shapefile/handler.py,sha256=wlKAIOpNkdLoaLNhNVRgdE5BK7nQy9jCAvwG7dLTWRA,5562
-importer/handlers/shapefile/serializer.py,sha256=mMQrfe6p3jcEIWIP1XBidzXaEMhkmEwehCXwE50tyvU,1039
-importer/handlers/shapefile/tests.py,sha256=boFIlI0eBeh8L5nrdhsx2OWJI39APtKA9fWMin33Y3k,5650
-importer/migrations/0001_initial.py,sha256=S7oonhRfSMiquei800v5umCSrqkNvfX7hqrGCWXi34U,1031
-importer/migrations/0002_resourcehandlerinfo_kwargs.py,sha256=fT3ncsBvO5LAJRhN4ECIL9WsLKXDNZ1AwFMpu9IqRYY,455
-importer/migrations/0003_resourcehandlerinfo_execution_id.py,sha256=V7Ow2Gaq3YdKGQcRM6XKifT4cIpM6ZbiTRb2MkuKWzQ,595
-importer/migrations/0004_rename_execution_id_resourcehandlerinfo_execution_request.py,sha256=_TwrOC_6ORRMWEa9fg8CqRPC6vDy2Hnbh6Wg9Jtq2kE,409
-importer/migrations/0005_fixup_dynamic_shema_table_names.py,sha256=9B2Me4Tkx3gnLBTP7UpjdUD0YVF6qUtVWLAm0EsO-hg,1106
-importer/migrations/0006_dataset_migration.py,sha256=Jn14JxlGL2FFTSwC-hcm7gJcESd_c62aTT9y6os_njo,1584
+importer/handlers/shapefile/handler.py,sha256=e_1KZrfI0mbBQHN2KpF3hQMa-XVNR14TNkRtlrtmT-c,5770
+importer/handlers/shapefile/serializer.py,sha256=XX2uKpwGcINfSmSIYytsttvXSjg3u9eN_JlNxpdUJaI,1111
+importer/handlers/shapefile/tests.py,sha256=XyBx3Pv-VNeEtPHF6mdq1BBI6e9Js-QYiEQLxzn2cao,5755
+importer/migrations/0001_initial.py,sha256=imLf8_-4c0ntRjtAPH_35Zd3ziiwlNZKtMgkgABY7hM,1030
+importer/migrations/0002_resourcehandlerinfo_kwargs.py,sha256=caoCKkdqcypGnxO6opArpUbFBZ_PBORODlIWzt5p0jA,501
+importer/migrations/0003_resourcehandlerinfo_execution_id.py,sha256=Ya6oRdwZIAQ3IIuGdI3nSl_vTGLrVmZaXql71hmmcgI,673
+importer/migrations/0004_rename_execution_id_resourcehandlerinfo_execution_request.py,sha256=z5Uh7ELS6nTyyFAx4lHzoSOiRegLWZCmyQWzPYTTinQ,408
+importer/migrations/0005_fixup_dynamic_shema_table_names.py,sha256=iKl2BEi3TNknQmfPClfoP_k4iT8Kl12jPrQvmSxx4JI,1097
+importer/migrations/0006_dataset_migration.py,sha256=bzQfT2-XJ0VME6EoClQbjyMDp4AUxmL__8Ig3VgsXTc,1571
 importer/migrations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-importer/tests/utils.py,sha256=biJXuf-Tyhzly_SklA4YzxsllGm50Vmqwm0HIPqGzWU,1815
+importer/tests/utils.py,sha256=6ipagaatvK-sltI_SVTe-aByucvS3sYMtDN4vlo1YZY,1814
 importer/tests/end2end/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-importer/tests/end2end/test_end2end.py,sha256=Qh4P3o9LChneY-pKdpP459EzmBv1XwGwYfiWWXT1-_s,6095
-importer/tests/end2end/test_end2end_copy.py,sha256=QDmUCZOuhYcB8ie_NDWAw2NIJhcAenlnWimpWzzrIA8,7707
+importer/tests/end2end/test_end2end.py,sha256=hJ8XRQbAQHTv8VsBAbnCBidu9UqbFdDrcqVNXHuckxU,6313
+importer/tests/end2end/test_end2end_copy.py,sha256=Gry4ZBqNSzorlnzVW2SidqVIcNuLrC71jP6fF8bUIck,7742
 importer/tests/unit/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-importer/tests/unit/test_models.py,sha256=cykml2VUVbiCmdeTTG7pqKAdZiCu0kAALxH7AVSWrC0,1234
+importer/tests/unit/test_models.py,sha256=n9uwSOzEv7fPy4qSPurXhvfZlAvaeKXrcJBixs2WcJM,1269
 importer/tests/unit/test_orchestrator.py,sha256=M5KueNjjCC3hPfQRepZZD7oo58tDbM7sakmYx-ffUo8,13739
 importer/tests/unit/test_publisher.py,sha256=u6juJPEWoQoKGhfjt6h4D160PDkF8dKuZiiPwwN3XZU,3606
-importer/tests/unit/test_task.py,sha256=AYl0z7IvUMfHKt_bmIdsCaLL0Rn6M6pXHF4zxD8MdsI,23904
-geonode_importer-1.0.3.dist-info/LICENSE,sha256=gMS2YAekulHmzbeQAk-e6U1_unOcCszd6aKY5bQJXO0,1069
-geonode_importer-1.0.3.dist-info/METADATA,sha256=AgcwitUJ67cib_T_OUXc11vAAbs9ZYce7cGh8zQLitY,712
-geonode_importer-1.0.3.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-geonode_importer-1.0.3.dist-info/top_level.txt,sha256=MRQ0MhtKdXF90IDYri2Z5uPJ8A9O0ITe5bEXA2ZLjM4,9
-geonode_importer-1.0.3.dist-info/RECORD,,
+importer/tests/unit/test_task.py,sha256=jW5oS093lYF8NfYxE1Q61P1q5QtVVx1wIdGg311D9AM,24454
+geonode_importer-1.0.4.dist-info/LICENSE,sha256=gMS2YAekulHmzbeQAk-e6U1_unOcCszd6aKY5bQJXO0,1069
+geonode_importer-1.0.4.dist-info/METADATA,sha256=iiOr1MqLsWkQaht6QY_Ud8bQ07GIhvTGSOcrbiq1tgA,712
+geonode_importer-1.0.4.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+geonode_importer-1.0.4.dist-info/top_level.txt,sha256=MRQ0MhtKdXF90IDYri2Z5uPJ8A9O0ITe5bEXA2ZLjM4,9
+geonode_importer-1.0.4.dist-info/RECORD,,
```

