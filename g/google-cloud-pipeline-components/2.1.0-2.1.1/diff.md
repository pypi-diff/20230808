# Comparing `tmp/google_cloud_pipeline_components-2.1.0-py3-none-any.whl.zip` & `tmp/google_cloud_pipeline_components-2.1.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,429 +1,434 @@
-Zip file size: 1206592 bytes, number of entries: 427
--rw-r--r--  2.0 unx      711 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/__init__.py
--rw-r--r--  2.0 unx      792 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_image.py
--rw-r--r--  2.0 unx    10685 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/utils.py
--rw-r--r--  2.0 unx      677 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/version.py
--rw-r--r--  2.0 unx      606 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/__init__.py
--rw-r--r--  2.0 unx      811 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model/__init__.py
--rw-r--r--  2.0 unx      662 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model/get_model/__init__.py
--rw-r--r--  2.0 unx     2307 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model/get_model/component.py
--rw-r--r--  2.0 unx     2478 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/__init__.py
--rw-r--r--  2.0 unx     3936 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/utils.py
--rw-r--r--  2.0 unx      963 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/version.py
--rw-r--r--  2.0 unx      669 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/__init__.py
--rw-r--r--  2.0 unx     5422 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/component.py
--rw-r--r--  2.0 unx      677 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/__init__.py
--rw-r--r--  2.0 unx     7004 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/component.py
--rw-r--r--  2.0 unx      671 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/error_analysis_annotation/__init__.py
--rw-r--r--  2.0 unx     4092 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/error_analysis_annotation/component.py
--rw-r--r--  2.0 unx      677 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/evaluated_annotation/__init__.py
--rw-r--r--  2.0 unx     5842 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/evaluated_annotation/component.py
--rw-r--r--  2.0 unx      674 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/feature_extractor/__init__.py
--rw-r--r--  2.0 unx     5539 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/feature_extractor/component.py
--rw-r--r--  2.0 unx      684 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluated_annotation/__init__.py
--rw-r--r--  2.0 unx     3053 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluated_annotation/component.py
--rw-r--r--  2.0 unx      674 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluation/__init__.py
--rw-r--r--  2.0 unx     6563 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluation/component.py
--rw-r--r--  2.0 unx      671 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/__init__.py
--rw-r--r--  2.0 unx     7492 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/component.py
--rw-r--r--  2.0 unx      682 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/__init__.py
--rw-r--r--  2.0 unx     5708 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/component.py
--rw-r--r--  2.0 unx      651 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/__init__.py
--rw-r--r--  2.0 unx      157 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/cloudbuild.yaml
--rw-r--r--  2.0 unx      659 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/_implementation/__init__.py
--rw-r--r--  2.0 unx      668 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/_implementation/model/__init__.py
--rw-r--r--  2.0 unx      685 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/_implementation/model/get_model/__init__.py
--rw-r--r--  2.0 unx     4989 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/_implementation/model/get_model/get_model.py
--rw-r--r--  2.0 unx      678 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/_implementation/model_evaluation/__init__.py
--rw-r--r--  2.0 unx    10196 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/_implementation/model_evaluation/import_evaluated_annotation.py
--rw-r--r--  2.0 unx    16626 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/_implementation/model_evaluation/import_model_evaluation.py
--rw-r--r--  2.0 unx      651 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/preview/__init__.py
--rw-r--r--  2.0 unx      655 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/preview/dataflow/__init__.py
--rw-r--r--  2.0 unx      698 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/preview/dataflow/flex_template/__init__.py
--rw-r--r--  2.0 unx     1914 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/preview/dataflow/flex_template/launcher.py
--rw-r--r--  2.0 unx     9632 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/preview/dataflow/flex_template/remote_runner.py
--rw-r--r--  2.0 unx      638 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/utils/__init__.py
--rw-r--r--  2.0 unx     1843 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/utils/execution_context.py
--rw-r--r--  2.0 unx      651 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/__init__.py
--rw-r--r--  2.0 unx      651 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/aiplatform/__init__.py
--rw-r--r--  2.0 unx    10719 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/aiplatform/remote_runner.py
--rw-r--r--  2.0 unx     5257 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/aiplatform/utils.py
--rw-r--r--  2.0 unx      696 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/batch_prediction_job/__init__.py
--rw-r--r--  2.0 unx     2364 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/batch_prediction_job/launcher.py
--rw-r--r--  2.0 unx     9386 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/batch_prediction_job/remote_runner.py
--rw-r--r--  2.0 unx      655 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/__init__.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/create_model/__init__.py
--rw-r--r--  2.0 unx     2502 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/create_model/launcher.py
--rw-r--r--  2.0 unx     3839 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/create_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/__init__.py
--rw-r--r--  2.0 unx     3284 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/launcher.py
--rw-r--r--  2.0 unx     5770 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/drop_model/__init__.py
--rw-r--r--  2.0 unx     2627 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/drop_model/launcher.py
--rw-r--r--  2.0 unx     3306 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/drop_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/__init__.py
--rw-r--r--  2.0 unx     3307 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/launcher.py
--rw-r--r--  2.0 unx     5588 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/__init__.py
--rw-r--r--  2.0 unx     2958 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/launcher.py
--rw-r--r--  2.0 unx     3432 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/__init__.py
--rw-r--r--  2.0 unx     3639 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/launcher.py
--rw-r--r--  2.0 unx     5843 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/export_model/__init__.py
--rw-r--r--  2.0 unx     2775 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/export_model/launcher.py
--rw-r--r--  2.0 unx     4706 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/export_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/feature_importance/__init__.py
--rw-r--r--  2.0 unx     2656 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/feature_importance/launcher.py
--rw-r--r--  2.0 unx     3576 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/feature_importance/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/__init__.py
--rw-r--r--  2.0 unx     2932 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/launcher.py
--rw-r--r--  2.0 unx     3384 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/global_explain/__init__.py
--rw-r--r--  2.0 unx     2807 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/global_explain/launcher.py
--rw-r--r--  2.0 unx     2676 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/global_explain/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/__init__.py
--rw-r--r--  2.0 unx     2653 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/launcher.py
--rw-r--r--  2.0 unx     4060 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/__init__.py
--rw-r--r--  2.0 unx     2655 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/launcher.py
--rw-r--r--  2.0 unx     3455 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/__init__.py
--rw-r--r--  2.0 unx     2822 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/launcher.py
--rw-r--r--  2.0 unx     4676 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/__init__.py
--rw-r--r--  2.0 unx     2780 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/launcher.py
--rw-r--r--  2.0 unx     4651 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/__init__.py
--rw-r--r--  2.0 unx     3319 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/launcher.py
--rw-r--r--  2.0 unx     4731 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/__init__.py
--rw-r--r--  2.0 unx     2641 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/launcher.py
--rw-r--r--  2.0 unx     4032 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/__init__.py
--rw-r--r--  2.0 unx     2676 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/launcher.py
--rw-r--r--  2.0 unx     3194 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/__init__.py
--rw-r--r--  2.0 unx     2665 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/launcher.py
--rw-r--r--  2.0 unx     3166 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_recommend/__init__.py
--rw-r--r--  2.0 unx     3086 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_recommend/launcher.py
--rw-r--r--  2.0 unx     4050 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_recommend/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_reconstruction_loss/__init__.py
--rw-r--r--  2.0 unx     3115 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_reconstruction_loss/launcher.py
--rw-r--r--  2.0 unx     4337 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_reconstruction_loss/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/__init__.py
--rw-r--r--  2.0 unx     3302 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/launcher.py
--rw-r--r--  2.0 unx     4613 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/__init__.py
--rw-r--r--  2.0 unx     2644 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/launcher.py
--rw-r--r--  2.0 unx     4046 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/__init__.py
--rw-r--r--  2.0 unx     2635 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/launcher.py
--rw-r--r--  2.0 unx     4225 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_weights/__init__.py
--rw-r--r--  2.0 unx     2627 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_weights/launcher.py
--rw-r--r--  2.0 unx     4015 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/ml_weights/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/predict_model/__init__.py
--rw-r--r--  2.0 unx     3302 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/predict_model/launcher.py
--rw-r--r--  2.0 unx     4578 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/predict_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/query_job/__init__.py
--rw-r--r--  2.0 unx     2473 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/query_job/launcher.py
--rw-r--r--  2.0 unx     2539 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/query_job/remote_runner.py
--rw-r--r--  2.0 unx      673 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/utils/__init__.py
--rw-r--r--  2.0 unx    12250 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/bigquery/utils/bigquery_util.py
--rw-r--r--  2.0 unx      686 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/custom_job/__init__.py
--rw-r--r--  2.0 unx     1917 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/custom_job/launcher.py
--rw-r--r--  2.0 unx     3586 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/custom_job/remote_runner.py
--rw-r--r--  2.0 unx      662 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataflow/__init__.py
--rw-r--r--  2.0 unx     3303 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataflow/dataflow_launcher.py
--rw-r--r--  2.0 unx     7415 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataflow/dataflow_python_job_remote_runner.py
--rw-r--r--  2.0 unx      699 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/__init__.py
--rw-r--r--  2.0 unx      705 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/create_pyspark_batch/__init__.py
--rw-r--r--  2.0 unx     2196 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/create_pyspark_batch/launcher.py
--rw-r--r--  2.0 unx      804 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/create_pyspark_batch/remote_runner.py
--rw-r--r--  2.0 unx      703 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/create_spark_batch/__init__.py
--rw-r--r--  2.0 unx     2188 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/create_spark_batch/launcher.py
--rw-r--r--  2.0 unx      800 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/create_spark_batch/remote_runner.py
--rw-r--r--  2.0 unx      705 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/create_spark_r_batch/__init__.py
--rw-r--r--  2.0 unx     2195 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/create_spark_r_batch/launcher.py
--rw-r--r--  2.0 unx      804 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/create_spark_r_batch/remote_runner.py
--rw-r--r--  2.0 unx      707 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/create_spark_sql_batch/__init__.py
--rw-r--r--  2.0 unx     2203 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/create_spark_sql_batch/launcher.py
--rw-r--r--  2.0 unx      808 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/create_spark_sql_batch/remote_runner.py
--rw-r--r--  2.0 unx      663 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/utils/__init__.py
--rw-r--r--  2.0 unx    12872 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/dataproc/utils/dataproc_util.py
--rw-r--r--  2.0 unx      655 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/endpoint/__init__.py
--rw-r--r--  2.0 unx      691 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/__init__.py
--rw-r--r--  2.0 unx     2288 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/launcher.py
--rw-r--r--  2.0 unx     2377 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/remote_runner.py
--rw-r--r--  2.0 unx      691 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/__init__.py
--rw-r--r--  2.0 unx     1938 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/launcher.py
--rw-r--r--  2.0 unx     2308 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/__init__.py
--rw-r--r--  2.0 unx     1926 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/launcher.py
--rw-r--r--  2.0 unx     2375 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/remote_runner.py
--rw-r--r--  2.0 unx      690 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/__init__.py
--rw-r--r--  2.0 unx     1934 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/launcher.py
--rw-r--r--  2.0 unx     4178 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/remote_runner.py
--rw-r--r--  2.0 unx      662 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/gcp_launcher/__init__.py
--rw-r--r--  2.0 unx     8350 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/gcp_launcher/job_remote_runner.py
--rw-r--r--  2.0 unx     4945 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/gcp_launcher/lro_remote_runner.py
--rw-r--r--  2.0 unx    10315 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/gcp_launcher/pipeline_remote_runner.py
--rw-r--r--  2.0 unx      660 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/__init__.py
--rw-r--r--  2.0 unx     2955 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/artifact_util.py
--rw-r--r--  2.0 unx      876 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/error_util.py
--rw-r--r--  2.0 unx     2193 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/gcp_labels_util.py
--rw-r--r--  2.0 unx     2951 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/json_util.py
--rw-r--r--  2.0 unx     1788 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/parser_util.py
--rw-r--r--  2.0 unx      701 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/__init__.py
--rw-r--r--  2.0 unx     2240 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/launcher.py
--rw-r--r--  2.0 unx     4402 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/remote_runner.py
--rw-r--r--  2.0 unx      696 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/infra_validation_job/__init__.py
--rw-r--r--  2.0 unx     2280 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/infra_validation_job/launcher.py
--rw-r--r--  2.0 unx     4068 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/infra_validation_job/remote_runner.py
--rw-r--r--  2.0 unx      652 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/model/__init__.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/model/delete_model/__init__.py
--rw-r--r--  2.0 unx     1923 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/model/delete_model/launcher.py
--rw-r--r--  2.0 unx     2307 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/model/delete_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/model/export_model/__init__.py
--rw-r--r--  2.0 unx     2201 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/model/export_model/launcher.py
--rw-r--r--  2.0 unx     2479 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/model/export_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/model/upload_model/__init__.py
--rw-r--r--  2.0 unx     2331 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/model/upload_model/launcher.py
--rw-r--r--  2.0 unx     4405 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/model/upload_model/remote_runner.py
--rw-r--r--  2.0 unx      662 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/vertex_notification_email/__init__.py
--rw-r--r--  2.0 unx     1074 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/vertex_notification_email/executor.py
--rw-r--r--  2.0 unx      694 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/wait_gcp_resources/__init__.py
--rw-r--r--  2.0 unx     1928 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/wait_gcp_resources/launcher.py
--rw-r--r--  2.0 unx     5968 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/container/v1/wait_gcp_resources/remote_runner.py
--rw-r--r--  2.0 unx      606 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/__init__.py
--rw-r--r--  2.0 unx      606 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/__init__.py
--rw-r--r--  2.0 unx     1230 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/forecasting/__init__.py
--rw-r--r--  2.0 unx     5827 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/forecasting/forecasting_ensemble.py
--rw-r--r--  2.0 unx     6656 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/forecasting/forecasting_stage_1_tuner.py
--rw-r--r--  2.0 unx     6456 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/forecasting/forecasting_stage_2_tuner.py
--rw-r--r--  2.0 unx   362451 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/forecasting/learn_to_learn_forecasting_pipeline.yaml
--rw-r--r--  2.0 unx   360628 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/forecasting/sequence_to_sequence_forecasting_pipeline.yaml
--rw-r--r--  2.0 unx   359730 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/forecasting/temporal_fusion_transformer_forecasting_pipeline.yaml
--rw-r--r--  2.0 unx   362527 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/forecasting/time_series_dense_encoder_forecasting_pipeline.yaml
--rw-r--r--  2.0 unx    53072 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/forecasting/utils.py
--rw-r--r--  2.0 unx     2114 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/__init__.py
--rw-r--r--  2.0 unx   515002 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/automl_tabular_feature_selection_pipeline.yaml
--rw-r--r--  2.0 unx   391133 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/automl_tabular_v2_pipeline.yaml
--rw-r--r--  2.0 unx     7674 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/feature_selection.py
--rw-r--r--  2.0 unx    51128 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/feature_transform_engine.py
--rw-r--r--  2.0 unx    10164 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/tabnet_hyperparameter_tuning_job.py
--rw-r--r--  2.0 unx   242992 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/tabnet_hyperparameter_tuning_job_pipeline.yaml
--rw-r--r--  2.0 unx    13060 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/tabnet_trainer.py
--rw-r--r--  2.0 unx   201451 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/tabnet_trainer_pipeline.yaml
--rw-r--r--  2.0 unx   165844 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/utils.py
--rw-r--r--  2.0 unx    10161 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_hyperparameter_tuning_job.py
--rw-r--r--  2.0 unx   196354 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_hyperparameter_tuning_job_pipeline.yaml
--rw-r--r--  2.0 unx    12170 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_trainer.py
--rw-r--r--  2.0 unx   191459 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_trainer_pipeline.yaml
--rw-r--r--  2.0 unx     4915 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/xgboost_hyperparameter_tuning_job.py
--rw-r--r--  2.0 unx   225088 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/xgboost_hyperparameter_tuning_job_pipeline.yaml
--rw-r--r--  2.0 unx     2341 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/xgboost_trainer.py
--rw-r--r--  2.0 unx   209094 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/xgboost_trainer_pipeline.yaml
--rw-r--r--  2.0 unx     3292 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_large_search_space.json
--rw-r--r--  2.0 unx     3261 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_medium_search_space.json
--rw-r--r--  2.0 unx     2975 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_small_search_space.json
--rw-r--r--  2.0 unx     3287 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_large_search_space.json
--rw-r--r--  2.0 unx     3266 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_medium_search_space.json
--rw-r--r--  2.0 unx     2974 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_small_search_space.json
--rw-r--r--  2.0 unx     3269 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_large_search_space.json
--rw-r--r--  2.0 unx     3254 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_medium_search_space.json
--rw-r--r--  2.0 unx     2966 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_small_search_space.json
--rw-r--r--  2.0 unx     2647 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/configs/wide_and_deep_params.json
--rw-r--r--  2.0 unx     7748 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/automl/tabular/configs/xgboost_params.json
--rw-r--r--  2.0 unx      828 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/dataflow/__init__.py
--rw-r--r--  2.0 unx      669 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/dataflow/flex_template/__init__.py
--rw-r--r--  2.0 unx    11929 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/dataflow/flex_template/component.py
--rw-r--r--  2.0 unx     1173 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/model_evaluation/__init__.py
--rw-r--r--  2.0 unx     5909 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/model_evaluation/data_bias_component.py
--rw-r--r--  2.0 unx     7509 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/model_evaluation/feature_attribution_component.py
--rw-r--r--  2.0 unx     6842 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/model_evaluation/model_bias_component.py
--rw-r--r--  2.0 unx     7567 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/preview/model_evaluation/utils.py
--rw-r--r--  2.0 unx      661 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/proto/__init__.py
--rw-r--r--  2.0 unx     7687 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/proto/gcp_resources_pb2.py
--rw-r--r--  2.0 unx      606 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/types/__init__.py
--rw-r--r--  2.0 unx    23260 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/types/artifact_types.py
--rw-r--r--  2.0 unx      765 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/__init__.py
--rw-r--r--  2.0 unx      631 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/__init__.py
--rw-r--r--  2.0 unx      806 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/forecasting/__init__.py
--rw-r--r--  2.0 unx    52018 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_predict_pipeline.yaml
--rw-r--r--  2.0 unx   246379 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_train_pipeline.yaml
--rw-r--r--  2.0 unx    94627 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/forecasting/prophet_predict_pipeline.yaml
--rw-r--r--  2.0 unx     8498 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer.py
--rw-r--r--  2.0 unx   144373 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer_pipeline.yaml
--rw-r--r--  2.0 unx    15028 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/forecasting/utils.py
--rw-r--r--  2.0 unx     2034 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/tabular/__init__.py
--rw-r--r--  2.0 unx   502269 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/tabular/automl_tabular_pipeline.yaml
--rw-r--r--  2.0 unx     6756 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/tabular/cv_trainer.py
--rw-r--r--  2.0 unx     6991 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/tabular/ensemble.py
--rw-r--r--  2.0 unx     3116 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/tabular/finalizer.py
--rw-r--r--  2.0 unx     1341 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/tabular/infra_validator.py
--rw-r--r--  2.0 unx     5571 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/tabular/split_materialized_data.py
--rw-r--r--  2.0 unx     7865 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/tabular/stage_1_tuner.py
--rw-r--r--  2.0 unx    13707 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/tabular/stats_and_example_gen.py
--rw-r--r--  2.0 unx    12002 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/tabular/training_configurator_and_validator.py
--rw-r--r--  2.0 unx     8305 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/tabular/transform.py
--rw-r--r--  2.0 unx    74707 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/tabular/utils.py
--rw-r--r--  2.0 unx   321170 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/tabular/deprecated/default_pipeline.json
--rw-r--r--  2.0 unx     1785 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/training_job/__init__.py
--rw-r--r--  2.0 unx      655 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/training_job/automl_forecasting_training_job/__init__.py
--rw-r--r--  2.0 unx    28961 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/training_job/automl_forecasting_training_job/component.py
--rw-r--r--  2.0 unx      649 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/training_job/automl_image_training_job/__init__.py
--rw-r--r--  2.0 unx    16657 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/training_job/automl_image_training_job/component.py
--rw-r--r--  2.0 unx      651 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/training_job/automl_tabular_training_job/__init__.py
--rw-r--r--  2.0 unx    22399 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/training_job/automl_tabular_training_job/component.py
--rw-r--r--  2.0 unx      648 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/training_job/automl_text_training_job/__init__.py
--rw-r--r--  2.0 unx    10058 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/training_job/automl_text_training_job/component.py
--rw-r--r--  2.0 unx      649 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/training_job/automl_video_training_job/__init__.py
--rw-r--r--  2.0 unx    10127 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/automl/training_job/automl_video_training_job/component.py
--rw-r--r--  2.0 unx      973 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/batch_predict_job/__init__.py
--rw-r--r--  2.0 unx    21662 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/batch_predict_job/component.py
--rw-r--r--  2.0 unx     5481 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/__init__.py
--rw-r--r--  2.0 unx      670 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/create_model/__init__.py
--rw-r--r--  2.0 unx     4129 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/create_model/component.py
--rw-r--r--  2.0 unx      680 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/detect_anomalies_model/__init__.py
--rw-r--r--  2.0 unx     7154 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/detect_anomalies_model/component.py
--rw-r--r--  2.0 unx      668 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/drop_model/__init__.py
--rw-r--r--  2.0 unx     4020 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/drop_model/component.py
--rw-r--r--  2.0 unx      672 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/evaluate_model/__init__.py
--rw-r--r--  2.0 unx     6437 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/evaluate_model/component.py
--rw-r--r--  2.0 unx      680 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/explain_forecast_model/__init__.py
--rw-r--r--  2.0 unx     5909 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/explain_forecast_model/component.py
--rw-r--r--  2.0 unx      679 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/explain_predict_model/__init__.py
--rw-r--r--  2.0 unx     7371 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/explain_predict_model/component.py
--rw-r--r--  2.0 unx      670 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/export_model/__init__.py
--rw-r--r--  2.0 unx     3995 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/export_model/component.py
--rw-r--r--  2.0 unx      676 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/feature_importance/__init__.py
--rw-r--r--  2.0 unx     5202 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/feature_importance/component.py
--rw-r--r--  2.0 unx      672 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/forecast_model/__init__.py
--rw-r--r--  2.0 unx     5844 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/forecast_model/component.py
--rw-r--r--  2.0 unx      672 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/global_explain/__init__.py
--rw-r--r--  2.0 unx     4325 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/global_explain/component.py
--rw-r--r--  2.0 unx      677 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_advanced_weights/__init__.py
--rw-r--r--  2.0 unx     4575 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_advanced_weights/component.py
--rw-r--r--  2.0 unx      679 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_arima_coefficients/__init__.py
--rw-r--r--  2.0 unx     4785 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_arima_coefficients/component.py
--rw-r--r--  2.0 unx      675 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_arima_evaluate/__init__.py
--rw-r--r--  2.0 unx     5755 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_arima_evaluate/component.py
--rw-r--r--  2.0 unx      670 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_centroids/__init__.py
--rw-r--r--  2.0 unx     5550 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_centroids/component.py
--rw-r--r--  2.0 unx      677 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_confusion_matrix/__init__.py
--rw-r--r--  2.0 unx     5555 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_confusion_matrix/component.py
--rw-r--r--  2.0 unx      673 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_feature_info/__init__.py
--rw-r--r--  2.0 unx     4540 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_feature_info/component.py
--rw-r--r--  2.0 unx      685 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_principal_component_info/__init__.py
--rw-r--r--  2.0 unx     5429 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_principal_component_info/component.py
--rw-r--r--  2.0 unx      681 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_principal_components/__init__.py
--rw-r--r--  2.0 unx     5375 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_principal_components/component.py
--rw-r--r--  2.0 unx      670 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_recommend/__init__.py
--rw-r--r--  2.0 unx     5918 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_recommend/component.py
--rw-r--r--  2.0 unx      680 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_reconstruction_loss/__init__.py
--rw-r--r--  2.0 unx     6021 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_reconstruction_loss/component.py
--rw-r--r--  2.0 unx      670 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_roc_curve/__init__.py
--rw-r--r--  2.0 unx     5439 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_roc_curve/component.py
--rw-r--r--  2.0 unx      674 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_training_info/__init__.py
--rw-r--r--  2.0 unx     4580 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_training_info/component.py
--rw-r--r--  2.0 unx      671 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_trial_info/__init__.py
--rw-r--r--  2.0 unx     5106 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_trial_info/component.py
--rw-r--r--  2.0 unx      668 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_weights/__init__.py
--rw-r--r--  2.0 unx     4511 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/ml_weights/component.py
--rw-r--r--  2.0 unx      671 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/predict_model/__init__.py
--rw-r--r--  2.0 unx     6400 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/predict_model/component.py
--rw-r--r--  2.0 unx      667 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/query_job/__init__.py
--rw-r--r--  2.0 unx     4985 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/bigquery/query_job/component.py
--rw-r--r--  2.0 unx     1208 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/custom_job/__init__.py
--rw-r--r--  2.0 unx     6178 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/custom_job/component.py
--rw-r--r--  2.0 unx    15856 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/custom_job/utils.py
--rw-r--r--  2.0 unx      878 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataflow/__init__.py
--rw-r--r--  2.0 unx      664 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataflow/python_job/__init__.py
--rw-r--r--  2.0 unx     2577 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataflow/python_job/component.py
--rw-r--r--  2.0 unx     1426 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataproc/__init__.py
--rw-r--r--  2.0 unx      670 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataproc/create_pyspark_batch/__init__.py
--rw-r--r--  2.0 unx     6861 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataproc/create_pyspark_batch/component.py
--rw-r--r--  2.0 unx      668 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataproc/create_spark_batch/__init__.py
--rw-r--r--  2.0 unx     6812 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataproc/create_spark_batch/component.py
--rw-r--r--  2.0 unx      669 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataproc/create_spark_r_batch/__init__.py
--rw-r--r--  2.0 unx     6356 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataproc/create_spark_r_batch/component.py
--rw-r--r--  2.0 unx      673 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataproc/create_spark_sql_batch/__init__.py
--rw-r--r--  2.0 unx     6136 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataproc/create_spark_sql_batch/component.py
--rw-r--r--  2.0 unx     3089 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/__init__.py
--rw-r--r--  2.0 unx      644 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/create_image_dataset/__init__.py
--rw-r--r--  2.0 unx     5845 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/create_image_dataset/component.py
--rw-r--r--  2.0 unx      646 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/create_tabular_dataset/__init__.py
--rw-r--r--  2.0 unx     4444 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/create_tabular_dataset/component.py
--rw-r--r--  2.0 unx      643 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/create_text_dataset/__init__.py
--rw-r--r--  2.0 unx     5728 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/create_text_dataset/component.py
--rw-r--r--  2.0 unx      642 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/create_time_series_dataset/__init__.py
--rw-r--r--  2.0 unx     4458 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/create_time_series_dataset/component.py
--rw-r--r--  2.0 unx      644 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/create_video_dataset/__init__.py
--rw-r--r--  2.0 unx     5725 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/create_video_dataset/component.py
--rw-r--r--  2.0 unx      644 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/export_image_dataset/__init__.py
--rw-r--r--  2.0 unx     2941 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/export_image_dataset/component.py
--rw-r--r--  2.0 unx      646 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/export_tabular_dataset/__init__.py
--rw-r--r--  2.0 unx     2945 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/export_tabular_dataset/component.py
--rw-r--r--  2.0 unx      643 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/export_text_dataset/__init__.py
--rw-r--r--  2.0 unx     2939 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/export_text_dataset/component.py
--rw-r--r--  2.0 unx      650 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/export_time_series_dataset/__init__.py
--rw-r--r--  2.0 unx     2950 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/export_time_series_dataset/component.py
--rw-r--r--  2.0 unx      644 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/export_video_dataset/__init__.py
--rw-r--r--  2.0 unx     2941 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/export_video_dataset/component.py
--rw-r--r--  2.0 unx      642 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/get_vertex_dataset/__init__.py
--rw-r--r--  2.0 unx     3886 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/get_vertex_dataset/component.py
--rw-r--r--  2.0 unx      644 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/import_image_dataset/__init__.py
--rw-r--r--  2.0 unx     4165 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/import_image_dataset/component.py
--rw-r--r--  2.0 unx      643 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/import_text_dataset/__init__.py
--rw-r--r--  2.0 unx     4178 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/import_text_dataset/component.py
--rw-r--r--  2.0 unx      644 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/import_video_dataset/__init__.py
--rw-r--r--  2.0 unx     4185 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/dataset/import_video_dataset/component.py
--rw-r--r--  2.0 unx     1362 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/endpoint/__init__.py
--rw-r--r--  2.0 unx      664 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/endpoint/create_endpoint/__init__.py
--rw-r--r--  2.0 unx     4782 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/endpoint/create_endpoint/component.py
--rw-r--r--  2.0 unx      639 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/endpoint/delete_endpoint/__init__.py
--rw-r--r--  2.0 unx     2324 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/endpoint/delete_endpoint/component.py
--rw-r--r--  2.0 unx      661 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/endpoint/deploy_model/__init__.py
--rw-r--r--  2.0 unx    10615 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/endpoint/deploy_model/component.py
--rw-r--r--  2.0 unx      638 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/endpoint/undeploy_model/__init__.py
--rw-r--r--  2.0 unx     3350 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/endpoint/undeploy_model/component.py
--rw-r--r--  2.0 unx     1274 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/forecasting/__init__.py
--rw-r--r--  2.0 unx      658 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/forecasting/prepare_data_for_train/__init__.py
--rw-r--r--  2.0 unx     7490 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/forecasting/prepare_data_for_train/component.py
--rw-r--r--  2.0 unx      646 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/forecasting/preprocess/__init__.py
--rw-r--r--  2.0 unx     2249 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/forecasting/preprocess/component.py
--rw-r--r--  2.0 unx      644 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/forecasting/validate/__init__.py
--rw-r--r--  2.0 unx     1891 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/forecasting/validate/component.py
--rw-r--r--  2.0 unx     1218 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/__init__.py
--rw-r--r--  2.0 unx     9978 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/component.py
--rw-r--r--  2.0 unx     3400 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/utils.py
--rw-r--r--  2.0 unx     1129 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model/__init__.py
--rw-r--r--  2.0 unx      638 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model/delete_model/__init__.py
--rw-r--r--  2.0 unx     2448 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model/delete_model/component.py
--rw-r--r--  2.0 unx      661 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model/export_model/__init__.py
--rw-r--r--  2.0 unx     5119 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model/export_model/component.py
--rw-r--r--  2.0 unx      661 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model/upload_model/__init__.py
--rw-r--r--  2.0 unx     6743 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model/upload_model/component.py
--rw-r--r--  2.0 unx     2359 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model_evaluation/__init__.py
--rw-r--r--  2.0 unx    12567 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model_evaluation/classification_component.py
--rw-r--r--  2.0 unx    20373 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model_evaluation/error_analysis_pipeline.py
--rw-r--r--  2.0 unx    11939 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model_evaluation/evaluated_annotation_pipeline.py
--rw-r--r--  2.0 unx    22182 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_feature_attribution_pipeline.py
--rw-r--r--  2.0 unx    15624 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_pipeline.py
--rw-r--r--  2.0 unx    18599 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_unstructure_data_pipeline.py
--rw-r--r--  2.0 unx    25056 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model_evaluation/evaluation_feature_attribution_pipeline.py
--rw-r--r--  2.0 unx    10258 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model_evaluation/forecasting_component.py
--rw-r--r--  2.0 unx     9311 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/model_evaluation/regression_component.py
--rw-r--r--  2.0 unx      863 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/vertex_notification_email/__init__.py
--rw-r--r--  2.0 unx     2175 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/vertex_notification_email/component.py
--rw-r--r--  2.0 unx      857 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/wait_gcp_resources/__init__.py
--rw-r--r--  2.0 unx     2538 b- defN 23-Jul-13 23:05 google_cloud_pipeline_components/v1/wait_gcp_resources/component.py
--rw-r--r--  2.0 unx    11357 b- defN 23-Jul-13 23:13 google_cloud_pipeline_components-2.1.0.dist-info/LICENSE
--rw-r--r--  2.0 unx     5839 b- defN 23-Jul-13 23:13 google_cloud_pipeline_components-2.1.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jul-13 23:13 google_cloud_pipeline_components-2.1.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       33 b- defN 23-Jul-13 23:13 google_cloud_pipeline_components-2.1.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    57995 b- defN 23-Jul-13 23:13 google_cloud_pipeline_components-2.1.0.dist-info/RECORD
-427 files, 6844684 bytes uncompressed, 1106518 bytes compressed:  83.8%
+Zip file size: 1220346 bytes, number of entries: 432
+-rw-r--r--  2.0 unx      711 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/__init__.py
+-rw-r--r--  2.0 unx      828 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_image.py
+-rw-r--r--  2.0 unx      859 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_placeholders.py
+-rw-r--r--  2.0 unx    11048 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/utils.py
+-rw-r--r--  2.0 unx      677 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/version.py
+-rw-r--r--  2.0 unx      606 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/__init__.py
+-rw-r--r--  2.0 unx      811 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model/__init__.py
+-rw-r--r--  2.0 unx      662 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model/get_model/__init__.py
+-rw-r--r--  2.0 unx     2307 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model/get_model/component.py
+-rw-r--r--  2.0 unx     2673 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/__init__.py
+-rw-r--r--  2.0 unx     3540 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/utils.py
+-rw-r--r--  2.0 unx      963 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/version.py
+-rw-r--r--  2.0 unx      669 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/__init__.py
+-rw-r--r--  2.0 unx     5452 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/component.py
+-rw-r--r--  2.0 unx      677 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/__init__.py
+-rw-r--r--  2.0 unx     7039 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/component.py
+-rw-r--r--  2.0 unx      671 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/error_analysis_annotation/__init__.py
+-rw-r--r--  2.0 unx     4092 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/error_analysis_annotation/component.py
+-rw-r--r--  2.0 unx      677 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/evaluated_annotation/__init__.py
+-rw-r--r--  2.0 unx     5842 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/evaluated_annotation/component.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/feature_extractor/__init__.py
+-rw-r--r--  2.0 unx     5539 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/feature_extractor/component.py
+-rw-r--r--  2.0 unx      684 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluated_annotation/__init__.py
+-rw-r--r--  2.0 unx     3053 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluated_annotation/component.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluation/__init__.py
+-rw-r--r--  2.0 unx     7277 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluation/component.py
+-rw-r--r--  2.0 unx      671 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/__init__.py
+-rw-r--r--  2.0 unx     7090 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/component.py
+-rw-r--r--  2.0 unx      662 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/rai_safety/__init__.py
+-rw-r--r--  2.0 unx     7605 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/rai_safety/safety_metrics_pipeline.py
+-rw-r--r--  2.0 unx      662 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/rai_safety/llm_safety_bias/__init__.py
+-rw-r--r--  2.0 unx     4185 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/rai_safety/llm_safety_bias/component.py
+-rw-r--r--  2.0 unx      682 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/__init__.py
+-rw-r--r--  2.0 unx     5738 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/component.py
+-rw-r--r--  2.0 unx      651 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/__init__.py
+-rw-r--r--  2.0 unx      659 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/_implementation/__init__.py
+-rw-r--r--  2.0 unx      668 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/_implementation/model/__init__.py
+-rw-r--r--  2.0 unx      685 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/_implementation/model/get_model/__init__.py
+-rw-r--r--  2.0 unx     4975 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/_implementation/model/get_model/get_model.py
+-rw-r--r--  2.0 unx      678 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/_implementation/model_evaluation/__init__.py
+-rw-r--r--  2.0 unx    10196 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/_implementation/model_evaluation/import_evaluated_annotation.py
+-rw-r--r--  2.0 unx    17219 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/_implementation/model_evaluation/import_model_evaluation.py
+-rw-r--r--  2.0 unx      651 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/preview/__init__.py
+-rw-r--r--  2.0 unx      655 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/preview/dataflow/__init__.py
+-rw-r--r--  2.0 unx      698 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/preview/dataflow/flex_template/__init__.py
+-rw-r--r--  2.0 unx     1914 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/preview/dataflow/flex_template/launcher.py
+-rw-r--r--  2.0 unx     9632 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/preview/dataflow/flex_template/remote_runner.py
+-rw-r--r--  2.0 unx      638 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/utils/__init__.py
+-rw-r--r--  2.0 unx     3027 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/utils/artifact_utils.py
+-rw-r--r--  2.0 unx     1843 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/utils/execution_context.py
+-rw-r--r--  2.0 unx      651 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/__init__.py
+-rw-r--r--  2.0 unx      651 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/aiplatform/__init__.py
+-rw-r--r--  2.0 unx    10719 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/aiplatform/remote_runner.py
+-rw-r--r--  2.0 unx     5257 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/aiplatform/utils.py
+-rw-r--r--  2.0 unx      696 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/batch_prediction_job/__init__.py
+-rw-r--r--  2.0 unx     2364 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/batch_prediction_job/launcher.py
+-rw-r--r--  2.0 unx     9372 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/batch_prediction_job/remote_runner.py
+-rw-r--r--  2.0 unx      655 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/__init__.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/create_model/__init__.py
+-rw-r--r--  2.0 unx     2502 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/create_model/launcher.py
+-rw-r--r--  2.0 unx     3825 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/create_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/__init__.py
+-rw-r--r--  2.0 unx     3284 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/launcher.py
+-rw-r--r--  2.0 unx     5770 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/drop_model/__init__.py
+-rw-r--r--  2.0 unx     2627 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/drop_model/launcher.py
+-rw-r--r--  2.0 unx     3306 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/drop_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/__init__.py
+-rw-r--r--  2.0 unx     3307 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/launcher.py
+-rw-r--r--  2.0 unx     5574 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/__init__.py
+-rw-r--r--  2.0 unx     2958 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/launcher.py
+-rw-r--r--  2.0 unx     3432 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/__init__.py
+-rw-r--r--  2.0 unx     3639 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/launcher.py
+-rw-r--r--  2.0 unx     5843 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/export_model/__init__.py
+-rw-r--r--  2.0 unx     2775 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/export_model/launcher.py
+-rw-r--r--  2.0 unx     4706 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/export_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/feature_importance/__init__.py
+-rw-r--r--  2.0 unx     2656 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/feature_importance/launcher.py
+-rw-r--r--  2.0 unx     3562 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/feature_importance/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/__init__.py
+-rw-r--r--  2.0 unx     2932 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/launcher.py
+-rw-r--r--  2.0 unx     3384 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/global_explain/__init__.py
+-rw-r--r--  2.0 unx     2807 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/global_explain/launcher.py
+-rw-r--r--  2.0 unx     2676 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/global_explain/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/__init__.py
+-rw-r--r--  2.0 unx     2653 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/launcher.py
+-rw-r--r--  2.0 unx     4046 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/__init__.py
+-rw-r--r--  2.0 unx     2655 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/launcher.py
+-rw-r--r--  2.0 unx     3441 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/__init__.py
+-rw-r--r--  2.0 unx     2822 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/launcher.py
+-rw-r--r--  2.0 unx     4662 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/__init__.py
+-rw-r--r--  2.0 unx     2780 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/launcher.py
+-rw-r--r--  2.0 unx     4637 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/__init__.py
+-rw-r--r--  2.0 unx     3319 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/launcher.py
+-rw-r--r--  2.0 unx     4731 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/__init__.py
+-rw-r--r--  2.0 unx     2641 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/launcher.py
+-rw-r--r--  2.0 unx     4018 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/__init__.py
+-rw-r--r--  2.0 unx     2676 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/launcher.py
+-rw-r--r--  2.0 unx     3194 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/__init__.py
+-rw-r--r--  2.0 unx     2665 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/launcher.py
+-rw-r--r--  2.0 unx     3166 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_recommend/__init__.py
+-rw-r--r--  2.0 unx     3086 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_recommend/launcher.py
+-rw-r--r--  2.0 unx     4050 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_recommend/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_reconstruction_loss/__init__.py
+-rw-r--r--  2.0 unx     3115 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_reconstruction_loss/launcher.py
+-rw-r--r--  2.0 unx     4337 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_reconstruction_loss/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/__init__.py
+-rw-r--r--  2.0 unx     3302 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/launcher.py
+-rw-r--r--  2.0 unx     4613 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/__init__.py
+-rw-r--r--  2.0 unx     2644 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/launcher.py
+-rw-r--r--  2.0 unx     4032 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/__init__.py
+-rw-r--r--  2.0 unx     2635 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/launcher.py
+-rw-r--r--  2.0 unx     4211 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_weights/__init__.py
+-rw-r--r--  2.0 unx     2627 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_weights/launcher.py
+-rw-r--r--  2.0 unx     4001 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/ml_weights/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/predict_model/__init__.py
+-rw-r--r--  2.0 unx     3302 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/predict_model/launcher.py
+-rw-r--r--  2.0 unx     4578 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/predict_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/query_job/__init__.py
+-rw-r--r--  2.0 unx     2473 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/query_job/launcher.py
+-rw-r--r--  2.0 unx     2539 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/query_job/remote_runner.py
+-rw-r--r--  2.0 unx      673 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/utils/__init__.py
+-rw-r--r--  2.0 unx    12236 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/bigquery/utils/bigquery_util.py
+-rw-r--r--  2.0 unx      686 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/custom_job/__init__.py
+-rw-r--r--  2.0 unx     1917 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/custom_job/launcher.py
+-rw-r--r--  2.0 unx     3586 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/custom_job/remote_runner.py
+-rw-r--r--  2.0 unx      662 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataflow/__init__.py
+-rw-r--r--  2.0 unx     3303 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataflow/dataflow_launcher.py
+-rw-r--r--  2.0 unx     7415 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataflow/dataflow_python_job_remote_runner.py
+-rw-r--r--  2.0 unx      699 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/__init__.py
+-rw-r--r--  2.0 unx      705 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/create_pyspark_batch/__init__.py
+-rw-r--r--  2.0 unx     2196 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/create_pyspark_batch/launcher.py
+-rw-r--r--  2.0 unx      804 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/create_pyspark_batch/remote_runner.py
+-rw-r--r--  2.0 unx      703 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/create_spark_batch/__init__.py
+-rw-r--r--  2.0 unx     2188 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/create_spark_batch/launcher.py
+-rw-r--r--  2.0 unx      800 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/create_spark_batch/remote_runner.py
+-rw-r--r--  2.0 unx      705 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/create_spark_r_batch/__init__.py
+-rw-r--r--  2.0 unx     2195 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/create_spark_r_batch/launcher.py
+-rw-r--r--  2.0 unx      804 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/create_spark_r_batch/remote_runner.py
+-rw-r--r--  2.0 unx      707 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/create_spark_sql_batch/__init__.py
+-rw-r--r--  2.0 unx     2203 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/create_spark_sql_batch/launcher.py
+-rw-r--r--  2.0 unx      808 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/create_spark_sql_batch/remote_runner.py
+-rw-r--r--  2.0 unx      663 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/utils/__init__.py
+-rw-r--r--  2.0 unx    12872 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/dataproc/utils/dataproc_util.py
+-rw-r--r--  2.0 unx      655 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/endpoint/__init__.py
+-rw-r--r--  2.0 unx      691 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/__init__.py
+-rw-r--r--  2.0 unx     2288 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/launcher.py
+-rw-r--r--  2.0 unx     2363 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/remote_runner.py
+-rw-r--r--  2.0 unx      691 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/__init__.py
+-rw-r--r--  2.0 unx     1938 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/launcher.py
+-rw-r--r--  2.0 unx     2308 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/__init__.py
+-rw-r--r--  2.0 unx     1926 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/launcher.py
+-rw-r--r--  2.0 unx     2375 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/remote_runner.py
+-rw-r--r--  2.0 unx      690 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/__init__.py
+-rw-r--r--  2.0 unx     1934 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/launcher.py
+-rw-r--r--  2.0 unx     4178 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/remote_runner.py
+-rw-r--r--  2.0 unx      662 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/gcp_launcher/__init__.py
+-rw-r--r--  2.0 unx     8350 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/gcp_launcher/job_remote_runner.py
+-rw-r--r--  2.0 unx     4945 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/gcp_launcher/lro_remote_runner.py
+-rw-r--r--  2.0 unx    10315 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/gcp_launcher/pipeline_remote_runner.py
+-rw-r--r--  2.0 unx      660 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/__init__.py
+-rw-r--r--  2.0 unx      876 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/error_util.py
+-rw-r--r--  2.0 unx     2193 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/gcp_labels_util.py
+-rw-r--r--  2.0 unx     2951 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/json_util.py
+-rw-r--r--  2.0 unx     1788 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/parser_util.py
+-rw-r--r--  2.0 unx      701 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/__init__.py
+-rw-r--r--  2.0 unx     2240 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/launcher.py
+-rw-r--r--  2.0 unx     4402 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/remote_runner.py
+-rw-r--r--  2.0 unx      696 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/infra_validation_job/__init__.py
+-rw-r--r--  2.0 unx     2280 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/infra_validation_job/launcher.py
+-rw-r--r--  2.0 unx     4068 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/infra_validation_job/remote_runner.py
+-rw-r--r--  2.0 unx      652 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/model/__init__.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/model/delete_model/__init__.py
+-rw-r--r--  2.0 unx     1923 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/model/delete_model/launcher.py
+-rw-r--r--  2.0 unx     2307 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/model/delete_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/model/export_model/__init__.py
+-rw-r--r--  2.0 unx     2201 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/model/export_model/launcher.py
+-rw-r--r--  2.0 unx     2479 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/model/export_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/model/upload_model/__init__.py
+-rw-r--r--  2.0 unx     2331 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/model/upload_model/launcher.py
+-rw-r--r--  2.0 unx     4391 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/model/upload_model/remote_runner.py
+-rw-r--r--  2.0 unx      662 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/vertex_notification_email/__init__.py
+-rw-r--r--  2.0 unx     1074 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/vertex_notification_email/executor.py
+-rw-r--r--  2.0 unx      694 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/wait_gcp_resources/__init__.py
+-rw-r--r--  2.0 unx     1928 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/wait_gcp_resources/launcher.py
+-rw-r--r--  2.0 unx     5968 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/container/v1/wait_gcp_resources/remote_runner.py
+-rw-r--r--  2.0 unx      606 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/__init__.py
+-rw-r--r--  2.0 unx      606 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/__init__.py
+-rw-r--r--  2.0 unx     1230 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/forecasting/__init__.py
+-rw-r--r--  2.0 unx     5827 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/forecasting/forecasting_ensemble.py
+-rw-r--r--  2.0 unx     6656 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/forecasting/forecasting_stage_1_tuner.py
+-rw-r--r--  2.0 unx     6456 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/forecasting/forecasting_stage_2_tuner.py
+-rw-r--r--  2.0 unx   362451 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/forecasting/learn_to_learn_forecasting_pipeline.yaml
+-rw-r--r--  2.0 unx   360628 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/forecasting/sequence_to_sequence_forecasting_pipeline.yaml
+-rw-r--r--  2.0 unx   359730 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/forecasting/temporal_fusion_transformer_forecasting_pipeline.yaml
+-rw-r--r--  2.0 unx   362527 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/forecasting/time_series_dense_encoder_forecasting_pipeline.yaml
+-rw-r--r--  2.0 unx    53072 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/forecasting/utils.py
+-rw-r--r--  2.0 unx     2114 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/__init__.py
+-rw-r--r--  2.0 unx   515002 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/automl_tabular_feature_selection_pipeline.yaml
+-rw-r--r--  2.0 unx   391133 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/automl_tabular_v2_pipeline.yaml
+-rw-r--r--  2.0 unx     7674 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/feature_selection.py
+-rw-r--r--  2.0 unx    51128 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/feature_transform_engine.py
+-rw-r--r--  2.0 unx    10164 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/tabnet_hyperparameter_tuning_job.py
+-rw-r--r--  2.0 unx   242992 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/tabnet_hyperparameter_tuning_job_pipeline.yaml
+-rw-r--r--  2.0 unx    13060 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/tabnet_trainer.py
+-rw-r--r--  2.0 unx   201451 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/tabnet_trainer_pipeline.yaml
+-rw-r--r--  2.0 unx   165844 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/utils.py
+-rw-r--r--  2.0 unx    10161 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_hyperparameter_tuning_job.py
+-rw-r--r--  2.0 unx   196354 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_hyperparameter_tuning_job_pipeline.yaml
+-rw-r--r--  2.0 unx    12170 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_trainer.py
+-rw-r--r--  2.0 unx   191459 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_trainer_pipeline.yaml
+-rw-r--r--  2.0 unx     4915 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/xgboost_hyperparameter_tuning_job.py
+-rw-r--r--  2.0 unx   225088 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/xgboost_hyperparameter_tuning_job_pipeline.yaml
+-rw-r--r--  2.0 unx     2341 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/xgboost_trainer.py
+-rw-r--r--  2.0 unx   209094 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/xgboost_trainer_pipeline.yaml
+-rw-r--r--  2.0 unx     3292 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_large_search_space.json
+-rw-r--r--  2.0 unx     3261 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_medium_search_space.json
+-rw-r--r--  2.0 unx     2975 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_small_search_space.json
+-rw-r--r--  2.0 unx     3287 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_large_search_space.json
+-rw-r--r--  2.0 unx     3266 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_medium_search_space.json
+-rw-r--r--  2.0 unx     2974 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_small_search_space.json
+-rw-r--r--  2.0 unx     3269 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_large_search_space.json
+-rw-r--r--  2.0 unx     3254 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_medium_search_space.json
+-rw-r--r--  2.0 unx     2966 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_small_search_space.json
+-rw-r--r--  2.0 unx     2647 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/configs/wide_and_deep_params.json
+-rw-r--r--  2.0 unx     7748 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/automl/tabular/configs/xgboost_params.json
+-rw-r--r--  2.0 unx      828 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/dataflow/__init__.py
+-rw-r--r--  2.0 unx      669 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/dataflow/flex_template/__init__.py
+-rw-r--r--  2.0 unx    11929 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/dataflow/flex_template/component.py
+-rw-r--r--  2.0 unx     1395 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/model_evaluation/__init__.py
+-rw-r--r--  2.0 unx     5955 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/model_evaluation/data_bias_component.py
+-rw-r--r--  2.0 unx     7509 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/model_evaluation/feature_attribution_component.py
+-rw-r--r--  2.0 unx    14063 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/model_evaluation/feature_attribution_graph_component.py
+-rw-r--r--  2.0 unx     6842 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/model_evaluation/model_bias_component.py
+-rw-r--r--  2.0 unx     7548 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/preview/model_evaluation/utils.py
+-rw-r--r--  2.0 unx      661 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/proto/__init__.py
+-rw-r--r--  2.0 unx     7687 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/proto/gcp_resources_pb2.py
+-rw-r--r--  2.0 unx      606 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/types/__init__.py
+-rw-r--r--  2.0 unx    23845 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/types/artifact_types.py
+-rw-r--r--  2.0 unx      765 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/__init__.py
+-rw-r--r--  2.0 unx      631 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/__init__.py
+-rw-r--r--  2.0 unx      806 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/forecasting/__init__.py
+-rw-r--r--  2.0 unx    52018 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_predict_pipeline.yaml
+-rw-r--r--  2.0 unx   246248 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_train_pipeline.yaml
+-rw-r--r--  2.0 unx    94627 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/forecasting/prophet_predict_pipeline.yaml
+-rw-r--r--  2.0 unx     8498 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer.py
+-rw-r--r--  2.0 unx   144373 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer_pipeline.yaml
+-rw-r--r--  2.0 unx    15028 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/forecasting/utils.py
+-rw-r--r--  2.0 unx     2034 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/tabular/__init__.py
+-rw-r--r--  2.0 unx   502269 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/tabular/automl_tabular_pipeline.yaml
+-rw-r--r--  2.0 unx     6756 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/tabular/cv_trainer.py
+-rw-r--r--  2.0 unx     6991 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/tabular/ensemble.py
+-rw-r--r--  2.0 unx     3116 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/tabular/finalizer.py
+-rw-r--r--  2.0 unx     1341 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/tabular/infra_validator.py
+-rw-r--r--  2.0 unx     5571 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/tabular/split_materialized_data.py
+-rw-r--r--  2.0 unx     7865 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/tabular/stage_1_tuner.py
+-rw-r--r--  2.0 unx    13707 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/tabular/stats_and_example_gen.py
+-rw-r--r--  2.0 unx    12002 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/tabular/training_configurator_and_validator.py
+-rw-r--r--  2.0 unx     8305 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/tabular/transform.py
+-rw-r--r--  2.0 unx    74707 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/tabular/utils.py
+-rw-r--r--  2.0 unx   321170 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/tabular/deprecated/default_pipeline.json
+-rw-r--r--  2.0 unx     1785 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/training_job/__init__.py
+-rw-r--r--  2.0 unx      655 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/training_job/automl_forecasting_training_job/__init__.py
+-rw-r--r--  2.0 unx    28961 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/training_job/automl_forecasting_training_job/component.py
+-rw-r--r--  2.0 unx      649 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/training_job/automl_image_training_job/__init__.py
+-rw-r--r--  2.0 unx    16657 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/training_job/automl_image_training_job/component.py
+-rw-r--r--  2.0 unx      651 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/training_job/automl_tabular_training_job/__init__.py
+-rw-r--r--  2.0 unx    22399 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/training_job/automl_tabular_training_job/component.py
+-rw-r--r--  2.0 unx      648 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/training_job/automl_text_training_job/__init__.py
+-rw-r--r--  2.0 unx    10058 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/training_job/automl_text_training_job/component.py
+-rw-r--r--  2.0 unx      649 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/training_job/automl_video_training_job/__init__.py
+-rw-r--r--  2.0 unx    10127 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/automl/training_job/automl_video_training_job/component.py
+-rw-r--r--  2.0 unx      973 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/batch_predict_job/__init__.py
+-rw-r--r--  2.0 unx    21662 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/batch_predict_job/component.py
+-rw-r--r--  2.0 unx     5481 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/__init__.py
+-rw-r--r--  2.0 unx      670 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/create_model/__init__.py
+-rw-r--r--  2.0 unx     4129 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/create_model/component.py
+-rw-r--r--  2.0 unx      680 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/detect_anomalies_model/__init__.py
+-rw-r--r--  2.0 unx     7154 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/detect_anomalies_model/component.py
+-rw-r--r--  2.0 unx      668 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/drop_model/__init__.py
+-rw-r--r--  2.0 unx     4020 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/drop_model/component.py
+-rw-r--r--  2.0 unx      672 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/evaluate_model/__init__.py
+-rw-r--r--  2.0 unx     6437 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/evaluate_model/component.py
+-rw-r--r--  2.0 unx      680 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/explain_forecast_model/__init__.py
+-rw-r--r--  2.0 unx     5909 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/explain_forecast_model/component.py
+-rw-r--r--  2.0 unx      679 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/explain_predict_model/__init__.py
+-rw-r--r--  2.0 unx     7371 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/explain_predict_model/component.py
+-rw-r--r--  2.0 unx      670 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/export_model/__init__.py
+-rw-r--r--  2.0 unx     3995 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/export_model/component.py
+-rw-r--r--  2.0 unx      676 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/feature_importance/__init__.py
+-rw-r--r--  2.0 unx     5202 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/feature_importance/component.py
+-rw-r--r--  2.0 unx      672 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/forecast_model/__init__.py
+-rw-r--r--  2.0 unx     5844 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/forecast_model/component.py
+-rw-r--r--  2.0 unx      672 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/global_explain/__init__.py
+-rw-r--r--  2.0 unx     4325 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/global_explain/component.py
+-rw-r--r--  2.0 unx      677 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_advanced_weights/__init__.py
+-rw-r--r--  2.0 unx     4575 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_advanced_weights/component.py
+-rw-r--r--  2.0 unx      679 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_arima_coefficients/__init__.py
+-rw-r--r--  2.0 unx     4785 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_arima_coefficients/component.py
+-rw-r--r--  2.0 unx      675 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_arima_evaluate/__init__.py
+-rw-r--r--  2.0 unx     5755 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_arima_evaluate/component.py
+-rw-r--r--  2.0 unx      670 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_centroids/__init__.py
+-rw-r--r--  2.0 unx     5550 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_centroids/component.py
+-rw-r--r--  2.0 unx      677 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_confusion_matrix/__init__.py
+-rw-r--r--  2.0 unx     5555 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_confusion_matrix/component.py
+-rw-r--r--  2.0 unx      673 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_feature_info/__init__.py
+-rw-r--r--  2.0 unx     4540 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_feature_info/component.py
+-rw-r--r--  2.0 unx      685 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_principal_component_info/__init__.py
+-rw-r--r--  2.0 unx     5429 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_principal_component_info/component.py
+-rw-r--r--  2.0 unx      681 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_principal_components/__init__.py
+-rw-r--r--  2.0 unx     5375 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_principal_components/component.py
+-rw-r--r--  2.0 unx      670 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_recommend/__init__.py
+-rw-r--r--  2.0 unx     5918 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_recommend/component.py
+-rw-r--r--  2.0 unx      680 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_reconstruction_loss/__init__.py
+-rw-r--r--  2.0 unx     6021 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_reconstruction_loss/component.py
+-rw-r--r--  2.0 unx      670 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_roc_curve/__init__.py
+-rw-r--r--  2.0 unx     5439 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_roc_curve/component.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_training_info/__init__.py
+-rw-r--r--  2.0 unx     4580 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_training_info/component.py
+-rw-r--r--  2.0 unx      671 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_trial_info/__init__.py
+-rw-r--r--  2.0 unx     5106 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_trial_info/component.py
+-rw-r--r--  2.0 unx      668 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_weights/__init__.py
+-rw-r--r--  2.0 unx     4511 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/ml_weights/component.py
+-rw-r--r--  2.0 unx      671 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/predict_model/__init__.py
+-rw-r--r--  2.0 unx     6400 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/predict_model/component.py
+-rw-r--r--  2.0 unx      667 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/query_job/__init__.py
+-rw-r--r--  2.0 unx     4985 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/bigquery/query_job/component.py
+-rw-r--r--  2.0 unx     1208 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/custom_job/__init__.py
+-rw-r--r--  2.0 unx     6178 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/custom_job/component.py
+-rw-r--r--  2.0 unx    15856 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/custom_job/utils.py
+-rw-r--r--  2.0 unx      878 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataflow/__init__.py
+-rw-r--r--  2.0 unx      664 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataflow/python_job/__init__.py
+-rw-r--r--  2.0 unx     2577 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataflow/python_job/component.py
+-rw-r--r--  2.0 unx     1426 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataproc/__init__.py
+-rw-r--r--  2.0 unx      670 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataproc/create_pyspark_batch/__init__.py
+-rw-r--r--  2.0 unx     6861 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataproc/create_pyspark_batch/component.py
+-rw-r--r--  2.0 unx      668 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataproc/create_spark_batch/__init__.py
+-rw-r--r--  2.0 unx     6812 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataproc/create_spark_batch/component.py
+-rw-r--r--  2.0 unx      669 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataproc/create_spark_r_batch/__init__.py
+-rw-r--r--  2.0 unx     6356 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataproc/create_spark_r_batch/component.py
+-rw-r--r--  2.0 unx      673 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataproc/create_spark_sql_batch/__init__.py
+-rw-r--r--  2.0 unx     6136 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataproc/create_spark_sql_batch/component.py
+-rw-r--r--  2.0 unx     3089 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/__init__.py
+-rw-r--r--  2.0 unx      644 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/create_image_dataset/__init__.py
+-rw-r--r--  2.0 unx     5845 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/create_image_dataset/component.py
+-rw-r--r--  2.0 unx      646 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/create_tabular_dataset/__init__.py
+-rw-r--r--  2.0 unx     4444 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/create_tabular_dataset/component.py
+-rw-r--r--  2.0 unx      643 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/create_text_dataset/__init__.py
+-rw-r--r--  2.0 unx     5728 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/create_text_dataset/component.py
+-rw-r--r--  2.0 unx      642 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/create_time_series_dataset/__init__.py
+-rw-r--r--  2.0 unx     4458 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/create_time_series_dataset/component.py
+-rw-r--r--  2.0 unx      644 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/create_video_dataset/__init__.py
+-rw-r--r--  2.0 unx     5725 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/create_video_dataset/component.py
+-rw-r--r--  2.0 unx      644 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/export_image_dataset/__init__.py
+-rw-r--r--  2.0 unx     2941 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/export_image_dataset/component.py
+-rw-r--r--  2.0 unx      646 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/export_tabular_dataset/__init__.py
+-rw-r--r--  2.0 unx     2945 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/export_tabular_dataset/component.py
+-rw-r--r--  2.0 unx      643 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/export_text_dataset/__init__.py
+-rw-r--r--  2.0 unx     2939 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/export_text_dataset/component.py
+-rw-r--r--  2.0 unx      650 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/export_time_series_dataset/__init__.py
+-rw-r--r--  2.0 unx     2950 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/export_time_series_dataset/component.py
+-rw-r--r--  2.0 unx      644 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/export_video_dataset/__init__.py
+-rw-r--r--  2.0 unx     2941 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/export_video_dataset/component.py
+-rw-r--r--  2.0 unx      642 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/get_vertex_dataset/__init__.py
+-rw-r--r--  2.0 unx     3886 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/get_vertex_dataset/component.py
+-rw-r--r--  2.0 unx      644 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/import_image_dataset/__init__.py
+-rw-r--r--  2.0 unx     4165 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/import_image_dataset/component.py
+-rw-r--r--  2.0 unx      643 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/import_text_dataset/__init__.py
+-rw-r--r--  2.0 unx     4178 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/import_text_dataset/component.py
+-rw-r--r--  2.0 unx      644 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/import_video_dataset/__init__.py
+-rw-r--r--  2.0 unx     4185 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/dataset/import_video_dataset/component.py
+-rw-r--r--  2.0 unx     1362 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/endpoint/__init__.py
+-rw-r--r--  2.0 unx      664 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/endpoint/create_endpoint/__init__.py
+-rw-r--r--  2.0 unx     4782 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/endpoint/create_endpoint/component.py
+-rw-r--r--  2.0 unx      639 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/endpoint/delete_endpoint/__init__.py
+-rw-r--r--  2.0 unx     2324 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/endpoint/delete_endpoint/component.py
+-rw-r--r--  2.0 unx      661 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/endpoint/deploy_model/__init__.py
+-rw-r--r--  2.0 unx    10615 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/endpoint/deploy_model/component.py
+-rw-r--r--  2.0 unx      638 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/endpoint/undeploy_model/__init__.py
+-rw-r--r--  2.0 unx     3350 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/endpoint/undeploy_model/component.py
+-rw-r--r--  2.0 unx     1274 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/forecasting/__init__.py
+-rw-r--r--  2.0 unx      658 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/forecasting/prepare_data_for_train/__init__.py
+-rw-r--r--  2.0 unx     7490 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/forecasting/prepare_data_for_train/component.py
+-rw-r--r--  2.0 unx      646 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/forecasting/preprocess/__init__.py
+-rw-r--r--  2.0 unx     2249 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/forecasting/preprocess/component.py
+-rw-r--r--  2.0 unx      644 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/forecasting/validate/__init__.py
+-rw-r--r--  2.0 unx     1891 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/forecasting/validate/component.py
+-rw-r--r--  2.0 unx     1218 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/__init__.py
+-rw-r--r--  2.0 unx     9978 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/component.py
+-rw-r--r--  2.0 unx     3400 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/utils.py
+-rw-r--r--  2.0 unx     1129 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model/__init__.py
+-rw-r--r--  2.0 unx      638 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model/delete_model/__init__.py
+-rw-r--r--  2.0 unx     2448 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model/delete_model/component.py
+-rw-r--r--  2.0 unx      661 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model/export_model/__init__.py
+-rw-r--r--  2.0 unx     5119 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model/export_model/component.py
+-rw-r--r--  2.0 unx      661 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model/upload_model/__init__.py
+-rw-r--r--  2.0 unx     6743 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model/upload_model/component.py
+-rw-r--r--  2.0 unx     2359 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model_evaluation/__init__.py
+-rw-r--r--  2.0 unx    12617 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model_evaluation/classification_component.py
+-rw-r--r--  2.0 unx    20408 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model_evaluation/error_analysis_pipeline.py
+-rw-r--r--  2.0 unx    11969 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model_evaluation/evaluated_annotation_pipeline.py
+-rw-r--r--  2.0 unx    47515 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_feature_attribution_pipeline.py
+-rw-r--r--  2.0 unx    38232 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_pipeline.py
+-rw-r--r--  2.0 unx    42441 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_unstructure_data_pipeline.py
+-rw-r--r--  2.0 unx    51528 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model_evaluation/evaluation_feature_attribution_pipeline.py
+-rw-r--r--  2.0 unx    10295 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model_evaluation/forecasting_component.py
+-rw-r--r--  2.0 unx     9341 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/model_evaluation/regression_component.py
+-rw-r--r--  2.0 unx      863 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/vertex_notification_email/__init__.py
+-rw-r--r--  2.0 unx     2175 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/vertex_notification_email/component.py
+-rw-r--r--  2.0 unx      857 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/wait_gcp_resources/__init__.py
+-rw-r--r--  2.0 unx     2538 b- defN 23-Aug-08 00:45 google_cloud_pipeline_components/v1/wait_gcp_resources/component.py
+-rw-r--r--  2.0 unx    11357 b- defN 23-Aug-08 01:21 google_cloud_pipeline_components-2.1.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx     5796 b- defN 23-Aug-08 01:21 google_cloud_pipeline_components-2.1.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Aug-08 01:21 google_cloud_pipeline_components-2.1.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       33 b- defN 23-Aug-08 01:21 google_cloud_pipeline_components-2.1.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    58751 b- defN 23-Aug-08 01:21 google_cloud_pipeline_components-2.1.1.dist-info/RECORD
+432 files, 6973462 bytes uncompressed, 1118948 bytes compressed:  84.0%
```

## zipnote {}

```diff
@@ -1,13 +1,16 @@
 Filename: google_cloud_pipeline_components/__init__.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/_image.py
 Comment: 
 
+Filename: google_cloud_pipeline_components/_placeholders.py
+Comment: 
+
 Filename: google_cloud_pipeline_components/utils.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/version.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/_implementation/__init__.py
@@ -75,26 +78,35 @@
 
 Filename: google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/__init__.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/component.py
 Comment: 
 
+Filename: google_cloud_pipeline_components/_implementation/model_evaluation/rai_safety/__init__.py
+Comment: 
+
+Filename: google_cloud_pipeline_components/_implementation/model_evaluation/rai_safety/safety_metrics_pipeline.py
+Comment: 
+
+Filename: google_cloud_pipeline_components/_implementation/model_evaluation/rai_safety/llm_safety_bias/__init__.py
+Comment: 
+
+Filename: google_cloud_pipeline_components/_implementation/model_evaluation/rai_safety/llm_safety_bias/component.py
+Comment: 
+
 Filename: google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/__init__.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/component.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/container/__init__.py
 Comment: 
 
-Filename: google_cloud_pipeline_components/container/cloudbuild.yaml
-Comment: 
-
 Filename: google_cloud_pipeline_components/container/_implementation/__init__.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/container/_implementation/model/__init__.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/container/_implementation/model/get_model/__init__.py
@@ -126,14 +138,17 @@
 
 Filename: google_cloud_pipeline_components/container/preview/dataflow/flex_template/remote_runner.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/container/utils/__init__.py
 Comment: 
 
+Filename: google_cloud_pipeline_components/container/utils/artifact_utils.py
+Comment: 
+
 Filename: google_cloud_pipeline_components/container/utils/execution_context.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/container/v1/__init__.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/container/v1/aiplatform/__init__.py
@@ -510,17 +525,14 @@
 
 Filename: google_cloud_pipeline_components/container/v1/gcp_launcher/pipeline_remote_runner.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/container/v1/gcp_launcher/utils/__init__.py
 Comment: 
 
-Filename: google_cloud_pipeline_components/container/v1/gcp_launcher/utils/artifact_util.py
-Comment: 
-
 Filename: google_cloud_pipeline_components/container/v1/gcp_launcher/utils/error_util.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/container/v1/gcp_launcher/utils/gcp_labels_util.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/container/v1/gcp_launcher/utils/json_util.py
@@ -726,14 +738,17 @@
 
 Filename: google_cloud_pipeline_components/preview/model_evaluation/data_bias_component.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/preview/model_evaluation/feature_attribution_component.py
 Comment: 
 
+Filename: google_cloud_pipeline_components/preview/model_evaluation/feature_attribution_graph_component.py
+Comment: 
+
 Filename: google_cloud_pipeline_components/preview/model_evaluation/model_bias_component.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/preview/model_evaluation/utils.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/proto/__init__.py
@@ -1260,23 +1275,23 @@
 
 Filename: google_cloud_pipeline_components/v1/wait_gcp_resources/__init__.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/v1/wait_gcp_resources/component.py
 Comment: 
 
-Filename: google_cloud_pipeline_components-2.1.0.dist-info/LICENSE
+Filename: google_cloud_pipeline_components-2.1.1.dist-info/LICENSE
 Comment: 
 
-Filename: google_cloud_pipeline_components-2.1.0.dist-info/METADATA
+Filename: google_cloud_pipeline_components-2.1.1.dist-info/METADATA
 Comment: 
 
-Filename: google_cloud_pipeline_components-2.1.0.dist-info/WHEEL
+Filename: google_cloud_pipeline_components-2.1.1.dist-info/WHEEL
 Comment: 
 
-Filename: google_cloud_pipeline_components-2.1.0.dist-info/top_level.txt
+Filename: google_cloud_pipeline_components-2.1.1.dist-info/top_level.txt
 Comment: 
 
-Filename: google_cloud_pipeline_components-2.1.0.dist-info/RECORD
+Filename: google_cloud_pipeline_components-2.1.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## google_cloud_pipeline_components/_image.py

```diff
@@ -7,11 +7,12 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+"""Constants for the GCPC image."""
 from google_cloud_pipeline_components import version
 
 GCPC_IMAGE_NAME = 'gcr.io/ml-pipeline/google-cloud-pipeline-components'
 GCPC_IMAGE_TAG = f'{GCPC_IMAGE_NAME}:{version.__version__}'
```

## google_cloud_pipeline_components/utils.py

```diff
@@ -12,19 +12,21 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Private utilities for component authoring."""
 
 import copy
 import json
 import re
-from typing import Any, Dict, List, Optional
+from typing import Any, Callable, Dict, List, Optional
 
 from google_cloud_pipeline_components import _image
 from kfp import components
 from kfp import dsl
+# do not follow this pattern!
+# we should not depend on non-public modules of the KFP SDK!
 from kfp.components import placeholders
 
 from google.protobuf import json_format
 
 
 # note: this is a slight dependency on KFP SDK implementation details
 # other code should not similarly depend on the stability of kfp.placeholders
@@ -139,20 +141,27 @@
   json_string = json.dumps(obj, default=custom_placeholder_encoder)
   return unquote_nonstring_placeholders(json_string, string_fields)
 
 
 def gcpc_output_name_converter(
     new_name: str,
     original_name: Optional[str] = None,
-):
+) -> Callable[["BaseComponent"], "BaseComponent"]:  # pytype: disable=name-error
   """Replace the output with original_name with a new_name in a component decorated with an @dsl.container_component decorator.
 
   Enables authoring components that have an input and output with the same
   key/name.
 
+  Args:
+    new_name: The new name for the output.
+    original_name: The original name of the output.
+
+  Returns:
+    A decorator that takes modifies a component in place.
+
   Example usage:
 
     @utils.gcpc_output_name_converter('output__gcp_resources', 'gcp_resources')
     @dsl.container_component
     def my_component(
         param: str,
         output__param: dsl.OutputPath(str),
@@ -250,16 +259,16 @@
                 new_name
             )
             component_spec.dag.outputs.parameters[new_name].CopyFrom(
                 modified_dag_output_parameter_spec
             )
 
       def replace_output_name_in_executor(
-          command: list,
-          args: list,
+          command: List[str],
+          args: List[str],
           original_name: str,
           new_name: str,
       ):
         def placeholder_replacer(string: str) -> str:
           param_pattern = rf"\{{\{{\$\.outputs\.parameters\[(?:''|'|\")({original_name})(?:''|'|\")]"
           param_replacement = f"{{{{$.outputs.parameters['{new_name}']"
```

## google_cloud_pipeline_components/version.py

```diff
@@ -9,8 +9,8 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Google Cloud Pipeline Components version."""
 
-__version__ = "2.1.0"
+__version__ = "2.1.1"
```

## google_cloud_pipeline_components/_implementation/model_evaluation/__init__.py

```diff
@@ -19,21 +19,23 @@
 from google_cloud_pipeline_components._implementation.model_evaluation.dataset_preprocessor.component import dataset_preprocessor_error_analysis as EvaluationDatasetPreprocessorOp
 from google_cloud_pipeline_components._implementation.model_evaluation.error_analysis_annotation.component import error_analysis_annotation as ErrorAnalysisAnnotationOp
 from google_cloud_pipeline_components._implementation.model_evaluation.evaluated_annotation.component import evaluated_annotation as EvaluatedAnnotationOp
 from google_cloud_pipeline_components._implementation.model_evaluation.feature_extractor.component import feature_extractor_error_analysis as FeatureExtractorOp
 from google_cloud_pipeline_components._implementation.model_evaluation.import_evaluated_annotation.component import evaluated_annotation_import as ModelImportEvaluatedAnnotationOp
 from google_cloud_pipeline_components._implementation.model_evaluation.import_evaluation.component import model_evaluation_import as ModelImportEvaluationOp
 from google_cloud_pipeline_components._implementation.model_evaluation.llm_evaluation.component import model_evaluation_text_generation as ModelEvaluationTextGenerationOp
+from google_cloud_pipeline_components._implementation.model_evaluation.rai_safety.llm_safety_bias.component import llm_safety_bias_metrics as LLMSafetyBiasMetricsOp
 from google_cloud_pipeline_components._implementation.model_evaluation.target_field_data_remover.component import target_field_data_remover as TargetFieldDataRemoverOp
 
 
 __all__ = [
     'EvaluationDataSamplerOp',
     'EvaluationDatasetPreprocessorOp',
     'ErrorAnalysisAnnotationOp',
     'EvaluatedAnnotationOp',
     'FeatureExtractorOp',
     'ModelEvaluationTextGenerationOp',
     'ModelImportEvaluatedAnnotationOp',
     'ModelImportEvaluationOp',
+    'LLMSafetyBiasMetricsOp',
     'TargetFieldDataRemoverOp',
 ]
```

## google_cloud_pipeline_components/_implementation/model_evaluation/utils.py

```diff
@@ -1,42 +1,37 @@
 """Utility functions used to create custom Kubeflow components."""
 
-from typing import Any
+from typing import Any, Dict, List
 
 from google_cloud_pipeline_components import _image
 
 
 def build_custom_job_payload(
     *,
     display_name: str,
     image_uri: str,
-    args: list[str],
+    args: List[str],
     machine_type: str = 'n1-standard-4',
     service_account: str = '',
     network: str = '',
-    reserved_ip_ranges: list[str] = [],
-    enable_web_access: bool = False,
-    encryption_spec_key_name: str = '',
+    reserved_ip_ranges: List[str] = [],
     accelerator_type: str = 'ACCELERATOR_TYPE_UNSPECIFIED',
     accelerator_count: int = 0,
-) -> dict[str, Any]:
+    encryption_spec_key_name: str = '',
+) -> Dict[str, Any]:
   """Generates payload for a CustomJob in a Sec4 horizontal compliant way.
 
   Args:
     display_name: CustomJob display name. Can contain up to 128 UTF-8
       characters.
+    image_uri: Docker image URI to use for the CustomJob.
+    args: Arguments to pass to the Docker image.
     machine_type: The type of the machine. See the list of machine types
       supported for custom training:
       https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types
-    accelerator_type: The type of accelerator(s) that may be attached to the
-      machine as per acceleratorCount.
-      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType
-    accelerator_count: The number of accelerators to attach to the machine.
-    image_uri: Docker image URI to use for the CustomJob.
-    args: Arguments to pass to the Docker image.
     service_account: Sets the default service account for workload run-as
       account. The service account running the pipeline
       (https://cloud.google.com/vertex-ai/docs/pipelines/configure-project#service-account)
       submitting jobs must have act-as permission on this run-as account. If
       unspecified, the Vertex AI Custom Code Service
       Agent(https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents)
       for the CustomJob's project.
@@ -46,18 +41,18 @@
       a project number, as in 12345, and {network} is a network name. Private
       services access must already be configured for the network. If left
       unspecified, the job is not peered with any network.
     reserved_ip_ranges: A list of names for the reserved ip ranges under the VPC
       network that can be used for this job. If set, we will deploy the job
       within the provided ip ranges. Otherwise, the job will be deployed to any
       ip ranges under the provided VPC network.
-    enable_web_access: Whether you want Vertex AI to enable [interactive shell
-      access](https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell)
-      to training containers. If set to `true`, you can access interactive
-      shells at the URIs given by [CustomJob.web_access_uris][].
+    accelerator_type: The type of accelerator(s) that may be attached to the
+      machine as per acceleratorCount.
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType
+    accelerator_count: The number of accelerators to attach to the machine.
     encryption_spec_key_name: Customer-managed encryption key options for the
       CustomJob. If this is set, then all resources created by the CustomJob
       will be encrypted with the provided encryption key.
 
   Returns:
     CustomJob payload dictionary.
   """
@@ -75,12 +70,11 @@
                   'image_uri': image_uri,
                   'args': args,
               },
           }],
           'service_account': str(service_account),
           'network': str(network),
           'reserved_ip_ranges': reserved_ip_ranges,
-          'enable_web_access': bool(enable_web_access),
       },
       'encryption_spec': {'kms_key_name': str(encryption_spec_key_name)},
   }
   return payload
```

## google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/component.py

```diff
@@ -8,14 +8,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import List
+
 from google_cloud_pipeline_components._implementation.model_evaluation import version
 from kfp.dsl import container_component
 from kfp.dsl import ContainerSpec
 from kfp.dsl import OutputPath
 from kfp.dsl import PIPELINE_JOB_ID_PLACEHOLDER
 from kfp.dsl import PIPELINE_ROOT_PLACEHOLDER
 from kfp.dsl import PIPELINE_TASK_ID_PLACEHOLDER
@@ -24,15 +26,15 @@
 @container_component
 def evaluation_data_sampler(
     gcp_resources: OutputPath(str),
     bigquery_output_table: OutputPath(str),
     gcs_output_directory: OutputPath(list),
     project: str,
     location: str = 'us-central1',
-    gcs_source_uris: list = [],
+    gcs_source_uris: List[str] = [],
     bigquery_source_uri: str = '',
     instances_format: str = 'jsonl',
     sample_size: int = 10000,
     dataflow_service_account: str = '',
     dataflow_subnetwork: str = '',
     dataflow_use_public_ips: bool = True,
     encryption_spec_key_name: str = '',
```

## google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/component.py

```diff
@@ -8,14 +8,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import List
+
 from google_cloud_pipeline_components._implementation.model_evaluation import version
 from google_cloud_pipeline_components.types.artifact_types import VertexDataset
 from kfp.dsl import container_component
 from kfp.dsl import ContainerSpec
 from kfp.dsl import IfPresentPlaceholder
 from kfp.dsl import Input
 from kfp.dsl import OutputPath
@@ -33,16 +35,16 @@
     training_data_items_storage_source: OutputPath(str),
     project: str,
     location: str = 'us-central1',
     test_dataset: Input[VertexDataset] = None,
     test_dataset_annotation_set_name: str = '',
     training_dataset: Input[VertexDataset] = None,
     training_dataset_annotation_set_name: str = '',
-    test_dataset_storage_source_uris: list = [],
-    training_dataset_storage_source_uris: list = [],
+    test_dataset_storage_source_uris: List[str] = [],
+    training_dataset_storage_source_uris: List[str] = [],
 ):
   # fmt: off
   """Preprocesses datasets for Vision Error Analysis pipelines.
 
   Args:
       project: GCP Project ID.
       location: GCP Region. If not set, defaulted to
```

## google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluation/component.py

```diff
@@ -9,15 +9,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
-from typing import Optional
+from typing import List, Optional
 
 from google_cloud_pipeline_components import _image
 from google_cloud_pipeline_components.types.artifact_types import ClassificationMetrics
 from google_cloud_pipeline_components.types.artifact_types import ForecastingMetrics
 from google_cloud_pipeline_components.types.artifact_types import RegressionMetrics
 from google_cloud_pipeline_components.types.artifact_types import VertexModel
 from kfp import dsl
@@ -25,28 +25,29 @@
 from kfp.dsl import Metrics
 
 
 @dsl.container_component
 def model_evaluation_import(
     model: Input[VertexModel],
     gcp_resources: dsl.OutputPath(str),
+    evaluation_resource_name: dsl.OutputPath(str),
     metrics: Optional[Input[Metrics]] = None,
     problem_type: Optional[str] = None,
     classification_metrics: Optional[Input[ClassificationMetrics]] = None,
     forecasting_metrics: Optional[Input[ForecastingMetrics]] = None,
     regression_metrics: Optional[Input[RegressionMetrics]] = None,
     text_generation_metrics: Optional[Input[Metrics]] = None,
     question_answering_metrics: Optional[Input[Metrics]] = None,
     summarization_metrics: Optional[Input[Metrics]] = None,
     explanation: Optional[Input[Metrics]] = None,
     feature_attributions: Optional[Input[Metrics]] = None,
-    display_name: Optional[str] = "",
-    dataset_path: Optional[str] = "",
-    dataset_paths: Optional[list] = [],
-    dataset_type: Optional[str] = "",
+    display_name: str = "",
+    dataset_path: str = "",
+    dataset_paths: List[str] = [],
+    dataset_type: str = "",
 ):
   # fmt: off
   """Imports a model evaluation artifact to an existing Vertex model with
   ModelService.ImportModelEvaluation.
 
   For more details, see
   https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models.evaluations
@@ -54,23 +55,33 @@
   classification_metrics, regression_metrics, or forecasting_metrics.
 
   Args:
     model: Vertex model resource that will be the parent resource of the
       uploaded evaluation.
     metrics: Path of metrics generated from an evaluation component.
     problem_type: The problem type of the metrics being imported to the
-      VertexModel. `classification`, `regression`, and `forecasting` are the
+      VertexModel. `classification`, `regression`, `forecasting`,
+      `text-generation`, `question-answering`, and `summarization` are the
       currently supported problem types. Must be provided when `metrics` is
       provided.
-    classification_metrics: Path of classification metrics generated from the
-      classification evaluation component.
-    forecasting_metrics: Path of forecasting metrics generated from the
-      forecasting evaluation component.
-    regression_metrics: Path of regression metrics generated from the regression
-      evaluation component.
+    classification_metrics: google.ClassificationMetrics artifact generated from
+      the ModelEvaluationClassificationOp component.
+    forecasting_metrics: google.ForecastingMetrics artifact generated from
+      the ModelEvaluationForecastingOp component.
+    regression_metrics: google.ClassificationMetrics artifact generated from
+      the ModelEvaluationRegressionOp component.
+    text_generation_metrics: system.Metrics artifact generated from
+      the ModelEvaluationTextGenerationOp component. Subject to change to
+      google.TextGenerationMetrics.
+    question_answering_metrics: system.Metrics artifact generated from
+      the ModelEvaluationTextGenerationOp component. Subject to change to
+      google.QuestionAnsweringMetrics.
+    summarization_metrics: system.Metrics artifact generated from
+      the ModelEvaluationTextGenerationOp component. Subject to change to
+      google.SummarizationMetrics.
     explanation: Path for model explanation metrics generated from an evaluation
       component.
     feature_attributions: The feature attributions metrics artifact generated
       from the feature attribution component.
     display_name: The display name for the uploaded model evaluation resource.
   """
   # fmt: on
@@ -134,15 +145,15 @@
                   question_answering_metrics.uri,
               ],
           ),
           dsl.IfPresentPlaceholder(
               input_name="summarization_metrics",
               then=[
                   "--summarization_metrics",
-                  "{{$.inputs.artifacts['summarization_metrics'].uri}}",
+                  summarization_metrics.uri,
               ],
           ),
           dsl.IfPresentPlaceholder(
               input_name="feature_attributions",
               then=[
                   "--feature_attributions",
                   feature_attributions.uri,
@@ -167,9 +178,11 @@
           dsl.PIPELINE_JOB_ID_PLACEHOLDER,
           "--pipeline_job_resource_name",
           dsl.PIPELINE_JOB_RESOURCE_NAME_PLACEHOLDER,
           "--model_name",
           model.metadata["resourceName"],
           "--gcp_resources",
           gcp_resources,
+          "--evaluation_resource_name",
+          evaluation_resource_name,
       ],
   )
```

## google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/component.py

```diff
@@ -7,17 +7,18 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
 """Text Generation LLM Evaluation component."""
 
+from typing import List
+
 from google_cloud_pipeline_components import utils as gcpc_utils
 from google_cloud_pipeline_components._implementation.model_evaluation import utils
 from google_cloud_pipeline_components._implementation.model_evaluation import version
 from kfp import dsl
 from kfp.dsl import container_component
 
 
@@ -33,17 +34,16 @@
     predictions_format: str = 'jsonl',
     joined_predictions_gcs_source: str = '',
     predictions_gcs_source: str = '',
     ground_truth_gcs_source: str = '',
     display_name: str = 'model-evaluation-text-generation',
     machine_type: str = 'e2-highmem-16',
     service_account: str = '',
-    enable_web_access: bool = True,
     network: str = '',
-    reserved_ip_ranges: list = [],
+    reserved_ip_ranges: List[str] = [],
     encryption_spec_key_name: str = '',
 ):
   """Computes evaluation metrics of a text generation model.
 
   Supports evaluating large language models performing the following generative
   tasks:
     `summarization`,`question-answering`,`text-generation`
@@ -83,19 +83,14 @@
       service_account (Optional[str]): Sets the default service account for
         workload run-as account. The service account running the pipeline
         (https://cloud.google.com/vertex-ai/docs/pipelines/configure-project#service-account)
         submitting jobs must have act-as permission on this run-as account. If
         unspecified, the Vertex AI Custom Code Service
         Agent(https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents)
         for the CustomJob's project.
-      enable_web_access (Optional[bool]): Whether you want Vertex AI to enable
-        [interactive shell
-        access](https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell)
-        to training containers. If set to `true`, you can access interactive
-        shells at the URIs given by [CustomJob.web_access_uris][].
       network (Optional[str]): The full name of the Compute Engine network to
         which the job should be peered. For example,
         projects/12345/global/networks/myVPC. Format is of the form
         projects/{project}/global/networks/{network}. Where {project} is a
         project number, as in 12345, and {network} is a network name. Private
         services access must already be configured for the network. If left
         unspecified, the job is not peered with any network.
@@ -134,12 +129,11 @@
               f'--ground_truth_gcs_source={ground_truth_gcs_source}',
               f'--evaluation_metrics_output_path={evaluation_metrics.path}',
               '--executor_input={{$.json_escape[1]}}',
           ],
           service_account=service_account,
           network=network,
           reserved_ip_ranges=reserved_ip_ranges,
-          enable_web_access=enable_web_access,
           encryption_spec_key_name=encryption_spec_key_name,
       ),
       gcp_resources=gcp_resources,
   )
```

## google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/component.py

```diff
@@ -8,14 +8,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import List
+
 from google_cloud_pipeline_components._implementation.model_evaluation import version
 from kfp.dsl import container_component
 from kfp.dsl import ContainerSpec
 from kfp.dsl import OutputPath
 from kfp.dsl import PIPELINE_JOB_ID_PLACEHOLDER
 from kfp.dsl import PIPELINE_ROOT_PLACEHOLDER
 from kfp.dsl import PIPELINE_TASK_ID_PLACEHOLDER
@@ -24,15 +26,15 @@
 @container_component
 def target_field_data_remover(
     gcp_resources: OutputPath(str),
     bigquery_output_table: OutputPath(str),
     gcs_output_directory: OutputPath(list),
     project: str,
     location: str = 'us-central1',
-    gcs_source_uris: list = [],
+    gcs_source_uris: List[str] = [],
     bigquery_source_uri: str = '',
     instances_format: str = 'jsonl',
     target_field_name: str = 'ground_truth',
     dataflow_service_account: str = '',
     dataflow_subnetwork: str = '',
     dataflow_use_public_ips: bool = True,
     encryption_spec_key_name: str = '',
```

## google_cloud_pipeline_components/container/_implementation/model/get_model/get_model.py

```diff
@@ -16,15 +16,15 @@
 import argparse
 import os
 import sys
 from typing import Optional
 
 from google.api_core import gapic_v1
 from google.cloud import aiplatform
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources
 from google_cloud_pipeline_components.types.artifact_types import VertexModel
 
 from google.protobuf import json_format
 
 
 RESOURCE_TYPE = 'Model'
@@ -116,15 +116,15 @@
     api_endpoint = location + '-aiplatform.googleapis.com'
     vertex_uri_prefix = f'https://{api_endpoint}/v1/'
 
   vertex_model = VertexModel.create(
       'model', vertex_uri_prefix + model_resource_name, model_resource_name
   )
   # TODO(b/266848949): Output Artifact should use correct MLMD artifact.
-  artifact_util.update_output_artifacts(executor_input, [vertex_model])
+  artifact_utils.update_output_artifacts(executor_input, [vertex_model])
 
   resources = GcpResources()
   model_resource = resources.resources.add()
   model_resource.resource_type = RESOURCE_TYPE
   model_resource.resource_uri = f'{vertex_uri_prefix}{model_resource_name}'
   with open(gcp_resources, 'w') as f:
     f.write(json_format.MessageToJson(resources))
```

## google_cloud_pipeline_components/container/_implementation/model_evaluation/import_model_evaluation.py

```diff
@@ -125,21 +125,28 @@
 parser.add_argument(
     '--gcp_resources',
     dest='gcp_resources',
     type=_make_parent_dirs_and_return_path,
     required=True,
     default=argparse.SUPPRESS,
 )
+parser.add_argument(
+    '--evaluation_resource_name',
+    dest='evaluation_resource_name',
+    type=_make_parent_dirs_and_return_path,
+    required=True,
+    default=argparse.SUPPRESS,
+)
 
 
 def main(argv):
   """Calls ModelService.ImportModelEvaluation."""
   parsed_args, _ = parser.parse_known_args(argv)
 
-  if parsed_args.model_name.startswith('publishers'):
+  if 'publishers/google' in parsed_args.model_name:
     return
 
   _, project_id, _, location, _, model_id = parsed_args.model_name.split('/')
   api_endpoint = location + '-aiplatform.googleapis.com'
   resource_uri_prefix = f'https://{api_endpoint}/v1/'
 
   if parsed_args.classification_metrics:
@@ -159,14 +166,19 @@
     problem_type = 'question-answering'
   elif parsed_args.summarization_metrics:
     metrics_file_path = parsed_args.summarization_metrics
     problem_type = 'summarization'
   else:
     metrics_file_path = parsed_args.metrics
     problem_type = parsed_args.problem_type
+    if problem_type not in PROBLEM_TYPE_TO_SCHEMA_URI:
+      raise ValueError(
+          'Unsupported problem_type: {}. Supported problem types are: {}'
+          .format(problem_type, list(PROBLEM_TYPE_TO_SCHEMA_URI.keys()))
+      )
 
   logging.info('metrics_file_path: %s', metrics_file_path)
   logging.info('problem_type: %s', problem_type)
   metrics_file_path = (
       metrics_file_path
       if not metrics_file_path.startswith('gs://')
       else '/gcs' + metrics_file_path[4:]
@@ -266,14 +278,18 @@
   )
   import_model_evaluation_response = client.import_model_evaluation(
       parent=parsed_args.model_name,
       model_evaluation=model_evaluation,
   )
   model_evaluation_name = import_model_evaluation_response.name
 
+  # Write the model evaluation resource to evaluation_resource_name output.
+  with open(parsed_args.evaluation_resource_name, 'w') as f:
+    f.write(model_evaluation_name)
+
   resources = GcpResources()
   # Write the model evaluation resource to GcpResources output.
   model_eval_resource = resources.resources.add()
   model_eval_resource.resource_type = MODEL_EVALUATION_RESOURCE_TYPE
   model_eval_resource.resource_uri = (
       f'{resource_uri_prefix}{model_evaluation_name}'
   )
```

## google_cloud_pipeline_components/container/v1/batch_prediction_job/remote_runner.py

```diff
@@ -15,16 +15,16 @@
 
 import json
 import logging
 import re
 
 from google.api_core import retry
 from google.cloud.aiplatform import explain
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.v1.gcp_launcher import job_remote_runner
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import error_util
 from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import gcp_labels_util
 from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import json_util
 from google_cloud_pipeline_components.types.artifact_types import BQTable
 from google_cloud_pipeline_components.types.artifact_types import VertexBatchPredictionJob
 from kfp import dsl
 
@@ -237,10 +237,10 @@
       output_artifacts.append(
           dsl.Artifact(
               'gcs_output_directory',
               get_job_response.output_info.gcs_output_directory,
           )
       )
 
-    artifact_util.update_output_artifacts(executor_input, output_artifacts)
+    artifact_utils.update_output_artifacts(executor_input, output_artifacts)
   except (ConnectionError, RuntimeError) as err:
     error_util.exit_with_internal_error(err.args[0])
```

## google_cloud_pipeline_components/container/v1/bigquery/create_model/remote_runner.py

```diff
@@ -11,16 +11,16 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 import google.auth
 import google.auth.transport.requests
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.v1.bigquery.utils import bigquery_util
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 from google_cloud_pipeline_components.types.artifact_types import BQMLModel
 
 
 def bigquery_create_model_job(
     type,
     project,
     location,
@@ -92,8 +92,8 @@
     )
 
   projectId = query_result['ddlTargetTable']['projectId']
   datasetId = query_result['ddlTargetTable']['datasetId']
   # tableId is the model ID
   modelId = query_result['ddlTargetTable']['tableId']
   bqml_model_artifact = BQMLModel.create('model', projectId, datasetId, modelId)
-  artifact_util.update_output_artifacts(executor_input, [bqml_model_artifact])
+  artifact_utils.update_output_artifacts(executor_input, [bqml_model_artifact])
```

## google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/remote_runner.py

```diff
@@ -13,16 +13,16 @@
 # limitations under the License.
 
 import json
 import logging
 
 import google.auth
 import google.auth.transport.requests
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.v1.bigquery.utils import bigquery_util
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 
 
 def bigquery_evaluate_model_job(
     type,
     project,
     location,
     model_name,
@@ -122,15 +122,15 @@
   # Poll bigquery job status until finished.
   job = bigquery_util.poll_job(job_uri, creds)
   logging.info('Getting query result for job ' + job['id'])
   _, job_id = job['id'].split('.')
   query_results = bigquery_util.get_query_results(
       project, job_id, location, creds
   )
-  artifact_util.update_output_artifact(
+  artifact_utils.update_output_artifact(
       executor_input,
       'evaluation_metrics',
       '',
       {
           bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA: query_results[
               bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA
           ],
```

## google_cloud_pipeline_components/container/v1/bigquery/feature_importance/remote_runner.py

```diff
@@ -13,16 +13,16 @@
 # limitations under the License.
 
 import json
 import logging
 
 import google.auth
 import google.auth.transport.requests
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.v1.bigquery.utils import bigquery_util
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 
 
 def bigquery_ml_feature_importance_job(
     type,
     project,
     location,
     model_name,
@@ -79,15 +79,15 @@
   # Poll bigquery job status until finished.
   job = bigquery_util.poll_job(job_uri, creds)
   logging.info('Getting query result for job ' + job['id'])
   _, job_id = job['id'].split('.')
   query_results = bigquery_util.get_query_results(
       project, job_id, location, creds
   )
-  artifact_util.update_output_artifact(
+  artifact_utils.update_output_artifact(
       executor_input,
       'feature_importance',
       '',
       {
           bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA: query_results[
               bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA
           ],
```

## google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/remote_runner.py

```diff
@@ -13,16 +13,16 @@
 # limitations under the License.
 
 import json
 import logging
 
 import google.auth
 import google.auth.transport.requests
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.v1.bigquery.utils import bigquery_util
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 
 
 def bigquery_ml_advanced_weights_job(
     type,
     project,
     location,
     model_name,
@@ -86,15 +86,15 @@
   # Poll bigquery job status until finished.
   job = bigquery_util.poll_job(job_uri, creds)
   logging.info('Getting query result for job ' + job['id'])
   _, job_id = job['id'].split('.')
   query_results = bigquery_util.get_query_results(
       project, job_id, location, creds
   )
-  artifact_util.update_output_artifact(
+  artifact_utils.update_output_artifact(
       executor_input,
       'advanced_weights',
       '',
       {
           bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA: query_results[
               bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA
           ],
```

## google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/remote_runner.py

```diff
@@ -13,16 +13,16 @@
 # limitations under the License.
 
 import json
 import logging
 
 import google.auth
 import google.auth.transport.requests
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.v1.bigquery.utils import bigquery_util
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 
 
 def bigquery_ml_arima_coefficients(
     type,
     project,
     location,
     model_name,
@@ -74,15 +74,15 @@
   # Poll bigquery job status until finished.
   job = bigquery_util.poll_job(job_uri, creds)
   logging.info('Getting query result for job ' + job['id'])
   _, job_id = job['id'].split('.')
   query_results = bigquery_util.get_query_results(
       project, job_id, location, creds
   )
-  artifact_util.update_output_artifact(
+  artifact_utils.update_output_artifact(
       executor_input,
       'arima_coefficients',
       '',
       {
           bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA: query_results[
               bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA
           ],
```

## google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/remote_runner.py

```diff
@@ -13,16 +13,16 @@
 # limitations under the License.
 
 import json
 import logging
 
 import google.auth
 import google.auth.transport.requests
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.v1.bigquery.utils import bigquery_util
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 
 
 def bigquery_ml_arima_evaluate_job(
     type,
     project,
     location,
     model_name,
@@ -99,15 +99,15 @@
   # Poll bigquery job status until finished.
   job = bigquery_util.poll_job(job_uri, creds)
   logging.info('Getting query result for job ' + job['id'])
   _, job_id = job['id'].split('.')
   query_results = bigquery_util.get_query_results(
       project, job_id, location, creds
   )
-  artifact_util.update_output_artifact(
+  artifact_utils.update_output_artifact(
       executor_input,
       'arima_evaluation_metrics',
       '',
       {
           bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA: query_results[
               bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA
           ],
```

## google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/remote_runner.py

```diff
@@ -13,16 +13,16 @@
 # limitations under the License.
 
 import json
 import logging
 
 import google.auth
 import google.auth.transport.requests
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.v1.bigquery.utils import bigquery_util
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 
 
 def bigquery_ml_centroids_job(
     type,
     project,
     location,
     model_name,
@@ -96,15 +96,15 @@
 
   # For ML.CENTROIDS job, the output only contains one row per feature per
   # centroid, which should be very small. Thus, we allow users to directly get
   # the result without writing into a BQ table.
   query_results = bigquery_util.get_query_results(
       project, job_id, location, creds
   )
-  artifact_util.update_output_artifact(
+  artifact_utils.update_output_artifact(
       executor_input,
       'centroids',
       '',
       {
           bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA: query_results[
               bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA
           ],
```

## google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/remote_runner.py

```diff
@@ -13,16 +13,16 @@
 # limitations under the License.
 
 import json
 import logging
 
 import google.auth
 import google.auth.transport.requests
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.v1.bigquery.utils import bigquery_util
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 
 
 def bigquery_ml_feature_info_job(
     type,
     project,
     location,
     model_name,
@@ -86,15 +86,15 @@
   # Poll bigquery job status until finished.
   job = bigquery_util.poll_job(job_uri, creds)
   logging.info('Getting query result for job ' + job['id'])
   _, job_id = job['id'].split('.')
   query_results = bigquery_util.get_query_results(
       project, job_id, location, creds
   )
-  artifact_util.update_output_artifact(
+  artifact_utils.update_output_artifact(
       executor_input,
       'feature_info',
       '',
       {
           bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA: query_results[
               bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA
           ],
```

## google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/remote_runner.py

```diff
@@ -13,16 +13,16 @@
 # limitations under the License.
 
 import json
 import logging
 
 import google.auth
 import google.auth.transport.requests
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.v1.bigquery.utils import bigquery_util
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 
 
 def bigquery_ml_training_info_job(
     type,
     project,
     location,
     model_name,
@@ -86,15 +86,15 @@
   # Poll bigquery job status until finished.
   job = bigquery_util.poll_job(job_uri, creds)
   logging.info('Getting query result for job %s', job['id'])
   _, job_id = job['id'].split('.')
   query_results = bigquery_util.get_query_results(
       project, job_id, location, creds
   )
-  artifact_util.update_output_artifact(
+  artifact_utils.update_output_artifact(
       executor_input,
       'ml_training_info',
       '',
       {
           bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA: query_results[
               bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA
           ],
```

## google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/remote_runner.py

```diff
@@ -13,16 +13,16 @@
 # limitations under the License.
 
 import json
 import logging
 
 import google.auth
 import google.auth.transport.requests
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.v1.bigquery.utils import bigquery_util
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 
 
 def bigquery_ml_trial_info_job(
     type,
     project,
     location,
     model_name,
@@ -90,15 +90,15 @@
 
   # For ML Trial Info job, as the returned results only contains num_trials
   # rows, which should be very small. In this case we allow users to directly
   # get the result without writing into a BQ table.
   query_results = bigquery_util.get_query_results(
       project, job_id, location, creds
   )
-  artifact_util.update_output_artifact(
+  artifact_utils.update_output_artifact(
       executor_input,
       'trial_info',
       '',
       {
           bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA: query_results[
               bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA
           ],
```

## google_cloud_pipeline_components/container/v1/bigquery/ml_weights/remote_runner.py

```diff
@@ -13,16 +13,16 @@
 # limitations under the License.
 
 import json
 import logging
 
 import google.auth
 import google.auth.transport.requests
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.v1.bigquery.utils import bigquery_util
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 
 
 def bigquery_ml_weights_job(
     type,
     project,
     location,
     model_name,
@@ -86,15 +86,15 @@
   # Poll bigquery job status until finished.
   job = bigquery_util.poll_job(job_uri, creds)
   logging.info('Getting query result for job ' + job['id'])
   _, job_id = job['id'].split('.')
   query_results = bigquery_util.get_query_results(
       project, job_id, location, creds
   )
-  artifact_util.update_output_artifact(
+  artifact_utils.update_output_artifact(
       executor_input,
       'weights',
       '',
       {
           bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA: query_results[
               bigquery_util.ARTIFACT_PROPERTY_KEY_SCHEMA
           ],
```

## google_cloud_pipeline_components/container/v1/bigquery/utils/bigquery_util.py

```diff
@@ -18,16 +18,16 @@
 from os import path
 import re
 import time
 from typing import Optional
 
 import google.auth
 import google.auth.transport.requests
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.utils import execution_context
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import gcp_labels_util
 from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import json_util
 from google_cloud_pipeline_components.proto import gcp_resources_pb2
 from google_cloud_pipeline_components.types.artifact_types import BQTable
 import requests
 
 from google.protobuf import json_format
@@ -330,8 +330,8 @@
   if 'destinationTable' in job['configuration']['query']:
     projectId = job['configuration']['query']['destinationTable']['projectId']
     datasetId = job['configuration']['query']['destinationTable']['datasetId']
     tableId = job['configuration']['query']['destinationTable']['tableId']
     bq_table_artifact = BQTable.create(
         artifact_name, projectId, datasetId, tableId
     )
-    artifact_util.update_output_artifacts(executor_input, [bq_table_artifact])
+    artifact_utils.update_output_artifacts(executor_input, [bq_table_artifact])
```

## google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/remote_runner.py

```diff
@@ -9,16 +9,16 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import json
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.v1.gcp_launcher import lro_remote_runner
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import gcp_labels_util
 from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import json_util
 from google_cloud_pipeline_components.types.artifact_types import VertexEndpoint
 
 _LABELS_PAYLOAD_KEY = 'labels'
 
 
@@ -52,8 +52,8 @@
   endpoint_resource_name = create_endpoint_lro['response']['name']
 
   vertex_endpoint = VertexEndpoint.create(
       'endpoint',
       vertex_uri_prefix + endpoint_resource_name,
       endpoint_resource_name,
   )
-  artifact_util.update_output_artifacts(executor_input, [vertex_endpoint])
+  artifact_utils.update_output_artifacts(executor_input, [vertex_endpoint])
```

## google_cloud_pipeline_components/container/v1/model/upload_model/remote_runner.py

```diff
@@ -11,16 +11,16 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import json
 from typing import Optional
 
+from google_cloud_pipeline_components.container.utils import artifact_utils
 from google_cloud_pipeline_components.container.v1.gcp_launcher import lro_remote_runner
-from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import artifact_util
 from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import error_util
 from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import gcp_labels_util
 from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import json_util
 from google_cloud_pipeline_components.types.artifact_types import VertexModel
 
 
 ARTIFACT_PROPERTY_KEY_UNMANAGED_CONTAINER_MODEL = 'unmanaged_container_model'
@@ -104,10 +104,10 @@
       model_resource_name += (
           f'@{upload_model_lro["response"]["model_version_id"]}'
       )
 
     vertex_model = VertexModel.create(
         'model', vertex_uri_prefix + model_resource_name, model_resource_name
     )
-    artifact_util.update_output_artifacts(executor_input, [vertex_model])
+    artifact_utils.update_output_artifacts(executor_input, [vertex_model])
   except (ConnectionError, RuntimeError) as err:
     error_util.exit_with_internal_error(err.args[0])
```

## google_cloud_pipeline_components/preview/model_evaluation/__init__.py

```diff
@@ -11,14 +11,16 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Model evaluation preview components."""
 
 from google_cloud_pipeline_components.preview.model_evaluation.data_bias_component import detect_data_bias as DetectDataBiasOp
 from google_cloud_pipeline_components.preview.model_evaluation.feature_attribution_component import feature_attribution as ModelEvaluationFeatureAttributionOp
+from google_cloud_pipeline_components.preview.model_evaluation.feature_attribution_graph_component import feature_attribution_graph_component as FeatureAttributionGraphComponentOp
 from google_cloud_pipeline_components.preview.model_evaluation.model_bias_component import detect_model_bias as DetectModelBiasOp
 
 __all__ = [
     'ModelEvaluationFeatureAttributionOp',
+    'FeatureAttributionGraphComponentOp',
     'DetectModelBiasOp',
     'DetectDataBiasOp',
 ]
```

## google_cloud_pipeline_components/preview/model_evaluation/data_bias_component.py

```diff
@@ -7,14 +7,17 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
+from typing import Any, List
+
 from google_cloud_pipeline_components._implementation.model_evaluation import version
 from google_cloud_pipeline_components.types.artifact_types import VertexDataset
 from kfp.dsl import Artifact
 from kfp.dsl import container_component
 from kfp.dsl import ContainerSpec
 from kfp.dsl import Input
 from kfp.dsl import Output
@@ -26,20 +29,20 @@
 
 @container_component
 def detect_data_bias(
     gcp_resources: OutputPath(str),
     data_bias_metrics: Output[Artifact],
     project: str,
     target_field_name: str,
-    bias_configs: list,
+    bias_configs: List[Any],
     location: str = 'us-central1',
     dataset_format: str = 'jsonl',
-    dataset_storage_source_uris: list = [],
+    dataset_storage_source_uris: List[str] = [],
     dataset: Input[VertexDataset] = None,
-    columns: list = [],
+    columns: List[str] = [],
     encryption_spec_key_name: str = '',
 ):
   # fmt: off
   """Detects data bias metrics in a dataset.
 
   Creates a Dataflow job with Apache Beam to category each data point in the
   dataset to the corresponding bucket based on bias configs, then compute data
```

## google_cloud_pipeline_components/preview/model_evaluation/utils.py

```diff
@@ -8,19 +8,20 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Any, Dict, List, Optional, Tuple, Union
+from typing import Any, Dict, List, Union
 
 from google.cloud.aiplatform_v1.types.model_evaluation_slice import ModelEvaluationSlice
+
+from google.protobuf.wrappers_pb2 import BoolValue
 from google.protobuf import json_format
-from google.protobuf import wrappers_pb2
 
 
 def create_slice_specs_list(
     list_of_feature_and_value: List[
         Dict[str, Union[float, int, str, List[float], bool]]
     ]
 ) -> List[ModelEvaluationSlice.Slice.SliceSpec]:
@@ -42,15 +43,15 @@
   slice_specs_list = []
   for feature_and_value in list_of_feature_and_value:
     configs = {}
     for feature, value in feature_and_value.items():
       if isinstance(value, bool):
         # Bool must be checked first, bool is a child of int in Python.
         configs[feature] = ModelEvaluationSlice.Slice.SliceSpec.SliceConfig(
-            all_values=wrappers_pb2.BoolValue(value=value)
+            all_values=BoolValue(value=value)
         )
       elif isinstance(value, int) or isinstance(value, float):
         configs[feature] = ModelEvaluationSlice.Slice.SliceSpec.SliceConfig(
             value=ModelEvaluationSlice.Slice.SliceSpec.Value(
                 float_value=float(value)
             )
         )
```

## google_cloud_pipeline_components/types/artifact_types.py

```diff
@@ -34,15 +34,15 @@
     'UnmanagedContainerModel',
     'ClassificationMetrics',
     'RegressionMetrics',
     'ForecastingMetrics',
 ]
 
 import textwrap
-from typing import Dict, Optional
+from typing import Any, Dict, Optional
 from kfp import dsl
 
 _RESOURCE_NAME_KEY = 'resourceName'
 
 
 class VertexModel(dsl.Artifact):
   """An artifact representing a Vertex AI `Model resource <https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models>`_."""
@@ -70,14 +70,17 @@
       https://{service-endpoint}/v1/projects/{project}/locations/{location}/models/{model},
         where {service-endpoint} is one of the supported service endpoints at
       https://cloud.google.com/vertex-ai/docs/reference/rest#rest_endpoints
       model_resource_name: The name of the Model resource, in a form of
         projects/{project}/locations/{location}/models/{model}. For more
         details, see
       https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models/get
+
+    Returns:
+      VertexModel instance.
     """
     return cls(
         name=name,
         uri=uri,
         metadata={_RESOURCE_NAME_KEY: model_resource_name},
     )
 
@@ -108,14 +111,17 @@
       https://{service-endpoint}/v1/projects/{project}/locations/{location}/endpoints/{endpoint},
         where {service-endpoint} is one of the supported service endpoints at
       https://cloud.google.com/vertex-ai/docs/reference/rest#rest_endpoints
       endpoint_resource_name: The name of the Endpoint resource, in a form of
         projects/{project}/locations/{location}/endpoints/{endpoint}. For more
         details, see
       https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/get
+
+    Returns:
+      VertexEndpoint instance.
     """
     return cls(
         name=name,
         uri=uri,
         metadata={_RESOURCE_NAME_KEY: endpoint_resource_name},
     )
 
@@ -168,14 +174,17 @@
       bq://projectId.bqDatasetId format, into which the prediction output is
         written. For more details, see
       https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#outputinfo
       gcs_output_directory: The full path of the Cloud Storage directory
         created, into which the prediction output is written. For more details,
         see
       https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#outputinfo
+
+    Returns:
+      VertexBatchPredictionJob instance.
     """
     return cls(
         name=name,
         uri=uri,
         metadata={
             _RESOURCE_NAME_KEY: job_resource_name,
             'bigqueryOutputTable': bigquery_output_table,
@@ -211,14 +220,17 @@
       https://{service-endpoint}/v1/projects/{project}/locations/{location}/datasets/{datasets_name},
         where {service-endpoint} is one of the supported service endpoints at
       https://cloud.google.com/vertex-ai/docs/reference/rest#rest_endpoints
       dataset_resource_name: The name of the Dataset resource, in a form of
         projects/{project}/locations/{location}/datasets/{datasets_name}. For
         more details, see
       https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.datasets/get
+
+    Returns:
+      VertexDataset instance.
     """
     return cls(
         uri=uri,
         name=name,
         metadata={_RESOURCE_NAME_KEY: dataset_resource_name},
     )
 
@@ -250,14 +262,17 @@
 
     Args:
       name: The artifact name.
       project_id: The ID of the project containing this model.
       dataset_id: The ID of the dataset containing this model.
       model_id: The ID of the model.  For more details, see
       https://cloud.google.com/bigquery/docs/reference/rest/v2/models#ModelReference
+
+    Returns:
+      BQMLModel instance.
     """
     return cls(
         name=name,
         uri=f'https://www.googleapis.com/bigquery/v2/projects/{project_id}/datasets/{dataset_id}/models/{model_id}',
         metadata={
             'projectId': project_id,
             'datasetId': dataset_id,
@@ -295,14 +310,17 @@
 
     Args:
       name: The artifact name.
       project_id: The ID of the project containing this table.
       dataset_id: The ID of the dataset containing this table.
       table_id: The ID of the table.  For more details, see
       https://cloud.google.com/bigquery/docs/reference/rest/v2/TableReference
+
+    Returns:
+      BQTable instance.
     """
     return cls(
         name=name,
         uri=f'https://www.googleapis.com/bigquery/v2/projects/{project_id}/datasets/{dataset_id}/tables/{table_id}',
         metadata={
             'projectId': project_id,
             'datasetId': dataset_id,
@@ -361,28 +379,31 @@
         type: string
       healthRoute:
         type: string""")
 
   @classmethod
   def create(
       cls,
-      predict_schemata: Dict,
-      container_spec: Dict,
+      predict_schemata: Dict[str, str],
+      container_spec: Dict[str, Any],
   ) -> 'UnmanagedContainerModel':
     """Create a UnmanagedContainerModel artifact instance.
 
     Args:
       predict_schemata: Contains the schemata used in Model's predictions and
         explanations via PredictionService.Predict, PredictionService.Explain
         and BatchPredictionJob. For more details, see
       https://cloud.google.com/vertex-ai/docs/reference/rest/v1/PredictSchemata
       container_spec: Specification of a container for serving predictions. Some
         fields in this message correspond to fields in the Kubernetes Container
         v1 core specification. For more details, see
       https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ModelContainerSpec
+
+    Returns:
+      UnmanagedContainerModel instance.
     """
     return cls(
         metadata={
             'predictSchemata': predict_schemata,
             'containerSpec': container_spec,
         }
     )
@@ -527,21 +548,25 @@
       au_prc: Optional[float] = None,
       au_roc: Optional[float] = None,
       log_loss: Optional[float] = None,
   ) -> 'ClassificationMetrics':
     """Create a ClassificationMetrics artifact instance.
 
     Args:
+      name: The artifact name.
       recall: Recall (True Positive Rate) for the given confidence threshold.
       precision: Precision for the given confidence threshold.
       f1_score: The harmonic mean of recall and precision.
       accuracy: Accuracy is the fraction of predictions given the correct label.
       au_prc: The Area Under Precision-Recall Curve metric.
       au_roc: The Area Under Receiver Operating Characteristic curve metric.
       log_loss: The Log Loss metric.
+
+    Returns:
+      ClassificationMetrics instance.
     """
     metadata = {}
     if recall is not None:
       metadata['recall'] = recall
     if precision is not None:
       metadata['precision'] = precision
     if f1_score is not None:
@@ -594,20 +619,24 @@
       mean_absolute_percentage_error: Optional[float] = None,
       r_squared: Optional[float] = None,
       root_mean_squared_log_error: Optional[float] = None,
   ) -> 'RegressionMetrics':
     """Create a RegressionMetrics artifact instance.
 
     Args:
+      name: The artifact name.
       root_mean_squared_error: Root Mean Squared Error (RMSE).
       mean_absolute_error: Mean Absolute Error (MAE).
       mean_absolute_percentage_error: Mean absolute percentage error.
       r_squared: Coefficient of determination as Pearson correlation
         coefficient.
       root_mean_squared_log_error: Root mean squared log error.
+
+    Returns:
+      RegressionMetrics instance.
     """
     metadata = {}
     if root_mean_squared_error is not None:
       metadata['rootMeanSquaredError'] = root_mean_squared_error
     if mean_absolute_error is not None:
       metadata['meanAbsoluteError'] = mean_absolute_error
     if mean_absolute_percentage_error is not None:
@@ -682,28 +711,32 @@
       weighted_absolute_percentage_error: Optional[float] = None,
       root_mean_squared_percentage_error: Optional[float] = None,
       symmetric_mean_absolute_percentage_error: Optional[float] = None,
   ) -> 'ForecastingMetrics':
     """Create a ForecastingMetrics artifact instance.
 
     Args:
+      name: The artifact name.
       root_mean_squared_error: Root Mean Squared Error (RMSE).
       mean_absolute_error: Mean Absolute Error (MAE).
       mean_absolute_percentage_error: Mean absolute percentage error.
       r_squared: Coefficient of determination as Pearson correlation
         coefficient.
       root_mean_squared_log_error: Root mean squared log error.
       weighted_absolute_percentage_error: Weighted Absolute Percentage Error.
         Does not use weights, this is just what the metric is called. Undefined
         if actual values sum to zero. Will be very large if actual values sum to
         a very small number.
       root_mean_squared_percentage_error: Root Mean Square Percentage Error.
         Square root of MSPE. Undefined/imaginary when MSPE is negative.
       symmetric_mean_absolute_percentage_error: Symmetric Mean Absolute
         Percentage Error.
+
+    Returns:
+      ForecastingMetrics instance.
     """
     metadata = {}
     if root_mean_squared_error is not None:
       metadata['rootMeanSquaredError'] = root_mean_squared_error
     if mean_absolute_error is not None:
       metadata['meanAbsoluteError'] = mean_absolute_error
     if mean_absolute_percentage_error is not None:
```

## google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_train_pipeline.yaml

```diff
@@ -4254,15 +4254,15 @@
           python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef create_metrics_artifact(\n    metrics_rows: List[Dict[str, str]],\n\
           \    evaluation_metrics: dsl.Output[dsl.Metrics],\n) -> None:\n  \"\"\"\
           Converts the rows of a metrics table into an Artifact.\"\"\"\n  # Use the\
-          \ Vertex Eval component's Metrics metadata naming from\n  # http://google3/third_party/py/google/cloud/aiplatform/aiplatform/metadata/schema/google/artifact_schema.py?cl=467006447&l=344\n\
+          \ Vertex Eval component's Metrics metadata naming from\n\
           \  metric_name_map = {\n      'MAE': 'meanAbsoluteError',\n      'RMSE':\
           \ 'rootMeanSquaredError',\n      'MAPE': 'meanAbsolutePercentageError',\n\
           \  }\n  metrics = {metric_name_map[k]: v for k, v in dict(metrics_rows[0]).items()}\n\
           \  evaluation_metrics.metadata = metrics\n\n"
         image: python:3.7-slim
     exec-feature-transform-engine:
       container:
```

## google_cloud_pipeline_components/v1/model_evaluation/classification_component.py

```diff
@@ -8,14 +8,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import Any, List
+
 from google_cloud_pipeline_components._implementation.model_evaluation import version
 from google_cloud_pipeline_components.types.artifact_types import BQTable
 from google_cloud_pipeline_components.types.artifact_types import ClassificationMetrics
 from google_cloud_pipeline_components.types.artifact_types import VertexModel
 from kfp import dsl
 from kfp.dsl import container_component
 
@@ -28,22 +30,22 @@
     target_field_name: str,
     model: dsl.Input[VertexModel] = None,
     location: str = 'us-central1',
     predictions_format: str = 'jsonl',
     predictions_gcs_source: dsl.Input[dsl.Artifact] = None,
     predictions_bigquery_source: dsl.Input[BQTable] = None,
     ground_truth_format: str = 'jsonl',
-    ground_truth_gcs_source: list = [],
+    ground_truth_gcs_source: List[str] = [],
     ground_truth_bigquery_source: str = '',
     classification_type: str = 'multiclass',
-    class_labels: list = [],
+    class_labels: List[str] = [],
     prediction_score_column: str = 'prediction.scores',
     prediction_label_column: str = 'prediction.classes',
-    slicing_specs: list = [],
-    positive_classes: list = [],
+    slicing_specs: List[Any] = [],
+    positive_classes: List[str] = [],
     dataflow_service_account: str = '',
     dataflow_disk_size_gb: int = 50,
     dataflow_machine_type: str = 'n1-standard-4',
     dataflow_workers_num: int = 1,
     dataflow_max_workers_num: int = 5,
     dataflow_subnetwork: str = '',
     dataflow_use_public_ips: bool = True,
```

## google_cloud_pipeline_components/v1/model_evaluation/error_analysis_pipeline.py

```diff
@@ -8,14 +8,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import List
+
 from google_cloud_pipeline_components._implementation.model import GetVertexModelOp
 from google_cloud_pipeline_components._implementation.model_evaluation import ErrorAnalysisAnnotationOp
 from google_cloud_pipeline_components._implementation.model_evaluation import EvaluatedAnnotationOp
 from google_cloud_pipeline_components._implementation.model_evaluation import EvaluationDatasetPreprocessorOp as DatasetPreprocessorOp
 from google_cloud_pipeline_components._implementation.model_evaluation import FeatureExtractorOp
 from google_cloud_pipeline_components._implementation.model_evaluation import ModelImportEvaluatedAnnotationOp
 from google_cloud_pipeline_components._implementation.model_evaluation import ModelImportEvaluationOp
@@ -32,16 +34,16 @@
     location: str,
     model_name: str,
     batch_predict_gcs_destination_output_uri: str,
     test_dataset_resource_name: str = '',
     test_dataset_annotation_set_name: str = '',
     training_dataset_resource_name: str = '',
     training_dataset_annotation_set_name: str = '',
-    test_dataset_storage_source_uris: list = [],
-    training_dataset_storage_source_uris: list = [],
+    test_dataset_storage_source_uris: List[str] = [],
+    training_dataset_storage_source_uris: List[str] = [],
     batch_predict_instances_format: str = 'jsonl',
     batch_predict_predictions_format: str = 'jsonl',
     batch_predict_machine_type: str = 'n1-standard-32',
     batch_predict_starting_replica_count: int = 5,
     batch_predict_max_replica_count: int = 10,
     batch_predict_accelerator_type: str = '',
     batch_predict_accelerator_count: int = 0,
```

## google_cloud_pipeline_components/v1/model_evaluation/evaluated_annotation_pipeline.py

```diff
@@ -8,14 +8,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import List
+
 from google_cloud_pipeline_components._implementation.model import GetVertexModelOp
 from google_cloud_pipeline_components._implementation.model_evaluation import EvaluatedAnnotationOp
 from google_cloud_pipeline_components._implementation.model_evaluation import EvaluationDatasetPreprocessorOp as DatasetPreprocessorOp
 from google_cloud_pipeline_components._implementation.model_evaluation import ModelImportEvaluatedAnnotationOp
 from google_cloud_pipeline_components._implementation.model_evaluation import ModelImportEvaluationOp
 from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp
 from google_cloud_pipeline_components.v1.dataset import GetVertexDatasetOp
@@ -27,15 +29,15 @@
 def evaluated_annotation_pipeline(
     project: str,
     location: str,
     model_name: str,
     batch_predict_gcs_destination_output_uri: str,
     test_dataset_resource_name: str = '',
     test_dataset_annotation_set_name: str = '',
-    test_dataset_storage_source_uris: list = [],
+    test_dataset_storage_source_uris: List[str] = [],
     batch_predict_instances_format: str = 'jsonl',
     batch_predict_predictions_format: str = 'jsonl',
     batch_predict_machine_type: str = 'n1-standard-32',
     batch_predict_starting_replica_count: int = 5,
     batch_predict_max_replica_count: int = 10,
     batch_predict_accelerator_type: str = '',
     batch_predict_accelerator_count: int = 0,
```

## google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_feature_attribution_pipeline.py

```diff
@@ -8,68 +8,74 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import Any, Dict, List, NamedTuple
+
 from google_cloud_pipeline_components._implementation.model import GetVertexModelOp
-from google_cloud_pipeline_components._implementation.model_evaluation import EvaluationDataSamplerOp
 from google_cloud_pipeline_components._implementation.model_evaluation import ModelImportEvaluationOp
-from google_cloud_pipeline_components.preview.model_evaluation import ModelEvaluationFeatureAttributionOp
+from google_cloud_pipeline_components.preview.model_evaluation import FeatureAttributionGraphComponentOp
+from google_cloud_pipeline_components.types.artifact_types import ClassificationMetrics
+from google_cloud_pipeline_components.types.artifact_types import RegressionMetrics
 from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp
 from google_cloud_pipeline_components.v1.model_evaluation.classification_component import model_evaluation_classification as ModelEvaluationClassificationOp
-from google_cloud_pipeline_components.v1.model_evaluation.forecasting_component import model_evaluation_forecasting as ModelEvaluationForecastingOp
 from google_cloud_pipeline_components.v1.model_evaluation.regression_component import model_evaluation_regression as ModelEvaluationRegressionOp
 import kfp
 
 
-@kfp.dsl.pipeline(name='evaluation-automl-tabular-feature-attribution-pipeline')
-def evaluation_automl_tabular_feature_attribution_pipeline(  # pylint: disable=dangerous-default-value
+@kfp.dsl.pipeline(
+    name='evaluation-automl-tabular-feature-attribution-classification-pipeline'
+)
+def evaluation_automl_tabular_feature_attribution_classification_pipeline(  # pylint: disable=dangerous-default-value
     project: str,
     location: str,
-    prediction_type: str,
     model_name: str,
     target_field_name: str,
     batch_predict_instances_format: str,
     batch_predict_gcs_destination_output_uri: str,
-    batch_predict_gcs_source_uris: list = [],  # pylint: disable=g-bare-generic
+    batch_predict_gcs_source_uris: List[str] = [],  # pylint: disable=g-bare-generic
     batch_predict_bigquery_source_uri: str = '',
     batch_predict_predictions_format: str = 'jsonl',
     batch_predict_bigquery_destination_output_uri: str = '',
     batch_predict_machine_type: str = 'n1-standard-16',
     batch_predict_starting_replica_count: int = 5,
     batch_predict_max_replica_count: int = 10,
-    batch_predict_explanation_metadata: dict = {},  # pylint: disable=g-bare-generic
-    batch_predict_explanation_parameters: dict = {},  # pylint: disable=g-bare-generic
+    batch_predict_explanation_metadata: Dict[str, Any] = {},  # pylint: disable=g-bare-generic
+    batch_predict_explanation_parameters: Dict[str, Any] = {},  # pylint: disable=g-bare-generic
     batch_predict_explanation_data_sample_size: int = 10000,
     batch_predict_accelerator_type: str = '',
     batch_predict_accelerator_count: int = 0,
-    slicing_specs: list = [],  # pylint: disable=g-bare-generic
+    slicing_specs: List[Any] = [],  # pylint: disable=g-bare-generic
     dataflow_machine_type: str = 'n1-standard-4',
     dataflow_max_num_workers: int = 5,
     dataflow_disk_size_gb: int = 50,
     dataflow_service_account: str = '',
     dataflow_subnetwork: str = '',
     dataflow_use_public_ips: bool = True,
     encryption_spec_key_name: str = '',
     force_runner_mode: str = '',
+) -> NamedTuple(
+    'outputs',
+    evaluation_metrics=ClassificationMetrics,
+    evaluation_resource_name=str,
 ):
-  """The evaluation AutoML tabular pipeline with feature attribution.
+  """The evaluation AutoML tabular pipeline with feature attribution for classification models.
 
   This pipeline guarantees support for AutoML Tabular models that contain a
-  valid explanation_spec. This pipeline does not include the data_splitter
-  component, which is needed for many tabular custom models.
+  valid explanation_spec. This pipeline does not include the
+  target_field_data_remover component, which is needed for many tabular custom
+  models.
 
 
   Args:
     project: The GCP project that runs the pipeline components.
     location: The GCP region that runs the pipeline components.
-    prediction_type: The type of prediction the model is to produce.
-      "classification", "regression", or "forecasting".
     model_name: The Vertex model resource name to be imported and used for batch
       prediction.
     target_field_name: The target field's name. Formatted to be able to find
       nested columns, delimited by ``.``. Prefixed with 'instance.' on the
       component for Vertex Batch Prediction.
     batch_predict_instances_format: The format in which instances are given,
       must be one of the Model's supportedInputStorageFormats. For more details
@@ -162,14 +168,20 @@
       attached to the machine as per ``batch_predict_accelerator_count``. Only
       used if ``batch_predict_machine_type`` is set. For more details about the
       machine spec, see
       https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
     batch_predict_accelerator_count: The number of accelerators to attach to the
       ``batch_predict_machine_type``. Only used if
       ``batch_predict_machine_type`` is set.
+    slicing_specs: List of
+      ``google.cloud.aiplatform_v1.types.ModelEvaluationSlice.SlicingSpec``.
+      When provided, compute metrics for each defined slice. See sample code in
+      https://cloud.google.com/vertex-ai/docs/pipelines/model-evaluation-component
+        For more details on configuring slices, see
+      https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.ModelEvaluationSlice.
     dataflow_machine_type: The Dataflow machine type for evaluation components.
     dataflow_max_num_workers: The max number of Dataflow workers for evaluation
       components.
     dataflow_disk_size_gb: Dataflow worker's disk size in GB for evaluation
       components.
     dataflow_service_account: Custom service account to run Dataflow jobs.
     dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty
@@ -181,15 +193,24 @@
       resources created by this pipeline will be encrypted with the provided
       encryption key. Has the form:
       ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
       The key needs to be in the same region as where the compute resource is
       created.
     force_runner_mode: Indicate the runner mode to use forcely. Valid options
       are ``Dataflow`` and ``DirectRunner``.
+
+  Returns:
+      A google.ClassificationMetrics artifact.
   """
+  outputs = NamedTuple(
+      'outputs',
+      evaluation_metrics=ClassificationMetrics,
+      evaluation_resource_name=str,
+  )
+
   evaluation_display_name = (
       'evaluation-automl-tabular-feature-attribution-pipeline'
   )
   get_model_task = GetVertexModelOp(model_name=model_name)
 
   # Run Batch Prediction.
   batch_predict_task = ModelBatchPredictOp(
@@ -207,207 +228,583 @@
       starting_replica_count=batch_predict_starting_replica_count,
       max_replica_count=batch_predict_max_replica_count,
       encryption_spec_key_name=encryption_spec_key_name,
       accelerator_type=batch_predict_accelerator_type,
       accelerator_count=batch_predict_accelerator_count,
   )
 
-  # Run the Batch Explain process (sampler -> batch explanation).
-  data_sampler_task = EvaluationDataSamplerOp(
+  # Run feature attribution steps.
+  feature_attribution_graph = FeatureAttributionGraphComponentOp(
       project=project,
       location=location,
-      gcs_source_uris=batch_predict_gcs_source_uris,
-      bigquery_source_uri=batch_predict_bigquery_source_uri,
-      instances_format=batch_predict_instances_format,
-      sample_size=batch_predict_explanation_data_sample_size,
+      prediction_type='classification',
+      vertex_model=get_model_task.outputs['model'],
+      batch_predict_instances_format=batch_predict_instances_format,
+      batch_predict_gcs_destination_output_uri=batch_predict_gcs_destination_output_uri,
+      batch_predict_gcs_source_uris=batch_predict_gcs_source_uris,
+      batch_predict_bigquery_source_uri=batch_predict_bigquery_source_uri,
+      batch_predict_predictions_format=batch_predict_predictions_format,
+      batch_predict_bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
+      batch_predict_machine_type=batch_predict_machine_type,
+      batch_predict_starting_replica_count=batch_predict_starting_replica_count,
+      batch_predict_max_replica_count=batch_predict_max_replica_count,
+      batch_predict_explanation_metadata=batch_predict_explanation_metadata,
+      batch_predict_explanation_parameters=batch_predict_explanation_parameters,
+      batch_predict_explanation_data_sample_size=batch_predict_explanation_data_sample_size,
+      batch_predict_accelerator_type=batch_predict_accelerator_type,
+      batch_predict_accelerator_count=batch_predict_accelerator_count,
+      dataflow_machine_type=dataflow_machine_type,
+      dataflow_max_num_workers=dataflow_max_num_workers,
+      dataflow_disk_size_gb=dataflow_disk_size_gb,
+      dataflow_service_account=dataflow_service_account,
+      dataflow_subnetwork=dataflow_subnetwork,
+      dataflow_use_public_ips=dataflow_use_public_ips,
+      encryption_spec_key_name=encryption_spec_key_name,
       force_runner_mode=force_runner_mode,
   )
-  batch_explain_task = ModelBatchPredictOp(
+
+  # Run evaluation for a classification model.
+  eval_task = ModelEvaluationClassificationOp(
       project=project,
       location=location,
-      model=get_model_task.outputs['model'],
-      job_display_name='model-registry-batch-explain-evaluation-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}',
-      gcs_source_uris=data_sampler_task.outputs['gcs_output_directory'],
-      bigquery_source_input_uri=data_sampler_task.outputs[
+      target_field_name=target_field_name,
+      predictions_format=batch_predict_predictions_format,
+      predictions_gcs_source=batch_predict_task.outputs['gcs_output_directory'],
+      predictions_bigquery_source=batch_predict_task.outputs[
           'bigquery_output_table'
       ],
+      dataflow_machine_type=dataflow_machine_type,
+      dataflow_max_workers_num=dataflow_max_num_workers,
+      dataflow_disk_size_gb=dataflow_disk_size_gb,
+      dataflow_service_account=dataflow_service_account,
+      dataflow_subnetwork=dataflow_subnetwork,
+      dataflow_use_public_ips=dataflow_use_public_ips,
+      encryption_spec_key_name=encryption_spec_key_name,
+      force_runner_mode=force_runner_mode,
+      model=get_model_task.outputs['model'],
+      slicing_specs=slicing_specs,
+  )
+
+  # Import the evaluation result to Vertex AI.
+  import_evaluation_task = ModelImportEvaluationOp(
+      classification_metrics=eval_task.outputs['evaluation_metrics'],
+      feature_attributions=feature_attribution_graph.outputs[
+          'feature_attributions'
+      ],
+      model=get_model_task.outputs['model'],
+      dataset_type=batch_predict_instances_format,
+      dataset_path=batch_predict_bigquery_source_uri,
+      dataset_paths=batch_predict_gcs_source_uris,
+      display_name=evaluation_display_name,
+  )
+
+  return outputs(
+      evaluation_metrics=eval_task.outputs['evaluation_metrics'],
+      evaluation_resource_name=import_evaluation_task.outputs[
+          'evaluation_resource_name'
+      ],
+  )
+
+
+@kfp.dsl.pipeline(
+    name='evaluation-automl-tabular-feature-attribution-regression-pipeline'
+)
+def evaluation_automl_tabular_feature_attribution_regression_pipeline(  # pylint: disable=dangerous-default-value
+    project: str,
+    location: str,
+    model_name: str,
+    target_field_name: str,
+    batch_predict_instances_format: str,
+    batch_predict_gcs_destination_output_uri: str,
+    batch_predict_gcs_source_uris: List[str] = [],  # pylint: disable=g-bare-generic
+    batch_predict_bigquery_source_uri: str = '',
+    batch_predict_predictions_format: str = 'jsonl',
+    batch_predict_bigquery_destination_output_uri: str = '',
+    batch_predict_machine_type: str = 'n1-standard-16',
+    batch_predict_starting_replica_count: int = 5,
+    batch_predict_max_replica_count: int = 10,
+    batch_predict_explanation_metadata: Dict[str, Any] = {},  # pylint: disable=g-bare-generic
+    batch_predict_explanation_parameters: Dict[str, Any] = {},  # pylint: disable=g-bare-generic
+    batch_predict_explanation_data_sample_size: int = 10000,
+    batch_predict_accelerator_type: str = '',
+    batch_predict_accelerator_count: int = 0,
+    dataflow_machine_type: str = 'n1-standard-4',
+    dataflow_max_num_workers: int = 5,
+    dataflow_disk_size_gb: int = 50,
+    dataflow_service_account: str = '',
+    dataflow_subnetwork: str = '',
+    dataflow_use_public_ips: bool = True,
+    encryption_spec_key_name: str = '',
+    force_runner_mode: str = '',
+) -> NamedTuple(
+    'outputs',
+    evaluation_metrics=RegressionMetrics,
+    evaluation_resource_name=str,
+):
+  """The evaluation AutoML tabular pipeline with feature attribution for regression models.
+
+  This pipeline guarantees support for AutoML Tabular models that contain a
+  valid explanation_spec. This pipeline does not include the
+  target_field_data_remover component, which is needed for many tabular custom
+  models.
+
+
+  Args:
+    project: The GCP project that runs the pipeline components.
+    location: The GCP region that runs the pipeline components.
+    model_name: The Vertex model resource name to be imported and used for batch
+      prediction.
+    target_field_name: The target field's name. Formatted to be able to find
+      nested columns, delimited by ``.``. Prefixed with 'instance.' on the
+      component for Vertex Batch Prediction.
+    batch_predict_instances_format: The format in which instances are given,
+      must be one of the Model's supportedInputStorageFormats. For more details
+      about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_gcs_destination_output_uri: The Google Cloud Storage location
+      of the directory where the output is to be written to. In the given
+      directory a new directory is created. Its name is
+      ``prediction-<model-display-name>-<job-create-time>``, where timestamp is
+      in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. Inside of it files
+      ``predictions_0001.<extension>``, ``predictions_0002.<extension>``, ...,
+      ``predictions_N.<extension>`` are created where ``<extension>`` depends on
+      chosen ``predictions_format``, and N may equal 0001 and depends on the
+      total number of successfully predicted instances. If the Model has both
+      ``instance`` and ``prediction`` schemata defined then each such file
+      contains predictions as per the ``predictions_format``. If prediction for
+      any instance failed (partially or completely), then an additional
+      ``errors_0001.<extension>``, ``errors_0002.<extension>``,...,
+      ``errors_N.<extension>`` files are created (N depends on total number of
+      failed predictions). These files contain the failed instances, as per
+      their schema, followed by an additional ``error`` field which as value has
+      ``google.rpc.Status`` containing only ``code`` and ``message`` fields. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_gcs_source_uris: Google Cloud Storage URI(-s) to your
+      instances to run batch prediction on. May contain wildcards. For more
+      information on wildcards, see
+      https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames. For
+        more details about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_bigquery_source_uri: Google BigQuery URI to your instances to
+      run batch prediction on. May contain wildcards. For more details about
+      this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_predictions_format: The format in which Vertex AI gives the
+      predictions. Must be one of the Model's supportedOutputStorageFormats. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_bigquery_destination_output_uri: The BigQuery project location
+      where the output is to be written to. In the given project a new dataset
+      is created with name ``prediction_<model-display-name>_<job-create-time>``
+      where is made BigQuery-dataset-name compatible (for example, most special
+      characters become underscores), and timestamp is in
+      YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset two
+      tables will be created, ``predictions``, and ``errors``. If the Model has
+      both ``instance`` and ``prediction`` schemata defined then the tables have
+      columns as follows: The ``predictions`` table contains instances for which
+      the prediction succeeded, it has columns as per a concatenation of the
+      Model's instance and prediction schemata. The ``errors`` table contains
+      rows for which the prediction has failed, it has instance columns, as per
+      the instance schema, followed by a single "errors" column, which as values
+      has ````google.rpc.Status`` <Status>``__ represented as a STRUCT, and
+      containing only ``code`` and ``message``.  For more details about this
+      output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_machine_type: The type of machine for running batch prediction
+      on dedicated resources. If the Model supports DEDICATED_RESOURCES this
+      config may be provided (and the job will use these resources). If the
+      Model doesn't support AUTOMATIC_RESOURCES, this config must be provided.
+      For more details about the BatchDedicatedResources, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchDedicatedResources.
+        For more details about the machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_starting_replica_count: The number of machine replicas used at
+      the start of the batch operation. If not set, Vertex AI decides starting
+      number, not greater than ``max_replica_count``. Only used if
+      ``machine_type`` is set.
+    batch_predict_max_replica_count: The maximum number of machine replicas the
+      batch operation may be scaled to. Only used if ``machine_type`` is set.
+    batch_predict_explanation_metadata: Explanation metadata configuration for
+      this BatchPredictionJob. Can be specified only if ``generate_explanation``
+      is set to ``True``. This value overrides the value of
+      ``Model.explanation_metadata``. All fields of ``explanation_metadata`` are
+      optional in the request. If a field of the ``explanation_metadata`` object
+      is not populated, the corresponding field of the
+      ``Model.explanation_metadata`` object is inherited. For more details, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata.
+    batch_predict_explanation_parameters: Parameters to configure explaining for
+      Model's predictions. Can be specified only if ``generate_explanation`` is
+      set to ``True``. This value overrides the value of
+      ``Model.explanation_parameters``. All fields of ``explanation_parameters``
+      are optional in the request. If a field of the ``explanation_parameters``
+      object is not populated, the corresponding field of the
+      ``Model.explanation_parameters`` object is inherited. For more details,
+      see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#ExplanationParameters.
+    batch_predict_explanation_data_sample_size: Desired size to downsample the
+      input dataset that will then be used for batch explanation.
+    batch_predict_accelerator_type: The type of accelerator(s) that may be
+      attached to the machine as per ``batch_predict_accelerator_count``. Only
+      used if ``batch_predict_machine_type`` is set. For more details about the
+      machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_accelerator_count: The number of accelerators to attach to the
+      ``batch_predict_machine_type``. Only used if
+      ``batch_predict_machine_type`` is set.
+    dataflow_machine_type: The Dataflow machine type for evaluation components.
+    dataflow_max_num_workers: The max number of Dataflow workers for evaluation
+      components.
+    dataflow_disk_size_gb: Dataflow worker's disk size in GB for evaluation
+      components.
+    dataflow_service_account: Custom service account to run Dataflow jobs.
+    dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty
+      the default subnetwork will be used. Example:
+      https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
+    dataflow_use_public_ips: Specifies whether Dataflow workers use public IP
+      addresses.
+    encryption_spec_key_name:  Customer-managed encryption key options. If set,
+      resources created by this pipeline will be encrypted with the provided
+      encryption key. Has the form:
+      ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
+      The key needs to be in the same region as where the compute resource is
+      created.
+    force_runner_mode: Indicate the runner mode to use forcely. Valid options
+      are ``Dataflow`` and ``DirectRunner``.
+
+  Returns:
+      A google.RegressionMetrics artifact.
+  """
+  outputs = NamedTuple(
+      'outputs',
+      evaluation_metrics=RegressionMetrics,
+      evaluation_resource_name=str,
+  )
+
+  evaluation_display_name = (
+      'evaluation-automl-tabular-feature-attribution-pipeline'
+  )
+  get_model_task = GetVertexModelOp(model_name=model_name)
+
+  # Run Batch Prediction.
+  batch_predict_task = ModelBatchPredictOp(
+      project=project,
+      location=location,
+      model=get_model_task.outputs['model'],
+      job_display_name='model-registry-batch-predict-evaluation-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}',
+      gcs_source_uris=batch_predict_gcs_source_uris,
+      bigquery_source_input_uri=batch_predict_bigquery_source_uri,
       instances_format=batch_predict_instances_format,
       predictions_format=batch_predict_predictions_format,
       gcs_destination_output_uri_prefix=batch_predict_gcs_destination_output_uri,
       bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
-      generate_explanation=True,
-      explanation_parameters=batch_predict_explanation_parameters,
-      explanation_metadata=batch_predict_explanation_metadata,
       machine_type=batch_predict_machine_type,
       starting_replica_count=batch_predict_starting_replica_count,
       max_replica_count=batch_predict_max_replica_count,
       encryption_spec_key_name=encryption_spec_key_name,
       accelerator_type=batch_predict_accelerator_type,
       accelerator_count=batch_predict_accelerator_count,
   )
 
-  # Run evaluation based on prediction type and feature attribution component.
-  # After, import the model evaluations to the Vertex model.
+  # Run feature attribution steps.
+  feature_attribution_graph = FeatureAttributionGraphComponentOp(
+      project=project,
+      location=location,
+      prediction_type='regression',
+      vertex_model=get_model_task.outputs['model'],
+      batch_predict_instances_format=batch_predict_instances_format,
+      batch_predict_gcs_destination_output_uri=batch_predict_gcs_destination_output_uri,
+      batch_predict_gcs_source_uris=batch_predict_gcs_source_uris,
+      batch_predict_bigquery_source_uri=batch_predict_bigquery_source_uri,
+      batch_predict_predictions_format=batch_predict_predictions_format,
+      batch_predict_bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
+      batch_predict_machine_type=batch_predict_machine_type,
+      batch_predict_starting_replica_count=batch_predict_starting_replica_count,
+      batch_predict_max_replica_count=batch_predict_max_replica_count,
+      batch_predict_explanation_metadata=batch_predict_explanation_metadata,
+      batch_predict_explanation_parameters=batch_predict_explanation_parameters,
+      batch_predict_explanation_data_sample_size=batch_predict_explanation_data_sample_size,
+      batch_predict_accelerator_type=batch_predict_accelerator_type,
+      batch_predict_accelerator_count=batch_predict_accelerator_count,
+      dataflow_machine_type=dataflow_machine_type,
+      dataflow_max_num_workers=dataflow_max_num_workers,
+      dataflow_disk_size_gb=dataflow_disk_size_gb,
+      dataflow_service_account=dataflow_service_account,
+      dataflow_subnetwork=dataflow_subnetwork,
+      dataflow_use_public_ips=dataflow_use_public_ips,
+      encryption_spec_key_name=encryption_spec_key_name,
+      force_runner_mode=force_runner_mode,
+  )
+
+  # Run evaluation for a regression model.
+  eval_task = ModelEvaluationRegressionOp(
+      project=project,
+      location=location,
+      target_field_name=target_field_name,
+      predictions_format=batch_predict_predictions_format,
+      predictions_gcs_source=batch_predict_task.outputs['gcs_output_directory'],
+      predictions_bigquery_source=batch_predict_task.outputs[
+          'bigquery_output_table'
+      ],
+      dataflow_machine_type=dataflow_machine_type,
+      dataflow_max_workers_num=dataflow_max_num_workers,
+      dataflow_disk_size_gb=dataflow_disk_size_gb,
+      dataflow_service_account=dataflow_service_account,
+      dataflow_subnetwork=dataflow_subnetwork,
+      dataflow_use_public_ips=dataflow_use_public_ips,
+      encryption_spec_key_name=encryption_spec_key_name,
+      force_runner_mode=force_runner_mode,
+      model=get_model_task.outputs['model'],
+  )
+
+  # Import the evaluation result to Vertex AI.
+  import_evaluation_task = ModelImportEvaluationOp(
+      regression_metrics=eval_task.outputs['evaluation_metrics'],
+      feature_attributions=feature_attribution_graph.outputs[
+          'feature_attributions'
+      ],
+      model=get_model_task.outputs['model'],
+      dataset_type=batch_predict_instances_format,
+      dataset_path=batch_predict_bigquery_source_uri,
+      dataset_paths=batch_predict_gcs_source_uris,
+      display_name=evaluation_display_name,
+  )
+
+  return outputs(
+      evaluation_metrics=eval_task.outputs['evaluation_metrics'],
+      evaluation_resource_name=import_evaluation_task.outputs[
+          'evaluation_resource_name'
+      ],
+  )
+
+
+@kfp.dsl.pipeline(name='evaluation-automl-tabular-feature-attribution-pipeline')
+def evaluation_automl_tabular_feature_attribution_pipeline(  # pylint: disable=dangerous-default-value
+    project: str,
+    location: str,
+    prediction_type: str,
+    model_name: str,
+    target_field_name: str,
+    batch_predict_instances_format: str,
+    batch_predict_gcs_destination_output_uri: str,
+    batch_predict_gcs_source_uris: List[str] = [],  # pylint: disable=g-bare-generic
+    batch_predict_bigquery_source_uri: str = '',
+    batch_predict_predictions_format: str = 'jsonl',
+    batch_predict_bigquery_destination_output_uri: str = '',
+    batch_predict_machine_type: str = 'n1-standard-16',
+    batch_predict_starting_replica_count: int = 5,
+    batch_predict_max_replica_count: int = 10,
+    batch_predict_explanation_metadata: Dict[str, Any] = {},  # pylint: disable=g-bare-generic
+    batch_predict_explanation_parameters: Dict[str, Any] = {},  # pylint: disable=g-bare-generic
+    batch_predict_explanation_data_sample_size: int = 10000,
+    batch_predict_accelerator_type: str = '',
+    batch_predict_accelerator_count: int = 0,
+    slicing_specs: List[Any] = [],  # pylint: disable=g-bare-generic
+    dataflow_machine_type: str = 'n1-standard-4',
+    dataflow_max_num_workers: int = 5,
+    dataflow_disk_size_gb: int = 50,
+    dataflow_service_account: str = '',
+    dataflow_subnetwork: str = '',
+    dataflow_use_public_ips: bool = True,
+    encryption_spec_key_name: str = '',
+    force_runner_mode: str = '',
+):
+  """The evaluation AutoML tabular pipeline with feature attribution.
+
+  This pipeline guarantees support for AutoML Tabular classification and
+  regression models that contain a valid explanation_spec. This pipeline does
+  not include the target_field_data_remover component, which is needed for many
+  tabular custom models.
+
+  Args:
+    project: The GCP project that runs the pipeline components.
+    location: The GCP region that runs the pipeline components.
+    prediction_type: The type of prediction the model is to produce.
+      "classification" or "regression".
+    model_name: The Vertex model resource name to be imported and used for batch
+      prediction.
+    target_field_name: The target field's name. Formatted to be able to find
+      nested columns, delimited by ``.``. Prefixed with 'instance.' on the
+      component for Vertex Batch Prediction.
+    batch_predict_instances_format: The format in which instances are given,
+      must be one of the Model's supportedInputStorageFormats. For more details
+      about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_gcs_destination_output_uri: The Google Cloud Storage location
+      of the directory where the output is to be written to. In the given
+      directory a new directory is created. Its name is
+      ``prediction-<model-display-name>-<job-create-time>``, where timestamp is
+      in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. Inside of it files
+      ``predictions_0001.<extension>``, ``predictions_0002.<extension>``, ...,
+      ``predictions_N.<extension>`` are created where ``<extension>`` depends on
+      chosen ``predictions_format``, and N may equal 0001 and depends on the
+      total number of successfully predicted instances. If the Model has both
+      ``instance`` and ``prediction`` schemata defined then each such file
+      contains predictions as per the ``predictions_format``. If prediction for
+      any instance failed (partially or completely), then an additional
+      ``errors_0001.<extension>``, ``errors_0002.<extension>``,...,
+      ``errors_N.<extension>`` files are created (N depends on total number of
+      failed predictions). These files contain the failed instances, as per
+      their schema, followed by an additional ``error`` field which as value has
+      ``google.rpc.Status`` containing only ``code`` and ``message`` fields. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_gcs_source_uris: Google Cloud Storage URI(-s) to your
+      instances to run batch prediction on. May contain wildcards. For more
+      information on wildcards, see
+      https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames. For
+        more details about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_bigquery_source_uri: Google BigQuery URI to your instances to
+      run batch prediction on. May contain wildcards. For more details about
+      this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_predictions_format: The format in which Vertex AI gives the
+      predictions. Must be one of the Model's supportedOutputStorageFormats. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_bigquery_destination_output_uri: The BigQuery project location
+      where the output is to be written to. In the given project a new dataset
+      is created with name ``prediction_<model-display-name>_<job-create-time>``
+      where is made BigQuery-dataset-name compatible (for example, most special
+      characters become underscores), and timestamp is in
+      YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset two
+      tables will be created, ``predictions``, and ``errors``. If the Model has
+      both ``instance`` and ``prediction`` schemata defined then the tables have
+      columns as follows: The ``predictions`` table contains instances for which
+      the prediction succeeded, it has columns as per a concatenation of the
+      Model's instance and prediction schemata. The ``errors`` table contains
+      rows for which the prediction has failed, it has instance columns, as per
+      the instance schema, followed by a single "errors" column, which as values
+      has ````google.rpc.Status`` <Status>``__ represented as a STRUCT, and
+      containing only ``code`` and ``message``.  For more details about this
+      output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_machine_type: The type of machine for running batch prediction
+      on dedicated resources. If the Model supports DEDICATED_RESOURCES this
+      config may be provided (and the job will use these resources). If the
+      Model doesn't support AUTOMATIC_RESOURCES, this config must be provided.
+      For more details about the BatchDedicatedResources, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchDedicatedResources.
+        For more details about the machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_starting_replica_count: The number of machine replicas used at
+      the start of the batch operation. If not set, Vertex AI decides starting
+      number, not greater than ``max_replica_count``. Only used if
+      ``machine_type`` is set.
+    batch_predict_max_replica_count: The maximum number of machine replicas the
+      batch operation may be scaled to. Only used if ``machine_type`` is set.
+    batch_predict_explanation_metadata: Explanation metadata configuration for
+      this BatchPredictionJob. Can be specified only if ``generate_explanation``
+      is set to ``True``. This value overrides the value of
+      ``Model.explanation_metadata``. All fields of ``explanation_metadata`` are
+      optional in the request. If a field of the ``explanation_metadata`` object
+      is not populated, the corresponding field of the
+      ``Model.explanation_metadata`` object is inherited. For more details, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata.
+    batch_predict_explanation_parameters: Parameters to configure explaining for
+      Model's predictions. Can be specified only if ``generate_explanation`` is
+      set to ``True``. This value overrides the value of
+      ``Model.explanation_parameters``. All fields of ``explanation_parameters``
+      are optional in the request. If a field of the ``explanation_parameters``
+      object is not populated, the corresponding field of the
+      ``Model.explanation_parameters`` object is inherited. For more details,
+      see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#ExplanationParameters.
+    batch_predict_explanation_data_sample_size: Desired size to downsample the
+      input dataset that will then be used for batch explanation.
+    batch_predict_accelerator_type: The type of accelerator(s) that may be
+      attached to the machine as per ``batch_predict_accelerator_count``. Only
+      used if ``batch_predict_machine_type`` is set. For more details about the
+      machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_accelerator_count: The number of accelerators to attach to the
+      ``batch_predict_machine_type``. Only used if
+      ``batch_predict_machine_type`` is set.
+    slicing_specs: List of
+      ``google.cloud.aiplatform_v1.types.ModelEvaluationSlice.SlicingSpec``.
+      When provided, compute metrics for each defined slice. See sample code in
+      https://cloud.google.com/vertex-ai/docs/pipelines/model-evaluation-component
+        For more details on configuring slices, see
+      https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.ModelEvaluationSlice.
+    dataflow_machine_type: The Dataflow machine type for evaluation components.
+    dataflow_max_num_workers: The max number of Dataflow workers for evaluation
+      components.
+    dataflow_disk_size_gb: Dataflow worker's disk size in GB for evaluation
+      components.
+    dataflow_service_account: Custom service account to run Dataflow jobs.
+    dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty
+      the default subnetwork will be used. Example:
+      https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
+    dataflow_use_public_ips: Specifies whether Dataflow workers use public IP
+      addresses.
+    encryption_spec_key_name:  Customer-managed encryption key options. If set,
+      resources created by this pipeline will be encrypted with the provided
+      encryption key. Has the form:
+      ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
+      The key needs to be in the same region as where the compute resource is
+      created.
+    force_runner_mode: Indicate the runner mode to use forcely. Valid options
+      are ``Dataflow`` and ``DirectRunner``.
+  """
   with kfp.dsl.Condition(
       prediction_type == 'classification', name='classification'
   ):
-    eval_task = ModelEvaluationClassificationOp(
+    evaluation_automl_tabular_feature_attribution_classification_pipeline(
         project=project,
         location=location,
+        model_name=model_name,
         target_field_name=target_field_name,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_predict_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_predict_task.outputs[
-            'bigquery_output_table'
-        ],
-        dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
-        dataflow_disk_size_gb=dataflow_disk_size_gb,
-        dataflow_service_account=dataflow_service_account,
-        dataflow_subnetwork=dataflow_subnetwork,
-        dataflow_use_public_ips=dataflow_use_public_ips,
-        encryption_spec_key_name=encryption_spec_key_name,
-        force_runner_mode=force_runner_mode,
-        model=get_model_task.outputs['model'],
+        batch_predict_instances_format=batch_predict_instances_format,
+        batch_predict_gcs_destination_output_uri=batch_predict_gcs_destination_output_uri,
+        batch_predict_gcs_source_uris=batch_predict_gcs_source_uris,
+        batch_predict_bigquery_source_uri=batch_predict_bigquery_source_uri,
+        batch_predict_predictions_format=batch_predict_predictions_format,
+        batch_predict_bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
+        batch_predict_machine_type=batch_predict_machine_type,
+        batch_predict_starting_replica_count=batch_predict_starting_replica_count,
+        batch_predict_max_replica_count=batch_predict_max_replica_count,
+        batch_predict_explanation_metadata=batch_predict_explanation_metadata,
+        batch_predict_explanation_parameters=batch_predict_explanation_parameters,
+        batch_predict_explanation_data_sample_size=batch_predict_explanation_data_sample_size,
+        batch_predict_accelerator_type=batch_predict_accelerator_type,
+        batch_predict_accelerator_count=batch_predict_accelerator_count,
         slicing_specs=slicing_specs,
-    )
-    feature_attribution_task = ModelEvaluationFeatureAttributionOp(
-        project=project,
-        location=location,
-        problem_type=prediction_type,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_explain_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_explain_task.outputs[
-            'bigquery_output_table'
-        ],
-        dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
-        dataflow_disk_size_gb=dataflow_disk_size_gb,
-        dataflow_service_account=dataflow_service_account,
-        dataflow_subnetwork=dataflow_subnetwork,
-        dataflow_use_public_ips=dataflow_use_public_ips,
-        encryption_spec_key_name=encryption_spec_key_name,
-        force_runner_mode=force_runner_mode,
-    )
-    ModelImportEvaluationOp(
-        classification_metrics=eval_task.outputs['evaluation_metrics'],
-        feature_attributions=feature_attribution_task.outputs[
-            'feature_attributions'
-        ],
-        model=get_model_task.outputs['model'],
-        dataset_type=batch_predict_instances_format,
-        dataset_path=batch_predict_bigquery_source_uri,
-        dataset_paths=batch_predict_gcs_source_uris,
-        display_name=evaluation_display_name,
-    )
-
-  with kfp.dsl.Condition(prediction_type == 'forecasting', name='forecasting'):
-    eval_task = ModelEvaluationForecastingOp(
-        project=project,
-        location=location,
-        target_field_name=target_field_name,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_predict_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_predict_task.outputs[
-            'bigquery_output_table'
-        ],
-        dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
-        dataflow_disk_size_gb=dataflow_disk_size_gb,
-        dataflow_service_account=dataflow_service_account,
-        dataflow_subnetwork=dataflow_subnetwork,
-        dataflow_use_public_ips=dataflow_use_public_ips,
-        encryption_spec_key_name=encryption_spec_key_name,
-        force_runner_mode=force_runner_mode,
-        model=get_model_task.outputs['model'],
-    )
-    feature_attribution_task = ModelEvaluationFeatureAttributionOp(
-        project=project,
-        location=location,
-        problem_type=prediction_type,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_explain_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_explain_task.outputs[
-            'bigquery_output_table'
-        ],
         dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
+        dataflow_max_num_workers=dataflow_max_num_workers,
         dataflow_disk_size_gb=dataflow_disk_size_gb,
         dataflow_service_account=dataflow_service_account,
         dataflow_subnetwork=dataflow_subnetwork,
         dataflow_use_public_ips=dataflow_use_public_ips,
         encryption_spec_key_name=encryption_spec_key_name,
         force_runner_mode=force_runner_mode,
     )
-    ModelImportEvaluationOp(
-        forecasting_metrics=eval_task.outputs['evaluation_metrics'],
-        feature_attributions=feature_attribution_task.outputs[
-            'feature_attributions'
-        ],
-        model=get_model_task.outputs['model'],
-        dataset_type=batch_predict_instances_format,
-        dataset_path=batch_predict_bigquery_source_uri,
-        dataset_paths=batch_predict_gcs_source_uris,
-        display_name=evaluation_display_name,
-    )
 
   with kfp.dsl.Condition(prediction_type == 'regression', name='regression'):
-    eval_task = ModelEvaluationRegressionOp(
+    evaluation_automl_tabular_feature_attribution_regression_pipeline(
         project=project,
         location=location,
+        model_name=model_name,
         target_field_name=target_field_name,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_predict_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_predict_task.outputs[
-            'bigquery_output_table'
-        ],
+        batch_predict_instances_format=batch_predict_instances_format,
+        batch_predict_gcs_destination_output_uri=batch_predict_gcs_destination_output_uri,
+        batch_predict_gcs_source_uris=batch_predict_gcs_source_uris,
+        batch_predict_bigquery_source_uri=batch_predict_bigquery_source_uri,
+        batch_predict_predictions_format=batch_predict_predictions_format,
+        batch_predict_bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
+        batch_predict_machine_type=batch_predict_machine_type,
+        batch_predict_starting_replica_count=batch_predict_starting_replica_count,
+        batch_predict_max_replica_count=batch_predict_max_replica_count,
+        batch_predict_explanation_metadata=batch_predict_explanation_metadata,
+        batch_predict_explanation_parameters=batch_predict_explanation_parameters,
+        batch_predict_explanation_data_sample_size=batch_predict_explanation_data_sample_size,
+        batch_predict_accelerator_type=batch_predict_accelerator_type,
+        batch_predict_accelerator_count=batch_predict_accelerator_count,
         dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
+        dataflow_max_num_workers=dataflow_max_num_workers,
         dataflow_disk_size_gb=dataflow_disk_size_gb,
         dataflow_service_account=dataflow_service_account,
         dataflow_subnetwork=dataflow_subnetwork,
         dataflow_use_public_ips=dataflow_use_public_ips,
         encryption_spec_key_name=encryption_spec_key_name,
         force_runner_mode=force_runner_mode,
-        model=get_model_task.outputs['model'],
-    )
-    feature_attribution_task = ModelEvaluationFeatureAttributionOp(
-        project=project,
-        location=location,
-        problem_type=prediction_type,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_explain_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_explain_task.outputs[
-            'bigquery_output_table'
-        ],
-        dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
-        dataflow_disk_size_gb=dataflow_disk_size_gb,
-        dataflow_service_account=dataflow_service_account,
-        dataflow_subnetwork=dataflow_subnetwork,
-        dataflow_use_public_ips=dataflow_use_public_ips,
-        encryption_spec_key_name=encryption_spec_key_name,
-        force_runner_mode=force_runner_mode,
-    )
-    ModelImportEvaluationOp(
-        regression_metrics=eval_task.outputs['evaluation_metrics'],
-        feature_attributions=feature_attribution_task.outputs[
-            'feature_attributions'
-        ],
-        model=get_model_task.outputs['model'],
-        dataset_type=batch_predict_instances_format,
-        dataset_path=batch_predict_bigquery_source_uri,
-        dataset_paths=batch_predict_gcs_source_uris,
-        display_name=evaluation_display_name,
     )
```

## google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_pipeline.py

```diff
@@ -8,63 +8,67 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import NamedTuple
+from typing import Any, List, NamedTuple
+
 from google_cloud_pipeline_components._implementation.model import GetVertexModelOp
 from google_cloud_pipeline_components._implementation.model_evaluation import ModelImportEvaluationOp
+from google_cloud_pipeline_components.types.artifact_types import ClassificationMetrics
+from google_cloud_pipeline_components.types.artifact_types import RegressionMetrics
 from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp
 from google_cloud_pipeline_components.v1.model_evaluation.classification_component import model_evaluation_classification as ModelEvaluationClassificationOp
 from google_cloud_pipeline_components.v1.model_evaluation.forecasting_component import model_evaluation_forecasting as ModelEvaluationForecastingOp
 from google_cloud_pipeline_components.v1.model_evaluation.regression_component import model_evaluation_regression as ModelEvaluationRegressionOp
 import kfp
 
 
-@kfp.dsl.pipeline(name='evaluation-automl-tabular-pipeline')
-def evaluation_automl_tabular_pipeline(  # pylint: disable=dangerous-default-value
+@kfp.dsl.pipeline(name='evaluation-automl-tabular-classification-pipeline')
+def evaluation_automl_tabular_classification_pipeline(  # pylint: disable=dangerous-default-value
     project: str,
     location: str,
-    prediction_type: str,
     model_name: str,
     target_field_name: str,
     batch_predict_instances_format: str,
     batch_predict_gcs_destination_output_uri: str,
-    batch_predict_gcs_source_uris: list = [],  # pylint: disable=g-bare-generic
+    batch_predict_gcs_source_uris: List[str] = [],  # pylint: disable=g-bare-generic
     batch_predict_bigquery_source_uri: str = '',
     batch_predict_predictions_format: str = 'jsonl',
     batch_predict_bigquery_destination_output_uri: str = '',
     batch_predict_machine_type: str = 'n1-standard-16',
     batch_predict_starting_replica_count: int = 5,
     batch_predict_max_replica_count: int = 10,
     batch_predict_accelerator_type: str = '',
     batch_predict_accelerator_count: int = 0,
-    slicing_specs: list = [],  # pylint: disable=g-bare-generic
+    slicing_specs: List[Any] = [],  # pylint: disable=g-bare-generic
     dataflow_machine_type: str = 'n1-standard-4',
     dataflow_max_num_workers: int = 5,
     dataflow_disk_size_gb: int = 50,
     dataflow_service_account: str = '',
     dataflow_subnetwork: str = '',
     dataflow_use_public_ips: bool = True,
     encryption_spec_key_name: str = '',
     force_runner_mode: str = '',
+) -> NamedTuple(
+    'outputs',
+    evaluation_metrics=ClassificationMetrics,
+    evaluation_resource_name=str,
 ):
-  """The evaluation AutoML tabular pipeline with no feature attribution.
+  """The evaluation AutoML tabular pipeline with no feature attribution for classification models.
 
   This pipeline guarantees support for AutoML Tabular models. This pipeline does
   not include the target_field_data_remover component, which is needed for many
   tabular custom models.
 
   Args:
     project: The GCP project that runs the pipeline components.
     location: The GCP region that runs the pipeline components.
-    prediction_type: The type of prediction the model is to produce.
-      "classification", "regression", or "forecasting".
     model_name: The Vertex model resource name to be imported and used for batch
       prediction.
     target_field_name: The target field's name. Formatted to be able to find
       nested columns, delimited by ``.``. Prefixed with 'instance.' on the
       component for Vertex Batch Prediction.
     batch_predict_instances_format: The format in which instances are given,
       must be one of the Model's supportedInputStorageFormats. For more details
@@ -138,14 +142,20 @@
       attached to the machine as per ``batch_predict_accelerator_count``. Only
       used if ``batch_predict_machine_type`` is set. For more details about the
       machine spec, see
       https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
     batch_predict_accelerator_count: The number of accelerators to attach to the
       ``batch_predict_machine_type``. Only used if
       ``batch_predict_machine_type`` is set.
+    slicing_specs: List of
+      ``google.cloud.aiplatform_v1.types.ModelEvaluationSlice.SlicingSpec``.
+      When provided, compute metrics for each defined slice. See sample code in
+      https://cloud.google.com/vertex-ai/docs/pipelines/model-evaluation-component
+        For more details on configuring slices, see
+      https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.ModelEvaluationSlice.
     dataflow_machine_type: The Dataflow machine type for evaluation components.
     dataflow_max_num_workers: The max number of Dataflow workers for evaluation
       components.
     dataflow_disk_size_gb: Dataflow worker's disk size in GB for evaluation
       components.
     dataflow_service_account: Custom service account to run Dataflow jobs.
     dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty
@@ -157,19 +167,31 @@
       resources created by this pipeline will be encrypted with the provided
       encryption key. Has the form:
       ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
       The key needs to be in the same region as where the compute resource is
       created.
     force_runner_mode: Indicate the runner mode to use forcely. Valid options
       are ``Dataflow`` and ``DirectRunner``.
+
+  Returns:
+      A google.ClassificationMetrics artifact and imported
+      evaluation_resource_name.
   """
+  outputs = NamedTuple(
+      'outputs',
+      evaluation_metrics=ClassificationMetrics,
+      evaluation_resource_name=str,
+  )
+
   evaluation_display_name = 'evaluation-automl-tabular-pipeline'
+
+  # Get the Vertex AI Model.
   get_model_task = GetVertexModelOp(model_name=model_name)
 
-  # Run Batch Prediction.
+  # Run Vertex AI Batch Prediction.
   batch_predict_task = ModelBatchPredictOp(
       project=project,
       location=location,
       model=get_model_task.outputs['model'],
       job_display_name='evaluation-batch-predict-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}',
       gcs_source_uris=batch_predict_gcs_source_uris,
       bigquery_source_input_uri=batch_predict_bigquery_source_uri,
@@ -181,104 +203,464 @@
       starting_replica_count=batch_predict_starting_replica_count,
       max_replica_count=batch_predict_max_replica_count,
       encryption_spec_key_name=encryption_spec_key_name,
       accelerator_type=batch_predict_accelerator_type,
       accelerator_count=batch_predict_accelerator_count,
   )
 
-  # Run evaluation based on prediction type.
-  # After, import the model evaluations to the Vertex model.
+  # Run evaluation for a classification model.
+  eval_task = ModelEvaluationClassificationOp(
+      project=project,
+      location=location,
+      target_field_name=target_field_name,
+      predictions_format=batch_predict_predictions_format,
+      predictions_gcs_source=batch_predict_task.outputs['gcs_output_directory'],
+      predictions_bigquery_source=batch_predict_task.outputs[
+          'bigquery_output_table'
+      ],
+      dataflow_machine_type=dataflow_machine_type,
+      dataflow_max_workers_num=dataflow_max_num_workers,
+      dataflow_disk_size_gb=dataflow_disk_size_gb,
+      dataflow_service_account=dataflow_service_account,
+      dataflow_subnetwork=dataflow_subnetwork,
+      dataflow_use_public_ips=dataflow_use_public_ips,
+      encryption_spec_key_name=encryption_spec_key_name,
+      force_runner_mode=force_runner_mode,
+      model=get_model_task.outputs['model'],
+      slicing_specs=slicing_specs,
+  )
+
+  # Import the evaluation result to Vertex AI.
+  import_evaluation_task = ModelImportEvaluationOp(
+      classification_metrics=eval_task.outputs['evaluation_metrics'],
+      model=get_model_task.outputs['model'],
+      dataset_type=batch_predict_instances_format,
+      dataset_path=batch_predict_bigquery_source_uri,
+      dataset_paths=batch_predict_gcs_source_uris,
+      display_name=evaluation_display_name,
+  )
+
+  return outputs(
+      evaluation_metrics=eval_task.outputs['evaluation_metrics'],
+      evaluation_resource_name=import_evaluation_task.outputs[
+          'evaluation_resource_name'
+      ],
+  )
+
+
+@kfp.dsl.pipeline(name='evaluation-automl-tabular-regression-pipeline')
+def evaluation_automl_tabular_regression_pipeline(  # pylint: disable=dangerous-default-value
+    project: str,
+    location: str,
+    model_name: str,
+    target_field_name: str,
+    batch_predict_instances_format: str,
+    batch_predict_gcs_destination_output_uri: str,
+    batch_predict_gcs_source_uris: List[str] = [],  # pylint: disable=g-bare-generic
+    batch_predict_bigquery_source_uri: str = '',
+    batch_predict_predictions_format: str = 'jsonl',
+    batch_predict_bigquery_destination_output_uri: str = '',
+    batch_predict_machine_type: str = 'n1-standard-16',
+    batch_predict_starting_replica_count: int = 5,
+    batch_predict_max_replica_count: int = 10,
+    batch_predict_accelerator_type: str = '',
+    batch_predict_accelerator_count: int = 0,
+    dataflow_machine_type: str = 'n1-standard-4',
+    dataflow_max_num_workers: int = 5,
+    dataflow_disk_size_gb: int = 50,
+    dataflow_service_account: str = '',
+    dataflow_subnetwork: str = '',
+    dataflow_use_public_ips: bool = True,
+    encryption_spec_key_name: str = '',
+    force_runner_mode: str = '',
+) -> NamedTuple(
+    'outputs',
+    evaluation_metrics=RegressionMetrics,
+    evaluation_resource_name=str,
+):
+  """The evaluation AutoML tabular pipeline with no feature attribution for regression models.
+
+  This pipeline guarantees support for AutoML Tabular models. This pipeline does
+  not include the target_field_data_remover component, which is needed for many
+  tabular custom models.
+
+  Args:
+    project: The GCP project that runs the pipeline components.
+    location: The GCP region that runs the pipeline components.
+    model_name: The Vertex model resource name to be imported and used for batch
+      prediction.
+    target_field_name: The target field's name. Formatted to be able to find
+      nested columns, delimited by ``.``. Prefixed with 'instance.' on the
+      component for Vertex Batch Prediction.
+    batch_predict_instances_format: The format in which instances are given,
+      must be one of the Model's supportedInputStorageFormats. For more details
+      about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_gcs_destination_output_uri: The Google Cloud Storage location
+      of the directory where the output is to be written to. In the given
+      directory a new directory is created. Its name is
+      ``prediction-<model-display-name>-<job-create-time>``, where timestamp is
+      in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. Inside of it files
+      ``predictions_0001.<extension>``, ``predictions_0002.<extension>``, ...,
+      ``predictions_N.<extension>`` are created where ``<extension>`` depends on
+      chosen ``predictions_format``, and N may equal 0001 and depends on the
+      total number of successfully predicted instances. If the Model has both
+      ``instance`` and ``prediction`` schemata defined then each such file
+      contains predictions as per the ``predictions_format``. If prediction for
+      any instance failed (partially or completely), then an additional
+      ``errors_0001.<extension>``, ``errors_0002.<extension>``,...,
+      ``errors_N.<extension>`` files are created (N depends on total number of
+      failed predictions). These files contain the failed instances, as per
+      their schema, followed by an additional ``error`` field which as value has
+      ``google.rpc.Status`` containing only ``code`` and ``message`` fields. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_gcs_source_uris: Google Cloud Storage URI(-s) to your
+      instances to run batch prediction on. May contain wildcards. For more
+      information on wildcards, see
+      https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames. For
+        more details about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_bigquery_source_uri: Google BigQuery URI to your instances to
+      run batch prediction on. May contain wildcards. For more details about
+      this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_predictions_format: The format in which Vertex AI gives the
+      predictions. Must be one of the Model's supportedOutputStorageFormats. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_bigquery_destination_output_uri: The BigQuery project location
+      where the output is to be written to. In the given project a new dataset
+      is created with name ``prediction_<model-display-name>_<job-create-time>``
+      where is made BigQuery-dataset-name compatible (for example, most special
+      characters become underscores), and timestamp is in
+      YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset two
+      tables will be created, ``predictions``, and ``errors``. If the Model has
+      both ``instance`` and ``prediction`` schemata defined then the tables have
+      columns as follows: The ``predictions`` table contains instances for which
+      the prediction succeeded, it has columns as per a concatenation of the
+      Model's instance and prediction schemata. The ``errors`` table contains
+      rows for which the prediction has failed, it has instance columns, as per
+      the instance schema, followed by a single "errors" column, which as values
+      has ````google.rpc.Status`` <Status>``__ represented as a STRUCT, and
+      containing only ``code`` and ``message``.  For more details about this
+      output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_machine_type: The type of machine for running batch prediction
+      on dedicated resources. If the Model supports DEDICATED_RESOURCES this
+      config may be provided (and the job will use these resources). If the
+      Model doesn't support AUTOMATIC_RESOURCES, this config must be provided.
+      For more details about the BatchDedicatedResources, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchDedicatedResources.
+        For more details about the machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_starting_replica_count: The number of machine replicas used at
+      the start of the batch operation. If not set, Vertex AI decides starting
+      number, not greater than ``max_replica_count``. Only used if
+      ``machine_type`` is set.
+    batch_predict_max_replica_count: The maximum number of machine replicas the
+      batch operation may be scaled to. Only used if ``machine_type`` is set.
+    batch_predict_accelerator_type: The type of accelerator(s) that may be
+      attached to the machine as per ``batch_predict_accelerator_count``. Only
+      used if ``batch_predict_machine_type`` is set. For more details about the
+      machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_accelerator_count: The number of accelerators to attach to the
+      ``batch_predict_machine_type``. Only used if
+      ``batch_predict_machine_type`` is set.
+    dataflow_machine_type: The Dataflow machine type for evaluation components.
+    dataflow_max_num_workers: The max number of Dataflow workers for evaluation
+      components.
+    dataflow_disk_size_gb: Dataflow worker's disk size in GB for evaluation
+      components.
+    dataflow_service_account: Custom service account to run Dataflow jobs.
+    dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty
+      the default subnetwork will be used. Example:
+      https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
+    dataflow_use_public_ips: Specifies whether Dataflow workers use public IP
+      addresses.
+    encryption_spec_key_name:  Customer-managed encryption key options. If set,
+      resources created by this pipeline will be encrypted with the provided
+      encryption key. Has the form:
+      ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
+      The key needs to be in the same region as where the compute resource is
+      created.
+    force_runner_mode: Indicate the runner mode to use forcely. Valid options
+      are ``Dataflow`` and ``DirectRunner``.
+
+  Returns:
+      A google.RegressionMetrics artifact and imported
+      evaluation_resource_name.
+  """
+  outputs = NamedTuple(
+      'outputs',
+      evaluation_metrics=RegressionMetrics,
+      evaluation_resource_name=str,
+  )
+
+  evaluation_display_name = 'evaluation-automl-tabular-pipeline'
+
+  # Get the Vertex AI Model.
+  get_model_task = GetVertexModelOp(model_name=model_name)
+
+  # Run Vertex AI Batch Prediction.
+  batch_predict_task = ModelBatchPredictOp(
+      project=project,
+      location=location,
+      model=get_model_task.outputs['model'],
+      job_display_name='evaluation-batch-predict-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}',
+      gcs_source_uris=batch_predict_gcs_source_uris,
+      bigquery_source_input_uri=batch_predict_bigquery_source_uri,
+      instances_format=batch_predict_instances_format,
+      predictions_format=batch_predict_predictions_format,
+      gcs_destination_output_uri_prefix=batch_predict_gcs_destination_output_uri,
+      bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
+      machine_type=batch_predict_machine_type,
+      starting_replica_count=batch_predict_starting_replica_count,
+      max_replica_count=batch_predict_max_replica_count,
+      encryption_spec_key_name=encryption_spec_key_name,
+      accelerator_type=batch_predict_accelerator_type,
+      accelerator_count=batch_predict_accelerator_count,
+  )
+
+  # Run evaluation for a regression model.
+  eval_task = ModelEvaluationRegressionOp(
+      project=project,
+      location=location,
+      target_field_name=target_field_name,
+      predictions_format=batch_predict_predictions_format,
+      predictions_gcs_source=batch_predict_task.outputs['gcs_output_directory'],
+      predictions_bigquery_source=batch_predict_task.outputs[
+          'bigquery_output_table'
+      ],
+      dataflow_machine_type=dataflow_machine_type,
+      dataflow_max_workers_num=dataflow_max_num_workers,
+      dataflow_disk_size_gb=dataflow_disk_size_gb,
+      dataflow_service_account=dataflow_service_account,
+      dataflow_subnetwork=dataflow_subnetwork,
+      dataflow_use_public_ips=dataflow_use_public_ips,
+      encryption_spec_key_name=encryption_spec_key_name,
+      force_runner_mode=force_runner_mode,
+      model=get_model_task.outputs['model'],
+  )
+
+  # Import the evaluation result to Vertex AI.
+  import_evaluation_task = ModelImportEvaluationOp(
+      regression_metrics=eval_task.outputs['evaluation_metrics'],
+      model=get_model_task.outputs['model'],
+      dataset_type=batch_predict_instances_format,
+      dataset_path=batch_predict_bigquery_source_uri,
+      dataset_paths=batch_predict_gcs_source_uris,
+      display_name=evaluation_display_name,
+  )
+
+  return outputs(
+      evaluation_metrics=eval_task.outputs['evaluation_metrics'],
+      evaluation_resource_name=import_evaluation_task.outputs[
+          'evaluation_resource_name'
+      ],
+  )
+
+
+@kfp.dsl.pipeline(name='evaluation-automl-tabular-pipeline')
+def evaluation_automl_tabular_pipeline(  # pylint: disable=dangerous-default-value
+    project: str,
+    location: str,
+    prediction_type: str,
+    model_name: str,
+    target_field_name: str,
+    batch_predict_instances_format: str,
+    batch_predict_gcs_destination_output_uri: str,
+    batch_predict_gcs_source_uris: List[str] = [],  # pylint: disable=g-bare-generic
+    batch_predict_bigquery_source_uri: str = '',
+    batch_predict_predictions_format: str = 'jsonl',
+    batch_predict_bigquery_destination_output_uri: str = '',
+    batch_predict_machine_type: str = 'n1-standard-16',
+    batch_predict_starting_replica_count: int = 5,
+    batch_predict_max_replica_count: int = 10,
+    batch_predict_accelerator_type: str = '',
+    batch_predict_accelerator_count: int = 0,
+    slicing_specs: List[Any] = [],  # pylint: disable=g-bare-generic
+    dataflow_machine_type: str = 'n1-standard-4',
+    dataflow_max_num_workers: int = 5,
+    dataflow_disk_size_gb: int = 50,
+    dataflow_service_account: str = '',
+    dataflow_subnetwork: str = '',
+    dataflow_use_public_ips: bool = True,
+    encryption_spec_key_name: str = '',
+    force_runner_mode: str = '',
+):
+  """The evaluation AutoML tabular pipeline with no feature attribution.
+
+  This pipeline guarantees support for AutoML Tabular classification and
+  regression models. This pipeline does not include the
+  target_field_data_remover component, which is needed for many tabular custom
+  models and AutoML Tabular Forecasting.
+
+  Args:
+    project: The GCP project that runs the pipeline components.
+    location: The GCP region that runs the pipeline components.
+    prediction_type: The type of prediction the model is to produce.
+      "classification" or "regression".
+    model_name: The Vertex model resource name to be imported and used for batch
+      prediction.
+    target_field_name: The target field's name. Formatted to be able to find
+      nested columns, delimited by ``.``. Prefixed with 'instance.' on the
+      component for Vertex Batch Prediction.
+    batch_predict_instances_format: The format in which instances are given,
+      must be one of the Model's supportedInputStorageFormats. For more details
+      about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_gcs_destination_output_uri: The Google Cloud Storage location
+      of the directory where the output is to be written to. In the given
+      directory a new directory is created. Its name is
+      ``prediction-<model-display-name>-<job-create-time>``, where timestamp is
+      in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. Inside of it files
+      ``predictions_0001.<extension>``, ``predictions_0002.<extension>``, ...,
+      ``predictions_N.<extension>`` are created where ``<extension>`` depends on
+      chosen ``predictions_format``, and N may equal 0001 and depends on the
+      total number of successfully predicted instances. If the Model has both
+      ``instance`` and ``prediction`` schemata defined then each such file
+      contains predictions as per the ``predictions_format``. If prediction for
+      any instance failed (partially or completely), then an additional
+      ``errors_0001.<extension>``, ``errors_0002.<extension>``,...,
+      ``errors_N.<extension>`` files are created (N depends on total number of
+      failed predictions). These files contain the failed instances, as per
+      their schema, followed by an additional ``error`` field which as value has
+      ``google.rpc.Status`` containing only ``code`` and ``message`` fields. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_gcs_source_uris: Google Cloud Storage URI(-s) to your
+      instances to run batch prediction on. May contain wildcards. For more
+      information on wildcards, see
+      https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames. For
+        more details about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_bigquery_source_uri: Google BigQuery URI to your instances to
+      run batch prediction on. May contain wildcards. For more details about
+      this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_predictions_format: The format in which Vertex AI gives the
+      predictions. Must be one of the Model's supportedOutputStorageFormats. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_bigquery_destination_output_uri: The BigQuery project location
+      where the output is to be written to. In the given project a new dataset
+      is created with name ``prediction_<model-display-name>_<job-create-time>``
+      where is made BigQuery-dataset-name compatible (for example, most special
+      characters become underscores), and timestamp is in
+      YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset two
+      tables will be created, ``predictions``, and ``errors``. If the Model has
+      both ``instance`` and ``prediction`` schemata defined then the tables have
+      columns as follows: The ``predictions`` table contains instances for which
+      the prediction succeeded, it has columns as per a concatenation of the
+      Model's instance and prediction schemata. The ``errors`` table contains
+      rows for which the prediction has failed, it has instance columns, as per
+      the instance schema, followed by a single "errors" column, which as values
+      has ````google.rpc.Status`` <Status>``__ represented as a STRUCT, and
+      containing only ``code`` and ``message``.  For more details about this
+      output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_machine_type: The type of machine for running batch prediction
+      on dedicated resources. If the Model supports DEDICATED_RESOURCES this
+      config may be provided (and the job will use these resources). If the
+      Model doesn't support AUTOMATIC_RESOURCES, this config must be provided.
+      For more details about the BatchDedicatedResources, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchDedicatedResources.
+        For more details about the machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_starting_replica_count: The number of machine replicas used at
+      the start of the batch operation. If not set, Vertex AI decides starting
+      number, not greater than ``max_replica_count``. Only used if
+      ``machine_type`` is set.
+    batch_predict_max_replica_count: The maximum number of machine replicas the
+      batch operation may be scaled to. Only used if ``machine_type`` is set.
+    batch_predict_accelerator_type: The type of accelerator(s) that may be
+      attached to the machine as per ``batch_predict_accelerator_count``. Only
+      used if ``batch_predict_machine_type`` is set. For more details about the
+      machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_accelerator_count: The number of accelerators to attach to the
+      ``batch_predict_machine_type``. Only used if
+      ``batch_predict_machine_type`` is set.
+    slicing_specs: List of
+      ``google.cloud.aiplatform_v1.types.ModelEvaluationSlice.SlicingSpec``.
+      When provided, compute metrics for each defined slice. See sample code in
+      https://cloud.google.com/vertex-ai/docs/pipelines/model-evaluation-component
+        For more details on configuring slices, see
+      https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.ModelEvaluationSlice.
+    dataflow_machine_type: The Dataflow machine type for evaluation components.
+    dataflow_max_num_workers: The max number of Dataflow workers for evaluation
+      components.
+    dataflow_disk_size_gb: Dataflow worker's disk size in GB for evaluation
+      components.
+    dataflow_service_account: Custom service account to run Dataflow jobs.
+    dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty
+      the default subnetwork will be used. Example:
+      https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
+    dataflow_use_public_ips: Specifies whether Dataflow workers use public IP
+      addresses.
+    encryption_spec_key_name:  Customer-managed encryption key options. If set,
+      resources created by this pipeline will be encrypted with the provided
+      encryption key. Has the form:
+      ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
+      The key needs to be in the same region as where the compute resource is
+      created.
+    force_runner_mode: Indicate the runner mode to use forcely. Valid options
+      are ``Dataflow`` and ``DirectRunner``.
+  """
   with kfp.dsl.Condition(
       prediction_type == 'classification', name='classification'
   ):
-    eval_task = ModelEvaluationClassificationOp(
+    evaluation_automl_tabular_classification_pipeline(
         project=project,
         location=location,
+        model_name=model_name,
         target_field_name=target_field_name,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_predict_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_predict_task.outputs[
-            'bigquery_output_table'
-        ],
-        dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
-        dataflow_disk_size_gb=dataflow_disk_size_gb,
-        dataflow_service_account=dataflow_service_account,
-        dataflow_subnetwork=dataflow_subnetwork,
-        dataflow_use_public_ips=dataflow_use_public_ips,
-        encryption_spec_key_name=encryption_spec_key_name,
-        force_runner_mode=force_runner_mode,
-        model=get_model_task.outputs['model'],
+        batch_predict_instances_format=batch_predict_instances_format,
+        batch_predict_gcs_destination_output_uri=batch_predict_gcs_destination_output_uri,
+        batch_predict_gcs_source_uris=batch_predict_gcs_source_uris,
+        batch_predict_bigquery_source_uri=batch_predict_bigquery_source_uri,
+        batch_predict_predictions_format=batch_predict_predictions_format,
+        batch_predict_bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
+        batch_predict_machine_type=batch_predict_machine_type,
+        batch_predict_starting_replica_count=batch_predict_starting_replica_count,
+        batch_predict_max_replica_count=batch_predict_max_replica_count,
+        batch_predict_accelerator_type=batch_predict_accelerator_type,
+        batch_predict_accelerator_count=batch_predict_accelerator_count,
         slicing_specs=slicing_specs,
-    )
-    ModelImportEvaluationOp(
-        classification_metrics=eval_task.outputs['evaluation_metrics'],
-        model=get_model_task.outputs['model'],
-        dataset_type=batch_predict_instances_format,
-        dataset_path=batch_predict_bigquery_source_uri,
-        dataset_paths=batch_predict_gcs_source_uris,
-        display_name=evaluation_display_name,
-    )
-
-  with kfp.dsl.Condition(prediction_type == 'forecasting', name='forecasting'):
-    eval_task = ModelEvaluationForecastingOp(
-        project=project,
-        location=location,
-        target_field_name=target_field_name,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_predict_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_predict_task.outputs[
-            'bigquery_output_table'
-        ],
         dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
+        dataflow_max_num_workers=dataflow_max_num_workers,
         dataflow_disk_size_gb=dataflow_disk_size_gb,
         dataflow_service_account=dataflow_service_account,
         dataflow_subnetwork=dataflow_subnetwork,
         dataflow_use_public_ips=dataflow_use_public_ips,
         encryption_spec_key_name=encryption_spec_key_name,
         force_runner_mode=force_runner_mode,
-        model=get_model_task.outputs['model'],
-    )
-    ModelImportEvaluationOp(
-        forecasting_metrics=eval_task.outputs['evaluation_metrics'],
-        model=get_model_task.outputs['model'],
-        dataset_type=batch_predict_instances_format,
-        dataset_path=batch_predict_bigquery_source_uri,
-        dataset_paths=batch_predict_gcs_source_uris,
-        display_name=evaluation_display_name,
     )
 
   with kfp.dsl.Condition(prediction_type == 'regression', name='regression'):
-    eval_task = ModelEvaluationRegressionOp(
+    evaluation_automl_tabular_regression_pipeline(
         project=project,
         location=location,
+        model_name=model_name,
         target_field_name=target_field_name,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_predict_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_predict_task.outputs[
-            'bigquery_output_table'
-        ],
+        batch_predict_instances_format=batch_predict_instances_format,
+        batch_predict_gcs_destination_output_uri=batch_predict_gcs_destination_output_uri,
+        batch_predict_gcs_source_uris=batch_predict_gcs_source_uris,
+        batch_predict_bigquery_source_uri=batch_predict_bigquery_source_uri,
+        batch_predict_predictions_format=batch_predict_predictions_format,
+        batch_predict_bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
+        batch_predict_machine_type=batch_predict_machine_type,
+        batch_predict_starting_replica_count=batch_predict_starting_replica_count,
+        batch_predict_max_replica_count=batch_predict_max_replica_count,
+        batch_predict_accelerator_type=batch_predict_accelerator_type,
+        batch_predict_accelerator_count=batch_predict_accelerator_count,
         dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
+        dataflow_max_num_workers=dataflow_max_num_workers,
         dataflow_disk_size_gb=dataflow_disk_size_gb,
         dataflow_service_account=dataflow_service_account,
         dataflow_subnetwork=dataflow_subnetwork,
         dataflow_use_public_ips=dataflow_use_public_ips,
         encryption_spec_key_name=encryption_spec_key_name,
         force_runner_mode=force_runner_mode,
-        model=get_model_task.outputs['model'],
-    )
-    ModelImportEvaluationOp(
-        regression_metrics=eval_task.outputs['evaluation_metrics'],
-        model=get_model_task.outputs['model'],
-        dataset_type=batch_predict_instances_format,
-        dataset_path=batch_predict_bigquery_source_uri,
-        dataset_paths=batch_predict_gcs_source_uris,
-        display_name=evaluation_display_name,
     )
```

## google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_unstructure_data_pipeline.py

```diff
@@ -8,65 +8,69 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import List, NamedTuple
+
 from google_cloud_pipeline_components._implementation.model import GetVertexModelOp
 from google_cloud_pipeline_components._implementation.model_evaluation import ModelImportEvaluationOp
 from google_cloud_pipeline_components._implementation.model_evaluation import TargetFieldDataRemoverOp
+from google_cloud_pipeline_components.types.artifact_types import ClassificationMetrics
+from google_cloud_pipeline_components.types.artifact_types import RegressionMetrics
 from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp
 from google_cloud_pipeline_components.v1.model_evaluation.classification_component import model_evaluation_classification as ModelEvaluationClassificationOp
-from google_cloud_pipeline_components.v1.model_evaluation.forecasting_component import model_evaluation_forecasting as ModelEvaluationForecastingOp
 from google_cloud_pipeline_components.v1.model_evaluation.regression_component import model_evaluation_regression as ModelEvaluationRegressionOp
 import kfp
 from kfp import dsl
 
 
-@kfp.dsl.pipeline(name='evaluation-automl-unstructure-data-pipeline')
-def evaluation_automl_unstructure_data_pipeline(  # pylint: disable=dangerous-default-value
+@kfp.dsl.pipeline(name='evaluation-classification-pipeline')
+def evaluation_automl_unstructure_data_classification_pipeline(  # pylint: disable=dangerous-default-value
     project: str,
     location: str,
-    prediction_type: str,
     model_name: str,
     target_field_name: str,
     batch_predict_instances_format: str,
     batch_predict_gcs_destination_output_uri: str,
-    batch_predict_gcs_source_uris: list = [],  # pylint: disable=g-bare-generic
+    batch_predict_gcs_source_uris: List[str] = [],  # pylint: disable=g-bare-generic
     batch_predict_bigquery_source_uri: str = '',
     batch_predict_predictions_format: str = 'jsonl',
     batch_predict_bigquery_destination_output_uri: str = '',
     batch_predict_machine_type: str = 'n1-standard-16',
     batch_predict_starting_replica_count: int = 5,
     batch_predict_max_replica_count: int = 10,
     batch_predict_accelerator_type: str = '',
     batch_predict_accelerator_count: int = 0,
     evaluation_prediction_label_column: str = '',
     evaluation_prediction_score_column: str = '',
-    evaluation_class_labels: list = [],  # pylint: disable=g-bare-generic
+    evaluation_class_labels: List[str] = [],  # pylint: disable=g-bare-generic
     dataflow_machine_type: str = 'n1-standard-4',
     dataflow_max_num_workers: int = 5,
     dataflow_disk_size_gb: int = 50,
     dataflow_service_account: str = '',
     dataflow_subnetwork: str = '',
     dataflow_use_public_ips: bool = True,
     encryption_spec_key_name: str = '',
     force_runner_mode: str = '',
+) -> NamedTuple(
+    'outputs',
+    evaluation_metrics=ClassificationMetrics,
+    evaluation_resource_name=str,
 ):
-  """The evaluation pipeline with ground truth and no feature attribution.
+  """The evaluation pipeline with ground truth and no feature attribution for classification models.
 
-  This pipeline is used for all unstructured AutoML models, including Text,
-  Video, Image and Custom imported models.
+  This pipeline is used for all classification unstructured AutoML models,
+  including Text, Video, Image and Custom models.
 
   Args:
     project: The GCP project that runs the pipeline components.
     location: The GCP region that runs the pipeline components.
-    prediction_type: The type of prediction the model is to produce.
-      "classification", "regression", or "forecasting".
     model_name: The Vertex model resource name to be imported and used for batch
       prediction. Formatted like
       projects/{project}/locations/{location}/models/{model} or
       projects/{project}/locations/{location}/models/{model}@{model_version_id_or_model_version_alias}.
     target_field_name: The target field's name. Formatted to be able to find
       nested columns, delimited by ``.``. Prefixed with 'instance.' on the
       component for Vertex Batch Prediction.
@@ -173,15 +177,270 @@
       resources created by this pipeline will be encrypted with the provided
       encryption key. Has the form:
       ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
       The key needs to be in the same region as where the compute resource is
       created.
     force_runner_mode: Indicate the runner mode to use forcely. Valid options
       are ``Dataflow`` and ``DirectRunner``.
+
+  Returns:
+      A Tuple of google.ClassificationMetrics artifact and the imported
+      evaluation metrics resource name.
+  """
+  outputs = NamedTuple(
+      'outputs',
+      evaluation_metrics=ClassificationMetrics,
+      evaluation_resource_name=str,
+  )
+
+  evaluation_display_name = 'evaluation_automl_unstructure_data_pipeline'
+  get_model_task = GetVertexModelOp(model_name=model_name)
+
+  # Remove the ground truth from the given GCS data.
+  # This is required for many models as Vertex Batch Prediction can not have the
+  # ground truth in the data to run, but later the evaluation component requires
+  # the ground truth data.
+  target_field_data_remover_task = TargetFieldDataRemoverOp(
+      project=project,
+      location=location,
+      target_field_name=target_field_name,
+      gcs_source_uris=batch_predict_gcs_source_uris,
+      bigquery_source_uri=batch_predict_bigquery_source_uri,
+      instances_format=batch_predict_instances_format,
+      dataflow_service_account=dataflow_service_account,
+      dataflow_subnetwork=dataflow_subnetwork,
+      dataflow_use_public_ips=dataflow_use_public_ips,
+      encryption_spec_key_name=encryption_spec_key_name,
+      force_runner_mode=force_runner_mode,
+  )
+
+  # Run Batch Prediction.
+  batch_predict_task = ModelBatchPredictOp(
+      project=project,
+      location=location,
+      model=get_model_task.outputs['model'],
+      job_display_name=f'evaluation-batch-predict-{dsl.PIPELINE_JOB_ID_PLACEHOLDER}-{dsl.PIPELINE_TASK_ID_PLACEHOLDER}',
+      gcs_source_uris=target_field_data_remover_task.outputs[
+          'gcs_output_directory'
+      ],
+      bigquery_source_input_uri=target_field_data_remover_task.outputs[
+          'bigquery_output_table'
+      ],
+      instances_format=batch_predict_instances_format,
+      predictions_format=batch_predict_predictions_format,
+      gcs_destination_output_uri_prefix=batch_predict_gcs_destination_output_uri,
+      bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
+      machine_type=batch_predict_machine_type,
+      starting_replica_count=batch_predict_starting_replica_count,
+      max_replica_count=batch_predict_max_replica_count,
+      encryption_spec_key_name=encryption_spec_key_name,
+      accelerator_type=batch_predict_accelerator_type,
+      accelerator_count=batch_predict_accelerator_count,
+  )
+
+  # Run evaluation for a classification model.
+  eval_task = ModelEvaluationClassificationOp(
+      project=project,
+      location=location,
+      class_labels=evaluation_class_labels,
+      prediction_label_column=evaluation_prediction_label_column,
+      prediction_score_column=evaluation_prediction_score_column,
+      target_field_name=target_field_name,
+      ground_truth_format=batch_predict_instances_format,
+      ground_truth_gcs_source=batch_predict_gcs_source_uris,
+      ground_truth_bigquery_source=batch_predict_bigquery_source_uri,
+      predictions_format=batch_predict_predictions_format,
+      predictions_gcs_source=batch_predict_task.outputs['gcs_output_directory'],
+      predictions_bigquery_source=batch_predict_task.outputs[
+          'bigquery_output_table'
+      ],
+      dataflow_machine_type=dataflow_machine_type,
+      dataflow_max_workers_num=dataflow_max_num_workers,
+      dataflow_disk_size_gb=dataflow_disk_size_gb,
+      dataflow_service_account=dataflow_service_account,
+      dataflow_subnetwork=dataflow_subnetwork,
+      dataflow_use_public_ips=dataflow_use_public_ips,
+      encryption_spec_key_name=encryption_spec_key_name,
+      force_runner_mode=force_runner_mode,
+      model=get_model_task.outputs['model'],
+  )
+
+  # Import the evaluation result to Vertex AI.
+  import_evaluation_task = ModelImportEvaluationOp(
+      classification_metrics=eval_task.outputs['evaluation_metrics'],
+      model=get_model_task.outputs['model'],
+      dataset_type=batch_predict_instances_format,
+      dataset_path=batch_predict_bigquery_source_uri,
+      dataset_paths=batch_predict_gcs_source_uris,
+      display_name=evaluation_display_name,
+  )
+
+  return outputs(
+      evaluation_metrics=eval_task.outputs['evaluation_metrics'],
+      evaluation_resource_name=import_evaluation_task.outputs[
+          'evaluation_resource_name'
+      ],
+  )
+
+
+@kfp.dsl.pipeline(name='evaluation-regression-pipeline')
+def evaluation_automl_unstructure_data_regression_pipeline(  # pylint: disable=dangerous-default-value
+    project: str,
+    location: str,
+    model_name: str,
+    target_field_name: str,
+    batch_predict_instances_format: str,
+    batch_predict_gcs_destination_output_uri: str,
+    batch_predict_gcs_source_uris: list = [],  # pylint: disable=g-bare-generic
+    batch_predict_bigquery_source_uri: str = '',
+    batch_predict_predictions_format: str = 'jsonl',
+    batch_predict_bigquery_destination_output_uri: str = '',
+    batch_predict_machine_type: str = 'n1-standard-16',
+    batch_predict_starting_replica_count: int = 5,
+    batch_predict_max_replica_count: int = 10,
+    batch_predict_accelerator_type: str = '',
+    batch_predict_accelerator_count: int = 0,
+    evaluation_prediction_score_column: str = '',
+    dataflow_machine_type: str = 'n1-standard-4',
+    dataflow_max_num_workers: int = 5,
+    dataflow_disk_size_gb: int = 50,
+    dataflow_service_account: str = '',
+    dataflow_subnetwork: str = '',
+    dataflow_use_public_ips: bool = True,
+    encryption_spec_key_name: str = '',
+    force_runner_mode: str = '',
+) -> NamedTuple(
+    'outputs',
+    evaluation_metrics=RegressionMetrics,
+    evaluation_resource_name=str,
+):
+  """The evaluation pipeline with ground truth and no feature attribution for regression models.
+
+  This pipeline is used for all custom tabular regression models.
+
+  Args:
+    project: The GCP project that runs the pipeline components.
+    location: The GCP region that runs the pipeline components.
+    model_name: The Vertex model resource name to be imported and used for batch
+      prediction. Formatted like
+      projects/{project}/locations/{location}/models/{model} or
+      projects/{project}/locations/{location}/models/{model}@{model_version_id_or_model_version_alias}.
+    target_field_name: The target field's name. Formatted to be able to find
+      nested columns, delimited by ``.``. Prefixed with 'instance.' on the
+      component for Vertex Batch Prediction.
+    batch_predict_instances_format: The format in which instances are given,
+      must be one of the Model's supportedInputStorageFormats. For more details
+      about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_gcs_destination_output_uri: The Google Cloud Storage location
+      of the directory where the output is to be written to. In the given
+      directory a new directory is created. Its name is
+      ``prediction-<model-display-name>-<job-create-time>``, where timestamp is
+      in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. Inside of it files
+      ``predictions_0001.<extension>``, ``predictions_0002.<extension>``, ...,
+      ``predictions_N.<extension>`` are created where ``<extension>`` depends on
+      chosen ``predictions_format``, and N may equal 0001 and depends on the
+      total number of successfully predicted instances. If the Model has both
+      ``instance`` and ``prediction`` schemata defined then each such file
+      contains predictions as per the ``predictions_format``. If prediction for
+      any instance failed (partially or completely), then an additional
+      ``errors_0001.<extension>``, ``errors_0002.<extension>``,...,
+      ``errors_N.<extension>`` files are created (N depends on total number of
+      failed predictions). These files contain the failed instances, as per
+      their schema, followed by an additional ``error`` field which as value has
+      ``google.rpc.Status`` containing only ``code`` and ``message`` fields. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_gcs_source_uris: Google Cloud Storage URI(-s) to your
+      instances data to run batch prediction on. The instances data should also
+      contain the ground truth (target) data, used for evaluation. May contain
+      wildcards. For more information on wildcards, see
+      https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames. For
+        more details about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_bigquery_source_uri: Google BigQuery URI to your instances to
+      run batch prediction on. May contain wildcards. For more details about
+      this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_predictions_format: The format in which Vertex AI gives the
+      predictions. Must be one of the Model's supportedOutputStorageFormats. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_bigquery_destination_output_uri: The BigQuery project location
+      where the output is to be written to. In the given project a new dataset
+      is created with name ``prediction_<model-display-name>_<job-create-time>``
+      where is made BigQuery-dataset-name compatible (for example, most special
+      characters become underscores), and timestamp is in
+      YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset two
+      tables will be created, ``predictions``, and ``errors``. If the Model has
+      both ``instance`` and ``prediction`` schemata defined then the tables have
+      columns as follows: The ``predictions`` table contains instances for which
+      the prediction succeeded, it has columns as per a concatenation of the
+      Model's instance and prediction schemata. The ``errors`` table contains
+      rows for which the prediction has failed, it has instance columns, as per
+      the instance schema, followed by a single "errors" column, which as values
+      has ````google.rpc.Status`` <Status>``__ represented as a STRUCT, and
+      containing only ``code`` and ``message``.  For more details about this
+      output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_machine_type: The type of machine for running batch prediction
+      on dedicated resources. If the Model supports DEDICATED_RESOURCES this
+      config may be provided (and the job will use these resources). If the
+      Model doesn't support AUTOMATIC_RESOURCES, this config must be provided.
+      For more details about the BatchDedicatedResources, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchDedicatedResources.
+        For more details about the machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_starting_replica_count: The number of machine replicas used at
+      the start of the batch operation. If not set, Vertex AI decides starting
+      number, not greater than ``max_replica_count``. Only used if
+      ``machine_type`` is set.
+    batch_predict_max_replica_count: The maximum number of machine replicas the
+      batch operation may be scaled to. Only used if ``machine_type`` is set.
+    batch_predict_accelerator_type: The type of accelerator(s) that may be
+      attached to the machine as per ``batch_predict_accelerator_count``. Only
+      used if ``batch_predict_machine_type`` is set. For more details about the
+      machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_accelerator_count: The number of accelerators to attach to the
+      ``batch_predict_machine_type``. Only used if
+      ``batch_predict_machine_type`` is set.
+    evaluation_prediction_score_column: The column name of the field containing
+      batch prediction scores. Formatted to be able to find nested columns,
+      delimited by ``.``.
+    dataflow_machine_type: The Dataflow machine type for evaluation components.
+    dataflow_max_num_workers: The max number of Dataflow workers for evaluation
+      components.
+    dataflow_disk_size_gb: Dataflow worker's disk size in GB for evaluation
+      components.
+    dataflow_service_account: Custom service account to run Dataflow jobs.
+    dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty
+      the default subnetwork will be used. Example:
+      https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
+    dataflow_use_public_ips: Specifies whether Dataflow workers use public IP
+      addresses.
+    encryption_spec_key_name:  Customer-managed encryption key options. If set,
+      resources created by this pipeline will be encrypted with the provided
+      encryption key. Has the form:
+      ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
+      The key needs to be in the same region as where the compute resource is
+      created.
+    force_runner_mode: Indicate the runner mode to use forcely. Valid options
+      are ``Dataflow`` and ``DirectRunner``.
+
+  Returns:
+      A Tuple of google.RegressionMetrics artifact and the imported evaluation
+      metrics resource name.
   """
+  outputs = NamedTuple(
+      'outputs',
+      evaluation_metrics=RegressionMetrics,
+      evaluation_resource_name=str,
+  )
+
   evaluation_display_name = 'evaluation_automl_unstructure_data_pipeline'
   get_model_task = GetVertexModelOp(model_name=model_name)
 
   # Remove the ground truth from the given GCS data.
   # This is required for many models as Vertex Batch Prediction can not have the
   # ground truth in the data to run, but later the evaluation component requires
   # the ground truth data.
@@ -219,117 +478,264 @@
       starting_replica_count=batch_predict_starting_replica_count,
       max_replica_count=batch_predict_max_replica_count,
       encryption_spec_key_name=encryption_spec_key_name,
       accelerator_type=batch_predict_accelerator_type,
       accelerator_count=batch_predict_accelerator_count,
   )
 
-  # Run evaluation based on prediction type.
-  # After, import the model evaluations to the Vertex model.
+  # Run evaluation for a regression model.
+  eval_task = ModelEvaluationRegressionOp(
+      project=project,
+      location=location,
+      target_field_name=target_field_name,
+      ground_truth_format=batch_predict_instances_format,
+      ground_truth_gcs_source=batch_predict_gcs_source_uris,
+      ground_truth_bigquery_source=batch_predict_bigquery_source_uri,
+      prediction_score_column=evaluation_prediction_score_column,
+      predictions_format=batch_predict_predictions_format,
+      predictions_gcs_source=batch_predict_task.outputs['gcs_output_directory'],
+      predictions_bigquery_source=batch_predict_task.outputs[
+          'bigquery_output_table'
+      ],
+      dataflow_machine_type=dataflow_machine_type,
+      dataflow_max_workers_num=dataflow_max_num_workers,
+      dataflow_disk_size_gb=dataflow_disk_size_gb,
+      dataflow_service_account=dataflow_service_account,
+      dataflow_subnetwork=dataflow_subnetwork,
+      dataflow_use_public_ips=dataflow_use_public_ips,
+      encryption_spec_key_name=encryption_spec_key_name,
+      force_runner_mode=force_runner_mode,
+      model=get_model_task.outputs['model'],
+  )
+
+  # Import the evaluation result to Vertex AI.
+  import_evaluation_task = ModelImportEvaluationOp(
+      regression_metrics=eval_task.outputs['evaluation_metrics'],
+      model=get_model_task.outputs['model'],
+      dataset_type=batch_predict_instances_format,
+      dataset_path=batch_predict_bigquery_source_uri,
+      dataset_paths=batch_predict_gcs_source_uris,
+      display_name=evaluation_display_name,
+  )
+
+  return outputs(
+      evaluation_metrics=eval_task.outputs['evaluation_metrics'],
+      evaluation_resource_name=import_evaluation_task.outputs[
+          'evaluation_resource_name'
+      ],
+  )
+
+
+@kfp.dsl.pipeline(name='evaluation-pipeline')
+def evaluation_automl_unstructure_data_pipeline(  # pylint: disable=dangerous-default-value
+    project: str,
+    location: str,
+    prediction_type: str,
+    model_name: str,
+    target_field_name: str,
+    batch_predict_instances_format: str,
+    batch_predict_gcs_destination_output_uri: str,
+    batch_predict_gcs_source_uris: List[str] = [],  # pylint: disable=g-bare-generic
+    batch_predict_bigquery_source_uri: str = '',
+    batch_predict_predictions_format: str = 'jsonl',
+    batch_predict_bigquery_destination_output_uri: str = '',
+    batch_predict_machine_type: str = 'n1-standard-16',
+    batch_predict_starting_replica_count: int = 5,
+    batch_predict_max_replica_count: int = 10,
+    batch_predict_accelerator_type: str = '',
+    batch_predict_accelerator_count: int = 0,
+    evaluation_prediction_label_column: str = '',
+    evaluation_prediction_score_column: str = '',
+    evaluation_class_labels: List[str] = [],  # pylint: disable=g-bare-generic
+    dataflow_machine_type: str = 'n1-standard-4',
+    dataflow_max_num_workers: int = 5,
+    dataflow_disk_size_gb: int = 50,
+    dataflow_service_account: str = '',
+    dataflow_subnetwork: str = '',
+    dataflow_use_public_ips: bool = True,
+    encryption_spec_key_name: str = '',
+    force_runner_mode: str = '',
+):
+  """The evaluation pipeline with ground truth and no feature attribution.
+
+  This pipeline is used for all unstructured AutoML models, including Text,
+  Video, Image and Custom models.
+
+  Args:
+    project: The GCP project that runs the pipeline components.
+    location: The GCP region that runs the pipeline components.
+    prediction_type: The type of prediction the model is to produce.
+      "classification" or "regression".
+    model_name: The Vertex model resource name to be imported and used for batch
+      prediction. Formatted like
+      projects/{project}/locations/{location}/models/{model} or
+      projects/{project}/locations/{location}/models/{model}@{model_version_id_or_model_version_alias}.
+    target_field_name: The target field's name. Formatted to be able to find
+      nested columns, delimited by ``.``. Prefixed with 'instance.' on the
+      component for Vertex Batch Prediction.
+    batch_predict_instances_format: The format in which instances are given,
+      must be one of the Model's supportedInputStorageFormats. For more details
+      about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_gcs_destination_output_uri: The Google Cloud Storage location
+      of the directory where the output is to be written to. In the given
+      directory a new directory is created. Its name is
+      ``prediction-<model-display-name>-<job-create-time>``, where timestamp is
+      in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. Inside of it files
+      ``predictions_0001.<extension>``, ``predictions_0002.<extension>``, ...,
+      ``predictions_N.<extension>`` are created where ``<extension>`` depends on
+      chosen ``predictions_format``, and N may equal 0001 and depends on the
+      total number of successfully predicted instances. If the Model has both
+      ``instance`` and ``prediction`` schemata defined then each such file
+      contains predictions as per the ``predictions_format``. If prediction for
+      any instance failed (partially or completely), then an additional
+      ``errors_0001.<extension>``, ``errors_0002.<extension>``,...,
+      ``errors_N.<extension>`` files are created (N depends on total number of
+      failed predictions). These files contain the failed instances, as per
+      their schema, followed by an additional ``error`` field which as value has
+      ``google.rpc.Status`` containing only ``code`` and ``message`` fields. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_gcs_source_uris: Google Cloud Storage URI(-s) to your
+      instances data to run batch prediction on. The instances data should also
+      contain the ground truth (target) data, used for evaluation. May contain
+      wildcards. For more information on wildcards, see
+      https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames. For
+        more details about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_bigquery_source_uri: Google BigQuery URI to your instances to
+      run batch prediction on. May contain wildcards. For more details about
+      this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_predictions_format: The format in which Vertex AI gives the
+      predictions. Must be one of the Model's supportedOutputStorageFormats. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_bigquery_destination_output_uri: The BigQuery project location
+      where the output is to be written to. In the given project a new dataset
+      is created with name ``prediction_<model-display-name>_<job-create-time>``
+      where is made BigQuery-dataset-name compatible (for example, most special
+      characters become underscores), and timestamp is in
+      YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset two
+      tables will be created, ``predictions``, and ``errors``. If the Model has
+      both ``instance`` and ``prediction`` schemata defined then the tables have
+      columns as follows: The ``predictions`` table contains instances for which
+      the prediction succeeded, it has columns as per a concatenation of the
+      Model's instance and prediction schemata. The ``errors`` table contains
+      rows for which the prediction has failed, it has instance columns, as per
+      the instance schema, followed by a single "errors" column, which as values
+      has ````google.rpc.Status`` <Status>``__ represented as a STRUCT, and
+      containing only ``code`` and ``message``.  For more details about this
+      output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_machine_type: The type of machine for running batch prediction
+      on dedicated resources. If the Model supports DEDICATED_RESOURCES this
+      config may be provided (and the job will use these resources). If the
+      Model doesn't support AUTOMATIC_RESOURCES, this config must be provided.
+      For more details about the BatchDedicatedResources, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchDedicatedResources.
+        For more details about the machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_starting_replica_count: The number of machine replicas used at
+      the start of the batch operation. If not set, Vertex AI decides starting
+      number, not greater than ``max_replica_count``. Only used if
+      ``machine_type`` is set.
+    batch_predict_max_replica_count: The maximum number of machine replicas the
+      batch operation may be scaled to. Only used if ``machine_type`` is set.
+    batch_predict_accelerator_type: The type of accelerator(s) that may be
+      attached to the machine as per ``batch_predict_accelerator_count``. Only
+      used if ``batch_predict_machine_type`` is set. For more details about the
+      machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_accelerator_count: The number of accelerators to attach to the
+      ``batch_predict_machine_type``. Only used if
+      ``batch_predict_machine_type`` is set.
+    evaluation_prediction_label_column: The column name of the field containing
+      classes the model is scoring. Formatted to be able to find nested columns,
+      delimited by ``.``.
+    evaluation_prediction_score_column: The column name of the field containing
+      batch prediction scores. Formatted to be able to find nested columns,
+      delimited by ``.``.
+    evaluation_class_labels: Required for classification prediction type. The
+      list of class names for the target_field_name, in the same order they
+      appear in a file in batch_predict_gcs_source_uris. For instance, if the
+      target_field_name could be either ``1`` or ``0``, then the class_labels
+      input will be ["1", "0"].
+    dataflow_machine_type: The Dataflow machine type for evaluation components.
+    dataflow_max_num_workers: The max number of Dataflow workers for evaluation
+      components.
+    dataflow_disk_size_gb: Dataflow worker's disk size in GB for evaluation
+      components.
+    dataflow_service_account: Custom service account to run Dataflow jobs.
+    dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty
+      the default subnetwork will be used. Example:
+      https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
+    dataflow_use_public_ips: Specifies whether Dataflow workers use public IP
+      addresses.
+    encryption_spec_key_name:  Customer-managed encryption key options. If set,
+      resources created by this pipeline will be encrypted with the provided
+      encryption key. Has the form:
+      ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
+      The key needs to be in the same region as where the compute resource is
+      created.
+    force_runner_mode: Indicate the runner mode to use forcely. Valid options
+      are ``Dataflow`` and ``DirectRunner``.
+  """
   with kfp.dsl.Condition(
       prediction_type == 'classification', name='classification'
   ):
-    eval_task = ModelEvaluationClassificationOp(
+    evaluation_automl_unstructure_data_classification_pipeline(
         project=project,
         location=location,
-        class_labels=evaluation_class_labels,
-        prediction_label_column=evaluation_prediction_label_column,
-        prediction_score_column=evaluation_prediction_score_column,
+        model_name=model_name,
         target_field_name=target_field_name,
-        ground_truth_format=batch_predict_instances_format,
-        ground_truth_gcs_source=batch_predict_gcs_source_uris,
-        ground_truth_bigquery_source=batch_predict_bigquery_source_uri,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_predict_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_predict_task.outputs[
-            'bigquery_output_table'
-        ],
+        batch_predict_instances_format=batch_predict_instances_format,
+        batch_predict_gcs_destination_output_uri=batch_predict_gcs_destination_output_uri,
+        batch_predict_gcs_source_uris=batch_predict_gcs_source_uris,
+        batch_predict_bigquery_source_uri=batch_predict_bigquery_source_uri,
+        batch_predict_predictions_format=batch_predict_predictions_format,
+        batch_predict_bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
+        batch_predict_machine_type=batch_predict_machine_type,
+        batch_predict_starting_replica_count=batch_predict_starting_replica_count,
+        batch_predict_max_replica_count=batch_predict_max_replica_count,
+        batch_predict_accelerator_type=batch_predict_accelerator_type,
+        batch_predict_accelerator_count=batch_predict_accelerator_count,
+        evaluation_prediction_label_column=evaluation_prediction_label_column,
+        evaluation_prediction_score_column=evaluation_prediction_score_column,
+        evaluation_class_labels=evaluation_class_labels,
         dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
+        dataflow_max_num_workers=dataflow_max_num_workers,
         dataflow_disk_size_gb=dataflow_disk_size_gb,
         dataflow_service_account=dataflow_service_account,
         dataflow_subnetwork=dataflow_subnetwork,
         dataflow_use_public_ips=dataflow_use_public_ips,
         encryption_spec_key_name=encryption_spec_key_name,
         force_runner_mode=force_runner_mode,
-        model=get_model_task.outputs['model'],
-    )
-    ModelImportEvaluationOp(
-        classification_metrics=eval_task.outputs['evaluation_metrics'],
-        model=get_model_task.outputs['model'],
-        dataset_type=batch_predict_instances_format,
-        dataset_path=batch_predict_bigquery_source_uri,
-        dataset_paths=batch_predict_gcs_source_uris,
-        display_name=evaluation_display_name,
-    )
-
-  with kfp.dsl.Condition(prediction_type == 'forecasting', name='forecasting'):
-    eval_task = ModelEvaluationForecastingOp(
-        project=project,
-        location=location,
-        target_field_name=target_field_name,
-        ground_truth_format=batch_predict_instances_format,
-        ground_truth_gcs_source=batch_predict_gcs_source_uris,
-        ground_truth_bigquery_source=batch_predict_bigquery_source_uri,
-        prediction_score_column=evaluation_prediction_score_column,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_predict_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_predict_task.outputs[
-            'bigquery_output_table'
-        ],
-        dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
-        dataflow_disk_size_gb=dataflow_disk_size_gb,
-        dataflow_service_account=dataflow_service_account,
-        dataflow_subnetwork=dataflow_subnetwork,
-        dataflow_use_public_ips=dataflow_use_public_ips,
-        encryption_spec_key_name=encryption_spec_key_name,
-        force_runner_mode=force_runner_mode,
-        model=get_model_task.outputs['model'],
-    )
-    ModelImportEvaluationOp(
-        forecasting_metrics=eval_task.outputs['evaluation_metrics'],
-        model=get_model_task.outputs['model'],
-        dataset_type=batch_predict_instances_format,
-        dataset_path=batch_predict_bigquery_source_uri,
-        dataset_paths=batch_predict_gcs_source_uris,
-        display_name=evaluation_display_name,
     )
 
   with kfp.dsl.Condition(prediction_type == 'regression', name='regression'):
-    eval_task = ModelEvaluationRegressionOp(
+    evaluation_automl_unstructure_data_regression_pipeline(
         project=project,
         location=location,
+        model_name=model_name,
         target_field_name=target_field_name,
-        ground_truth_format=batch_predict_instances_format,
-        ground_truth_gcs_source=batch_predict_gcs_source_uris,
-        ground_truth_bigquery_source=batch_predict_bigquery_source_uri,
-        prediction_score_column=evaluation_prediction_score_column,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_predict_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_predict_task.outputs[
-            'bigquery_output_table'
-        ],
+        batch_predict_instances_format=batch_predict_instances_format,
+        batch_predict_gcs_destination_output_uri=batch_predict_gcs_destination_output_uri,
+        batch_predict_gcs_source_uris=batch_predict_gcs_source_uris,
+        batch_predict_bigquery_source_uri=batch_predict_bigquery_source_uri,
+        batch_predict_predictions_format=batch_predict_predictions_format,
+        batch_predict_bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
+        batch_predict_machine_type=batch_predict_machine_type,
+        batch_predict_starting_replica_count=batch_predict_starting_replica_count,
+        batch_predict_max_replica_count=batch_predict_max_replica_count,
+        batch_predict_accelerator_type=batch_predict_accelerator_type,
+        batch_predict_accelerator_count=batch_predict_accelerator_count,
+        evaluation_prediction_score_column=evaluation_prediction_score_column,
         dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
+        dataflow_max_num_workers=dataflow_max_num_workers,
         dataflow_disk_size_gb=dataflow_disk_size_gb,
         dataflow_service_account=dataflow_service_account,
         dataflow_subnetwork=dataflow_subnetwork,
         dataflow_use_public_ips=dataflow_use_public_ips,
         encryption_spec_key_name=encryption_spec_key_name,
         force_runner_mode=force_runner_mode,
-        model=get_model_task.outputs['model'],
-    )
-    ModelImportEvaluationOp(
-        regression_metrics=eval_task.outputs['evaluation_metrics'],
-        model=get_model_task.outputs['model'],
-        dataset_type=batch_predict_instances_format,
-        dataset_path=batch_predict_bigquery_source_uri,
-        dataset_paths=batch_predict_gcs_source_uris,
-        display_name=evaluation_display_name,
     )
```

## google_cloud_pipeline_components/v1/model_evaluation/evaluation_feature_attribution_pipeline.py

```diff
@@ -8,71 +8,73 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import Any, Dict, List, NamedTuple
+
 from google_cloud_pipeline_components._implementation.model import GetVertexModelOp
-from google_cloud_pipeline_components._implementation.model_evaluation import EvaluationDataSamplerOp
 from google_cloud_pipeline_components._implementation.model_evaluation import ModelImportEvaluationOp
 from google_cloud_pipeline_components._implementation.model_evaluation import TargetFieldDataRemoverOp
-from google_cloud_pipeline_components.preview.model_evaluation import ModelEvaluationFeatureAttributionOp
+from google_cloud_pipeline_components.preview.model_evaluation import FeatureAttributionGraphComponentOp
+from google_cloud_pipeline_components.types.artifact_types import ClassificationMetrics
+from google_cloud_pipeline_components.types.artifact_types import RegressionMetrics
 from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp
 from google_cloud_pipeline_components.v1.model_evaluation.classification_component import model_evaluation_classification as ModelEvaluationClassificationOp
-from google_cloud_pipeline_components.v1.model_evaluation.forecasting_component import model_evaluation_forecasting as ModelEvaluationForecastingOp
 from google_cloud_pipeline_components.v1.model_evaluation.regression_component import model_evaluation_regression as ModelEvaluationRegressionOp
 import kfp
 
 
-@kfp.dsl.pipeline(name='evaluation-feature-attribution-pipeline')
-def evaluation_feature_attribution_pipeline(  # pylint: disable=dangerous-default-value
+@kfp.dsl.pipeline(name='evaluation-feature-attribution-classification-pipeline')
+def evaluation_feature_attribution_classification_pipeline(  # pylint: disable=dangerous-default-value
     project: str,
     location: str,
-    prediction_type: str,
     model_name: str,
     target_field_name: str,
     batch_predict_instances_format: str,
     batch_predict_gcs_destination_output_uri: str,
-    batch_predict_gcs_source_uris: list = [],  # pylint: disable=g-bare-generic
+    batch_predict_gcs_source_uris: List[str] = [],  # pylint: disable=g-bare-generic
     batch_predict_bigquery_source_uri: str = '',
     batch_predict_predictions_format: str = 'jsonl',
     batch_predict_bigquery_destination_output_uri: str = '',
     batch_predict_machine_type: str = 'n1-standard-16',
     batch_predict_starting_replica_count: int = 5,
     batch_predict_max_replica_count: int = 10,
-    batch_predict_explanation_metadata: dict = {},  # pylint: disable=g-bare-generic
-    batch_predict_explanation_parameters: dict = {},  # pylint: disable=g-bare-generic
+    batch_predict_explanation_metadata: Dict[str, Any] = {},  # pylint: disable=g-bare-generic
+    batch_predict_explanation_parameters: Dict[str, Any] = {},  # pylint: disable=g-bare-generic
     batch_predict_explanation_data_sample_size: int = 10000,
     batch_predict_accelerator_type: str = '',
     batch_predict_accelerator_count: int = 0,
     evaluation_prediction_label_column: str = '',
     evaluation_prediction_score_column: str = '',
-    evaluation_class_labels: list = [],  # pylint: disable=g-bare-generic
+    evaluation_class_labels: List[str] = [],  # pylint: disable=g-bare-generic
     dataflow_machine_type: str = 'n1-standard-4',
     dataflow_max_num_workers: int = 5,
     dataflow_disk_size_gb: int = 50,
     dataflow_service_account: str = '',
     dataflow_subnetwork: str = '',
     dataflow_use_public_ips: bool = True,
     encryption_spec_key_name: str = '',
     force_runner_mode: str = '',
+) -> NamedTuple(
+    'outputs',
+    evaluation_metrics=ClassificationMetrics,
+    evaluation_resource_name=str,
 ):
-  """The evaluation AutoML tabular pipeline with feature attribution.
+  """The evaluation custom tabular pipeline with feature attribution for classification models.
 
-  This pipeline guarantees support for AutoML Tabular models that contain a
-  valid explanation_spec. This pipeline does not include the
-  target_field_data_remover component, which is needed for many tabular custom
-  models.
+  This pipeline gives support for custom models that contain a
+  valid explanation_spec. This pipeline includes the target_field_data_remover
+  component, which is needed for many tabular custom models.
 
   Args:
     project: The GCP project that runs the pipeline components.
     location: The GCP region that runs the pipeline components.
-    prediction_type: The type of prediction the model is to produce.
-      "classification", "regression", or "forecasting".
     model_name: The Vertex model resource name to be imported and used for batch
       prediction.
     target_field_name: The target field's name. Formatted to be able to find
       nested columns, delimited by ``.``. Prefixed with 'instance.' on the
       component for Vertex Batch Prediction.
     batch_predict_instances_format: The format in which instances are given,
       must be one of the Model's supportedInputStorageFormats. For more details
@@ -196,19 +198,28 @@
       resources created by this pipeline will be encrypted with the provided
       encryption key. Has the form:
       ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
       The key needs to be in the same region as where the compute resource is
       created.
     force_runner_mode: Indicate the runner mode to use forcely. Valid options
       are ``Dataflow`` and ``DirectRunner``.
+
+  Returns:
+      A google.ClassificationMetrics artifact.
   """
+  outputs = NamedTuple(
+      'outputs',
+      evaluation_metrics=ClassificationMetrics,
+      evaluation_resource_name=str,
+  )
+
   evaluation_display_name = 'evaluation-feature-attribution-pipeline'
   get_model_task = GetVertexModelOp(model_name=model_name)
 
-  # Remove the ground truth from the given GCS data.
+  # Remove the ground truth from the given GCS or BQ data.
   # This is required for many models as Vertex Batch Prediction can not have the
   # ground truth in the data to run, but later the evaluation component requires
   # the ground truth data.
   target_field_data_remover_task = TargetFieldDataRemoverOp(
       project=project,
       location=location,
       target_field_name=target_field_name,
@@ -242,224 +253,631 @@
       starting_replica_count=batch_predict_starting_replica_count,
       max_replica_count=batch_predict_max_replica_count,
       encryption_spec_key_name=encryption_spec_key_name,
       accelerator_type=batch_predict_accelerator_type,
       accelerator_count=batch_predict_accelerator_count,
   )
 
-  # Run the Batch Explain process (sampler -> batch explanation).
-  data_sampler_task = EvaluationDataSamplerOp(
+  # Run feature attribution steps.
+  feature_attribution_graph = FeatureAttributionGraphComponentOp(
       project=project,
       location=location,
-      gcs_source_uris=target_field_data_remover_task.outputs[
+      prediction_type='classification',
+      vertex_model=get_model_task.outputs['model'],
+      batch_predict_instances_format=batch_predict_instances_format,
+      batch_predict_gcs_destination_output_uri=batch_predict_gcs_destination_output_uri,
+      batch_predict_gcs_source_uris=target_field_data_remover_task.outputs[
           'gcs_output_directory'
       ],
-      bigquery_source_uri=target_field_data_remover_task.outputs[
+      batch_predict_bigquery_source_uri=target_field_data_remover_task.outputs[
+          'bigquery_output_table'
+      ],
+      batch_predict_predictions_format=batch_predict_predictions_format,
+      batch_predict_bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
+      batch_predict_machine_type=batch_predict_machine_type,
+      batch_predict_starting_replica_count=batch_predict_starting_replica_count,
+      batch_predict_max_replica_count=batch_predict_max_replica_count,
+      batch_predict_explanation_metadata=batch_predict_explanation_metadata,
+      batch_predict_explanation_parameters=batch_predict_explanation_parameters,
+      batch_predict_explanation_data_sample_size=batch_predict_explanation_data_sample_size,
+      batch_predict_accelerator_type=batch_predict_accelerator_type,
+      batch_predict_accelerator_count=batch_predict_accelerator_count,
+      dataflow_machine_type=dataflow_machine_type,
+      dataflow_max_num_workers=dataflow_max_num_workers,
+      dataflow_disk_size_gb=dataflow_disk_size_gb,
+      dataflow_service_account=dataflow_service_account,
+      dataflow_subnetwork=dataflow_subnetwork,
+      dataflow_use_public_ips=dataflow_use_public_ips,
+      encryption_spec_key_name=encryption_spec_key_name,
+      force_runner_mode=force_runner_mode,
+  )
+
+  # Run evaluation for a classification model.
+  eval_task = ModelEvaluationClassificationOp(
+      project=project,
+      location=location,
+      class_labels=evaluation_class_labels,
+      prediction_label_column=evaluation_prediction_label_column,
+      prediction_score_column=evaluation_prediction_score_column,
+      target_field_name=target_field_name,
+      ground_truth_format=batch_predict_instances_format,
+      ground_truth_gcs_source=batch_predict_gcs_source_uris,
+      ground_truth_bigquery_source=batch_predict_bigquery_source_uri,
+      predictions_format=batch_predict_predictions_format,
+      predictions_gcs_source=batch_predict_task.outputs['gcs_output_directory'],
+      predictions_bigquery_source=batch_predict_task.outputs[
           'bigquery_output_table'
       ],
+      dataflow_machine_type=dataflow_machine_type,
+      dataflow_max_workers_num=dataflow_max_num_workers,
+      dataflow_disk_size_gb=dataflow_disk_size_gb,
+      dataflow_service_account=dataflow_service_account,
+      dataflow_subnetwork=dataflow_subnetwork,
+      dataflow_use_public_ips=dataflow_use_public_ips,
+      encryption_spec_key_name=encryption_spec_key_name,
+      force_runner_mode=force_runner_mode,
+      model=get_model_task.outputs['model'],
+  )
+
+  # Import the evaluation result to Vertex AI.
+  import_evaluation_task = ModelImportEvaluationOp(
+      classification_metrics=eval_task.outputs['evaluation_metrics'],
+      feature_attributions=feature_attribution_graph.outputs[
+          'feature_attributions'
+      ],
+      model=get_model_task.outputs['model'],
+      dataset_type=batch_predict_instances_format,
+      dataset_path=batch_predict_bigquery_source_uri,
+      dataset_paths=batch_predict_gcs_source_uris,
+      display_name=evaluation_display_name,
+  )
+
+  return outputs(
+      evaluation_metrics=eval_task.outputs['evaluation_metrics'],
+      evaluation_resource_name=import_evaluation_task.outputs[
+          'evaluation_resource_name'
+      ],
+  )
+
+
+@kfp.dsl.pipeline(name='evaluation-feature-attribution-regression-pipeline')
+def evaluation_feature_attribution_regression_pipeline(  # pylint: disable=dangerous-default-value
+    project: str,
+    location: str,
+    model_name: str,
+    target_field_name: str,
+    batch_predict_instances_format: str,
+    batch_predict_gcs_destination_output_uri: str,
+    batch_predict_gcs_source_uris: List[str] = [],  # pylint: disable=g-bare-generic
+    batch_predict_bigquery_source_uri: str = '',
+    batch_predict_predictions_format: str = 'jsonl',
+    batch_predict_bigquery_destination_output_uri: str = '',
+    batch_predict_machine_type: str = 'n1-standard-16',
+    batch_predict_starting_replica_count: int = 5,
+    batch_predict_max_replica_count: int = 10,
+    batch_predict_explanation_metadata: Dict[str, Any] = {},  # pylint: disable=g-bare-generic
+    batch_predict_explanation_parameters: Dict[str, Any] = {},  # pylint: disable=g-bare-generic
+    batch_predict_explanation_data_sample_size: int = 10000,
+    batch_predict_accelerator_type: str = '',
+    batch_predict_accelerator_count: int = 0,
+    evaluation_prediction_score_column: str = '',
+    dataflow_machine_type: str = 'n1-standard-4',
+    dataflow_max_num_workers: int = 5,
+    dataflow_disk_size_gb: int = 50,
+    dataflow_service_account: str = '',
+    dataflow_subnetwork: str = '',
+    dataflow_use_public_ips: bool = True,
+    encryption_spec_key_name: str = '',
+    force_runner_mode: str = '',
+) -> NamedTuple(
+    'outputs',
+    evaluation_metrics=RegressionMetrics,
+    evaluation_resource_name=str,
+):
+  """The evaluation custom tabular pipeline with feature attribution for regression models.
+
+  This pipeline gives support for custom models that contain a
+  valid explanation_spec. This pipeline includes the target_field_data_remover
+  component, which is needed for many tabular custom models.
+
+  Args:
+    project: The GCP project that runs the pipeline components.
+    location: The GCP region that runs the pipeline components.
+    model_name: The Vertex model resource name to be imported and used for batch
+      prediction.
+    target_field_name: The target field's name. Formatted to be able to find
+      nested columns, delimited by ``.``. Prefixed with 'instance.' on the
+      component for Vertex Batch Prediction.
+    batch_predict_instances_format: The format in which instances are given,
+      must be one of the Model's supportedInputStorageFormats. For more details
+      about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_gcs_destination_output_uri: The Google Cloud Storage location
+      of the directory where the output is to be written to. In the given
+      directory a new directory is created. Its name is
+      ``prediction-<model-display-name>-<job-create-time>``, where timestamp is
+      in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. Inside of it files
+      ``predictions_0001.<extension>``, ``predictions_0002.<extension>``, ...,
+      ``predictions_N.<extension>`` are created where ``<extension>`` depends on
+      chosen ``predictions_format``, and N may equal 0001 and depends on the
+      total number of successfully predicted instances. If the Model has both
+      ``instance`` and ``prediction`` schemata defined then each such file
+      contains predictions as per the ``predictions_format``. If prediction for
+      any instance failed (partially or completely), then an additional
+      ``errors_0001.<extension>``, ``errors_0002.<extension>``,...,
+      ``errors_N.<extension>`` files are created (N depends on total number of
+      failed predictions). These files contain the failed instances, as per
+      their schema, followed by an additional ``error`` field which as value has
+      ``google.rpc.Status`` containing only ``code`` and ``message`` fields. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_gcs_source_uris: Google Cloud Storage URI(-s) to your
+      instances data to run batch prediction on. The instances data should also
+      contain the ground truth (target) data, used for evaluation. May contain
+      wildcards. For more information on wildcards, see
+      https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames. For
+        more details about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_bigquery_source_uri: Google BigQuery URI to your instances to
+      run batch prediction on. May contain wildcards. For more details about
+      this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_predictions_format: The format in which Vertex AI gives the
+      predictions. Must be one of the Model's supportedOutputStorageFormats. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_bigquery_destination_output_uri: The BigQuery project location
+      where the output is to be written to. In the given project a new dataset
+      is created with name ``prediction_<model-display-name>_<job-create-time>``
+      where is made BigQuery-dataset-name compatible (for example, most special
+      characters become underscores), and timestamp is in
+      YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset two
+      tables will be created, ``predictions``, and ``errors``. If the Model has
+      both ``instance`` and ``prediction`` schemata defined then the tables have
+      columns as follows: The ``predictions`` table contains instances for which
+      the prediction succeeded, it has columns as per a concatenation of the
+      Model's instance and prediction schemata. The ``errors`` table contains
+      rows for which the prediction has failed, it has instance columns, as per
+      the instance schema, followed by a single "errors" column, which as values
+      has ````google.rpc.Status`` <Status>``__ represented as a STRUCT, and
+      containing only ``code`` and ``message``.  For more details about this
+      output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_machine_type: The type of machine for running batch prediction
+      on dedicated resources. If the Model supports DEDICATED_RESOURCES this
+      config may be provided (and the job will use these resources). If the
+      Model doesn't support AUTOMATIC_RESOURCES, this config must be provided.
+      For more details about the BatchDedicatedResources, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchDedicatedResources.
+        For more details about the machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_starting_replica_count: The number of machine replicas used at
+      the start of the batch operation. If not set, Vertex AI decides starting
+      number, not greater than ``max_replica_count``. Only used if
+      ``machine_type`` is set.
+    batch_predict_max_replica_count: The maximum number of machine replicas the
+      batch operation may be scaled to. Only used if ``machine_type`` is set.
+    batch_predict_explanation_metadata: Explanation metadata configuration for
+      this BatchPredictionJob. Can be specified only if ``generate_explanation``
+      is set to ``True``. This value overrides the value of
+      ``Model.explanation_metadata``. All fields of ``explanation_metadata`` are
+      optional in the request. If a field of the ``explanation_metadata`` object
+      is not populated, the corresponding field of the
+      ``Model.explanation_metadata`` object is inherited. For more details, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata.
+    batch_predict_explanation_parameters: Parameters to configure explaining for
+      Model's predictions. Can be specified only if ``generate_explanation`` is
+      set to ``True``. This value overrides the value of
+      ``Model.explanation_parameters``. All fields of ``explanation_parameters``
+      are optional in the request. If a field of the ``explanation_parameters``
+      object is not populated, the corresponding field of the
+      ``Model.explanation_parameters`` object is inherited. For more details,
+      see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#ExplanationParameters.
+    batch_predict_explanation_data_sample_size: Desired size to downsample the
+      input dataset that will then be used for batch explanation.
+    batch_predict_accelerator_type: The type of accelerator(s) that may be
+      attached to the machine as per ``batch_predict_accelerator_count``. Only
+      used if ``batch_predict_machine_type`` is set. For more details about the
+      machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_accelerator_count: The number of accelerators to attach to the
+      ``batch_predict_machine_type``. Only used if
+      ``batch_predict_machine_type`` is set.
+    evaluation_prediction_score_column: The column name of the field containing
+      batch prediction scores. Formatted to be able to find nested columns,
+      delimited by ``.``.
+    dataflow_machine_type: The Dataflow machine type for evaluation components.
+    dataflow_max_num_workers: The max number of Dataflow workers for evaluation
+      components.
+    dataflow_disk_size_gb: Dataflow worker's disk size in GB for evaluation
+      components.
+    dataflow_service_account: Custom service account to run Dataflow jobs.
+    dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty
+      the default subnetwork will be used. Example:
+      https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
+    dataflow_use_public_ips: Specifies whether Dataflow workers use public IP
+      addresses.
+    encryption_spec_key_name:  Customer-managed encryption key options. If set,
+      resources created by this pipeline will be encrypted with the provided
+      encryption key. Has the form:
+      ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
+      The key needs to be in the same region as where the compute resource is
+      created.
+    force_runner_mode: Indicate the runner mode to use forcely. Valid options
+      are ``Dataflow`` and ``DirectRunner``.
+
+  Returns:
+      A google.RegressionMetrics artifact.
+  """
+  outputs = NamedTuple(
+      'outputs',
+      evaluation_metrics=RegressionMetrics,
+      evaluation_resource_name=str,
+  )
+
+  evaluation_display_name = 'evaluation-feature-attribution-pipeline'
+  get_model_task = GetVertexModelOp(model_name=model_name)
+
+  # Remove the ground truth from the given GCS or BQ data.
+  # This is required for many models as Vertex Batch Prediction can not have the
+  # ground truth in the data to run, but later the evaluation component requires
+  # the ground truth data.
+  target_field_data_remover_task = TargetFieldDataRemoverOp(
+      project=project,
+      location=location,
+      target_field_name=target_field_name,
+      gcs_source_uris=batch_predict_gcs_source_uris,
+      bigquery_source_uri=batch_predict_bigquery_source_uri,
       instances_format=batch_predict_instances_format,
-      sample_size=batch_predict_explanation_data_sample_size,
+      dataflow_service_account=dataflow_service_account,
+      dataflow_subnetwork=dataflow_subnetwork,
+      dataflow_use_public_ips=dataflow_use_public_ips,
+      encryption_spec_key_name=encryption_spec_key_name,
       force_runner_mode=force_runner_mode,
   )
-  batch_explain_task = ModelBatchPredictOp(
+
+  # Run Batch Prediction.
+  batch_predict_task = ModelBatchPredictOp(
       project=project,
       location=location,
       model=get_model_task.outputs['model'],
-      job_display_name='model-registry-batch-explain-evaluation-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}',
-      gcs_source_uris=data_sampler_task.outputs['gcs_output_directory'],
-      bigquery_source_input_uri=data_sampler_task.outputs[
+      job_display_name='model-registry-batch-predict-evaluation-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}',
+      gcs_source_uris=target_field_data_remover_task.outputs[
+          'gcs_output_directory'
+      ],
+      bigquery_source_input_uri=target_field_data_remover_task.outputs[
           'bigquery_output_table'
       ],
       instances_format=batch_predict_instances_format,
       predictions_format=batch_predict_predictions_format,
       gcs_destination_output_uri_prefix=batch_predict_gcs_destination_output_uri,
       bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
-      generate_explanation=True,
-      explanation_parameters=batch_predict_explanation_parameters,
-      explanation_metadata=batch_predict_explanation_metadata,
       machine_type=batch_predict_machine_type,
       starting_replica_count=batch_predict_starting_replica_count,
       max_replica_count=batch_predict_max_replica_count,
       encryption_spec_key_name=encryption_spec_key_name,
       accelerator_type=batch_predict_accelerator_type,
       accelerator_count=batch_predict_accelerator_count,
   )
 
-  # Run evaluation based on prediction type and feature attribution component.
-  # After, import the model evaluations to the Vertex model.
+  # Run feature attribution steps.
+  feature_attribution_graph = FeatureAttributionGraphComponentOp(
+      project=project,
+      location=location,
+      prediction_type='regression',
+      vertex_model=get_model_task.outputs['model'],
+      batch_predict_instances_format=batch_predict_instances_format,
+      batch_predict_gcs_destination_output_uri=batch_predict_gcs_destination_output_uri,
+      batch_predict_gcs_source_uris=target_field_data_remover_task.outputs[
+          'gcs_output_directory'
+      ],
+      batch_predict_bigquery_source_uri=target_field_data_remover_task.outputs[
+          'bigquery_output_table'
+      ],
+      batch_predict_predictions_format=batch_predict_predictions_format,
+      batch_predict_bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
+      batch_predict_machine_type=batch_predict_machine_type,
+      batch_predict_starting_replica_count=batch_predict_starting_replica_count,
+      batch_predict_max_replica_count=batch_predict_max_replica_count,
+      batch_predict_explanation_metadata=batch_predict_explanation_metadata,
+      batch_predict_explanation_parameters=batch_predict_explanation_parameters,
+      batch_predict_explanation_data_sample_size=batch_predict_explanation_data_sample_size,
+      batch_predict_accelerator_type=batch_predict_accelerator_type,
+      batch_predict_accelerator_count=batch_predict_accelerator_count,
+      dataflow_machine_type=dataflow_machine_type,
+      dataflow_max_num_workers=dataflow_max_num_workers,
+      dataflow_disk_size_gb=dataflow_disk_size_gb,
+      dataflow_service_account=dataflow_service_account,
+      dataflow_subnetwork=dataflow_subnetwork,
+      dataflow_use_public_ips=dataflow_use_public_ips,
+      encryption_spec_key_name=encryption_spec_key_name,
+      force_runner_mode=force_runner_mode,
+  )
+
+  # Run evaluation for a regression model.
+  eval_task = ModelEvaluationRegressionOp(
+      project=project,
+      location=location,
+      target_field_name=target_field_name,
+      ground_truth_format=batch_predict_instances_format,
+      ground_truth_gcs_source=batch_predict_gcs_source_uris,
+      ground_truth_bigquery_source=batch_predict_bigquery_source_uri,
+      prediction_score_column=evaluation_prediction_score_column,
+      predictions_format=batch_predict_predictions_format,
+      predictions_gcs_source=batch_predict_task.outputs['gcs_output_directory'],
+      predictions_bigquery_source=batch_predict_task.outputs[
+          'bigquery_output_table'
+      ],
+      dataflow_machine_type=dataflow_machine_type,
+      dataflow_max_workers_num=dataflow_max_num_workers,
+      dataflow_disk_size_gb=dataflow_disk_size_gb,
+      dataflow_service_account=dataflow_service_account,
+      dataflow_subnetwork=dataflow_subnetwork,
+      dataflow_use_public_ips=dataflow_use_public_ips,
+      encryption_spec_key_name=encryption_spec_key_name,
+      force_runner_mode=force_runner_mode,
+      model=get_model_task.outputs['model'],
+  )
+
+  # Import the evaluation result to Vertex AI.
+  import_evaluation_task = ModelImportEvaluationOp(
+      regression_metrics=eval_task.outputs['evaluation_metrics'],
+      feature_attributions=feature_attribution_graph.outputs[
+          'feature_attributions'
+      ],
+      model=get_model_task.outputs['model'],
+      dataset_type=batch_predict_instances_format,
+      dataset_path=batch_predict_bigquery_source_uri,
+      dataset_paths=batch_predict_gcs_source_uris,
+      display_name=evaluation_display_name,
+  )
+
+  return outputs(
+      evaluation_metrics=eval_task.outputs['evaluation_metrics'],
+      evaluation_resource_name=import_evaluation_task.outputs[
+          'evaluation_resource_name'
+      ],
+  )
+
+
+@kfp.dsl.pipeline(name='evaluation-feature-attribution-pipeline')
+def evaluation_feature_attribution_pipeline(  # pylint: disable=dangerous-default-value
+    project: str,
+    location: str,
+    prediction_type: str,
+    model_name: str,
+    target_field_name: str,
+    batch_predict_instances_format: str,
+    batch_predict_gcs_destination_output_uri: str,
+    batch_predict_gcs_source_uris: List[str] = [],  # pylint: disable=g-bare-generic
+    batch_predict_bigquery_source_uri: str = '',
+    batch_predict_predictions_format: str = 'jsonl',
+    batch_predict_bigquery_destination_output_uri: str = '',
+    batch_predict_machine_type: str = 'n1-standard-16',
+    batch_predict_starting_replica_count: int = 5,
+    batch_predict_max_replica_count: int = 10,
+    batch_predict_explanation_metadata: Dict[str, Any] = {},  # pylint: disable=g-bare-generic
+    batch_predict_explanation_parameters: Dict[str, Any] = {},  # pylint: disable=g-bare-generic
+    batch_predict_explanation_data_sample_size: int = 10000,
+    batch_predict_accelerator_type: str = '',
+    batch_predict_accelerator_count: int = 0,
+    evaluation_prediction_label_column: str = '',
+    evaluation_prediction_score_column: str = '',
+    evaluation_class_labels: List[str] = [],  # pylint: disable=g-bare-generic
+    dataflow_machine_type: str = 'n1-standard-4',
+    dataflow_max_num_workers: int = 5,
+    dataflow_disk_size_gb: int = 50,
+    dataflow_service_account: str = '',
+    dataflow_subnetwork: str = '',
+    dataflow_use_public_ips: bool = True,
+    encryption_spec_key_name: str = '',
+    force_runner_mode: str = '',
+):
+  """The evaluation custom tabular pipeline with feature attribution.
+
+  This pipeline gives support for custom models that contain a
+  valid explanation_spec. This pipeline includes the target_field_data_remover
+  component, which is needed for many tabular custom models.
+
+  Args:
+    project: The GCP project that runs the pipeline components.
+    location: The GCP region that runs the pipeline components.
+    prediction_type: The type of prediction the model is to produce.
+      "classification" or "regression".
+    model_name: The Vertex model resource name to be imported and used for batch
+      prediction.
+    target_field_name: The target field's name. Formatted to be able to find
+      nested columns, delimited by ``.``. Prefixed with 'instance.' on the
+      component for Vertex Batch Prediction.
+    batch_predict_instances_format: The format in which instances are given,
+      must be one of the Model's supportedInputStorageFormats. For more details
+      about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_gcs_destination_output_uri: The Google Cloud Storage location
+      of the directory where the output is to be written to. In the given
+      directory a new directory is created. Its name is
+      ``prediction-<model-display-name>-<job-create-time>``, where timestamp is
+      in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. Inside of it files
+      ``predictions_0001.<extension>``, ``predictions_0002.<extension>``, ...,
+      ``predictions_N.<extension>`` are created where ``<extension>`` depends on
+      chosen ``predictions_format``, and N may equal 0001 and depends on the
+      total number of successfully predicted instances. If the Model has both
+      ``instance`` and ``prediction`` schemata defined then each such file
+      contains predictions as per the ``predictions_format``. If prediction for
+      any instance failed (partially or completely), then an additional
+      ``errors_0001.<extension>``, ``errors_0002.<extension>``,...,
+      ``errors_N.<extension>`` files are created (N depends on total number of
+      failed predictions). These files contain the failed instances, as per
+      their schema, followed by an additional ``error`` field which as value has
+      ``google.rpc.Status`` containing only ``code`` and ``message`` fields. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_gcs_source_uris: Google Cloud Storage URI(-s) to your
+      instances data to run batch prediction on. The instances data should also
+      contain the ground truth (target) data, used for evaluation. May contain
+      wildcards. For more information on wildcards, see
+      https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames. For
+        more details about this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_bigquery_source_uri: Google BigQuery URI to your instances to
+      run batch prediction on. May contain wildcards. For more details about
+      this input config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_predictions_format: The format in which Vertex AI gives the
+      predictions. Must be one of the Model's supportedOutputStorageFormats. For
+      more details about this output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_bigquery_destination_output_uri: The BigQuery project location
+      where the output is to be written to. In the given project a new dataset
+      is created with name ``prediction_<model-display-name>_<job-create-time>``
+      where is made BigQuery-dataset-name compatible (for example, most special
+      characters become underscores), and timestamp is in
+      YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset two
+      tables will be created, ``predictions``, and ``errors``. If the Model has
+      both ``instance`` and ``prediction`` schemata defined then the tables have
+      columns as follows: The ``predictions`` table contains instances for which
+      the prediction succeeded, it has columns as per a concatenation of the
+      Model's instance and prediction schemata. The ``errors`` table contains
+      rows for which the prediction has failed, it has instance columns, as per
+      the instance schema, followed by a single "errors" column, which as values
+      has ````google.rpc.Status`` <Status>``__ represented as a STRUCT, and
+      containing only ``code`` and ``message``.  For more details about this
+      output config, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
+    batch_predict_machine_type: The type of machine for running batch prediction
+      on dedicated resources. If the Model supports DEDICATED_RESOURCES this
+      config may be provided (and the job will use these resources). If the
+      Model doesn't support AUTOMATIC_RESOURCES, this config must be provided.
+      For more details about the BatchDedicatedResources, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchDedicatedResources.
+        For more details about the machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_starting_replica_count: The number of machine replicas used at
+      the start of the batch operation. If not set, Vertex AI decides starting
+      number, not greater than ``max_replica_count``. Only used if
+      ``machine_type`` is set.
+    batch_predict_max_replica_count: The maximum number of machine replicas the
+      batch operation may be scaled to. Only used if ``machine_type`` is set.
+    batch_predict_explanation_metadata: Explanation metadata configuration for
+      this BatchPredictionJob. Can be specified only if ``generate_explanation``
+      is set to ``True``. This value overrides the value of
+      ``Model.explanation_metadata``. All fields of ``explanation_metadata`` are
+      optional in the request. If a field of the ``explanation_metadata`` object
+      is not populated, the corresponding field of the
+      ``Model.explanation_metadata`` object is inherited. For more details, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata.
+    batch_predict_explanation_parameters: Parameters to configure explaining for
+      Model's predictions. Can be specified only if ``generate_explanation`` is
+      set to ``True``. This value overrides the value of
+      ``Model.explanation_parameters``. All fields of ``explanation_parameters``
+      are optional in the request. If a field of the ``explanation_parameters``
+      object is not populated, the corresponding field of the
+      ``Model.explanation_parameters`` object is inherited. For more details,
+      see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#ExplanationParameters.
+    batch_predict_explanation_data_sample_size: Desired size to downsample the
+      input dataset that will then be used for batch explanation.
+    batch_predict_accelerator_type: The type of accelerator(s) that may be
+      attached to the machine as per ``batch_predict_accelerator_count``. Only
+      used if ``batch_predict_machine_type`` is set. For more details about the
+      machine spec, see
+      https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec
+    batch_predict_accelerator_count: The number of accelerators to attach to the
+      ``batch_predict_machine_type``. Only used if
+      ``batch_predict_machine_type`` is set.
+    evaluation_prediction_label_column: The column name of the field containing
+      classes the model is scoring. Formatted to be able to find nested columns,
+      delimited by ``.``.
+    evaluation_prediction_score_column: The column name of the field containing
+      batch prediction scores. Formatted to be able to find nested columns,
+      delimited by ``.``.
+    evaluation_class_labels: Required for classification prediction type. The
+      list of class names for the target_field_name, in the same order they
+      appear in a file in batch_predict_gcs_source_uris. For instance, if the
+      target_field_name could be either ``1`` or ``0``, then the class_labels
+      input will be ["1", "0"].
+    dataflow_machine_type: The Dataflow machine type for evaluation components.
+    dataflow_max_num_workers: The max number of Dataflow workers for evaluation
+      components.
+    dataflow_disk_size_gb: Dataflow worker's disk size in GB for evaluation
+      components.
+    dataflow_service_account: Custom service account to run Dataflow jobs.
+    dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty
+      the default subnetwork will be used. Example:
+      https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
+    dataflow_use_public_ips: Specifies whether Dataflow workers use public IP
+      addresses.
+    encryption_spec_key_name:  Customer-managed encryption key options. If set,
+      resources created by this pipeline will be encrypted with the provided
+      encryption key. Has the form:
+      ``projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key``.
+      The key needs to be in the same region as where the compute resource is
+      created.
+    force_runner_mode: Indicate the runner mode to use forcely. Valid options
+      are ``Dataflow`` and ``DirectRunner``.
+  """
   with kfp.dsl.Condition(
       prediction_type == 'classification', name='classification'
   ):
-    eval_task = ModelEvaluationClassificationOp(
-        project=project,
-        location=location,
-        class_labels=evaluation_class_labels,
-        prediction_label_column=evaluation_prediction_label_column,
-        prediction_score_column=evaluation_prediction_score_column,
-        target_field_name=target_field_name,
-        ground_truth_format=batch_predict_instances_format,
-        ground_truth_gcs_source=batch_predict_gcs_source_uris,
-        ground_truth_bigquery_source=batch_predict_bigquery_source_uri,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_predict_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_predict_task.outputs[
-            'bigquery_output_table'
-        ],
-        dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
-        dataflow_disk_size_gb=dataflow_disk_size_gb,
-        dataflow_service_account=dataflow_service_account,
-        dataflow_subnetwork=dataflow_subnetwork,
-        dataflow_use_public_ips=dataflow_use_public_ips,
-        encryption_spec_key_name=encryption_spec_key_name,
-        force_runner_mode=force_runner_mode,
-        model=get_model_task.outputs['model'],
-    )
-    feature_attribution_task = ModelEvaluationFeatureAttributionOp(
-        project=project,
-        location=location,
-        problem_type=prediction_type,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_explain_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_explain_task.outputs[
-            'bigquery_output_table'
-        ],
-        dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
-        dataflow_disk_size_gb=dataflow_disk_size_gb,
-        dataflow_service_account=dataflow_service_account,
-        dataflow_subnetwork=dataflow_subnetwork,
-        dataflow_use_public_ips=dataflow_use_public_ips,
-        encryption_spec_key_name=encryption_spec_key_name,
-        force_runner_mode=force_runner_mode,
-    )
-    ModelImportEvaluationOp(
-        classification_metrics=eval_task.outputs['evaluation_metrics'],
-        feature_attributions=feature_attribution_task.outputs[
-            'feature_attributions'
-        ],
-        model=get_model_task.outputs['model'],
-        dataset_type=batch_predict_instances_format,
-        dataset_path=batch_predict_bigquery_source_uri,
-        dataset_paths=batch_predict_gcs_source_uris,
-        display_name=evaluation_display_name,
-    )
-
-  with kfp.dsl.Condition(prediction_type == 'forecasting', name='forecasting'):
-    eval_task = ModelEvaluationForecastingOp(
+    evaluation_feature_attribution_classification_pipeline(
         project=project,
         location=location,
+        model_name=model_name,
         target_field_name=target_field_name,
-        ground_truth_format=batch_predict_instances_format,
-        ground_truth_gcs_source=batch_predict_gcs_source_uris,
-        ground_truth_bigquery_source=batch_predict_bigquery_source_uri,
-        prediction_score_column=evaluation_prediction_score_column,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_predict_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_predict_task.outputs[
-            'bigquery_output_table'
-        ],
+        batch_predict_instances_format=batch_predict_instances_format,
+        batch_predict_gcs_destination_output_uri=batch_predict_gcs_destination_output_uri,
+        batch_predict_gcs_source_uris=batch_predict_gcs_source_uris,
+        batch_predict_bigquery_source_uri=batch_predict_bigquery_source_uri,
+        batch_predict_predictions_format=batch_predict_predictions_format,
+        batch_predict_bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
+        batch_predict_machine_type=batch_predict_machine_type,
+        batch_predict_starting_replica_count=batch_predict_starting_replica_count,
+        batch_predict_max_replica_count=batch_predict_max_replica_count,
+        batch_predict_explanation_metadata=batch_predict_explanation_metadata,
+        batch_predict_explanation_parameters=batch_predict_explanation_parameters,
+        batch_predict_explanation_data_sample_size=batch_predict_explanation_data_sample_size,
+        batch_predict_accelerator_type=batch_predict_accelerator_type,
+        batch_predict_accelerator_count=batch_predict_accelerator_count,
+        evaluation_prediction_label_column=evaluation_prediction_label_column,
+        evaluation_prediction_score_column=evaluation_prediction_score_column,
+        evaluation_class_labels=evaluation_class_labels,
         dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
+        dataflow_max_num_workers=dataflow_max_num_workers,
         dataflow_disk_size_gb=dataflow_disk_size_gb,
         dataflow_service_account=dataflow_service_account,
         dataflow_subnetwork=dataflow_subnetwork,
         dataflow_use_public_ips=dataflow_use_public_ips,
         encryption_spec_key_name=encryption_spec_key_name,
         force_runner_mode=force_runner_mode,
-        model=get_model_task.outputs['model'],
-    )
-    feature_attribution_task = ModelEvaluationFeatureAttributionOp(
-        project=project,
-        location=location,
-        problem_type=prediction_type,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_explain_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_explain_task.outputs[
-            'bigquery_output_table'
-        ],
-        dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
-        dataflow_disk_size_gb=dataflow_disk_size_gb,
-        dataflow_service_account=dataflow_service_account,
-        dataflow_subnetwork=dataflow_subnetwork,
-        dataflow_use_public_ips=dataflow_use_public_ips,
-        encryption_spec_key_name=encryption_spec_key_name,
-        force_runner_mode=force_runner_mode,
-    )
-    ModelImportEvaluationOp(
-        forecasting_metrics=eval_task.outputs['evaluation_metrics'],
-        feature_attributions=feature_attribution_task.outputs[
-            'feature_attributions'
-        ],
-        model=get_model_task.outputs['model'],
-        dataset_type=batch_predict_instances_format,
-        dataset_path=batch_predict_bigquery_source_uri,
-        dataset_paths=batch_predict_gcs_source_uris,
-        display_name=evaluation_display_name,
     )
 
   with kfp.dsl.Condition(prediction_type == 'regression', name='regression'):
-    eval_task = ModelEvaluationRegressionOp(
+    evaluation_feature_attribution_regression_pipeline(
         project=project,
         location=location,
+        model_name=model_name,
         target_field_name=target_field_name,
-        ground_truth_format=batch_predict_instances_format,
-        ground_truth_gcs_source=batch_predict_gcs_source_uris,
-        ground_truth_bigquery_source=batch_predict_bigquery_source_uri,
-        prediction_score_column=evaluation_prediction_score_column,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_predict_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_predict_task.outputs[
-            'bigquery_output_table'
-        ],
-        dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
-        dataflow_disk_size_gb=dataflow_disk_size_gb,
-        dataflow_service_account=dataflow_service_account,
-        dataflow_subnetwork=dataflow_subnetwork,
-        dataflow_use_public_ips=dataflow_use_public_ips,
-        encryption_spec_key_name=encryption_spec_key_name,
-        force_runner_mode=force_runner_mode,
-        model=get_model_task.outputs['model'],
-    )
-    feature_attribution_task = ModelEvaluationFeatureAttributionOp(
-        project=project,
-        location=location,
-        problem_type=prediction_type,
-        predictions_format=batch_predict_predictions_format,
-        predictions_gcs_source=batch_explain_task.outputs[
-            'gcs_output_directory'
-        ],
-        predictions_bigquery_source=batch_explain_task.outputs[
-            'bigquery_output_table'
-        ],
+        batch_predict_instances_format=batch_predict_instances_format,
+        batch_predict_gcs_destination_output_uri=batch_predict_gcs_destination_output_uri,
+        batch_predict_gcs_source_uris=batch_predict_gcs_source_uris,
+        batch_predict_bigquery_source_uri=batch_predict_bigquery_source_uri,
+        batch_predict_predictions_format=batch_predict_predictions_format,
+        batch_predict_bigquery_destination_output_uri=batch_predict_bigquery_destination_output_uri,
+        batch_predict_machine_type=batch_predict_machine_type,
+        batch_predict_starting_replica_count=batch_predict_starting_replica_count,
+        batch_predict_max_replica_count=batch_predict_max_replica_count,
+        batch_predict_explanation_metadata=batch_predict_explanation_metadata,
+        batch_predict_explanation_parameters=batch_predict_explanation_parameters,
+        batch_predict_explanation_data_sample_size=batch_predict_explanation_data_sample_size,
+        batch_predict_accelerator_type=batch_predict_accelerator_type,
+        batch_predict_accelerator_count=batch_predict_accelerator_count,
+        evaluation_prediction_score_column=evaluation_prediction_score_column,
         dataflow_machine_type=dataflow_machine_type,
-        dataflow_max_workers_num=dataflow_max_num_workers,
+        dataflow_max_num_workers=dataflow_max_num_workers,
         dataflow_disk_size_gb=dataflow_disk_size_gb,
         dataflow_service_account=dataflow_service_account,
         dataflow_subnetwork=dataflow_subnetwork,
         dataflow_use_public_ips=dataflow_use_public_ips,
         encryption_spec_key_name=encryption_spec_key_name,
         force_runner_mode=force_runner_mode,
     )
-    ModelImportEvaluationOp(
-        regression_metrics=eval_task.outputs['evaluation_metrics'],
-        feature_attributions=feature_attribution_task.outputs[
-            'feature_attributions'
-        ],
-        model=get_model_task.outputs['model'],
-        dataset_type=batch_predict_instances_format,
-        dataset_path=batch_predict_bigquery_source_uri,
-        dataset_paths=batch_predict_gcs_source_uris,
-        display_name=evaluation_display_name,
-    )
```

## google_cloud_pipeline_components/v1/model_evaluation/forecasting_component.py

```diff
@@ -8,14 +8,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import List
+
 from google_cloud_pipeline_components._implementation.model_evaluation import version
 from google_cloud_pipeline_components.types.artifact_types import BQTable
 from google_cloud_pipeline_components.types.artifact_types import ForecastingMetrics
 from google_cloud_pipeline_components.types.artifact_types import VertexModel
 from kfp import dsl
 from kfp.dsl import container_component
 
@@ -28,18 +30,18 @@
     target_field_name: str,
     model: dsl.Input[VertexModel] = None,
     location: str = 'us-central1',
     predictions_format: str = 'jsonl',
     predictions_gcs_source: dsl.Input[dsl.Artifact] = None,
     predictions_bigquery_source: dsl.Input[BQTable] = None,
     ground_truth_format: str = 'jsonl',
-    ground_truth_gcs_source: list = [],
+    ground_truth_gcs_source: List[str] = [],
     ground_truth_bigquery_source: str = '',
     forecasting_type: str = 'point',
-    forecasting_quantiles: list = [],
+    forecasting_quantiles: List[float] = [],
     point_evaluation_quantile: float = 0.5,
     prediction_score_column: str = 'prediction.value',
     dataflow_service_account: str = '',
     dataflow_disk_size_gb: int = 50,
     dataflow_machine_type: str = 'n1-standard-4',
     dataflow_workers_num: int = 1,
     dataflow_max_workers_num: int = 5,
```

## google_cloud_pipeline_components/v1/model_evaluation/regression_component.py

```diff
@@ -8,14 +8,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import List
+
 from google_cloud_pipeline_components._implementation.model_evaluation import version
 from google_cloud_pipeline_components.types.artifact_types import BQTable
 from google_cloud_pipeline_components.types.artifact_types import RegressionMetrics
 from google_cloud_pipeline_components.types.artifact_types import VertexModel
 from kfp import dsl
 from kfp.dsl import container_component
 
@@ -28,15 +30,15 @@
     target_field_name: str,
     model: dsl.Input[VertexModel] = None,
     location: str = 'us-central1',
     predictions_format: str = 'jsonl',
     predictions_gcs_source: dsl.Input[dsl.Artifact] = None,
     predictions_bigquery_source: dsl.Input[BQTable] = None,
     ground_truth_format: str = 'jsonl',
-    ground_truth_gcs_source: list = [],
+    ground_truth_gcs_source: List[str] = [],
     ground_truth_bigquery_source: str = '',
     prediction_score_column: str = 'prediction.value',
     dataflow_service_account: str = '',
     dataflow_disk_size_gb: int = 50,
     dataflow_machine_type: str = 'n1-standard-4',
     dataflow_workers_num: int = 1,
     dataflow_max_workers_num: int = 5,
```

## Comparing `google_cloud_pipeline_components/container/v1/gcp_launcher/utils/artifact_util.py` & `google_cloud_pipeline_components/container/utils/artifact_utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,24 +8,26 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+"""Utilities for working with artifacts."""
+
 import json
 import os
 
 
 def update_output_artifact(
     executor_input: str,
     target_artifact_name: str,
     uri: str,
     metadata: dict = {},
-):
+) -> None:
   """Updates the output artifact with the new uri and metadata."""
   executor_input_json = json.loads(executor_input)
   executor_output = {'artifacts': {}}
   for name, artifacts in (
       executor_input_json.get('outputs', {}).get('artifacts', {}).items()
   ):
     artifacts_list = artifacts.get('artifacts')
@@ -43,15 +45,18 @@
       exist_ok=True,
   )
   with open(executor_input_json['outputs']['outputFile'], 'w') as f:
     f.write(json.dumps(executor_output))
 
 
 # Writes a list of Artifacts to the executor output file.
-def update_output_artifacts(executor_input: str, artifacts: list):
+def update_output_artifacts(
+    executor_input: str,
+    artifacts: list,
+) -> None:
   """Updates a list of Artifacts to the executor output file."""
   executor_input_json = json.loads(executor_input)
   executor_output = {'artifacts': {}}
   output_artifacts = executor_input_json.get('outputs', {}).get('artifacts', {})
   # This assumes that no other output artifact exists.
   for artifact in artifacts:
     if artifact.name in output_artifacts.keys():
```

## Comparing `google_cloud_pipeline_components-2.1.0.dist-info/LICENSE` & `google_cloud_pipeline_components-2.1.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `google_cloud_pipeline_components-2.1.0.dist-info/METADATA` & `google_cloud_pipeline_components-2.1.1.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: google-cloud-pipeline-components
-Version: 2.1.0
+Version: 2.1.1
 Summary: This SDK enables a set of First Party (Google owned) pipeline components that allow users to take their experience from Vertex AI SDK and other Google Cloud services and create a corresponding pipeline using KFP or Managed Pipelines.
 Home-page: https://github.com/kubeflow/pipelines/tree/master/components/google-cloud
 Author: The Google Cloud Pipeline Components authors
 Author-email: google-cloud-pipeline-components@google.com
 License: Apache License 2.0
 Project-URL: User Documentation, https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction
 Project-URL: Reference Documentation, https://google-cloud-pipeline-components.readthedocs.io/
@@ -58,15 +58,15 @@
 ## Documentation
 
 ### User documentation
 
 Please see the [Google Cloud Pipeline Components user guide](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction).
 
 ### API documentation
-Please see the [Google Cloud Pipeline Components API reference documentation](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.41/).
+Please see the [Google Cloud Pipeline Components API reference documentation](https://google-cloud-pipeline-components.readthedocs.io/).
 
 ### Release details
 For details about previous and upcoming releases, please see the [release notes](https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/RELEASE.md).
 
 ## Examples
 *   [Train an image classification model using Vertex AI AutoML](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/pipelines/google_cloud_pipeline_components_automl_images.ipynb).
 *   [Train a classification model using tabular data and Vertex AI AutoML](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/pipelines/automl_tabular_classification_beans.ipynb).
```

## Comparing `google_cloud_pipeline_components-2.1.0.dist-info/RECORD` & `google_cloud_pipeline_components-2.1.1.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,108 +1,113 @@
 google_cloud_pipeline_components/__init__.py,sha256=3m702tYaSzOwehjieES8NyIdJULy3B0v3XssTc1hm5g,711
-google_cloud_pipeline_components/_image.py,sha256=43ZYPpwWFvz2J0R8E7vjbQU1vGGjXTxiD4xEq8Ui2BU,792
-google_cloud_pipeline_components/utils.py,sha256=tv5r1rppmj23JI0PsqVFaXL67H5hW5bTOaVCYHjFAzQ,10685
-google_cloud_pipeline_components/version.py,sha256=JwdWrNyI1Bec6p5xrrXuvgccvhlmIfsL1zzrpdAEOwk,677
+google_cloud_pipeline_components/_image.py,sha256=lANDYNk1WSuGZSoTTRcWdjsUvCkkA-PmwouTM9Et7fY,828
+google_cloud_pipeline_components/_placeholders.py,sha256=MoglmqYGMu8Jwf1kAX5Kas5gOfFOBTkrKGwn_ZqHmm4,859
+google_cloud_pipeline_components/utils.py,sha256=mktx_gGdnpf1AehUpWJxQFFFqkttvTXbRqouGAml6iQ,11048
+google_cloud_pipeline_components/version.py,sha256=O97kFYs2425SIf2erVQsXd9GA73AVuCKsBj0wTiuTUg,677
 google_cloud_pipeline_components/_implementation/__init__.py,sha256=sb6SfJl6rt3AKjiWxd-KO9DSiZ3PzGZRcsqKuc1A2Cg,606
 google_cloud_pipeline_components/_implementation/model/__init__.py,sha256=KmOW74re0WZ93DWM1lqqQYbv6w1aIW66BMV3gaAdg3s,811
 google_cloud_pipeline_components/_implementation/model/get_model/__init__.py,sha256=cXMkDUZHVSbXeXSa3qsI6Ef8Tad9nmusw5NUZaYORdE,662
 google_cloud_pipeline_components/_implementation/model/get_model/component.py,sha256=H2sbMTWCw8nMDMT-Ni9-pdzVXEFmHYjtP3z1LcI5m5w,2307
-google_cloud_pipeline_components/_implementation/model_evaluation/__init__.py,sha256=hnj1iIFJjtQZCA6qbWPI0XbNyyL-LLrfsHg5SUcbXiU,2478
-google_cloud_pipeline_components/_implementation/model_evaluation/utils.py,sha256=fdesuN7sVmmVJy5xqsCHj2B7IBqgknubCWjI0FYenQI,3936
+google_cloud_pipeline_components/_implementation/model_evaluation/__init__.py,sha256=Du4H9IIme9OLrMKP9o5582DFDUR3lh_EG5LkSf7U6LU,2673
+google_cloud_pipeline_components/_implementation/model_evaluation/utils.py,sha256=9V34RtPZSRNeBwcsImaZM6YC3T7CafT_E00Iby4KHxw,3540
 google_cloud_pipeline_components/_implementation/model_evaluation/version.py,sha256=mwMobBM7GmKtvmx6SCYEJWHajvYewe52g6kXXbwO-BU,963
 google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/__init__.py,sha256=wBCzT0CQxnhyR2lJQ6220nZEP9iAn_OqjYT_-bJTFSc,669
-google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/component.py,sha256=lmjeeb6QPO1A-GgzFCgSQZsNLO_n9FFxIeIzJjfiyf4,5422
+google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/component.py,sha256=y3CzEm94k3o76SKqKlD2yYhObBb5SWw0zThD2sx_n74,5452
 google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/__init__.py,sha256=s7-HxTa9wuPsOcxs9iN1gKrjIlAhoPLje96aFSqM1Vs,677
-google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/component.py,sha256=wo0rnPoHxJ7iozs7KcjSPtMLQQgCRN-cC0AdmeqC5pQ,7004
+google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/component.py,sha256=bBJfDO5RNgRnyIpbSyTEERDnzf5F5blvuZz0UycKsr0,7039
 google_cloud_pipeline_components/_implementation/model_evaluation/error_analysis_annotation/__init__.py,sha256=LSc-h1veE7xA7pVsP0LlamhQ4cTeN6X-ghAKyazdN7Q,671
 google_cloud_pipeline_components/_implementation/model_evaluation/error_analysis_annotation/component.py,sha256=xyJ89zWKj2yrQjTlbTgYd2DzJ6rqXe2MQGK-EETFP9E,4092
 google_cloud_pipeline_components/_implementation/model_evaluation/evaluated_annotation/__init__.py,sha256=UBMB8ThxUAXaLxUe4RP_0LpiQRdBuFpKBVC3ldaP7CM,677
 google_cloud_pipeline_components/_implementation/model_evaluation/evaluated_annotation/component.py,sha256=bmEEHGe_QtOshJBIktRq9F0TCRd9GJvLBEGiz2sjbhs,5842
 google_cloud_pipeline_components/_implementation/model_evaluation/feature_extractor/__init__.py,sha256=pIXVwg8rbGCJASN_DIapQ5czufmFP5SEcd5UtS-ffqc,674
 google_cloud_pipeline_components/_implementation/model_evaluation/feature_extractor/component.py,sha256=u4P3hSGBnFXtn8MKb8g1J9FAA74VpowBmPTmSDFq9J4,5539
 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluated_annotation/__init__.py,sha256=-znXeLR3Uhon4BsRcAqHdVhcSUCqkY7rFouQFLD6Atg,684
 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluated_annotation/component.py,sha256=d1QlNA_hpSJNgvwRJFGKZnVi0zc0KhZEHzVkq5wJWXQ,3053
 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluation/__init__.py,sha256=eG4D5YbXHkveV6PP1uX8ZL48LCZ1mUtKGKb520dpILg,674
-google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluation/component.py,sha256=I7u-xHU4ibdbbGHuygwGpwzbRzQ0vdtPG3p7Ax-PqTk,6563
+google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluation/component.py,sha256=9GbXoPqdLEg7AMmAEe9cYrczsH1rc0Y6NwAUuoZMz3g,7277
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/__init__.py,sha256=gDyltb_vTZRncaVZbMUkXYBHZsEg_CuaPAQVWUOGy7c,671
-google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/component.py,sha256=Z8r_E4AFrz1RUzcxYsoENyZIRLrOEgb-7XZU5yGqN4Q,7492
+google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/component.py,sha256=Aw5lteI4zJ2W-mssG-A3AQGJWogT2C4N1CmWeQJXexw,7090
+google_cloud_pipeline_components/_implementation/model_evaluation/rai_safety/__init__.py,sha256=Pz8FVgv-3ip6k98HqTmcmfPxOzJ5WSgvJMbx-RUPJko,662
+google_cloud_pipeline_components/_implementation/model_evaluation/rai_safety/safety_metrics_pipeline.py,sha256=d4kvVlIaF5yGniCljJ1ZviPyr7DH6lRxpBZ8sgAFOKI,7605
+google_cloud_pipeline_components/_implementation/model_evaluation/rai_safety/llm_safety_bias/__init__.py,sha256=Le723TFFExxtdhfmlAjc768Gpp5J5g1gVemVX-dfLzU,662
+google_cloud_pipeline_components/_implementation/model_evaluation/rai_safety/llm_safety_bias/component.py,sha256=tJfJPIPBJoKOWuK6aXMaiUNxrSQ26xE1fL0fakcRGoE,4185
 google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/__init__.py,sha256=B4VuH-pN_qGbJjaVNWB5b2vfdPP5yqqTphRNLukMY6o,682
-google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/component.py,sha256=42tolf76Yg5o1jqF5BQtT8hcjp6H4Xk9u21zjAYDzEI,5708
+google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/component.py,sha256=OeMON9Oms1xso5Emm4W5q6oUgcix9XWLlKoTmN-OUfI,5738
 google_cloud_pipeline_components/container/__init__.py,sha256=5X_ymPsBAGK9hpepKd1PR0opJLeeMmHvB2DR5YjAtCQ,651
-google_cloud_pipeline_components/container/cloudbuild.yaml,sha256=mREDp5CF73CybE-8QrmpcpmBB9rdc-MMeMVrdCVddPE,157
 google_cloud_pipeline_components/container/_implementation/__init__.py,sha256=Yz9TE5TRKlmUV6auolXWipHdEffdDa9XbXJ1P2uAjpo,659
 google_cloud_pipeline_components/container/_implementation/model/__init__.py,sha256=dqk5qgFRu3SpAWqLpiPjHke349yWKEWNVXiOmaZXxnY,668
 google_cloud_pipeline_components/container/_implementation/model/get_model/__init__.py,sha256=QuvCb1P_AiO6Wi5hZSaHhGICbzRqI5BMNIUe0PuDPzc,685
-google_cloud_pipeline_components/container/_implementation/model/get_model/get_model.py,sha256=Ts6wj_-KMQH4f-vCAb1baHzaNiMI_d28wssdWAu7ao8,4989
+google_cloud_pipeline_components/container/_implementation/model/get_model/get_model.py,sha256=BHZHxaXMFUPlhsRTmQZBrMf0DZd0R5rvMgG3enw70p8,4975
 google_cloud_pipeline_components/container/_implementation/model_evaluation/__init__.py,sha256=d0va9RCvobpiyxTwpxBhxrslwAUqm8SsWI-AUx2La5A,678
 google_cloud_pipeline_components/container/_implementation/model_evaluation/import_evaluated_annotation.py,sha256=PvTrdoklV6ZBaUZIwJn0_hv9pV3b2XYJMlTmafnEvMQ,10196
-google_cloud_pipeline_components/container/_implementation/model_evaluation/import_model_evaluation.py,sha256=WsJlxNcifnIM_LPQWdkHk_MlptG3EFwN0vdtesS5ts4,16626
+google_cloud_pipeline_components/container/_implementation/model_evaluation/import_model_evaluation.py,sha256=4qdRcmPnSU6CnQCPadfYr79Zep2sZdGtlC2cVU2w3n4,17219
 google_cloud_pipeline_components/container/preview/__init__.py,sha256=tOrlxtIqA4e41GJDdcldd7y32wrWjZvwT6Hq8R33l1I,651
 google_cloud_pipeline_components/container/preview/dataflow/__init__.py,sha256=Dg_Omfhb8BmeW2N65q1TS7JBamUDVbjNrAy7sNdUcTo,655
 google_cloud_pipeline_components/container/preview/dataflow/flex_template/__init__.py,sha256=1X5undw-V6YQ4FedmyVmEqdaPg3dG-f08krz1UOAHfw,698
 google_cloud_pipeline_components/container/preview/dataflow/flex_template/launcher.py,sha256=yB1ZfK3lGtvNGrrN_9V2bYc-mjJ2eWUKiN_PxKy6GCM,1914
 google_cloud_pipeline_components/container/preview/dataflow/flex_template/remote_runner.py,sha256=1_aAu2ZEeHjoeOeqjUxbwkesqyn9fItCAwrR8Gk4cqg,9632
 google_cloud_pipeline_components/container/utils/__init__.py,sha256=yqPd0QW1ITPpcgd-0Fv5665kTX1HRPNutCfkBXnA_uA,638
+google_cloud_pipeline_components/container/utils/artifact_utils.py,sha256=VEb89NTTcyiy_ouO0ysosUv3nDwj9QfI_EbZnX9kHZk,3027
 google_cloud_pipeline_components/container/utils/execution_context.py,sha256=KRB33N2wCOk95zHDbo5WS6cMB_jxaXEAEW0VhJSlX9U,1843
 google_cloud_pipeline_components/container/v1/__init__.py,sha256=iEvOrKaUA31vCoQeMAr7EBPKPkIwv5iLQ1tChsJok8w,651
 google_cloud_pipeline_components/container/v1/aiplatform/__init__.py,sha256=5X_ymPsBAGK9hpepKd1PR0opJLeeMmHvB2DR5YjAtCQ,651
 google_cloud_pipeline_components/container/v1/aiplatform/remote_runner.py,sha256=FciZ9QKRRDu7CI8Op-NjQaQbfpP28PxJxg1Tj-_mD-M,10719
 google_cloud_pipeline_components/container/v1/aiplatform/utils.py,sha256=RAuXI-y_gKZRLMfYLSkIvkYXG_VM6UrKLBybXhtu8b4,5257
 google_cloud_pipeline_components/container/v1/batch_prediction_job/__init__.py,sha256=em4AGJlxjcHXhBO6Ot1DYFOKQIfS9FZcXC03UEEY36g,696
 google_cloud_pipeline_components/container/v1/batch_prediction_job/launcher.py,sha256=5G1WjhKfCHutXIJmDz0TH2S3eYszoEy5fXEekIZ4h0s,2364
-google_cloud_pipeline_components/container/v1/batch_prediction_job/remote_runner.py,sha256=asn1jIJatQ6aEiaPkFb-60hpgNE9MYxRozcw-D6VpSM,9386
+google_cloud_pipeline_components/container/v1/batch_prediction_job/remote_runner.py,sha256=Ip_usUbbb6YCYLqsAnPWfEZRZ-gGnTFtOgOr2sdPq4M,9372
 google_cloud_pipeline_components/container/v1/bigquery/__init__.py,sha256=tAYaZ_eLY0QvAPZV21l2WO4q6K9B1rR7vPaHCU4wpq0,655
 google_cloud_pipeline_components/container/v1/bigquery/create_model/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/create_model/launcher.py,sha256=X4cozg8LyIeu-x8441AYxlf2HRlwieWobB7NZ1IxSuA,2502
-google_cloud_pipeline_components/container/v1/bigquery/create_model/remote_runner.py,sha256=M0uxqEZLZIpb_fYoTjra-TIlboYihQmhXx7-zZHoLSQ,3839
+google_cloud_pipeline_components/container/v1/bigquery/create_model/remote_runner.py,sha256=WicYUPxbdGWW6l1i4N7kvDWlHJU1McBDIFLs9WZGVH0,3825
 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/launcher.py,sha256=6-NO6fadITZJw4C5kbO9oDWe3PSuuhddQrXWWvcNimk,3284
 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/remote_runner.py,sha256=cs9P92jxu9KGYuaBzedw-K6ki9lgD8uHEDBvfn0r4WM,5770
 google_cloud_pipeline_components/container/v1/bigquery/drop_model/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/drop_model/launcher.py,sha256=5IYPtwyGzz27BfQZz5a7oif-079weIGJGsSQ7dRa1lE,2627
 google_cloud_pipeline_components/container/v1/bigquery/drop_model/remote_runner.py,sha256=H7SDJ1JGjnJijaeXQc6m6Iha99Y98VRBJBlM0FG_OUU,3306
 google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/launcher.py,sha256=rHqmp6b4LyiPj1G7UY6xyvqur_Nj93NSjk9lDyDu5V4,3307
-google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/remote_runner.py,sha256=D0o36dRDiygDHN4ODT3BeJGkchVLFbJyeh43A9lqKEs,5588
+google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/remote_runner.py,sha256=n0nLhDQicaoViEZ0JX1Z9pLaQRBgPgc6r7cVyKEZWf8,5574
 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/launcher.py,sha256=gDjInVz0chZ6OCesX0w51G-bfAzAQfHNcKLf-SrvmsE,2958
 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/remote_runner.py,sha256=wIKyzObg_eum0eERerkKI7u58qhcbe5GM8grb0RUVQQ,3432
 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/launcher.py,sha256=45cRkHVdPLSYyiiJAKNy5rVhAFuhJjrfq3DaK8fIun4,3639
 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/remote_runner.py,sha256=SIEoI6LsR-xvOqUj-vt6mqOgIWuaH27v0TEwIf88yAU,5843
 google_cloud_pipeline_components/container/v1/bigquery/export_model/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/export_model/launcher.py,sha256=Blxbakmw41jHIPshplAZeeTmB2Pg7lAN0b8LRLdDhFQ,2775
 google_cloud_pipeline_components/container/v1/bigquery/export_model/remote_runner.py,sha256=1DHBsQhvcHmj3-fBnnY8fPVmW067OTwewpOvLRSB9pY,4706
 google_cloud_pipeline_components/container/v1/bigquery/feature_importance/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/feature_importance/launcher.py,sha256=YJ-o-rDrIfDm4T7lH_oBBBekZXVjhp1T00BHOzWEHoU,2656
-google_cloud_pipeline_components/container/v1/bigquery/feature_importance/remote_runner.py,sha256=8eizuRxrtkrDiZbDJwczSvpPehi6bv6aB_uO99wb2as,3576
+google_cloud_pipeline_components/container/v1/bigquery/feature_importance/remote_runner.py,sha256=9Zy3q1kAsc4_OerzwGz7WxNBx18J_RuYvE_B3FNhDVY,3562
 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/launcher.py,sha256=rI5LWG3A3L92Hj4OIwtN7rgWY3Bqv0XOClXdByFDzuA,2932
 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/remote_runner.py,sha256=eN8kTRWz9mTXzR9Y1BMcH_VKlv9yKPSNTKdRwLMHqwc,3384
 google_cloud_pipeline_components/container/v1/bigquery/global_explain/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/global_explain/launcher.py,sha256=wNT9G2FfEtDfbNfcfcbmvjiLfbkmsTPYjlKKYxpioeM,2807
 google_cloud_pipeline_components/container/v1/bigquery/global_explain/remote_runner.py,sha256=NHXEI_YtnXTHTh9aiOWq_GzvDpwXnHEmt_LaPw2ckyk,2676
 google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/launcher.py,sha256=dek6bTsONFxz5zouOaItBw6KEkeBCnEpiFTE1iTwQeg,2653
-google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/remote_runner.py,sha256=ErP1uBKiNuJYlDnjIOnQR7T4099uAbCjglxDWd6ap9o,4060
+google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/remote_runner.py,sha256=ry2CVw-CPsFt0100-DXmCA004jrxP3uc6akxsCK7vbM,4046
 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/launcher.py,sha256=jNUlI8Yw24wtC1AShBAzkIalLQItxyX_PdbJefsTLZw,2655
-google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/remote_runner.py,sha256=qkn8G1xUUlOIHyDQ2woM0f84mNYhj94jkADzQ9bGCDA,3455
+google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/remote_runner.py,sha256=4JPpRrirTZNkBrYPInXQJF71dXnKwqACXmEklBtVWC0,3441
 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/launcher.py,sha256=iKQ5u0PCAF8Y6EhAJwryYhKchcxj3gos0GfJ_mDJ4jY,2822
-google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/remote_runner.py,sha256=NNG28Qj034DfG3P0TsDjSciX_ot4-x3_b5tJPRie7lQ,4676
+google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/remote_runner.py,sha256=-2cj2lb2BuN1f63BhZ_rIrrVmxdUYo9GRfl5QgmWcow,4662
 google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/launcher.py,sha256=YOwaKfQq76JvnuHPwHlVgfTtRQi2GyZMIrnLOk6uAvk,2780
-google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/remote_runner.py,sha256=4Rioe7TEzTC6Zk00FVl79WcAkTpaLwurokcbsUl4VUs,4651
+google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/remote_runner.py,sha256=N1vkExxwyPbgwdtVgD_oD8UlLzlzP0bOLpkLV5EnzsY,4637
 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/launcher.py,sha256=UMrQUxvyipiZWEgVET5bPxZSq97Kl78p-Jl6LUTTN3g,3319
 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/remote_runner.py,sha256=FNwQh7kL2tOdcufwQocuV6mLq7mggY_NecS_0hqfdzQ,4731
 google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/launcher.py,sha256=qkJbYooxBo_tzOoy2V29WC_Wd37ZuZ5rWo40t3_GBK8,2641
-google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/remote_runner.py,sha256=Di-occnXoCJxESEchhtdrjoOxkIDH5GSInuOwrooh9c,4032
+google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/remote_runner.py,sha256=1diR2kW8TgAYTZPXBk0x1ENhm2Z5M7ehWwSnm7AXiL4,4018
 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/launcher.py,sha256=LUdUeBWr2XJ64LABKRk7YDDLlXSXuXGopifUiV0RlmI,2676
 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/remote_runner.py,sha256=4wS4a4G54zviVg9h8LvZv_WRzba2LiuzPncYyMN-GCI,3194
 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/launcher.py,sha256=896aaJBWU2S0Cc-y8cStBVfQBEhkZUipMXRnjMNU89o,2665
 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/remote_runner.py,sha256=kPssPHPndKr4fXNXkNd6BWRJwx3wmxUwCpG731730Zg,3166
 google_cloud_pipeline_components/container/v1/bigquery/ml_recommend/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
@@ -112,29 +117,29 @@
 google_cloud_pipeline_components/container/v1/bigquery/ml_reconstruction_loss/launcher.py,sha256=Pzp9iCNXen4xiS8uPCJOptUf0KhImeFwyc--RZ41tRY,3115
 google_cloud_pipeline_components/container/v1/bigquery/ml_reconstruction_loss/remote_runner.py,sha256=-0aM2moIF8WnX9NHdmCAEOdSwUrz8AMWd17-jIFWcQM,4337
 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/launcher.py,sha256=6xtAhB4CQEc44ueyjrYHniPaBERCst2vekiMbOvk-Lo,3302
 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/remote_runner.py,sha256=0VW5mobOcktuzzipv-fjezSNe2__CU26gfNBKCv8rck,4613
 google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/launcher.py,sha256=iX4GQUR_o0wB0AOzVUg_CuyiIbntNrAPkpEaODsjLkM,2644
-google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/remote_runner.py,sha256=sG7-hbRv9cGbII-uY8FwTOzpv1LibqJwSjWFn_x0NCo,4046
+google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/remote_runner.py,sha256=TcqHSpmRAZ9W1prl_2zkOqleMLgFuOE9vgnBJ7gQ2Rw,4032
 google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/launcher.py,sha256=KnZBgIt2txWZhOJ0XI5Uxfmi_xHBNiGlJI3FRu7Iy8g,2635
-google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/remote_runner.py,sha256=75rdmo0nVEdm29ZOQicaYRyjvUKoW20P-Wb4oTutw4M,4225
+google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/remote_runner.py,sha256=tNw7H_SqmSNgPAaykfz179RrhN5QPoMi15KWN05OwsA,4211
 google_cloud_pipeline_components/container/v1/bigquery/ml_weights/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/ml_weights/launcher.py,sha256=teSvUVQNyi_g3WShvbMyg--U_EtMnpO7yWzbUZZuPA4,2627
-google_cloud_pipeline_components/container/v1/bigquery/ml_weights/remote_runner.py,sha256=pE5tQW-3DO3mU7bMIXfcgtB7wCKDOFZhW0xBsHNQVC4,4015
+google_cloud_pipeline_components/container/v1/bigquery/ml_weights/remote_runner.py,sha256=tpwgPRmlxuyOfdgVXZVNtm_s6M8IHLGeKzAK29wKQN8,4001
 google_cloud_pipeline_components/container/v1/bigquery/predict_model/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/predict_model/launcher.py,sha256=AP24wlQhegJvbFoP5wKYDXWz_aQvVqaAx8ufMgh46hU,3302
 google_cloud_pipeline_components/container/v1/bigquery/predict_model/remote_runner.py,sha256=2_wd3gvTAGorkx-pgkL92iQbXk310yV7guEXzKZwES8,4578
 google_cloud_pipeline_components/container/v1/bigquery/query_job/__init__.py,sha256=CC9fUYXpc6qvZxp1JAmE9xoVSDbhbPQZvvmUZ9em2m0,688
 google_cloud_pipeline_components/container/v1/bigquery/query_job/launcher.py,sha256=izrzvX3cUQ--M343UdnQZbGUKrtXynFR5hLc3HIsKdo,2473
 google_cloud_pipeline_components/container/v1/bigquery/query_job/remote_runner.py,sha256=Mgk4eCR0hr0n6alc0MMoO1_RpfDxQnYnY7xuIvecviQ,2539
 google_cloud_pipeline_components/container/v1/bigquery/utils/__init__.py,sha256=8RQBa9PX4YO75nH5qDD9Qm2IgfJA5WqtOutxN3XCQw8,673
-google_cloud_pipeline_components/container/v1/bigquery/utils/bigquery_util.py,sha256=JuCAfE5AttfjMEJlSgn96vw1DlLr134m_WMgdo66UJI,12250
+google_cloud_pipeline_components/container/v1/bigquery/utils/bigquery_util.py,sha256=FWiZqhSZOmtKi-wdFDGeqYSNc0Y-4R6xfiz13915fWA,12236
 google_cloud_pipeline_components/container/v1/custom_job/__init__.py,sha256=KK7TRANpB8dE3H8VNxzSUDIVWaLBHNM9BfDF8JvcwXk,686
 google_cloud_pipeline_components/container/v1/custom_job/launcher.py,sha256=oVtxYK-67fpq3SWxh1e5yVhJBT8m5HtxUHcc2MwnEQo,1917
 google_cloud_pipeline_components/container/v1/custom_job/remote_runner.py,sha256=Qt0lMUlvqn9CPaPwp9uV1l_uHzIgvaJ8y3XEyqmJW9s,3586
 google_cloud_pipeline_components/container/v1/dataflow/__init__.py,sha256=jEGX3Bu1tJZ0jZbLIS-PSKGqP77BLMtKwCjosm-hBMI,662
 google_cloud_pipeline_components/container/v1/dataflow/dataflow_launcher.py,sha256=0XDqt8bN4-0y5jGjXeh8aWf81wUse-4J04NrVu0orQo,3303
 google_cloud_pipeline_components/container/v1/dataflow/dataflow_python_job_remote_runner.py,sha256=VBpBsnRMW6MX7ZFkZl8JjYHe4yi1Zc8b2TejigaYo7g,7415
 google_cloud_pipeline_components/container/v1/dataproc/__init__.py,sha256=86deeMjtoAhOHV43oG-8KHKSt6KLAi-gJK3s4QbmLm8,699
@@ -151,30 +156,29 @@
 google_cloud_pipeline_components/container/v1/dataproc/create_spark_sql_batch/launcher.py,sha256=YmrSvObs4xq1jAJlBORGbvDgSmhIqYSg54yELLWOrvs,2203
 google_cloud_pipeline_components/container/v1/dataproc/create_spark_sql_batch/remote_runner.py,sha256=nFxLbQ_3vhUSzDAIUMP0dwMVOknQ-OIE_EF9K0nrVS0,808
 google_cloud_pipeline_components/container/v1/dataproc/utils/__init__.py,sha256=a7yfDdTKIg8eL1tuSIREMYTYLos5VuFRL2hCPf7KMN4,663
 google_cloud_pipeline_components/container/v1/dataproc/utils/dataproc_util.py,sha256=Mg0BRx12xzg23xitxXokQco3r6aMWfZzUPSZPZgc7ho,12872
 google_cloud_pipeline_components/container/v1/endpoint/__init__.py,sha256=00o_h8FE3ULeXd3u5m-xVR6IXSNVxee6Vs2By8I_Pf4,655
 google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/__init__.py,sha256=fpx23qlR-Z3-hZjOHxMugKAi7GdF0NoQc_mXK--oAcI,691
 google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/launcher.py,sha256=6C4PA9VzuNxujy0ESAS3zoH-naqneBFYuPeeHy3o1w8,2288
-google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/remote_runner.py,sha256=NUvcaHBbLf3FUGJuZTu8m6zdV4snPdsi8iCTTSG88Hg,2377
+google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/remote_runner.py,sha256=WbIkZy00TIA3jYCN_UUYJbh5Sp4thxaJ1HE9uieBeLs,2363
 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/__init__.py,sha256=zf_zkAIQA90Bo98oxBZIL33SDuNJOrY3f4XP9hQwO1A,691
 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/launcher.py,sha256=A9siAG_Zzcuvh23jmKLOwJzOvZKvqwvJEyG6rQmBxrc,1938
 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/remote_runner.py,sha256=fQy0iV1ooDLBT5J7CzvFj6vrRbd4mAhDOtmOXsJSmQc,2308
 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/__init__.py,sha256=3VOp10rPtpBzhxum0Jidrv0UtvVa-b3Y0zbs6XxsFUY,688
 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/launcher.py,sha256=GIyHuo7s2l-IirHRaE6_9d-Ir0F1z3pjS6iRSIgkEJE,1926
 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/remote_runner.py,sha256=FZCbHX_qXLVJBETlI2zuIdvYlHsUPjDxTxs79pbGyzM,2375
 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/__init__.py,sha256=HCDEiDq8pcnCovppueFD_AT8NPfDjLBpq6XxKXb83uY,690
 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/launcher.py,sha256=3BBWj0LjypOuD1BYA9IhfiV512D2wiFMNd7SYQnfT9g,1934
 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/remote_runner.py,sha256=965Cayk5CtrRNhwZALUfFxApxv3nPYkZoVDVMe4UNwg,4178
 google_cloud_pipeline_components/container/v1/gcp_launcher/__init__.py,sha256=ztZbE-kSAQ9oltydyygstqjCyXudx3YadDCmkG6DVVY,662
 google_cloud_pipeline_components/container/v1/gcp_launcher/job_remote_runner.py,sha256=Oy87-QV-PczvkQZFeC-TXiJg3vQ8KyAf6TIMN-QglaE,8350
 google_cloud_pipeline_components/container/v1/gcp_launcher/lro_remote_runner.py,sha256=omVnaLxEU0g2xH2M3oiOOqOUabuXOPJNASp-yYu14JI,4945
 google_cloud_pipeline_components/container/v1/gcp_launcher/pipeline_remote_runner.py,sha256=62VBsT335ivoMFZX3rxc6eJeaQMqjnX9XawULuYvcS8,10315
 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/__init__.py,sha256=8-SKVsI-q5QCyXBtuS4d6OlrfzlgnOrICbbPnCQfKi4,660
-google_cloud_pipeline_components/container/v1/gcp_launcher/utils/artifact_util.py,sha256=MZWB_AH5_jhpOoWCI2xXUiYEzdZW_Sj568pIOd0HGIk,2955
 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/error_util.py,sha256=Vvqro5eU6Gs5fkLVVFMCl1cUckeIKC4caRFriqyX4_o,876
 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/gcp_labels_util.py,sha256=S3uLQ7vVd9hRlvrMFJDDmhMGIdTHgdAF9geLAFV0nVk,2193
 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/json_util.py,sha256=QPbmiPTyFbiQEmDYbjQK0E2hx0GrWfQfYh_EyxWoHdc,2951
 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/parser_util.py,sha256=p3J2-9Rbl8xnJd7n6zeC-Tuh3fdhy8H3WLzF61pzGSg,1788
 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/__init__.py,sha256=RJ-GWSeh3uYEAQg8s_SWCR05NqBTPAaEFs0lAZxvPJQ,701
 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/launcher.py,sha256=1tPL1udEfXtDMQCwIcdECnXQdbRMFaXd6pu6BOhvAnc,2240
 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/remote_runner.py,sha256=nnL4PPGgNw0ONMVjjJKQgp4VQsY_jtuhPpjk4dLKZxQ,4402
@@ -186,15 +190,15 @@
 google_cloud_pipeline_components/container/v1/model/delete_model/launcher.py,sha256=KW_URGz1eyH8bFG-NnqE5wMdftntXlKK1KYSKR479rk,1923
 google_cloud_pipeline_components/container/v1/model/delete_model/remote_runner.py,sha256=PKzaVpwm6nXyyV7QfSiSbtigLFvMMjv0PwZe7at5LUM,2307
 google_cloud_pipeline_components/container/v1/model/export_model/__init__.py,sha256=X-GJYtcyiTQhUZR9LK3iQfMg4uQWKC0QJJHxuuDEWSM,688
 google_cloud_pipeline_components/container/v1/model/export_model/launcher.py,sha256=vlK1KjGyse10PR8r0ieLnajdF2GcFqjO5ELz6AVnjnw,2201
 google_cloud_pipeline_components/container/v1/model/export_model/remote_runner.py,sha256=YJwkjq5UhJtpX43pjqGs09dNPWpjJqct6cRAHDFBmZk,2479
 google_cloud_pipeline_components/container/v1/model/upload_model/__init__.py,sha256=NZJ1pbjANZ1XDV-RvjtJcKX7Bdhw39_Eh_DRMq6vOko,688
 google_cloud_pipeline_components/container/v1/model/upload_model/launcher.py,sha256=QwnM9K43By8MNM9_iiIJH7LexeI6WLA5-V8iRS2S-0Y,2331
-google_cloud_pipeline_components/container/v1/model/upload_model/remote_runner.py,sha256=iqCMymDwcIV0laRBxEsLEFhRSSWuOHEaYogzlxeIS9Q,4405
+google_cloud_pipeline_components/container/v1/model/upload_model/remote_runner.py,sha256=Nn9R_QGueldfoB03ofhsu1mn5K2PcfP_TA5CgyyfL0M,4391
 google_cloud_pipeline_components/container/v1/vertex_notification_email/__init__.py,sha256=p3LsPxd6b8FpOTqoFdnGDE9pH7FbVntFT1G0vWqQM_o,662
 google_cloud_pipeline_components/container/v1/vertex_notification_email/executor.py,sha256=NWlooFYXMgsBFw_XEiBYJN8pEOwjPMBTseXnUXJBYSE,1074
 google_cloud_pipeline_components/container/v1/wait_gcp_resources/__init__.py,sha256=AEioMAXo4wGRE2vvdduUOIraafLiS5GngRk_0_HZN88,694
 google_cloud_pipeline_components/container/v1/wait_gcp_resources/launcher.py,sha256=Emm-dcZM5VhADG4ygWMQeWvf4kvZiettecr58lN9EBE,1928
 google_cloud_pipeline_components/container/v1/wait_gcp_resources/remote_runner.py,sha256=T9hsFy-Uih360V_Rv3OQILFONSOrFEcz2VzEEgi-ua0,5968
 google_cloud_pipeline_components/preview/__init__.py,sha256=sb6SfJl6rt3AKjiWxd-KO9DSiZ3PzGZRcsqKuc1A2Cg,606
 google_cloud_pipeline_components/preview/automl/__init__.py,sha256=sb6SfJl6rt3AKjiWxd-KO9DSiZ3PzGZRcsqKuc1A2Cg,606
@@ -235,28 +239,29 @@
 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_medium_search_space.json,sha256=MktCbOyMLDqG_3bvqP104HJL-4gnAEsYIsKpHZlLSgg,3254
 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_small_search_space.json,sha256=58I8OBUYitt8-nFWU_C1ujBvAtetbbAY5kIDlmVJX50,2966
 google_cloud_pipeline_components/preview/automl/tabular/configs/wide_and_deep_params.json,sha256=g8P6ZBjoQvN8ZDHmUKl2rHZLGMXgoTIEoj9Xmrob0qk,2647
 google_cloud_pipeline_components/preview/automl/tabular/configs/xgboost_params.json,sha256=r5Uwl49bs4vGKlWGm-LrpyJHZRBD2uap1QH6-tzqa_g,7748
 google_cloud_pipeline_components/preview/dataflow/__init__.py,sha256=R0LJ89NGicO8093QKjWtA_An_23OBakyrPuFS4zhj_k,828
 google_cloud_pipeline_components/preview/dataflow/flex_template/__init__.py,sha256=uG42x7_0zehtVV7f_fHvPHBJ48aqi3jJwLY6tplH8jk,669
 google_cloud_pipeline_components/preview/dataflow/flex_template/component.py,sha256=AmLChb4dne5_DruIn7q3qnhR-LhpRbw1gc49Wdiyyjc,11929
-google_cloud_pipeline_components/preview/model_evaluation/__init__.py,sha256=gl3NSSf_aTEPJwFeHM23BJOv_ugD1U40HKeJeMyqSDQ,1173
-google_cloud_pipeline_components/preview/model_evaluation/data_bias_component.py,sha256=lBcdprCRqDV0CO62Lny-X9yx62XFw_4cbP-GWsa_5zI,5909
+google_cloud_pipeline_components/preview/model_evaluation/__init__.py,sha256=YIiFmGdGQEMUjJEsJ1goOmJJTHigX9yakVz9h7mVjUo,1395
+google_cloud_pipeline_components/preview/model_evaluation/data_bias_component.py,sha256=YfnWwZsAsM18HWHnPitIM4TguBKTYGadc8DxFnIANWs,5955
 google_cloud_pipeline_components/preview/model_evaluation/feature_attribution_component.py,sha256=F4MEULNzoPIv1mLl0R0cSNOitf6BTXN19-C3AiDhYjY,7509
+google_cloud_pipeline_components/preview/model_evaluation/feature_attribution_graph_component.py,sha256=2um1sWzoeJwy5a-0Zw-Z5NnXar62HETypvh3w8CN92g,14063
 google_cloud_pipeline_components/preview/model_evaluation/model_bias_component.py,sha256=bND8J-EaG12iF2W_OURra_jFPJJGKvFlV9aXzXc1Ffs,6842
-google_cloud_pipeline_components/preview/model_evaluation/utils.py,sha256=9YRWLKmx0Ej-z8nlM_zng3D6ClMpoHAAmHEBsZJiEY0,7567
+google_cloud_pipeline_components/preview/model_evaluation/utils.py,sha256=KLMvS2nxMCGLQT6JZnqL0hphfzxz70YafVVX3nhRk50,7548
 google_cloud_pipeline_components/proto/__init__.py,sha256=aiPUc6gpQwG9cRTYfw3ChFCJfDr3vAIsm2eMYUDJjJQ,661
 google_cloud_pipeline_components/proto/gcp_resources_pb2.py,sha256=EYn1KnHwuH6-Q7eHurgqYx_zA-tjBCqN2yncg07SAAY,7687
 google_cloud_pipeline_components/types/__init__.py,sha256=1WFkL49QEy-gNb6ywQOE4yZkD7DoULAeiL1tLdb3S28,606
-google_cloud_pipeline_components/types/artifact_types.py,sha256=HK1nlCyGIg3F1TsPu5FZ6eQD2Vqac6y6wS1b8M5Ykv4,23260
+google_cloud_pipeline_components/types/artifact_types.py,sha256=DN-MpMvilEYzczL6uqe8a8COaZ_DWPYIwVd6EAJNN6I,23845
 google_cloud_pipeline_components/v1/__init__.py,sha256=E1Fie3Gq3KKLHEBGUUBeuao-Eo1uwuBfAWWhx2siwaE,765
 google_cloud_pipeline_components/v1/automl/__init__.py,sha256=l8whL8MMhZ-KMyacLpGzO-5cNxxD2wUW6PtMVx0C_mI,631
 google_cloud_pipeline_components/v1/automl/forecasting/__init__.py,sha256=cGZ52eyY83aDKd-fTIkQCDsAX6n7feK9KcKkMg3SHeE,806
 google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_predict_pipeline.yaml,sha256=3LYn-i8QSoCFwwTjewrlg25gt9NYvmr960OIz6_OMUQ,52018
-google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_train_pipeline.yaml,sha256=utZZTNTXIjo6Iwc-x7S5hWJGPsGekIQ0xlKZD_YNToU,246379
+google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_train_pipeline.yaml,sha256=yh6b--sPAkW-026rOJkt1D1mQSmXpSujFSiLnVlr4Y8,246248
 google_cloud_pipeline_components/v1/automl/forecasting/prophet_predict_pipeline.yaml,sha256=nLrMCPvtuCLzpFEIK-KbruLElD3a3Gk5fI7a3G_ZLZw,94627
 google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer.py,sha256=JJaYo2F3K12zZ2CuBxesLaBWq5fY5d1H2d0y-QnTkok,8498
 google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer_pipeline.yaml,sha256=zi1j2OJEDemVKWLammA_9ifrP8uQLMPMi52xthFOmg4,144373
 google_cloud_pipeline_components/v1/automl/forecasting/utils.py,sha256=xf3nJR2riWDDxep8mFvaqOeTztRGrf7QLFEIxLkqSBU,15028
 google_cloud_pipeline_components/v1/automl/tabular/__init__.py,sha256=d2M85nOLZc_yySj3B5TyWC0KGB9os9aFRVE5NddGfa0,2034
 google_cloud_pipeline_components/v1/automl/tabular/automl_tabular_pipeline.yaml,sha256=BuSWaGzz3XL0_WIuWNRALhnn7uyrAYU8Nk3KhW_s708,502269
 google_cloud_pipeline_components/v1/automl/tabular/cv_trainer.py,sha256=eFB---KimYsKRW5K7QDwZIUdirrpa7QMjwWapDUDTrU,6756
@@ -403,25 +408,25 @@
 google_cloud_pipeline_components/v1/model/delete_model/__init__.py,sha256=vH1ffcOuoYGAkK3Jtks2QCQqBswzyDS-zkHIrT9MpyA,638
 google_cloud_pipeline_components/v1/model/delete_model/component.py,sha256=huTJqMhUdrng4wpwERpZULX-laQXMsyjrNPF8nCH0Dc,2448
 google_cloud_pipeline_components/v1/model/export_model/__init__.py,sha256=c4fG2x5dxyMDY4m9YrqJK67qqSVa3W7HA0E0jCO2OYQ,661
 google_cloud_pipeline_components/v1/model/export_model/component.py,sha256=ZRW1mJs7Rf4oQzhsIE7whhHTpLgPTVJxM1VS-7j22s0,5119
 google_cloud_pipeline_components/v1/model/upload_model/__init__.py,sha256=6uwVQw6h3TXxei5imUE4JaS97XXzDRPQyNnTE-qFjck,661
 google_cloud_pipeline_components/v1/model/upload_model/component.py,sha256=4X7EFYFyJszfAAIdh2QJv_X1rQ1gCIdSRBfdIKh8k14,6743
 google_cloud_pipeline_components/v1/model_evaluation/__init__.py,sha256=uJjE6fTAgrnI5hyf1DtSV-syWh14Jn1I-o72pddIifA,2359
-google_cloud_pipeline_components/v1/model_evaluation/classification_component.py,sha256=AqDCSH7j5lUOmkapwXkPL38MezjTzjL_9KOXZCjw3H8,12567
-google_cloud_pipeline_components/v1/model_evaluation/error_analysis_pipeline.py,sha256=XPHUF8YL6nYfhMtmttZRbdc83IPP50Rtxl1rpH6ZTE4,20373
-google_cloud_pipeline_components/v1/model_evaluation/evaluated_annotation_pipeline.py,sha256=r85TZRqJn-m7zLjEjcMtNIafYB_rJjQW7B_alvOn4wk,11939
-google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_feature_attribution_pipeline.py,sha256=Vh4uoE66_cOIK1CVBamiJmcRAUWOw7SA6GNav9lAhFI,22182
-google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_pipeline.py,sha256=HuVusl14syZaEG50brVH4hYJ066ZWP7zfGNMl3-yUJ4,15624
-google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_unstructure_data_pipeline.py,sha256=UxMWemFLFW8AgxWOP6vJHmRQNkqFtyEfL4E91mlQi08,18599
-google_cloud_pipeline_components/v1/model_evaluation/evaluation_feature_attribution_pipeline.py,sha256=q7baxkvp229mp-ekJJG8HlRmRbOdk6LUC4jOpnfJaTI,25056
-google_cloud_pipeline_components/v1/model_evaluation/forecasting_component.py,sha256=eDNk8nsr8CEA_MqEAt00LCH1CulieQof0j1y28Suk6Y,10258
-google_cloud_pipeline_components/v1/model_evaluation/regression_component.py,sha256=rgq_q6ANTjibaQkVz1-VrkN6dBbiy7dxuqKu9Sx8om8,9311
+google_cloud_pipeline_components/v1/model_evaluation/classification_component.py,sha256=GRwcZYPIdV0MJKXT6jFoOfONIAOg3-4TEyNZYwlVLrE,12617
+google_cloud_pipeline_components/v1/model_evaluation/error_analysis_pipeline.py,sha256=bByf975FIcZ2Bqi10ivN6RDaNZFcFPVQFJtXNd6Er84,20408
+google_cloud_pipeline_components/v1/model_evaluation/evaluated_annotation_pipeline.py,sha256=_srccVmteigjc0_8VhMdY2svo8cRkQHv5HhD-5kAxoY,11969
+google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_feature_attribution_pipeline.py,sha256=nh1Y3iIzH3KFqz_m3-XvWUs9e1_nVungnsmqYDXxx3I,47515
+google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_pipeline.py,sha256=96aLoR_v0hVETbYmQJ02sjf-o5nBTLXYMKIMolJUuBI,38232
+google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_unstructure_data_pipeline.py,sha256=0vD_6bb5jrdkVvJfSwFu_6N-WU2Cy7inFySisSUQZbg,42441
+google_cloud_pipeline_components/v1/model_evaluation/evaluation_feature_attribution_pipeline.py,sha256=jPU6RbMTAMamEzL2Ryu9a_TW3E6v-vo3mJyDMn0Iev8,51528
+google_cloud_pipeline_components/v1/model_evaluation/forecasting_component.py,sha256=I54odNcSUnCwOYhnFeXKDg-OCXyihvbM11_MB-q9DHM,10295
+google_cloud_pipeline_components/v1/model_evaluation/regression_component.py,sha256=Jnb_y4I4LWUDZegEuoro-ywI5ksxOiOFBql6yP-GuFQ,9341
 google_cloud_pipeline_components/v1/vertex_notification_email/__init__.py,sha256=YIRljNy_oHY_vRda-kfhm5QiulNd_SIIPbmpzOiYJ0k,863
 google_cloud_pipeline_components/v1/vertex_notification_email/component.py,sha256=QFSJLUuyePqR4cHg-c5dzuc7SRMA-KWohzhoLWHW7x8,2175
 google_cloud_pipeline_components/v1/wait_gcp_resources/__init__.py,sha256=4mUizfOaBMg6RNUjFsz8y8fUHefws7FdLxr3Vo-Us7E,857
 google_cloud_pipeline_components/v1/wait_gcp_resources/component.py,sha256=36EvKauOpo-DtF0IscVvVeS1-i6uDo2U8TGOswdvsNs,2538
-google_cloud_pipeline_components-2.1.0.dist-info/LICENSE,sha256=WNHhf_5RCaeuKWyq_K39vmp9F28LxKsB4SpomwSZ2L0,11357
-google_cloud_pipeline_components-2.1.0.dist-info/METADATA,sha256=-C4yuOzimcUQBeg5fMS7bMbVpa2VNMKZEfVeGqrbo1o,5839
-google_cloud_pipeline_components-2.1.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-google_cloud_pipeline_components-2.1.0.dist-info/top_level.txt,sha256=E8T4T8KGMGLXbHvt2goa98oezRpxryPC6QhWBZ27Hhc,33
-google_cloud_pipeline_components-2.1.0.dist-info/RECORD,,
+google_cloud_pipeline_components-2.1.1.dist-info/LICENSE,sha256=WNHhf_5RCaeuKWyq_K39vmp9F28LxKsB4SpomwSZ2L0,11357
+google_cloud_pipeline_components-2.1.1.dist-info/METADATA,sha256=fOs6e1UzShX_LJSKHL3LoMxoMf7Xuvi-iMW3VZV4uCE,5796
+google_cloud_pipeline_components-2.1.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+google_cloud_pipeline_components-2.1.1.dist-info/top_level.txt,sha256=E8T4T8KGMGLXbHvt2goa98oezRpxryPC6QhWBZ27Hhc,33
+google_cloud_pipeline_components-2.1.1.dist-info/RECORD,,
```

