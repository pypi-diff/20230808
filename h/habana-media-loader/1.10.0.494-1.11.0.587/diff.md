# Comparing `tmp/habana_media_loader-1.10.0.494-py3-none-any.whl.zip` & `tmp/habana_media_loader-1.11.0.587-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,79 +1,79 @@
-Zip file size: 142685 bytes, number of entries: 77
--rw-r--r--  2.0 unx     1326 b- defN 23-May-24 04:06 habana_frameworks/medialoaders/__init__.py
--rw-r--r--  2.0 unx     1326 b- defN 23-May-24 04:06 habana_frameworks/medialoaders/tensorflow/__init__.py
--rw-r--r--  2.0 unx    16734 b- defN 23-May-24 04:06 habana_frameworks/medialoaders/tensorflow/media_resnet_pipe.py
--rw-r--r--  2.0 unx     1370 b- defN 23-May-24 04:06 habana_frameworks/medialoaders/torch/__init__.py
--rw-r--r--  2.0 unx    49827 b- defN 23-May-24 04:06 habana_frameworks/medialoaders/torch/media_dataloader_mediapipe.py
--rw-r--r--  2.0 unx    59665 b- defN 23-May-24 04:06 habana_frameworks/medialoaders/torch/mediapipe_unet_3d.py
--rw-r--r--  2.0 unx    18643 b- defN 23-May-24 04:06 habana_frameworks/medialoaders/torch/mediapipe_unet_mlperf.py
--rw-r--r--  2.0 unx    15239 b- defN 23-May-24 04:06 habana_frameworks/medialoaders/torch/mediapipe_unet_mlperf_accuracy_test.py
--rw-r--r--  2.0 unx    19998 b- defN 23-May-24 04:06 habana_frameworks/medialoaders/torch/mediapipe_unet_mlperf_cpp.py
--rw-r--r--  2.0 unx     1424 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/__init__.py
--rw-r--r--  2.0 unx     4865 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/fn.py
--rw-r--r--  2.0 unx     2062 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/media_proxy.py
--rw-r--r--  2.0 unx     1697 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/media_types.py
--rw-r--r--  2.0 unx    11526 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/mediapipe.py
--rw-r--r--  2.0 unx     1542 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/backend/__init__.py
--rw-r--r--  2.0 unx    33564 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/backend/cal.py
--rw-r--r--  2.0 unx    26361 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/backend/graph.py
--rw-r--r--  2.0 unx     7535 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/backend/graph_cpu.py
--rw-r--r--  2.0 unx     1813 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/backend/iterator.py
--rw-r--r--  2.0 unx     9019 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/backend/legacy.py
--rw-r--r--  2.0 unx      151 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/backend/logger.py
--rw-r--r--  2.0 unx    11221 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/backend/nodes.py
--rw-r--r--  2.0 unx     8034 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/backend/operator_specs.py
--rw-r--r--  2.0 unx      801 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/backend/proxy_impl.py
--rw-r--r--  2.0 unx     1837 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/backend/tensor.py
--rw-r--r--  2.0 unx     3482 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/backend/tensor_cpu.py
--rw-r--r--  2.0 unx      859 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/backend/tracing.py
--rw-r--r--  2.0 unx    11476 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/backend/utils.py
--rw-r--r--  2.0 unx     1482 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/__init__.py
--rw-r--r--  2.0 unx     8517 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/media_nodes.py
--rw-r--r--  2.0 unx      492 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/media_params.py
--rw-r--r--  2.0 unx      414 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/media_schema.py
--rw-r--r--  2.0 unx     1326 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/cpu_nodes/__init__.py
--rw-r--r--  2.0 unx     4078 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/cpu_nodes/basic_crop.py
--rw-r--r--  2.0 unx     2257 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/cpu_nodes/cpu_node_params.py
--rw-r--r--  2.0 unx     3816 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/cpu_nodes/cpu_node_schema.py
--rw-r--r--  2.0 unx    10933 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/cpu_nodes/cpu_nodes.py
--rw-r--r--  2.0 unx     1235 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/cpu_nodes/cpu_ops_node.py
--rw-r--r--  2.0 unx     2578 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/cpu_nodes/gather_video.py
--rw-r--r--  2.0 unx     4362 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/cpu_nodes/random_biased_crop.py
--rw-r--r--  2.0 unx     3533 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/cpu_nodes/random_flip.py
--rw-r--r--  2.0 unx     3781 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/cpu_nodes/zoom.py
--rw-r--r--  2.0 unx     1326 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/decoder_nodes/__init__.py
--rw-r--r--  2.0 unx     1164 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/decoder_nodes/decoder_node_params.py
--rw-r--r--  2.0 unx      882 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/decoder_nodes/decoder_node_schema.py
--rw-r--r--  2.0 unx     6651 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/decoder_nodes/decoder_nodes.py
--rw-r--r--  2.0 unx     1326 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/hpu_nodes/__init__.py
--rw-r--r--  2.0 unx     3889 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/hpu_nodes/hpu_node_params.py
--rw-r--r--  2.0 unx     6929 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/hpu_nodes/hpu_node_schema.py
--rw-r--r--  2.0 unx     2978 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/hpu_nodes/hpu_nodes.py
--rw-r--r--  2.0 unx     1326 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/metadata_nodes/__init__.py
--rw-r--r--  2.0 unx      324 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/metadata_nodes/metadata_node_params.py
--rw-r--r--  2.0 unx      517 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/metadata_nodes/metadata_node_schema.py
--rw-r--r--  2.0 unx     3933 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/metadata_nodes/metadata_nodes.py
--rw-r--r--  2.0 unx     1482 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/reader_nodes/__init__.py
--rw-r--r--  2.0 unx    14600 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/reader_nodes/coco_reader.py
--rw-r--r--  2.0 unx    10945 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/reader_nodes/read_image_from_dir.py
--rw-r--r--  2.0 unx    10427 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/reader_nodes/read_image_from_dir_buf.py
--rw-r--r--  2.0 unx     8855 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/reader_nodes/read_image_jpeg.py
--rw-r--r--  2.0 unx    14054 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/reader_nodes/read_numpy_from_dir.py
--rw-r--r--  2.0 unx     3466 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/reader_nodes/read_video_from_dir.py
--rw-r--r--  2.0 unx     1794 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/reader_nodes/reader_cpu_ops_node.py
--rw-r--r--  2.0 unx     2342 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/reader_nodes/reader_node_params.py
--rw-r--r--  2.0 unx     2653 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/reader_nodes/reader_node_schema.py
--rw-r--r--  2.0 unx     5734 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/reader_nodes/reader_nodes.py
--rw-r--r--  2.0 unx     3813 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/operators/reader_nodes/reader_utils.py
--rw-r--r--  2.0 unx     1359 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/plugins/__init__.py
--rw-r--r--  2.0 unx    16014 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/plugins/iterator_pytorch.py
--rw-r--r--  2.0 unx     1359 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/plugins/readers/__init__.py
--rw-r--r--  2.0 unx     3993 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/plugins/readers/tfrecord_coco_reader_cpp.py
--rw-r--r--  2.0 unx    10599 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/plugins/readers/tfrecord_reader.py
--rw-r--r--  2.0 unx     3203 b- defN 23-May-24 04:06 habana_frameworks/mediapipe/plugins/readers/tfrecord_reader_cpp.py
--rw-rw-r--  2.0 unx    22291 b- defN 23-May-24 04:07 habana_media_loader-1.10.0.494.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx     9326 b- defN 23-May-24 04:07 habana_media_loader-1.10.0.494.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-24 04:07 habana_media_loader-1.10.0.494.dist-info/WHEEL
--rw-r--r--  2.0 unx       18 b- defN 23-May-24 04:07 habana_media_loader-1.10.0.494.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     8807 b- defN 23-May-24 04:07 habana_media_loader-1.10.0.494.dist-info/RECORD
-77 files, 560672 bytes uncompressed, 127881 bytes compressed:  77.2%
+Zip file size: 150891 bytes, number of entries: 77
+-rw-r--r--  2.0 unx     1326 b- defN 23-Aug-03 22:43 habana_frameworks/medialoaders/__init__.py
+-rw-r--r--  2.0 unx     1326 b- defN 23-Aug-03 22:43 habana_frameworks/medialoaders/tensorflow/__init__.py
+-rw-r--r--  2.0 unx    16734 b- defN 23-Aug-03 22:43 habana_frameworks/medialoaders/tensorflow/media_resnet_pipe.py
+-rw-r--r--  2.0 unx     1370 b- defN 23-Aug-03 22:43 habana_frameworks/medialoaders/torch/__init__.py
+-rw-r--r--  2.0 unx    49827 b- defN 23-Aug-03 22:43 habana_frameworks/medialoaders/torch/media_dataloader_mediapipe.py
+-rw-r--r--  2.0 unx    18643 b- defN 23-Aug-03 22:43 habana_frameworks/medialoaders/torch/mediapipe_unet.py
+-rw-r--r--  2.0 unx    59665 b- defN 23-Aug-03 22:43 habana_frameworks/medialoaders/torch/mediapipe_unet_3d.py
+-rw-r--r--  2.0 unx    58037 b- defN 23-Aug-03 22:43 habana_frameworks/medialoaders/torch/mediapipe_unet_3d_cpp.py
+-rw-r--r--  2.0 unx    10618 b- defN 23-Aug-03 22:43 habana_frameworks/medialoaders/torch/mediapipe_unet_cpp.py
+-rw-r--r--  2.0 unx     1424 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/__init__.py
+-rw-r--r--  2.0 unx     4865 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/fn.py
+-rw-r--r--  2.0 unx     2062 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/media_proxy.py
+-rw-r--r--  2.0 unx     1825 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/media_types.py
+-rw-r--r--  2.0 unx    11563 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/mediapipe.py
+-rw-r--r--  2.0 unx     1542 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/backend/__init__.py
+-rw-r--r--  2.0 unx    33814 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/backend/cal.py
+-rw-r--r--  2.0 unx    26410 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/backend/graph.py
+-rw-r--r--  2.0 unx     7711 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/backend/graph_cpu.py
+-rw-r--r--  2.0 unx     1666 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/backend/iterator.py
+-rw-r--r--  2.0 unx     9019 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/backend/legacy.py
+-rw-r--r--  2.0 unx      151 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/backend/logger.py
+-rw-r--r--  2.0 unx    11221 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/backend/nodes.py
+-rw-r--r--  2.0 unx     8034 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/backend/operator_specs.py
+-rw-r--r--  2.0 unx      801 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/backend/proxy_impl.py
+-rw-r--r--  2.0 unx     1837 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/backend/tensor.py
+-rw-r--r--  2.0 unx     3579 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/backend/tensor_cpu.py
+-rw-r--r--  2.0 unx      859 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/backend/tracing.py
+-rw-r--r--  2.0 unx    11544 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/backend/utils.py
+-rw-r--r--  2.0 unx     1482 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/__init__.py
+-rw-r--r--  2.0 unx     8517 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/media_nodes.py
+-rw-r--r--  2.0 unx      492 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/media_params.py
+-rw-r--r--  2.0 unx      414 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/media_schema.py
+-rw-r--r--  2.0 unx     1326 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/cpu_nodes/__init__.py
+-rw-r--r--  2.0 unx     4078 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/cpu_nodes/basic_crop.py
+-rw-r--r--  2.0 unx     2423 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/cpu_nodes/cpu_node_params.py
+-rw-r--r--  2.0 unx     4011 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/cpu_nodes/cpu_node_schema.py
+-rw-r--r--  2.0 unx    10933 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/cpu_nodes/cpu_nodes.py
+-rw-r--r--  2.0 unx     1235 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/cpu_nodes/cpu_ops_node.py
+-rw-r--r--  2.0 unx     2578 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/cpu_nodes/gather_video.py
+-rw-r--r--  2.0 unx     4362 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/cpu_nodes/random_biased_crop.py
+-rw-r--r--  2.0 unx     3533 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/cpu_nodes/random_flip.py
+-rw-r--r--  2.0 unx     3781 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/cpu_nodes/zoom.py
+-rw-r--r--  2.0 unx     1326 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/decoder_nodes/__init__.py
+-rw-r--r--  2.0 unx     1164 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/decoder_nodes/decoder_node_params.py
+-rw-r--r--  2.0 unx      882 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/decoder_nodes/decoder_node_schema.py
+-rw-r--r--  2.0 unx     6651 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/decoder_nodes/decoder_nodes.py
+-rw-r--r--  2.0 unx     1326 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/hpu_nodes/__init__.py
+-rw-r--r--  2.0 unx     3889 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/hpu_nodes/hpu_node_params.py
+-rw-r--r--  2.0 unx     6934 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/hpu_nodes/hpu_node_schema.py
+-rw-r--r--  2.0 unx     2978 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/hpu_nodes/hpu_nodes.py
+-rw-r--r--  2.0 unx     1326 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/metadata_nodes/__init__.py
+-rw-r--r--  2.0 unx      324 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/metadata_nodes/metadata_node_params.py
+-rw-r--r--  2.0 unx      517 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/metadata_nodes/metadata_node_schema.py
+-rw-r--r--  2.0 unx     3933 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/metadata_nodes/metadata_nodes.py
+-rw-r--r--  2.0 unx     1482 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/reader_nodes/__init__.py
+-rw-r--r--  2.0 unx    14600 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/reader_nodes/coco_reader.py
+-rw-r--r--  2.0 unx    10933 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/reader_nodes/read_image_from_dir.py
+-rw-r--r--  2.0 unx    10427 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/reader_nodes/read_image_from_dir_buf.py
+-rw-r--r--  2.0 unx     8855 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/reader_nodes/read_image_jpeg.py
+-rw-r--r--  2.0 unx    14054 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/reader_nodes/read_numpy_from_dir.py
+-rw-r--r--  2.0 unx    25837 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/reader_nodes/read_video_from_dir.py
+-rw-r--r--  2.0 unx     1794 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/reader_nodes/reader_cpu_ops_node.py
+-rw-r--r--  2.0 unx     2816 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/reader_nodes/reader_node_params.py
+-rw-r--r--  2.0 unx     2653 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/reader_nodes/reader_node_schema.py
+-rw-r--r--  2.0 unx     5734 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/reader_nodes/reader_nodes.py
+-rw-r--r--  2.0 unx     3813 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/operators/reader_nodes/reader_utils.py
+-rw-r--r--  2.0 unx     1359 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/plugins/__init__.py
+-rw-r--r--  2.0 unx    17505 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/plugins/iterator_pytorch.py
+-rw-r--r--  2.0 unx     1359 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/plugins/readers/__init__.py
+-rw-r--r--  2.0 unx     3993 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/plugins/readers/tfrecord_coco_reader_cpp.py
+-rw-r--r--  2.0 unx    10599 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/plugins/readers/tfrecord_reader.py
+-rw-r--r--  2.0 unx     3203 b- defN 23-Aug-03 22:43 habana_frameworks/mediapipe/plugins/readers/tfrecord_reader_cpp.py
+-rw-rw-r--  2.0 unx    22291 b- defN 23-Aug-03 22:43 habana_media_loader-1.11.0.587.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     9354 b- defN 23-Aug-03 22:43 habana_media_loader-1.11.0.587.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Aug-03 22:43 habana_media_loader-1.11.0.587.dist-info/WHEEL
+-rw-r--r--  2.0 unx       18 b- defN 23-Aug-03 22:43 habana_media_loader-1.11.0.587.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     8780 b- defN 23-Aug-03 22:43 habana_media_loader-1.11.0.587.dist-info/RECORD
+77 files, 619439 bytes uncompressed, 136143 bytes compressed:  78.0%
```

## zipnote {}

```diff
@@ -9,24 +9,24 @@
 
 Filename: habana_frameworks/medialoaders/torch/__init__.py
 Comment: 
 
 Filename: habana_frameworks/medialoaders/torch/media_dataloader_mediapipe.py
 Comment: 
 
-Filename: habana_frameworks/medialoaders/torch/mediapipe_unet_3d.py
+Filename: habana_frameworks/medialoaders/torch/mediapipe_unet.py
 Comment: 
 
-Filename: habana_frameworks/medialoaders/torch/mediapipe_unet_mlperf.py
+Filename: habana_frameworks/medialoaders/torch/mediapipe_unet_3d.py
 Comment: 
 
-Filename: habana_frameworks/medialoaders/torch/mediapipe_unet_mlperf_accuracy_test.py
+Filename: habana_frameworks/medialoaders/torch/mediapipe_unet_3d_cpp.py
 Comment: 
 
-Filename: habana_frameworks/medialoaders/torch/mediapipe_unet_mlperf_cpp.py
+Filename: habana_frameworks/medialoaders/torch/mediapipe_unet_cpp.py
 Comment: 
 
 Filename: habana_frameworks/mediapipe/__init__.py
 Comment: 
 
 Filename: habana_frameworks/mediapipe/fn.py
 Comment: 
@@ -210,23 +210,23 @@
 
 Filename: habana_frameworks/mediapipe/plugins/readers/tfrecord_reader.py
 Comment: 
 
 Filename: habana_frameworks/mediapipe/plugins/readers/tfrecord_reader_cpp.py
 Comment: 
 
-Filename: habana_media_loader-1.10.0.494.dist-info/LICENSE.txt
+Filename: habana_media_loader-1.11.0.587.dist-info/LICENSE.txt
 Comment: 
 
-Filename: habana_media_loader-1.10.0.494.dist-info/METADATA
+Filename: habana_media_loader-1.11.0.587.dist-info/METADATA
 Comment: 
 
-Filename: habana_media_loader-1.10.0.494.dist-info/WHEEL
+Filename: habana_media_loader-1.11.0.587.dist-info/WHEEL
 Comment: 
 
-Filename: habana_media_loader-1.10.0.494.dist-info/top_level.txt
+Filename: habana_media_loader-1.11.0.587.dist-info/top_level.txt
 Comment: 
 
-Filename: habana_media_loader-1.10.0.494.dist-info/RECORD
+Filename: habana_media_loader-1.11.0.587.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## habana_frameworks/mediapipe/media_types.py

```diff
@@ -89,7 +89,16 @@
 class decoderType:
     """
     Class defining media decoder types.
 
     """
     IMAGE_DECODER = "image_decoder"
     VIDEO_DECODER = "video_decoder"
+
+
+class clipSampler:
+    """
+    Class defining sampler for video clips
+
+    """
+    RANDOM_SAMPLER = 0
+    UNIFORM_SAMPLER = 1
```

## habana_frameworks/mediapipe/mediapipe.py

```diff
@@ -132,27 +132,27 @@
         # call user defined graph function
         output_tensors = self.definegraph()
         if isinstance(output_tensors, tuple):
             output_tensors = list(output_tensors)
         elif not isinstance(output_tensors, list):
             output_tensors = [output_tensors]
         self._gp_ = self.__graph_processor__(
-            self._device_type_, output_tensors)
+            self._device_type_, output_tensors, self._fw_type_, self._proxy_)
         self._gp_.segment_graph()
         self._gp_.process_and_validate_graph(self._batch_size_,
                                              self._queue_depth_,
                                              self._num_threads_)
         self._recipe_ = self._gp_.compile()
         self._gp_.process_recipe()
 
     def set_proxy(self, fw_type, proxy):
         """
         Setter method to set media proxy.
 
-        :params fw_type: framework to be used by media. <SYNAPSE_FW/TF_FW/PYTHON_FW>
+        :params fw_type: framework to be used by media. <SYNAPSE_FW/TF_FW/PYTHON_FW/PYT_FW>
         :params proxy : c++ proxy address.
         """
         if MediaPipe.framework_type is None:
             MediaPipe.framework_type = get_media_fw_type(fw_type)
             if not isinstance(MediaPipe.framework_type, mppy.fwType):
                 raise RuntimeError(" Invalid proxy type.")
             self._fw_type_ = MediaPipe.framework_type
```

## habana_frameworks/mediapipe/backend/cal.py

```diff
@@ -637,14 +637,16 @@
         # if self._py_proxy_ is not None:
         if self._fw_type_ == mppy.fwType.PYTHON_FW:
             self._py_proxy_.delete_tensor(dev_addr)
         elif self._fw_type_ == mppy.fwType.SYNAPSE_FW:
             self._pm_.freeRawDevBuffer(dev_addr)
         elif self._fw_type_ == mppy.fwType.TF_FW:
             pass
+        elif self._fw_type_ == mppy.fwType.PYT_FW:
+            pass
         else:
             raise RuntimeError("unknown FW type ", self._fw_type_)
 
     def get_output(self):
         """
         Method to catch the processed output from device.
 
@@ -721,20 +723,22 @@
 
         """
         ret = self._pm_.releaseDevice()
         is_ok("releaseDevice", ret)
 
 
 class graph_handler():
-    def __init__(self, batch_size, reader_nodes, cpu_nodes, hpu_nodes, outputs):
+    def __init__(self, batch_size, reader_nodes, cpu_nodes, hpu_nodes, outputs, fw_type, proxy):
         self.__reader_nodes__ = reader_nodes
         self.__cpu_nodes__ = cpu_nodes
         self.__hpu_nodes__ = hpu_nodes
         self.__outputs__ = outputs
         self._batch_size_ = batch_size
+        self._fw_type_ = fw_type
+        self._proxy_ = proxy
         self.__c_outputs__ = []
 
         for r in self.__reader_nodes__:
             self.__create_c_nodes__(
                 r, mpn.NodeType_t.NODE_READER, mpn.Device_t.DEVICE_CPU)
         for op in self.__cpu_nodes__:
             self.__create_c_nodes__(
@@ -751,15 +755,17 @@
             return
         outputs = mpn.tensor_node_list()
         for o in self.__outputs__:
             outputs.append(o.c_t)
         self.gp = mpn.GraphProcessor(self._batch_size_,
                                      queue_depth,
                                      num_threads,
-                                     outputs)
+                                     outputs,
+                                     self._fw_type_,
+                                     self._proxy_)
         self.gp.PreProcessGraph()
         self.is_compiled = True
 
     def get_recipe(self):
         if(self.is_compiled == False):
             raise RuntimeError("Recipe Compile not happened yet")
         self.cookbook = self.gp.GetCookBook()
```

## habana_frameworks/mediapipe/backend/graph.py

```diff
@@ -68,21 +68,22 @@
 
 class graph_processor(object):
     """
     Class defining compile time processing of media nodes.
 
     """
 
-    def __init__(self, device_type, output_tensors):
+    def __init__(self, device_type, output_tensors, fw_type, proxy):
         """
         Constructor method.
 
         """
         self._device_type_ = device_type
         self._output_tensors_ = output_tensors
+        self._fw_type_ = fw_type
         self._mm_ = media_manager(self._device_type_)
         self._ops_ = []
         self._readers_ = []
         self._const_inputs_ = []
         self._func_inputs_ = []
         self._decoders_ = []
         self._hpu_ops_ = []
```

## habana_frameworks/mediapipe/backend/graph_cpu.py

```diff
@@ -16,21 +16,23 @@
 
 class graph_processor(object):
     """
     Class defining compile time processing of media nodes.
 
     """
 
-    def __init__(self, device_type, output_tensors):
+    def __init__(self, device_type, output_tensors, fw_type, proxy):
         """
         Constructor method.
 
         """
         self._device_type_ = device_type
         self._output_tensors_ = output_tensors
+        self._fw_type_ = fw_type
+        self._proxy_ = proxy
         self._ops_ = []
         self._readers_ = []
         self._const_inputs_ = []
         self._func_inputs_ = []
         self._cpu_ops_ = []
         self._transfer_ops_ = []
         self._hpu_ops_ = []
@@ -123,15 +125,17 @@
         Method to compile graph.
 
         """
         self._gh_ = graph_handler(self._batch_size_,
                                   self._readers_,
                                   self._cpu_ops_,
                                   self._hpu_ops_,
-                                  self._output_tensors_)
+                                  self._output_tensors_,
+                                  self._fw_type_,
+                                  self._proxy_)
         self._gh_.compile(self._queue_depth_, self._num_threads_)
         return self._gh_.get_recipe()
 
     def process_recipe(self):
         """
         Getter method to get graph recipe.
```

## habana_frameworks/mediapipe/backend/iterator.py

```diff
@@ -55,28 +55,24 @@
 
     def __init__(self, mediapipe):
         """
         Constructor method.
 
         :params mediapipe: mediapipe
         """
+        mediapipe.set_proxy(mppy.fwType.SYNAPSE_FW, 0)
         mediapipe.build()
-        self.proxy_set = False
         super().__init__(_pipeline=mediapipe)
 
     def __iter__(self):
         """
         Method to initialize mediapipe iterator.
 
         :returns : iterator for mediapipe
         """
-        if self.proxy_set == False:
-            # this must be 0 and not None
-            self.pipe.set_proxy(mppy.fwType.SYNAPSE_FW, 0)
-            self.proxy_set = True
         self.pipe.iter_init()
         return self
 
     def __next__(self):
         """
         Method to run mediapipe iterator over one batch of dataset and return the output tensors.
```

## habana_frameworks/mediapipe/backend/tensor_cpu.py

```diff
@@ -66,14 +66,15 @@
                                 self.__tensor__.GetDtype(), shape[::-1])
         return np_arr
 
     def __del__(self):
         # data_ptr = self.tensor.GetDataPtr()
         # print("CPU tensor delete :",flush=True)
         self.__tensor__.Free()
+        del self.__tensor__
 
 
 class HPUTensor:
     def __init__(self, tensor):
         # print("CPU tensor create : ", flush=True)
         self.name = tensor.name
         self.__tensor__ = tensor
@@ -91,14 +92,17 @@
         dtype = self.__tensor__.GetDtype()
         cputensor = mpn.TensorCPU(self.name + '_py', dtype, 1.0, 0.0, size)
         cputensor.Reshape(shape)
         cputensor.ToHost(self.__tensor__)
         tensor = CPUTensor(cputensor)
         return tensor
 
+    def get_addr(self):
+        return self.__tensor__.GetBusAddr()
+
     def __del__(self):
         # data_ptr = self.tensor.GetDataPtr()
         # print("CPU tensor delete :",flush=True)
         self.__tensor__.Free()
 
 
 def TensorPacker(tensors):
```

## habana_frameworks/mediapipe/backend/utils.py

```diff
@@ -302,14 +302,16 @@
     fw_type = ""
     if (in_type == "TF_FW"):
         fw_type = mppy.fwType.TF_FW
     elif(in_type == "SYNAPSE_FW"):
         fw_type = mppy.fwType.SYNAPSE_FW
     elif(in_type == "PYTHON_FW"):
         fw_type = mppy.fwType.PYTHON_FW
+    elif(in_type == "PYT_FW"):
+        fw_type = mppy.fwType.PYT_FW
     else:
         raise ValueError("invalid fw type {}".format(in_type))
     return fw_type
 
 
 def get_media_fw_type(in_type):
     """
```

## habana_frameworks/mediapipe/operators/cpu_nodes/cpu_node_params.py

```diff
@@ -27,15 +27,15 @@
     'seed': 0,
     'shape': [1],
     # 'unique_number': 0, this will be populated by framework can be used with seed to get unique seed
     'priv_params': {}  # user defined params can be passed here
 }
 
 random_biased_crop_params = {
-    'patch_size': [0, 0, 0],  # do not share batch_size
+    'patch_size': [0, 0, 0],  # i.e [fcd, y, z], do not share batch_size
     'over_sampling': 0.33,
     'num_channels': 1,
     'seed': 0,
     'num_workers': 1,
     'cache_bboxes': False,
     'cache_bboxes_at_first_run': False
 }
@@ -65,14 +65,22 @@
 }
 
 random_bernoulli_in_keys = ["probability"]
 random_bernoulli_params = {
     'seed': 100
 }
 
+gaussian_filter_in_keys = ["sigma"]
+gaussian_filter_params = {
+    'min_sigma': 0.5,
+    'max_sigma': 1.5,
+    'channels': 0,
+    'input_depth':0
+}
+
 random_uniform_in_keys = ["seed"]
 random_uniform_params = {
     'seed': 0,
     'low': 0,
     'high': 1
 }
```

## habana_frameworks/mediapipe/operators/cpu_nodes/cpu_node_schema.py

```diff
@@ -62,7 +62,10 @@
                         empty_params, mpn.EmptyParams_t, cpu_ops_node, dt.NDT)
 
 schema.add_operator_cpu("Crop", "crop", 1, 1, generic_in1_keys, 1,
                         crop_params, mpn.CropParams_t, cpu_ops_node, dt.NDT)
 
 schema.add_operator_cpu("Constant", "constant", 0, 0, generic_in0_keys, 1,
                         constant_params, mpn.ConstantParams_t, cpu_ops_node, dt.NDT)
+
+schema.add_operator_cpu("GaussianFilter", "gaussian_filter", 1, 1, gaussian_filter_in_keys,  1,
+                        gaussian_filter_params, mpn.GaussianFilterParams_t, cpu_ops_node, dt.NDT)
```

## habana_frameworks/mediapipe/operators/hpu_nodes/hpu_node_schema.py

```diff
@@ -31,15 +31,15 @@
 schema.add_operator("Brightness", 'brightness',
                     1, 2, brightness_in_key, 1, brightness_params, mpp.brightnessParams, media_hpu_ops, dt.UINT8)
 
 schema.add_operator("Saturation", 'saturation',
                     1, 2, saturation_in_key, 1, saturation_params, mpp.saturationParams, media_hpu_ops, dt.UINT8)
 
 schema.add_operator("Transpose", 'transpose', 1, 1, generic_in1_key, 1,
-                    transpose_params, mpp.transposeParams, media_hpu_ops, dt.UINT8)
+                    transpose_params, mpp.mediaTransposeParams, media_hpu_ops, dt.UINT8)
 
 schema.add_operator("Slice", 'slice', 1, 1, generic_in1_key, 1,
                     slice_params, mpp.mediaSliceParams, media_hpu_ops, dt.UINT8)
 
 schema.add_operator("Concat", 'concat', 2, 2, generic_in2_key, 1,
                     concat_params, mpp.mediaConcatParams, media_hpu_ops, dt.UINT8)
```

## habana_frameworks/mediapipe/operators/reader_nodes/read_image_from_dir.py

```diff
@@ -12,15 +12,15 @@
 
 # user defined functions
 def gen_class_list(dir, class_list=None):
     """
     Method to get list of classes present.
 
     """
-    if class_list is not None:
+    if class_list:
         return np.array(class_list)
 
     data_dir = pathlib.Path(dir)
     return np.array(sorted(
         [item.name for item in data_dir.glob('*') if item.is_dir() == True], key=lambda x: ''.join(x.split())))
```

## habana_frameworks/mediapipe/operators/reader_nodes/read_video_from_dir.py

```diff
@@ -1,15 +1,347 @@
 from habana_frameworks.mediapipe.backend.nodes import opnode_tensor_info
-from habana_frameworks.mediapipe.operators.reader_nodes.read_image_from_dir import read_image_from_dir
+from habana_frameworks.mediapipe.operators.media_nodes import MediaReaderNode
+from habana_frameworks.mediapipe.operators.reader_nodes.read_image_from_dir import gen_class_list, gen_image_list, gen_label_list
 from habana_frameworks.mediapipe.media_types import dtype as dt
 from habana_frameworks.mediapipe.media_types import readerOutType as ro
+from habana_frameworks.mediapipe.media_types import clipSampler as cs
+from habana_frameworks.mediapipe.operators.reader_nodes.reader_utils import dataset_shuffler
+from fractions import Fraction
 import numpy as np
+import time
+import av
+import math
+from typing import Union, List, Tuple
+import warnings
+import bisect
 
+g_use_torch_for_fps = False
 
-class read_video_from_dir(read_image_from_dir):
+# ToDo: May not be needed
+if g_use_torch_for_fps == True:
+
+    import torch.utils.data
+    from torchvision.datasets.utils import tqdm
+    from typing import TypeVar
+
+    T = TypeVar("T")
+
+    def _collate_fn(x: T) -> T:
+        # Dummy collate function to be used with _VideoFrameRateDataset
+        return x
+
+    def videoFrameNumFrameRate(file_path: str) -> Tuple[int, float]:
+        try:
+            with av.open(file_path, metadata_errors="ignore") as container:
+                video_stream = container.streams.video[0]
+                total_frames = video_stream.frames
+                avg_frame_rate = float(video_stream.average_rate)
+        except av.AVError:
+            print("Got AVError in File ", file_path)
+            # Ignore Video with AV Error
+            total_frames = 0
+            avg_frame_rate = 1.0
+        return total_frames, avg_frame_rate
+
+    class _VideoFrameRateDataset:
+        """
+        Dataset used to parallelize the reading of the number of frames and frame rate
+        of a list of videos, given their paths in the filesystem.
+        """
+
+        def __init__(self, video_paths: np.ndarray) -> None:
+            self.video_paths = video_paths
+
+        def __len__(self) -> int:
+            return len(self.video_paths)
+
+        def __getitem__(self, idx: int) -> Tuple[int, float]:
+            return videoFrameNumFrameRate(self.video_paths[idx])
+
+    def compute_num_frame_torch(vid_list: np.ndarray) -> Tuple[List[int], List[float]]:
+        """
+        get number of frames and average frame rate of each video using torch.utils.data.DataLoader
+        """
+
+        num_frame_list = []
+        avg_frame_rate_list = []
+        num_workers = 4  # ToDo: Update if needed
+
+        # use a DataLoader to parallelize videoFrameNumFrameRate, so need to create a dummy dataset first
+        dl: torch.utils.data.DataLoader = torch.utils.data.DataLoader(
+            _VideoFrameRateDataset(vid_list),
+            batch_size=16,
+            num_workers=num_workers,
+            collate_fn=_collate_fn,
+        )
+
+        with tqdm(total=len(dl)) as pbar:
+            for batch in dl:
+                pbar.update(1)
+                num_frames, fps = list(zip(*batch))
+                num_frame_list.extend(num_frames)
+                avg_frame_rate_list.extend(fps)
+
+        return num_frame_list, avg_frame_rate_list
+
+
+def compute_num_frame(vid_list: np.ndarray) -> Tuple[List[int], List[float]]:
+    """
+    get number of frames and average frame rate of each video.
+    """
+    num_frame_list = []
+    avg_frame_rate_list = []
+    for file in vid_list:
+        try:
+            with av.open(file, metadata_errors="ignore") as container:
+                video_stream = container.streams.video[0]
+                total_frames = video_stream.frames
+                avg_frame_rate = float(video_stream.average_rate)
+        except av.AVError:
+            print("Got AVError in File ", file)
+            # Ignore Video with AV Error
+            total_frames = 0
+            avg_frame_rate = 1.0
+        num_frame_list.append(total_frames)
+        avg_frame_rate_list.append(avg_frame_rate)
+    return num_frame_list, avg_frame_rate_list
+
+
+def unfold(idx_array: np.ndarray, size: int, step: int, dilation: int = 1) -> np.ndarray:
+    """
+    Returns all consecutive windows of `size` elements, with `step` between windows. The distance between each element
+    in a window is given by `dilation`.
+    """
+    if idx_array.ndim != 1:
+        raise ValueError(
+            f"expected 1 dimension instead of {idx_array.ndim}")
+
+    o_stride = idx_array.strides[0]
+    numel = idx_array.size
+    new_stride = (step * o_stride, dilation * o_stride)
+    new_size = ((numel - (dilation * (size - 1) + 1)) // step + 1, size)
+    if new_size[0] < 1:
+        new_size = (0, size)
+
+    return np.lib.stride_tricks.as_strided(idx_array, new_size, new_stride, writeable=False)
+
+
+def compute_clips_for_video(num_frame_vid: int, clip_length: int, step: int, fps_video: float, target_frame_rate: int) -> np.ndarray:
+    """
+    Compute all consecutive sequences of clips of clip_length from num_frame_vid.
+    """
+    if target_frame_rate == 0:
+        target_frame_rate = fps_video
+    total_frames = num_frame_vid * (target_frame_rate / fps_video)
+    _idxs = _resample_video_idx(
+        int(math.floor(total_frames)), fps_video, target_frame_rate)
+
+    if isinstance(_idxs, slice):
+        idxs_temp = np.arange(num_frame_vid, dtype=np.int32)
+        idxs_temp = idxs_temp[_idxs]
+        idxs = unfold(idxs_temp, clip_length, step)
+
+    else:
+        idxs = unfold(_idxs, clip_length, step)
+
+    if idxs.size == 0:
+        warnings.warn(
+            "video reader: There aren't enough frames in the current video to get a clip for the given clip length (frames_per_clip)."
+            "The video (and potentially others) will be skipped."
+        )
+
+    return idxs
+
+
+def _resample_video_idx(num_frame_vid: int, original_fps: float, new_fps: int) -> Union[slice, np.ndarray]:
+    """
+    calculate resample index for video with num_frame_vid frames at original_fps to new_fps
+    """
+    step = original_fps / new_fps
+
+    if step.is_integer():
+        # optimization: if step is integer, don't need to perform
+        # advanced indexing
+        step = int(step)
+        return slice(None, None, step)
+
+    idxs = np.arange(num_frame_vid, dtype=np.float32) * step
+    idxs = np.floor(idxs).astype(np.int32)
+    return idxs
+
+
+class RandomSampler:
+    """
+    Samples at most max_clips_per_video clips for each video randomly
+    """
+
+    def __init__(self, video_clips: List[np.ndarray], max_clips_per_video: int, seed: int) -> None:
+        self.max_clips_per_video = max_clips_per_video
+        self.vid_clips_list = video_clips
+        self.rng = np.random.default_rng(seed)
+
+    def get_iter_array(self) -> np.ndarray:
+        idxs = []
+        s = 0
+        # select at most self.max_clips_per_video for each video, randomly
+        for c in self.vid_clips_list:
+            length = len(c)
+            size = min(length, self.max_clips_per_video)
+
+            sampled = self.rng.permutation(length)[:size] + s
+            s += length
+            idxs.append(sampled)
+        idxs = np.concatenate(idxs)
+        # shuffle all clips randomly
+        perm = self.rng.permutation(len(idxs))
+        idxs_np = np.array(idxs[perm], dtype=np.uint32)
+        return idxs_np
+
+    def get_length(self) -> int:
+        """
+        Calculates number of clips that will be generated at each iter
+        """
+        sampler_length = 0
+        for c in self.vid_clips_list:
+            length = len(c)
+            size = min(length, self.max_clips_per_video)
+            sampler_length += size
+
+        return sampler_length
+
+
+class UniformSampler:
+    """
+    Samples num_clips_per_video clips for each video.
+    """
+
+    def __init__(self, video_clips: List[np.ndarray], num_clips_per_video: int) -> None:
+        self.num_clips_per_video = num_clips_per_video
+        self.vid_clips_list = video_clips
+
+    def get_iter_array(self) -> np.ndarray:
+        """
+        Sample self.num_clips_per_video clips for each video, equally spaced.
+        """
+        idxs = []
+        s = 0
+        # select self.num_clips_per_video for each video, uniformly spaced
+        for c in self.vid_clips_list:
+            length = len(c)
+            if length == 0:
+                continue
+
+            sampled = np.floor(np.linspace(
+                s, s + length - 1, num=self.num_clips_per_video, dtype=np.float32)).astype(np.int32)
+
+            s += length
+            idxs.append(sampled)
+        idxs = np.concatenate(idxs)
+        idxs_np = np.array(idxs, dtype=np.uint32)
+        return idxs_np
+
+    def get_length(self) -> int:
+        """
+        Calculates number of clips that will be generated at each iter
+        """
+        sampler_length = 0
+        for c in self.vid_clips_list:
+            length = len(c)
+            if length == 0:
+                continue
+            sampler_length += self.num_clips_per_video
+        return sampler_length
+
+
+class VideoClips:
+    def __init__(self, frames: int, step: int, frame_rate: int, vid_list: np.ndarray) -> None:
+        """
+        Given a list of video files 'vid_list', compute all consecutive subvideos of size
+        'frames_per_clip', where the distance between each subvideo in the
+        same video is defined by `step_between_clips`.
+
+        frames_per_clip: size of a clip in number of frames
+        step_between_clips: step (in frames) between each clip
+        target_frame_rate: If specified(i.e. not 0), resample the videos so that they have the same `target_frame_rate`.
+        If 0, resampling based on fps is not done.
+        """
+
+        self.frames_per_clip = frames
+        self.step_between_clips = step
+        self.target_frame_rate = frame_rate
+
+        # start_time = time.time()
+        print("compute num_frame, fps ...")
+        if g_use_torch_for_fps == False:
+            num_frame, avg_frame_rate = compute_num_frame(vid_list)
+        else:
+            num_frame, avg_frame_rate = compute_num_frame_torch(vid_list)
+        # total_time = time.time() - start_time
+        print("compute num_frame, fps Done")
+        # print("compute num_frame, fps Done, time {}s".format(total_time))
+
+        assert len(vid_list) == len(num_frame), "wrong num frame"
+        assert len(vid_list) == len(avg_frame_rate), "wrong frame rate"
+
+        self.vid_list_num_frame_np = np.array(num_frame, dtype=np.uint32)
+        self.vid_list_avg_frame_rate_np = np.array(
+            avg_frame_rate, dtype=np.float64)
+
+        max_frame_rate = np.max(self.vid_list_avg_frame_rate_np)
+
+        print("max frame rate {} in video files".format(max_frame_rate))
+
+        self.compute_clips(self.frames_per_clip,
+                           self.step_between_clips, self.target_frame_rate)
+        # print("compute_clips Done")
+
+    def compute_clips(self, num_frames: int, step: int, target_frame_rate: int):
+        """
+        Compute all consecutive sequences of clips from self.vid_list_num_frame_np.
+
+        Args:
+            num_frames (int): number of frames for the clip
+            step (int): distance between two clips
+            target_frame_rate (int): frame rate at which all video are to be resampled.
+                                     If 0, frame resampling based on fps is not done.
+        """
+
+        self.vid_clips_list = []
+
+        for num_frame_vid, fps_vid in zip(self.vid_list_num_frame_np, self.vid_list_avg_frame_rate_np):
+            clips = compute_clips_for_video(
+                num_frame_vid, num_frames, step, fps_vid, target_frame_rate)
+            self.vid_clips_list.append(clips)
+
+        clip_lengths = np.array([len(v)
+                                for v in self.vid_clips_list], dtype=np.uint32)
+        self.vid_clips_list_cumulative_sizes = clip_lengths.cumsum(0).tolist()
+
+    def get_video_clip_list(self) -> List[np.ndarray]:
+        return self.vid_clips_list
+
+    def get_clip_location(self, idx: int) -> Tuple[int, int]:
+        """
+        Converts a flattened representation of the indices into a video_idx, clip_idx
+        representation.
+        """
+        # assert idx <= (self.vid_clips_list_cumulative_sizes[-1] - 1), "Error: Got invalid clip index {} max idx {}".format(
+        #    idx, (self.vid_clips_list_cumulative_sizes[-1] - 1))
+
+        video_idx = bisect.bisect_right(
+            self.vid_clips_list_cumulative_sizes, idx)
+        if video_idx == 0:
+            clip_idx = idx
+        else:
+            clip_idx = idx - \
+                self.vid_clips_list_cumulative_sizes[video_idx - 1]
+        return video_idx, clip_idx
+
+
+class read_video_from_dir(MediaReaderNode):
     """
     Class defining read video from directory node.
 
     """
 
     def __init__(self, name, guid, device, inputs, params, cparams, node_attr):
         """
@@ -18,69 +350,312 @@
         :params name: node name.
         :params guid: guid of node.
         :params device: device on which this node should execute.
         :params params: node specific params.
         :params cparams: backend params.
         :params node_attr: node output information
         """
-        self.frames_per_clip = params['frames_per_clip']
+        super().__init__(
+            name, guid, device, inputs, params, cparams, node_attr)
+
+        self.batch_size = 1
+        self.dir = params['dir']
+        self.seed = params['seed']
+        self.drop_remainder = params['drop_remainder']
+        self.pad_remainder = params['pad_remainder']
+        self.format = params['format']
+        self.meta_dtype = params["label_dtype"]
+        self.num_slices = params['num_slices']
+        self.slice_index = params['slice_index']
+        self.class_list = params['class_list']
+        self.vid_list = params['file_list']
+        self.file_classes = params['file_classes']
+
+        # fixed_clip_mode output clips: start_frame_index to frames_per_clip
+        self.fixed_clip_mode = params['fixed_clip_mode']
+        self.shuffle = params['shuffle']  # used only for fixed_clip_mode
+        # used only for fixed_clip_mode
         self.start_frame_index = params['start_frame_index']
-        if self.start_frame_index != 0:
-            raise ValueError("start frame index 0 only supported")
+
+        # clip_length_in_frames
+        self.frames_per_clip = params['frames_per_clip']
+        self.clips_per_video = params['clips_per_video']
+        # if target_frame_rate set to 0, frame resampling based on fps is not done
+        self.target_frame_rate = params['target_frame_rate']
+        self.frames_between_clips = params['step_between_clips']
+
+        self.sampler = params['sampler']
+
+        # self.max_file = params['max_file']
+        # self.file_sizes = params['file_sizes']
+
+        if (self.seed == None):
+            if self.num_slices > 1:
+                raise ValueError("seed not set")
+            else:
+                # max supported seed value is 32bit so modulo
+                self.seed = int(time.time_ns() % (2**31 - 1))
+
+        if ((self.class_list != [] and self.vid_list == []) or (self.vid_list != [] and self.class_list == [])):
+            raise ValueError("Both class_list and file_list must be shared")
+        elif((self.vid_list != []) and (self.file_classes != []) and (len(self.vid_list) != len(self.file_classes))):
+            raise ValueError(
+                "Both file_list and file_classes must be of same length")
+        elif(self.vid_list == [] and self.file_classes != []):
+            raise ValueError(
+                "file_classes must be shared only if file_list is shared")
+        elif(self.vid_list == [] and self.dir == ""):
+            raise ValueError("Atleast file_list or dir must be shared")
+        elif (self.vid_list != [] and self.dir != ""):
+            raise ValueError("Only file_list or dir must be shared")
+
+        # self.rng = np.random.default_rng(self.seed)
+        print("Finding classes ...", end=" ")
+        self.class_list = gen_class_list(self.dir, self.class_list)
+        print("Done!")
+        print("Finding videos ...", end=" ")
+        self.vid_list = gen_image_list(self.dir, self.format, self.vid_list)
+        print("Done!")
+        print("Generating labels ...", end=" ")
+        self.lbl_list = gen_label_list(
+            self.vid_list, self.class_list, self.meta_dtype, self.file_classes)
+        print("Done!")
+
+        num_imgs = len(self.vid_list)
+        print("Total media files/labels {} classes {}".format(num_imgs,
+              len(self.class_list)))
+        assert len(self.vid_list) == len(
+            self.lbl_list), "wrong num label/video"
+
+        self.iter_loc = 0
+        if num_imgs == 0:
+            raise ValueError("video list is empty")
+        self.num_batches_slice = int(num_imgs / self.batch_size)
+        if(self.num_slices < 1):
+            raise ValueError("num slice cannot be less then 1")
+        if(self.slice_index >= self.num_slices):
+            raise ValueError("slice_index cannot be >= num_slices")
+        print("seed {} num_slices {} slice_index {}".format(
+            self.seed, self.num_slices, self.slice_index))
+
+        self.first_file = self.vid_list[0]  # ToDo: Update largest file
 
         self.resample_dtype = dt.INT32
         self.dec_frame_offset_dtype = dt.UINT32
 
-        super().__init__(
-            name, guid, device, inputs, params, cparams, node_attr)
+        self.is_modulo_slice = True
+
+        assert self.frames_per_clip > 0, "frames_per_clip > 0 expected"
 
-        print("video reader frames_per_clip {} start_frame_index {}".format(
-            self.frames_per_clip, self.start_frame_index))
+        if self.fixed_clip_mode:
+            """
+            Each video is used to generate a clip from 'start_frame_index' of length 'frames_per_clip'
+            """
+            assert self.start_frame_index >= 0, "start_frame_index >= 0 expected"
+
+            print("video reader: Fixed Clip mode, start_frame_index: {} frames_per_clip: {} shuffle: {}".format(
+                self.start_frame_index, self.frames_per_clip, self.shuffle))
+        else:
+
+            assert self.clips_per_video > 0, "clips_per_video > 0 expected"
+            assert self.frames_between_clips > 0, "step_between_clips > 0 expected"
+
+            if self.sampler == cs.RANDOM_SAMPLER:
+                print("video reader sampler: Random frames_per_clip: {} target_frame_rate: {}".format(
+                    self.frames_per_clip, self.target_frame_rate))
+            elif self.sampler == cs.UNIFORM_SAMPLER:
+                print("video reader sampler: Uniform frames_per_clip: {} target_frame_rate: {}".format(
+                    self.frames_per_clip, self.target_frame_rate))
+            else:
+                raise ValueError("unsupported sampler ", self.sampler)
 
     def gen_output_info(self):
         """
         Method to generate output type information.
 
         :returns : output tensor information of type "opnode_tensor_info".
         """
 
-        out_info_ip = super().gen_output_info()
+        # out_info_ip = super().gen_output_info()
+        out_info = []
+        o = opnode_tensor_info(dt.NDT,
+                               np.array([self.batch_size],
+                                        dtype=np.uint32),
+                               "")
+        out_info.append(o)
+        o = opnode_tensor_info(self.meta_dtype,
+                               np.array([self.batch_size],
+                                        dtype=np.uint32),
+                               "")
+        out_info.append(o)
 
         self.resample_shape = [self.frames_per_clip, self.batch_size]
         self.resample_shape_np = self.resample_shape[::-1]
         o = opnode_tensor_info(self.resample_dtype, np.array(
             self.resample_shape, dtype=np.uint32), "")
-        out_info_ip.append(o)
+        out_info.append(o)
 
         self.dec_frame_offset_shape = [2, self.batch_size]
         self.dec_frame_offset_shape_np = self.dec_frame_offset_shape[::-1]
         o = opnode_tensor_info(self.dec_frame_offset_dtype, np.array(
             self.dec_frame_offset_shape, dtype=np.uint32), "")
-        out_info_ip.append(o)
+        out_info.append(o)
+
+        return out_info
+
+    def get_largest_file(self):
+        """
+        Method to get largest media in the dataset.
+
+        :returns : largest media element in the dataset.
+        """
+        return self.first_file
+
+    def get_media_output_type(self):
+        """
+        Method to specify type of media output produced by the reader.
+
+        returns: type of media output which is produced by this reader.
+        """
+        return ro.FILE_LIST
+
+    def __len__(self):
+        """
+        Method to get dataset length.
+
+        returns: length of dataset in units of batch_size.
+        """
+        return self.num_batches_slice
+
+    def set_params(self, params):
+        """
+        Setter method to set mediapipe specific params.
+
+        :params params: mediapipe params of type "opnode_params".
+        """
+
+        self.batch_size = params.batch_size
+
+        if self.fixed_clip_mode == False:
+
+            self.video_clips = VideoClips(
+                self.frames_per_clip, self.frames_between_clips, self.target_frame_rate, self.vid_list)
+
+            self.video_clips_list = self.video_clips.get_video_clip_list()
+            if self.sampler == cs.RANDOM_SAMPLER:
+                self.sampler_inst = RandomSampler(
+                    self.video_clips_list, self.clips_per_video, self.seed)
+            else:
+                self.sampler_inst = UniformSampler(
+                    self.video_clips_list, self.clips_per_video)
+
+            sampler_clip_length = self.sampler_inst.get_length()
+
+            self.shuffler = dataset_shuffler(None,  # self.seed_set_epoch
+                                             False,  # shuffle is False
+                                             self.slice_index,
+                                             self.num_slices,
+                                             self.batch_size,
+                                             sampler_clip_length,
+                                             self.drop_remainder,
+                                             self.pad_remainder,
+                                             False,  # shuffle_across_dataset
+                                             self.is_modulo_slice)
+
+        else:
+
+            sampler_clip_length = len(self.vid_list)
+            self.shuffler = dataset_shuffler(self.seed,
+                                             self.shuffle,
+                                             self.slice_index,
+                                             self.num_slices,
+                                             self.batch_size,
+                                             sampler_clip_length,
+                                             self.drop_remainder,
+                                             self.pad_remainder,
+                                             self.shuffle,  # shuffle_across_dataset
+                                             self.is_modulo_slice)
+
+        # idxs not needed here, only len needed
+        self.num_batches_slice = self.shuffler.get_num_iterable_elements() // self.batch_size
+        print("num video: {} num clips: {} batch size: {} sliced, rounded clips: {} num batches: {}".format(len(self.vid_list), sampler_clip_length,
+              self.batch_size, self.shuffler.get_num_iterable_elements(), self.num_batches_slice))
+
+    def __iter__(self):
+        """
+        Method to initialize iterator.
+
+        """
+
+        if self.fixed_clip_mode == False:
+            print("Iter ...",  end=" ")
+            clip_dataset_iter = self.sampler_inst.get_iter_array()
+            idxs = self.shuffler.gen_idx_list()
 
-        return out_info_ip
+            self.vid_list_slice_iter = clip_dataset_iter[idxs]
+            print("Done!")
+
+        else:
+
+            shuffle_idx = self.shuffler.gen_idx_list()
+
+            self.vid_list_slice_iter = self.vid_list[shuffle_idx]
+            self.lbl_list_slice_iter = self.lbl_list[shuffle_idx]
+
+        self.iter_loc = 0
+        return self
 
     def __next__(self):
         """
         Method to get one batch of dataset ouput from iterator.
 
         """
-        try:
-            vid_list, lbl_list = super().__next__()
-        except StopIteration:
+
+        if self.iter_loc > (len(self.vid_list_slice_iter) - 1):
             raise StopIteration
 
-        # resample_idx are frame indexes(frames_per_clip) to be used for each video in vid_list
+        # resample_idx are frame indexes to be used for each clip in vid_list
         resample_idx = np.zeros(self.resample_shape_np,
                                 dtype=self.resample_dtype)
-        resample_single_vid = np.array(np.arange(self.start_frame_index, (
-            self.start_frame_index + self.frames_per_clip)), dtype=self.resample_dtype)
 
-        for i in range(self.batch_size):
-            resample_idx[i] = resample_single_vid
+        start = self.iter_loc
+        end = self.iter_loc + self.batch_size
+        self.iter_loc += self.batch_size
+
+        if self.fixed_clip_mode == False:
+
+            clip_list = self.vid_list_slice_iter[start:end]
+            lbl_list = np.zeros([self.batch_size], dtype=self.meta_dtype)
+            vid_list = []
+
+            for idx, clip_idx_list in enumerate(clip_list):
+                video_index, clip_index = self.video_clips.get_clip_location(
+                    clip_idx_list)
+                video_path = self.vid_list[video_index]
+                vid_list.append(video_path)
+                lbl_list[idx] = self.lbl_list[video_index]
+                clip_resample_idx = self.video_clips_list[video_index][clip_index]
+                # print("reader next for {} clip idx {} video {} clip {} path {} lbl {} resampling {}".format(
+                #    idx, clip_idx_list, video_index, clip_index, video_path, lbl_list[idx], clip_resample_idx))
+
+                clip_resample_idx_np = np.array(
+                    clip_resample_idx, dtype=self.resample_dtype)
+
+                resample_idx[idx] = clip_resample_idx_np
+
+        else:
+
+            vid_list = self.vid_list_slice_iter[start:end]
+            lbl_list = self.lbl_list_slice_iter[start:end]
+
+            resample_single_vid = np.arange(self.start_frame_index, (
+                self.start_frame_index + self.frames_per_clip), dtype=self.resample_dtype)
+
+            for i in range(self.batch_size):
+                resample_idx[i] = resample_single_vid
 
         # dec_frame_offset_idx is used to configure decoder for frames to output(start, length) for each video in vid_list
         dec_frame_offset_idx = np.zeros(
             self.dec_frame_offset_shape_np, dtype=self.dec_frame_offset_dtype)
         for i in range(self.batch_size):
             dec_frame_offset_idx[i][0] = resample_idx[i][0]
             dec_frame_offset_idx[i][1] = (
```

## habana_frameworks/mediapipe/operators/reader_nodes/reader_node_params.py

```diff
@@ -1,8 +1,9 @@
 from habana_frameworks.mediapipe.media_types import dtype as dt
+from habana_frameworks.mediapipe.media_types import clipSampler as cs
 
 # INFO: Here we will give params and its default arguments order doesnt matter
 # INFO: if any parameter is not set here it will be set to zero
 
 generic_in0_keys = []
 
 media_ext_reader_op_params = {
@@ -26,30 +27,34 @@
     'class_list': None,
     'file_sizes': None,
     'file_classes': None
 }
 
 
 read_video_from_dir_params = {
-    'dir': "/",
-    'format': "mp4",  # updated for video
-    'shuffle': True,
+    'dir': "",
+    'format': "mp4",      # updated for video
     'seed': None,
-    'max_file': None,
     'drop_remainder': False,
     'pad_remainder': False,
     'label_dtype': dt.UINT64,
     'num_slices': 1,
     'slice_index': 0,
-    'file_list': None,
-    'class_list': None,
-    'file_sizes': None,
-    'file_classes': None,
-    'frames_per_clip': 1,  # added for video
-    'start_frame_index': 0  # added for video
+    'file_list': [],
+    'class_list': [],
+    'file_classes': [],
+    'frames_per_clip': 1,      # added for video
+    'clips_per_video': 1,      # added for video for fixed_clip_mode=False
+    'target_frame_rate': 0,    # added for video for fixed_clip_mode=False
+    'step_between_clips': 1,   # added for video for fixed_clip_mode=False
+    'sampler': cs.RANDOM_SAMPLER,  # added for video for fixed_clip_mode=False
+    'fixed_clip_mode': False,  # added for video
+    'start_frame_index': 0,    # added for video for fixed_clip_mode=True
+    'shuffle': True            # for fixed_clip_mode=True
+    # 'max_file', 'file_sizes'
 }
 
 coco_reader_params = {
     'root':         "",
     'annfile':   "",
     'drop_remainder': False,
     'pad_remainder': False,
```

## habana_frameworks/mediapipe/plugins/iterator_pytorch.py

```diff
@@ -1,9 +1,10 @@
 import torch
 import habana_frameworks.torch.utils.experimental as htexp
+#import media_pyt_bridge as mpytpx
 from habana_frameworks.mediapipe.backend.iterator import _MediaIterator
 from habana_frameworks.mediapipe.media_proxy import HPUProxy
 from habana_frameworks.mediapipe.media_types import dtype as dt
 import media_pipe_proxy as mppy
 import time
 import os
 from ctypes import *
@@ -208,28 +209,28 @@
 
         :params mediapipe: mediapipe
         """
         device_id = mediapipe.getDeviceId()
         if HPUGenericPytorchIterator.proxy_device is None:
             HPUGenericPytorchIterator.proxy_device = PytorchDevice(
                 name='hpu', device_id=device_id)
+
+        mediapipe.set_proxy(mppy.fwType.PYTHON_FW,
+                            HPUGenericPytorchIterator.proxy_device)
+
         mediapipe.build()
-        self.proxy_set = False
+
         super().__init__(_pipeline=mediapipe)
 
     def __iter__(self):
         """
         Method to initialize mediapipe iterator.
 
         :returns : iterator for mediapipe
         """
-        if self.proxy_set == False:
-            self.pipe.set_proxy(mppy.fwType.PYTHON_FW,
-                                HPUGenericPytorchIterator.proxy_device)
-            self.proxy_set = True
         self.pipe.iter_init()
         return self
 
     def __next__(self):
         """
         Method to run mediapipe iterator over one batch of dataset and return the output tensors.
 
@@ -240,16 +241,44 @@
         for output in outputs:
             tensor = HPUGenericPytorchIterator.proxy_device.get_tensor(
                 output.dev_addr)
             output_tensors.append(tensor)
         return output_tensors
 
 
+"""class PytorchIteratorBase(_MediaIterator):
+    
+    proxy_device = None
+
+    def __init__(self, mediapipe):
+        device_id = mediapipe.getDeviceId()
+        if PytorchIteratorBase.proxy_device is None:
+            PytorchIteratorBase.proxy_device = mpytpx.CreatePytMediaProxy(
+                device_id)
+
+        mediapipe.set_proxy(mppy.fwType.PYT_FW,
+                            PytorchIteratorBase.proxy_device)
+
+        mediapipe.build()
+        super().__init__(_pipeline=mediapipe)
+
+    def __iter__(self):
+        self.pipe.iter_init()
+        return self
+
+    def __next__(self):
+        outputs = self.pipe.run()
+        output_tensors = []
+        for output in outputs:
+            tensor = mpytpx.get_out_tensor(output.get_addr())
+            output_tensors.append(tensor)
+        return output_tensors"""
+
+
 class HPUResnetPytorchIterator(HPUGenericPytorchIterator):
-    # ToDo: check if HPUResnetPytorchIterator needed
     """
     Class defining Resnet mediapipe iterator for Pytorch framework.
     This class provides functionality to get output tensors from mediapipe.
 
     """
 
     def __init__(self, mediapipe):
@@ -324,18 +353,17 @@
             img_size[1] = torch.narrow(img_size[1], 0, 0, batch)
             boxes_tensor = torch.narrow(boxes_tensor, 0, 0, batch)
             labels_tensor = torch.narrow(labels_tensor, 0, 0, batch)
         # TODO: check if labels_tensor.long() needed
         return images_tensor, ids_tensor, img_size, boxes_tensor, labels_tensor.long()
 
 
-class HPUUnet3DMlPerfPytorchIterator(HPUGenericPytorchIterator):
-    # ToDo: check if HPUResnetPytorchIterator needed
+class HPUUnetPytorchIterator(HPUGenericPytorchIterator):
     """
-    Class defining Unet 3D mlperf mediapipe iterator for Pytorch framework.
+    Class defining Unet mediapipe iterator for Pytorch framework.
     This class provides functionality to get output tensors from mediapipe.
 
     """
 
     def __init__(self, mediapipe):
         """
         Constructor method.
@@ -381,14 +409,30 @@
             self.iter_loc += 1
             return batch
         else:
             self.iter_loc = 0
             raise StopIteration
 
 
+"""class HPUUnet3DPytorchIteratorPTBridge(PytorchIteratorBase):
+
+    def __init__(self, mediapipe):
+        super().__init__(mediapipe=mediapipe)
+
+    def __next__(self):
+        images, labels = self.pipe.run()
+        images_tensor = mpytpx.get_out_tensor(images.get_addr())
+        labels_tensor = mpytpx.get_out_tensor(labels.get_addr())
+
+        dict = {}
+        dict["image"] = images_tensor
+        dict["label"] = labels_tensor
+        return dict
+"""
+
 class HPUUnet3DPytorchIterator(HPUGenericPytorchIterator):
     """
     Class defining Unet 3D mediapipe iterator for Pytorch framework.
     This class provides functionality to get output tensors from mediapipe.
 
     """
 
@@ -410,14 +454,31 @@
         images_tensor = self.proxy_device.get_tensor(images.dev_addr)
         labels_tensor = self.proxy_device.get_tensor(labels.dev_addr)
         dict = {}
         dict["image"] = images_tensor
         dict["label"] = labels_tensor
         return dict
 
+
+"""class CPUUnet3DPytorchIteratorPTBridge(PytorchIteratorBase):
+
+    def __init__(self, mediapipe):
+        super().__init__(mediapipe=mediapipe)
+
+    def __next__(self):
+        images, labels = self.pipe.run()
+        images_tensor = mpytpx.get_out_tensor(images.get_addr())
+        labels_tensor = mpytpx.get_out_tensor(labels.get_addr())
+
+        dict = {}
+        dict["image"] = images_tensor
+        dict["label"] = labels_tensor
+        return dict"""
+
+
 class CPUUnet3DPytorchIterator(HPUGenericPytorchIterator):
     """
     Class defining Unet 3D mediapipe iterator for Pytorch framework.
     This class provides functionality to get output tensors from mediapipe.
 
     """
 
@@ -441,14 +502,15 @@
         images = torch.from_numpy(images)
         labels = torch.from_numpy(labels)
         dict = {}
         dict["image"] = images
         dict["label"] = labels
         return dict
 
+
 class CPUGenericPytorchIterator(_MediaIterator):
     """
     Class defining mediapipe iterator for Pytorch framework.
     This class provides functionality to get output tensors from cpu mediapipe.
 
     """
 
@@ -471,18 +533,18 @@
         """
         self.pipe.iter_init()
         return self
 
     def __next__(self):
         return
 
-class CPUUnet3DMlPerfPytorchIterator(CPUGenericPytorchIterator):
-    # ToDo: check if HPUResnetPytorchIterator needed
+
+class CPUUnetPytorchIterator(CPUGenericPytorchIterator):
     """
-    Class defining Unet 3D mlperf mediapipe iterator for Pytorch framework.
+    Class defining Unet mediapipe iterator for Pytorch framework.
     This class provides functionality to get output tensors from mediapipe.
     """
 
     def __init__(self, mediapipe):
         """
         Constructor method.
 
@@ -508,18 +570,18 @@
 
         :returns : output tensors.
         """
         try:
             images, labels = self.pipe.run()
             images = images.as_nparray()
             labels = labels.as_nparray()
-            #images_tensor = torch.tensor(images, dtype=torch.float32)
-            #labels_tensor = torch.tensor(labels, dtype=torch.uint8)
-            #del images
-            #del labels
+            # images_tensor = torch.tensor(images, dtype=torch.float32)
+            # labels_tensor = torch.tensor(labels, dtype=torch.uint8)
+            # del images
+            # del labels
             images_tensor = torch.from_numpy(images)
             labels_tensor = torch.from_numpy(labels)
             batch = {}
             batch["image"] = images_tensor
             batch["label"] = labels_tensor
             self.iter_loc += 1
             return batch
```

## Comparing `habana_frameworks/medialoaders/torch/mediapipe_unet_mlperf.py` & `habana_frameworks/medialoaders/torch/mediapipe_unet.py`

 * *Files identical despite different names*

## Comparing `habana_media_loader-1.10.0.494.dist-info/LICENSE.txt` & `habana_media_loader-1.11.0.587.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `habana_media_loader-1.10.0.494.dist-info/METADATA` & `habana_media_loader-1.11.0.587.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 Metadata-Version: 2.1
 Name: habana-media-loader
-Version: 1.10.0.494
+Version: 1.11.0.587
 Summary: Dataloader using Habana hardware media pipeline
 Home-page: https://habana.ai/
 Author: Habana Labs Ltd., an Intel Company
 Author-email: support@habana.ai
 License: See LICENSE.txt
 Platform: UNKNOWN
 Description-Content-Type: text/markdown
 License-File: LICENSE.txt
 Requires-Dist: pillow
+Requires-Dist: av (==9.2.0)
 
 # Habana Media Python package
 
 `habana_media_loader` is a package designed for easy integration of media processing on Gaudi2.
 Main entry point (Python import) is `habana_frameworks.mediapipe` module that contains all the necessary functions to work with Gaudi2.
 
 ## Structure
```

## Comparing `habana_media_loader-1.10.0.494.dist-info/RECORD` & `habana_media_loader-1.11.0.587.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,77 +1,77 @@
 habana_frameworks/medialoaders/__init__.py,sha256=3uxTxL_YvH-zJd-D4Wz4bewWq24QU5GSRyGEatkk-y4,1326
 habana_frameworks/medialoaders/tensorflow/__init__.py,sha256=3uxTxL_YvH-zJd-D4Wz4bewWq24QU5GSRyGEatkk-y4,1326
 habana_frameworks/medialoaders/tensorflow/media_resnet_pipe.py,sha256=9pBhraALztv1y7GZnLQj7r4oUxD68eabfMf8n-t791A,16734
 habana_frameworks/medialoaders/torch/__init__.py,sha256=kXJFg2zX_iuskBny_frNG8czZChf5Z11EbJ8PXxBgTQ,1370
 habana_frameworks/medialoaders/torch/media_dataloader_mediapipe.py,sha256=QrC8FjIm3CDl-2xFKdxA81URXuFcBebSfweqOw4b0x0,49827
+habana_frameworks/medialoaders/torch/mediapipe_unet.py,sha256=ISn7xgsYDI1P69kRaVnZaCzqax7Uzft7CURmpi-VFuE,18643
 habana_frameworks/medialoaders/torch/mediapipe_unet_3d.py,sha256=YLJpWpnVGKjw5bovcGt8GEcEH_aiGlLBzsXREy8LPmc,59665
-habana_frameworks/medialoaders/torch/mediapipe_unet_mlperf.py,sha256=ISn7xgsYDI1P69kRaVnZaCzqax7Uzft7CURmpi-VFuE,18643
-habana_frameworks/medialoaders/torch/mediapipe_unet_mlperf_accuracy_test.py,sha256=sIV9-OIAOuVtZzmPANH9bx7TdSKzjKIZ8NtNxiwIP5Y,15239
-habana_frameworks/medialoaders/torch/mediapipe_unet_mlperf_cpp.py,sha256=E_QqEStuGBPyjkxjSElm3qGL-TBQYN_lD9-crlqgzhs,19998
+habana_frameworks/medialoaders/torch/mediapipe_unet_3d_cpp.py,sha256=sZ4kYQ9Hb6ceArhYyWZi_VoWyyTPXt7blp6Fj3zYS9A,58037
+habana_frameworks/medialoaders/torch/mediapipe_unet_cpp.py,sha256=5V0-qrs8LWhH9q_JUS8SXXliOLR1geeb8Rqy0E9RnGs,10618
 habana_frameworks/mediapipe/__init__.py,sha256=Z4r2Q5UntCMjJ3omQj6MIpyfAtwz4v9f0veq387IBm0,1424
 habana_frameworks/mediapipe/fn.py,sha256=zXcVLLcP0NicDCKbD8mO9nT_UPJAABvCW64y6HRB5K4,4865
 habana_frameworks/mediapipe/media_proxy.py,sha256=_uZLrThs2lRTeySbniB6gD_ruN8yumfk98l939B85nU,2062
-habana_frameworks/mediapipe/media_types.py,sha256=pv61UWIIVmEFdVA1DjXPg41VfDVyAa8g9DH8yyOvv-o,1697
-habana_frameworks/mediapipe/mediapipe.py,sha256=xx_GxoPwrU-QdU9ZT89WkpNYKtvF7L780pS9LZQDOzU,11526
+habana_frameworks/mediapipe/media_types.py,sha256=ooQ6_FVY8_rwVHDSZkiYagUUmsYpPKvIP-EggG0awJ0,1825
+habana_frameworks/mediapipe/mediapipe.py,sha256=ynNXlOfPsBL_t8z-Derhbu5kj_Iy53IkHWMoAqJCwiY,11563
 habana_frameworks/mediapipe/backend/__init__.py,sha256=l0x3AYJ2AH8GwLwwMcNmfA2-vQWJ9K3HctXVH3ZpVNs,1542
-habana_frameworks/mediapipe/backend/cal.py,sha256=xANdsYJlMLe6mG8ZBbtJNNzljPwGmE7KoRfL-1DLPs4,33564
-habana_frameworks/mediapipe/backend/graph.py,sha256=tz-bPTTYJo3rI0Y_TfggHDYnO66ZZfHCQSJtFKoGpuw,26361
-habana_frameworks/mediapipe/backend/graph_cpu.py,sha256=cloL6_dGGH0pV3NPRFk3FldhH4YGi51Nqv9KeKj-IzQ,7535
-habana_frameworks/mediapipe/backend/iterator.py,sha256=HDRIrU6G0LdRHsP-MO2PhLaSGVUZN-1AL-yqGuV_BHg,1813
+habana_frameworks/mediapipe/backend/cal.py,sha256=4PypKqhkZW3ulidY7yS4uh5kaEi_S2JVnjKFn9jxTRM,33814
+habana_frameworks/mediapipe/backend/graph.py,sha256=jHiylCrL2-REH_FSowUW2iZn6B8o1AozWyUXrSvZUE8,26410
+habana_frameworks/mediapipe/backend/graph_cpu.py,sha256=7k8UR-6lvG3CtdjRm7oW7bldilcBCWtZqR1ScIy9l80,7711
+habana_frameworks/mediapipe/backend/iterator.py,sha256=RPNK1kM4O2ssfqsV41NrwL2mIzF0JS1475t5QzuLiWk,1666
 habana_frameworks/mediapipe/backend/legacy.py,sha256=chJvzfrA1S4o4p61r9vZIdMlpCRnUKDjbcW7iU0BT9M,9019
 habana_frameworks/mediapipe/backend/logger.py,sha256=zPVZ660GSRsHfC_haU7J9mjHzmi2WIbMFOy81fnWjDg,151
 habana_frameworks/mediapipe/backend/nodes.py,sha256=wpuvgRHz9RacJdAl7s19JvXC4H4QFerw9YEATFNSLGc,11221
 habana_frameworks/mediapipe/backend/operator_specs.py,sha256=AW8EEDIUZ5VqV1zFfQANcEarf__rm6bTMPVuAbvWZ9c,8034
 habana_frameworks/mediapipe/backend/proxy_impl.py,sha256=fVcaXYcK61kxZodMhzcss4CBRfxNwJerbYRmfdgqhIc,801
 habana_frameworks/mediapipe/backend/tensor.py,sha256=S_o6lwnd0W5uAq0KDFQSHmDAhmT7mXBnol4t42BIjcs,1837
-habana_frameworks/mediapipe/backend/tensor_cpu.py,sha256=_NteLIm0DgHcAEq5AzpyeX8SU4mec2XnUBsg5AYDYkk,3482
+habana_frameworks/mediapipe/backend/tensor_cpu.py,sha256=AXj08zYJMJj0hn2AVYeaQGVM_9P9u5WjwRx3L7QtFyQ,3579
 habana_frameworks/mediapipe/backend/tracing.py,sha256=H3GieNTvCVWEBBzkF6gW9wa7OrBsoOWfG9MYVwG1Dwk,859
-habana_frameworks/mediapipe/backend/utils.py,sha256=sshq7980mOI5ETGt2iG2ieW6_k71Qw53OKQkQaS6Unw,11476
+habana_frameworks/mediapipe/backend/utils.py,sha256=Wf9Z5Ag3d8MT86G9iPdSGE2wOoVxdICXdd_Xgl4ukp8,11544
 habana_frameworks/mediapipe/operators/__init__.py,sha256=9z9vJY5V4W1BrgfkK6G5JiNLQmNHRks8wvbexb8NohU,1482
 habana_frameworks/mediapipe/operators/media_nodes.py,sha256=QqYeI_tlAOHt6FLlkT1T7j_ash1vP47h8Hn3_pp5lbI,8517
 habana_frameworks/mediapipe/operators/media_params.py,sha256=rY2dTVtoOcV2n2gJ7yxG7tKvtet6-USbOcg7KoFydPk,492
 habana_frameworks/mediapipe/operators/media_schema.py,sha256=nu-y72LFW7IjoE_CYuocYlkReGHXtnbOLk7FFyEykiA,414
 habana_frameworks/mediapipe/operators/cpu_nodes/__init__.py,sha256=3uxTxL_YvH-zJd-D4Wz4bewWq24QU5GSRyGEatkk-y4,1326
 habana_frameworks/mediapipe/operators/cpu_nodes/basic_crop.py,sha256=b0pCGJ9AGkHzrKhtGe9eC31RmRMkuhjiVlMsbt-Hc5s,4078
-habana_frameworks/mediapipe/operators/cpu_nodes/cpu_node_params.py,sha256=dMwCFxxtnVMICZ3PvMrON-HEMnTrf9xPPGWgAdhipP4,2257
-habana_frameworks/mediapipe/operators/cpu_nodes/cpu_node_schema.py,sha256=7-ONGnKJN7eGTbrIlKgg1F6flInY4gElq2pMs16JE6Q,3816
+habana_frameworks/mediapipe/operators/cpu_nodes/cpu_node_params.py,sha256=Ib4OBu6MabZVdpRR529evu2bgxVpAeKB3yg6eY2XiFE,2423
+habana_frameworks/mediapipe/operators/cpu_nodes/cpu_node_schema.py,sha256=BS8uEvQr7mw_71TtdgnTOhszLeCY8-vdnF307seYvPM,4011
 habana_frameworks/mediapipe/operators/cpu_nodes/cpu_nodes.py,sha256=xLG7Ubx6-OEA-RQCmMl4Ea0Ug5aEslxrhURCYP331EU,10933
 habana_frameworks/mediapipe/operators/cpu_nodes/cpu_ops_node.py,sha256=M8_qU_vH8C5HkGjoASSSNufWYX4PZTjd4FyXBbYNVtc,1235
 habana_frameworks/mediapipe/operators/cpu_nodes/gather_video.py,sha256=zFJ60X42_pgxj8RLmrPffg4FWU06WYJ3psEDQZZxaRg,2578
 habana_frameworks/mediapipe/operators/cpu_nodes/random_biased_crop.py,sha256=1RJ_moDbWXCGX5fRfxOylIVVMWbZqaZyaAePo7sQ9-E,4362
 habana_frameworks/mediapipe/operators/cpu_nodes/random_flip.py,sha256=ZUf2K9E5kfRc9kC5S2z-f_2PpiiCozQoflLY56fL7EY,3533
 habana_frameworks/mediapipe/operators/cpu_nodes/zoom.py,sha256=p5g-nrVyj5-slQUb0CB4YItghGRMrIbfT9LH7Zv2aa8,3781
 habana_frameworks/mediapipe/operators/decoder_nodes/__init__.py,sha256=3uxTxL_YvH-zJd-D4Wz4bewWq24QU5GSRyGEatkk-y4,1326
 habana_frameworks/mediapipe/operators/decoder_nodes/decoder_node_params.py,sha256=wXAezIxb8Aln0N1vreFOVJa40d6hhJ8Wbfqwsxt7WIQ,1164
 habana_frameworks/mediapipe/operators/decoder_nodes/decoder_node_schema.py,sha256=b--73wGVt-v_6yo-me8L9TjbVXe2PFCnALQgdW5sJCY,882
 habana_frameworks/mediapipe/operators/decoder_nodes/decoder_nodes.py,sha256=NkDOqYL59Nv-hoDLo4Coppu4bjdlUc2D6U4aXosguOg,6651
 habana_frameworks/mediapipe/operators/hpu_nodes/__init__.py,sha256=3uxTxL_YvH-zJd-D4Wz4bewWq24QU5GSRyGEatkk-y4,1326
 habana_frameworks/mediapipe/operators/hpu_nodes/hpu_node_params.py,sha256=50XY8MD79gQfUg4JDjAecCLbTNyLQuOuS1DCjoQspjE,3889
-habana_frameworks/mediapipe/operators/hpu_nodes/hpu_node_schema.py,sha256=JYvflea_M-e4WtRp4L3ZT3El0kXMFKHWvhqLeUeqveA,6929
+habana_frameworks/mediapipe/operators/hpu_nodes/hpu_node_schema.py,sha256=AWVihANkQtSXR6p1kucmeAGNNUancWO2aVB-4hPrD7A,6934
 habana_frameworks/mediapipe/operators/hpu_nodes/hpu_nodes.py,sha256=1pVPb95EPTi5IU0WtNkcW5TP5014WjhlLbx4WBerA-Q,2978
 habana_frameworks/mediapipe/operators/metadata_nodes/__init__.py,sha256=3uxTxL_YvH-zJd-D4Wz4bewWq24QU5GSRyGEatkk-y4,1326
 habana_frameworks/mediapipe/operators/metadata_nodes/metadata_node_params.py,sha256=gFpn1trbb6MOqZVAuaNVSqepB3brFGylylwVfc9DBDs,324
 habana_frameworks/mediapipe/operators/metadata_nodes/metadata_node_schema.py,sha256=bmm8hSn6fYE_D4C0a1HXyqZXqTTutn26OsJ6DzjQUF8,517
 habana_frameworks/mediapipe/operators/metadata_nodes/metadata_nodes.py,sha256=WP00AFgovoM1DcpALpQ6bvGQZ6Xzrh3qPpsVhgsKb30,3933
 habana_frameworks/mediapipe/operators/reader_nodes/__init__.py,sha256=9z9vJY5V4W1BrgfkK6G5JiNLQmNHRks8wvbexb8NohU,1482
 habana_frameworks/mediapipe/operators/reader_nodes/coco_reader.py,sha256=0QIOK4oRYhajO91O-PvbVM-OQPS6Q_rUXXFmfqlQapQ,14600
-habana_frameworks/mediapipe/operators/reader_nodes/read_image_from_dir.py,sha256=CuDD2TIU4kK1vaZpZP3kIErqN4H2X9tv4INdLjDAPI0,10945
+habana_frameworks/mediapipe/operators/reader_nodes/read_image_from_dir.py,sha256=GxIceFt4Evs2kXAPWbI_m3J9-kpenV_NtI25c5DJ-qw,10933
 habana_frameworks/mediapipe/operators/reader_nodes/read_image_from_dir_buf.py,sha256=d2XlicEEB0jCUSYL-wfeI1eHWsNPhVteKfCXC3mPMMQ,10427
 habana_frameworks/mediapipe/operators/reader_nodes/read_image_jpeg.py,sha256=YwTYZhJotcD-TB2MAW5QbBYqO1bSzRS9Cf1_4FCZZZ0,8855
 habana_frameworks/mediapipe/operators/reader_nodes/read_numpy_from_dir.py,sha256=xOAlGvvYoUn0zvmaw6LySABT_3pQ4WmzKVpY74cTJIY,14054
-habana_frameworks/mediapipe/operators/reader_nodes/read_video_from_dir.py,sha256=60maT57-llfAufjWV_GNOpbYUdMTDpJV1wSGXcdGRbQ,3466
+habana_frameworks/mediapipe/operators/reader_nodes/read_video_from_dir.py,sha256=b5SZRamV75mgbVXP4Cu6ZTKhaJSXVcW2kD7V-C4oZC0,25837
 habana_frameworks/mediapipe/operators/reader_nodes/reader_cpu_ops_node.py,sha256=Cic03onmgfcOfMA10-AP84F7Zh4Iop0NGXlTjOezkEY,1794
-habana_frameworks/mediapipe/operators/reader_nodes/reader_node_params.py,sha256=7o_fja7jEZPrhemVPPSij19UqMPsxwhcyJCQHyaTnjo,2342
+habana_frameworks/mediapipe/operators/reader_nodes/reader_node_params.py,sha256=138I7oCHqVyoFWfO943M2b7ZoW0CQvdlRxasKYXmemk,2816
 habana_frameworks/mediapipe/operators/reader_nodes/reader_node_schema.py,sha256=nIxPJi8pHmIOyco1ftAwLd8UEiAVse3VeIz_fqYgr1o,2653
 habana_frameworks/mediapipe/operators/reader_nodes/reader_nodes.py,sha256=8HueRHf3fWDhhEAP2-NsVwrqivhfG7L52pgSEtYXQbE,5734
 habana_frameworks/mediapipe/operators/reader_nodes/reader_utils.py,sha256=e6mZTARVzk3Wwh1JXyMDRikEmHco6aEIj_fokPP63g4,3813
 habana_frameworks/mediapipe/plugins/__init__.py,sha256=5lFXiTrB1MrQkZgBudIpYUpHhDDvhEOSd6wn6_Q9IXc,1359
-habana_frameworks/mediapipe/plugins/iterator_pytorch.py,sha256=npaixYzKrZ5BJOt6uU91Y4CC6u86JnAJzbARTT3wDX0,16014
+habana_frameworks/mediapipe/plugins/iterator_pytorch.py,sha256=tFgbX8VOKGhfLEZxFAmWG_sSDvpfAPsoweBDjW-rI9U,17505
 habana_frameworks/mediapipe/plugins/readers/__init__.py,sha256=5lFXiTrB1MrQkZgBudIpYUpHhDDvhEOSd6wn6_Q9IXc,1359
 habana_frameworks/mediapipe/plugins/readers/tfrecord_coco_reader_cpp.py,sha256=5U5Z5nPpGBO-lZEADFPaCNl8l3XxFekX-ihPn7HW4uI,3993
 habana_frameworks/mediapipe/plugins/readers/tfrecord_reader.py,sha256=AElmbhDqZzYWwqXrkkQZBxDKMCQtjCkY0ba44QOCSNI,10599
 habana_frameworks/mediapipe/plugins/readers/tfrecord_reader_cpp.py,sha256=iLSeAdwf8OQJTCic7AgZIq_GCUEaR2DQUx_ghs1zkdE,3203
-habana_media_loader-1.10.0.494.dist-info/LICENSE.txt,sha256=cEpyZIUtQwrv_aB6ayHP7NvBSek4jlD9audqM2sFQqY,22291
-habana_media_loader-1.10.0.494.dist-info/METADATA,sha256=ZCteOGZXLKJE0HshYMGqT7PA7ewmhBx9FyxjjXG9p24,9326
-habana_media_loader-1.10.0.494.dist-info/WHEEL,sha256=ewwEueio1C2XeHTvT17n8dZUJgOvyCWCt0WVNLClP9o,92
-habana_media_loader-1.10.0.494.dist-info/top_level.txt,sha256=wCc-GPYwJuCjuubEh4j46vF28TJORdbD7_2WKsX6t5Y,18
-habana_media_loader-1.10.0.494.dist-info/RECORD,,
+habana_media_loader-1.11.0.587.dist-info/LICENSE.txt,sha256=cEpyZIUtQwrv_aB6ayHP7NvBSek4jlD9audqM2sFQqY,22291
+habana_media_loader-1.11.0.587.dist-info/METADATA,sha256=wzG33Qtc-RlKB_jeqMia8kW1SDwE59XDDkNXzhsKb_k,9354
+habana_media_loader-1.11.0.587.dist-info/WHEEL,sha256=ewwEueio1C2XeHTvT17n8dZUJgOvyCWCt0WVNLClP9o,92
+habana_media_loader-1.11.0.587.dist-info/top_level.txt,sha256=wCc-GPYwJuCjuubEh4j46vF28TJORdbD7_2WKsX6t5Y,18
+habana_media_loader-1.11.0.587.dist-info/RECORD,,
```

