# Comparing `tmp/lightly-1.4.7.tar.gz` & `tmp/lightly-1.4.8.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "lightly-1.4.7.tar", last modified: Wed Jun  7 08:25:57 2023, max compression
+gzip compressed data, was "lightly-1.4.8.tar", last modified: Tue Jun 13 17:21:23 2023, max compression
```

## Comparing `lightly-1.4.7.tar` & `lightly-1.4.8.tar`

### file list

```diff
@@ -1,404 +1,404 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.156761 lightly-1.4.7/
--rw-r--r--   0 runner    (1001) docker     (123)     1073 2023-06-07 08:25:48.000000 lightly-1.4.7/LICENSE.txt
--rw-r--r--   0 runner    (1001) docker     (123)      129 2023-06-07 08:25:48.000000 lightly-1.4.7/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (123)     2266 2023-06-07 08:25:57.156761 lightly-1.4.7/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    20415 2023-06-07 08:25:48.000000 lightly-1.4.7/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.124760 lightly-1.4.7/lightly/
--rw-r--r--   0 runner    (1001) docker     (123)     3910 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.124760 lightly-1.4.7/lightly/active_learning/
--rw-r--r--   0 runner    (1001) docker     (123)      391 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/active_learning/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.124760 lightly-1.4.7/lightly/active_learning/config/
--rw-r--r--   0 runner    (1001) docker     (123)      200 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/active_learning/config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2001 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/active_learning/config/selection_config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.124760 lightly-1.4.7/lightly/api/
--rw-r--r--   0 runner    (1001) docker     (123)      535 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    15532 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/api_workflow_artifacts.py
--rw-r--r--   0 runner    (1001) docker     (123)    10844 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/api_workflow_client.py
--rw-r--r--   0 runner    (1001) docker     (123)     3310 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/api_workflow_collaboration.py
--rw-r--r--   0 runner    (1001) docker     (123)    24922 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/api_workflow_compute_worker.py
--rw-r--r--   0 runner    (1001) docker     (123)    16198 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/api_workflow_datasets.py
--rw-r--r--   0 runner    (1001) docker     (123)    32401 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/api_workflow_datasources.py
--rw-r--r--   0 runner    (1001) docker     (123)    11452 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/api_workflow_download_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)    13211 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/api_workflow_export.py
--rw-r--r--   0 runner    (1001) docker     (123)     6614 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/api_workflow_predictions.py
--rw-r--r--   0 runner    (1001) docker     (123)     7321 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/api_workflow_selection.py
--rw-r--r--   0 runner    (1001) docker     (123)     9825 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/api_workflow_tags.py
--rw-r--r--   0 runner    (1001) docker     (123)    12930 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/api_workflow_upload_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)    10013 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/api_workflow_upload_embeddings.py
--rw-r--r--   0 runner    (1001) docker     (123)     9368 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/api_workflow_upload_metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     6802 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/bitmask.py
--rw-r--r--   0 runner    (1001) docker     (123)    19950 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/download.py
--rw-r--r--   0 runner    (1001) docker     (123)     2375 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/patch.py
--rw-r--r--   0 runner    (1001) docker     (123)     3616 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/prediction_singletons.py
--rw-r--r--   0 runner    (1001) docker     (123)     2380 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/swagger_api_client.py
--rw-r--r--   0 runner    (1001) docker     (123)     4847 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/swagger_rest_client.py
--rw-r--r--   0 runner    (1001) docker     (123)     6419 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2524 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/api/version_checking.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.128760 lightly-1.4.7/lightly/cli/
--rw-r--r--   0 runner    (1001) docker     (123)      444 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/cli/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1181 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/cli/_cli_simclr.py
--rw-r--r--   0 runner    (1001) docker     (123)     7330 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/cli/_helpers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.128760 lightly-1.4.7/lightly/cli/config/
--rw-r--r--   0 runner    (1001) docker     (123)      196 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/cli/config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8870 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/cli/config/config.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      427 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/cli/config/get_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     4468 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/cli/crop_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)     4840 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/cli/download_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)     3908 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/cli/embed_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)     1888 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/cli/lightly_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)     6372 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/cli/train_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)      677 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/cli/version_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)     7435 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/core.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.128760 lightly-1.4.7/lightly/data/
--rw-r--r--   0 runner    (1001) docker     (123)      687 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4189 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/data/_helpers.py
--rw-r--r--   0 runner    (1001) docker     (123)     3811 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/data/_image.py
--rw-r--r--   0 runner    (1001) docker     (123)      851 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/data/_image_loaders.py
--rw-r--r--   0 runner    (1001) docker     (123)     1203 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/data/_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    25874 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/data/_video.py
--rw-r--r--   0 runner    (1001) docker     (123)    49619 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/data/collate.py
--rw-r--r--   0 runner    (1001) docker     (123)    13255 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/data/dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     2569 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/data/lightly_subset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3022 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/data/multi_view_collate.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.128760 lightly-1.4.7/lightly/embedding/
--rw-r--r--   0 runner    (1001) docker     (123)      366 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/embedding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3778 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/embedding/_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     2775 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/embedding/callbacks.py
--rw-r--r--   0 runner    (1001) docker     (123)     5426 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/embedding/embedding.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.128760 lightly-1.4.7/lightly/loss/
--rw-r--r--   0 runner    (1001) docker     (123)      805 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2625 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/barlow_twins_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     9091 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/dcl_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     5289 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/dino_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     3477 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/hypersphere_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     3923 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/memory_bank.py
--rw-r--r--   0 runner    (1001) docker     (123)     8522 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/msn_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     1344 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/negative_cosine_similarity.py
--rw-r--r--   0 runner    (1001) docker     (123)     7186 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/ntx_ent_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     5790 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/pmsn_loss.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.128760 lightly-1.4.7/lightly/loss/regularizer/
--rw-r--r--   0 runner    (1001) docker     (123)      230 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/regularizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5522 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/regularizer/co2.py
--rw-r--r--   0 runner    (1001) docker     (123)     5741 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/swav_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     2308 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/sym_neg_cos_sim_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     3887 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/tico_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     4817 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/vicreg_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)    17701 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/loss/vicregl_loss.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.132760 lightly-1.4.7/lightly/models/
--rw-r--r--   0 runner    (1001) docker     (123)     1071 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3144 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/_momentum.py
--rw-r--r--   0 runner    (1001) docker     (123)     3976 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/barlowtwins.py
--rw-r--r--   0 runner    (1001) docker     (123)     2714 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/batchnorm.py
--rw-r--r--   0 runner    (1001) docker     (123)     5349 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/byol.py
--rw-r--r--   0 runner    (1001) docker     (123)     4373 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/moco.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.132760 lightly-1.4.7/lightly/models/modules/
--rw-r--r--   0 runner    (1001) docker     (123)      987 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    23563 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/modules/heads.py
--rw-r--r--   0 runner    (1001) docker     (123)    14929 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/modules/masked_autoencoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2108 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/modules/nn_memory_bank.py
--rw-r--r--   0 runner    (1001) docker     (123)     7095 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/nnclr.py
--rw-r--r--   0 runner    (1001) docker     (123)     8383 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/resnet.py
--rw-r--r--   0 runner    (1001) docker     (123)     3399 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/simclr.py
--rw-r--r--   0 runner    (1001) docker     (123)     4169 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/simsiam.py
--rw-r--r--   0 runner    (1001) docker     (123)    18480 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1573 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/models/zoo.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.124760 lightly-1.4.7/lightly/openapi_generated/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.132760 lightly-1.4.7/lightly/openapi_generated/swagger_client/
--rw-r--r--   0 runner    (1001) docker     (123)    25152 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.132760 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/
--rw-r--r--   0 runner    (1001) docker     (123)     1513 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    15030 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/collaboration_api.py
--rw-r--r--   0 runner    (1001) docker     (123)    48008 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/datasets_api.py
--rw-r--r--   0 runner    (1001) docker     (123)    78621 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/datasources_api.py
--rw-r--r--   0 runner    (1001) docker     (123)   160419 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/docker_api.py
--rw-r--r--   0 runner    (1001) docker     (123)    16861 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/embeddings2d_api.py
--rw-r--r--   0 runner    (1001) docker     (123)    34963 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/embeddings_api.py
--rw-r--r--   0 runner    (1001) docker     (123)     7739 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/jobs_api.py
--rw-r--r--   0 runner    (1001) docker     (123)     5858 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/mappings_api.py
--rw-r--r--   0 runner    (1001) docker     (123)    20444 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/meta_data_configurations_api.py
--rw-r--r--   0 runner    (1001) docker     (123)    38782 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/predictions_api.py
--rw-r--r--   0 runner    (1001) docker     (123)     4208 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/quota_api.py
--rw-r--r--   0 runner    (1001) docker     (123)    52413 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/samples_api.py
--rw-r--r--   0 runner    (1001) docker     (123)     6517 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/samplings_api.py
--rw-r--r--   0 runner    (1001) docker     (123)    16937 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/scores_api.py
--rw-r--r--   0 runner    (1001) docker     (123)    97324 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/tags_api.py
--rw-r--r--   0 runner    (1001) docker     (123)     5000 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/teams_api.py
--rw-r--r--   0 runner    (1001) docker     (123)     7619 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api/versioning_api.py
--rw-r--r--   0 runner    (1001) docker     (123)    25332 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/api_client.py
--rw-r--r--   0 runner    (1001) docker     (123)     8842 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/configuration.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.152760 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/
--rw-r--r--   0 runner    (1001) docker     (123)    23521 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2881 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/access_role.py
--rw-r--r--   0 runner    (1001) docker     (123)     4843 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/active_learning_score_create_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     7181 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/active_learning_score_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     2946 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/active_learning_score_type.py
--rw-r--r--   0 runner    (1001) docker     (123)     2931 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/active_learning_scores.py
--rw-r--r--   0 runner    (1001) docker     (123)     6996 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/api_error_code.py
--rw-r--r--   0 runner    (1001) docker     (123)     6275 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/api_error_response.py
--rw-r--r--   0 runner    (1001) docker     (123)     3648 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/async_task_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     2886 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/bounding_box.py
--rw-r--r--   0 runner    (1001) docker     (123)     2881 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/category_id.py
--rw-r--r--   0 runner    (1001) docker     (123)     2891 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/category_name.py
--rw-r--r--   0 runner    (1001) docker     (123)     7076 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/configuration_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     7078 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/configuration_entry.py
--rw-r--r--   0 runner    (1001) docker     (123)     4574 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/configuration_set_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     3340 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/configuration_value_data_type.py
--rw-r--r--   0 runner    (1001) docker     (123)     7266 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/create_docker_worker_registry_entry_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     3658 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/create_entity_response.py
--rw-r--r--   0 runner    (1001) docker     (123)     4873 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/create_sample_with_write_urls_response.py
--rw-r--r--   0 runner    (1001) docker     (123)     3075 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/creator.py
--rw-r--r--   0 runner    (1001) docker     (123)     9296 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/crop_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     2931 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/custom_sample_meta_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     6632 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dataset_create_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     3110 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dataset_creator.py
--rw-r--r--   0 runner    (1001) docker     (123)    15469 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dataset_data.py
--rw-r--r--   0 runner    (1001) docker     (123)    16339 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dataset_data_enriched.py
--rw-r--r--   0 runner    (1001) docker     (123)     7328 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dataset_embedding_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     2886 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dataset_name.py
--rw-r--r--   0 runner    (1001) docker     (123)     2911 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dataset_name_query.py
--rw-r--r--   0 runner    (1001) docker     (123)     2991 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dataset_type.py
--rw-r--r--   0 runner    (1001) docker     (123)     3694 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dataset_update_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     3700 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     4957 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_config_azure.py
--rw-r--r--   0 runner    (1001) docker     (123)     8360 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_config_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     5244 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_config_gcs.py
--rw-r--r--   0 runner    (1001) docker     (123)     2946 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_config_lightly.py
--rw-r--r--   0 runner    (1001) docker     (123)     2936 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_config_local.py
--rw-r--r--   0 runner    (1001) docker     (123)     6526 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_config_obs.py
--rw-r--r--   0 runner    (1001) docker     (123)     7430 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_config_s3.py
--rw-r--r--   0 runner    (1001) docker     (123)     7300 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_config_s3_delegated_access.py
--rw-r--r--   0 runner    (1001) docker     (123)     7243 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_config_verify_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     6055 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_config_verify_data_errors.py
--rw-r--r--   0 runner    (1001) docker     (123)     4307 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_processed_until_timestamp_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     4316 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_processed_until_timestamp_response.py
--rw-r--r--   0 runner    (1001) docker     (123)     3035 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_purpose.py
--rw-r--r--   0 runner    (1001) docker     (123)     6024 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_raw_samples_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     4699 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_raw_samples_data_row.py
--rw-r--r--   0 runner    (1001) docker     (123)     6212 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_raw_samples_metadata_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     4803 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_raw_samples_metadata_data_row.py
--rw-r--r--   0 runner    (1001) docker     (123)     6276 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_raw_samples_predictions_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     4842 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_raw_samples_predictions_data_row.py
--rw-r--r--   0 runner    (1001) docker     (123)     3069 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dimensionality_reduction_method.py
--rw-r--r--   0 runner    (1001) docker     (123)     6051 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_license_information.py
--rw-r--r--   0 runner    (1001) docker     (123)     4799 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_artifact_create_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     4909 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_artifact_created_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     6061 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_artifact_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     3225 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_artifact_type.py
--rw-r--r--   0 runner    (1001) docker     (123)     7581 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_create_request.py
--rw-r--r--   0 runner    (1001) docker     (123)    11742 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     4456 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_log_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     5980 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_log_entry_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     3083 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_log_level.py
--rw-r--r--   0 runner    (1001) docker     (123)     6321 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_scheduled_create_request.py
--rw-r--r--   0 runner    (1001) docker     (123)    10613 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_scheduled_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     3078 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_scheduled_priority.py
--rw-r--r--   0 runner    (1001) docker     (123)     3071 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_scheduled_state.py
--rw-r--r--   0 runner    (1001) docker     (123)     5409 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_scheduled_update_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     3974 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_state.py
--rw-r--r--   0 runner    (1001) docker     (123)     4419 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_update_request.py
--rw-r--r--   0 runner    (1001) docker     (123)    11503 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_task_description.py
--rw-r--r--   0 runner    (1001) docker     (123)     7790 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_user_stats.py
--rw-r--r--   0 runner    (1001) docker     (123)     6780 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     4581 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_create_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     6742 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     6059 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)     4613 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_create_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     6804 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_data.py
--rw-r--r--   0 runner    (1001) docker     (123)    16176 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_docker.py
--rw-r--r--   0 runner    (1001) docker     (123)     5470 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_docker_object_level.py
--rw-r--r--   0 runner    (1001) docker     (123)     4718 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_docker_stopping_condition.py
--rw-r--r--   0 runner    (1001) docker     (123)     7729 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_lightly.py
--rw-r--r--   0 runner    (1001) docker     (123)    13793 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_lightly_collate.py
--rw-r--r--   0 runner    (1001) docker     (123)     5810 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_lightly_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     5281 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_lightly_trainer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6059 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3.py
--rw-r--r--   0 runner    (1001) docker     (123)     4613 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_create_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     6804 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_data.py
--rw-r--r--   0 runner    (1001) docker     (123)    15967 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_docker.py
--rw-r--r--   0 runner    (1001) docker     (123)     4063 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_docker_corruptness_check.py
--rw-r--r--   0 runner    (1001) docker     (123)     5715 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_docker_datasource.py
--rw-r--r--   0 runner    (1001) docker     (123)     3765 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_docker_training.py
--rw-r--r--   0 runner    (1001) docker     (123)     9498 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly.py
--rw-r--r--   0 runner    (1001) docker     (123)     4019 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_checkpoint_callback.py
--rw-r--r--   0 runner    (1001) docker     (123)    14601 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_collate.py
--rw-r--r--   0 runner    (1001) docker     (123)     3830 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_criterion.py
--rw-r--r--   0 runner    (1001) docker     (123)     6063 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_loader.py
--rw-r--r--   0 runner    (1001) docker     (123)     5810 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     4488 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_optimizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5281 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_trainer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2921 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_labels.py
--rw-r--r--   0 runner    (1001) docker     (123)     2911 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_name.py
--rw-r--r--   0 runner    (1001) docker     (123)    10855 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_registry_entry_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     3041 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_state.py
--rw-r--r--   0 runner    (1001) docker     (123)     2970 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_type.py
--rw-r--r--   0 runner    (1001) docker     (123)     2941 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/embedding2d_coordinates.py
--rw-r--r--   0 runner    (1001) docker     (123)     7646 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/embedding2d_create_request.py
--rw-r--r--   0 runner    (1001) docker     (123)    10520 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/embedding2d_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     5110 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/embedding_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     3048 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/file_name_format.py
--rw-r--r--   0 runner    (1001) docker     (123)     2990 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/file_output_format.py
--rw-r--r--   0 runner    (1001) docker     (123)     4582 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/filename_and_read_url.py
--rw-r--r--   0 runner    (1001) docker     (123)     2926 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/filename_and_read_urls.py
--rw-r--r--   0 runner    (1001) docker     (123)     2911 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/general_job_result.py
--rw-r--r--   0 runner    (1001) docker     (123)     2981 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/image_type.py
--rw-r--r--   0 runner    (1001) docker     (123)     5823 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/initial_tag_create_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     3099 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/job_result_type.py
--rw-r--r--   0 runner    (1001) docker     (123)     3032 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/job_state.py
--rw-r--r--   0 runner    (1001) docker     (123)    10716 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/job_status_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     4335 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/job_status_data_result.py
--rw-r--r--   0 runner    (1001) docker     (123)     6185 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/job_status_meta.py
--rw-r--r--   0 runner    (1001) docker     (123)     3061 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/job_status_upload_method.py
--rw-r--r--   0 runner    (1001) docker     (123)     8096 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/jobs_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     4762 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/label_box_data_row.py
--rw-r--r--   0 runner    (1001) docker     (123)     2911 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/label_box_data_rows.py
--rw-r--r--   0 runner    (1001) docker     (123)     5437 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/label_box_v4_data_row.py
--rw-r--r--   0 runner    (1001) docker     (123)     2921 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/label_box_v4_data_rows.py
--rw-r--r--   0 runner    (1001) docker     (123)     4482 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/label_studio_task.py
--rw-r--r--   0 runner    (1001) docker     (123)     5653 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/label_studio_task_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     2911 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/label_studio_tasks.py
--rw-r--r--   0 runner    (1001) docker     (123)     3056 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/lightly_model_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)     3056 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/lightly_model_v3.py
--rw-r--r--   0 runner    (1001) docker     (123)     3027 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/lightly_trainer_precision_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)     3027 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/lightly_trainer_precision_v3.py
--rw-r--r--   0 runner    (1001) docker     (123)     2896 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/mongo_object_id.py
--rw-r--r--   0 runner    (1001) docker     (123)     2871 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/object_id.py
--rw-r--r--   0 runner    (1001) docker     (123)     2891 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/path_safe_name.py
--rw-r--r--   0 runner    (1001) docker     (123)     3732 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/prediction_singleton.py
--rw-r--r--   0 runner    (1001) docker     (123)     8807 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/prediction_singleton_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     3867 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/prediction_singleton_classification.py
--rw-r--r--   0 runner    (1001) docker     (123)     6027 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/prediction_singleton_instance_segmentation.py
--rw-r--r--   0 runner    (1001) docker     (123)     5068 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/prediction_singleton_keypoint_detection.py
--rw-r--r--   0 runner    (1001) docker     (123)     4695 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/prediction_singleton_object_detection.py
--rw-r--r--   0 runner    (1001) docker     (123)     5188 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/prediction_singleton_semantic_segmentation.py
--rw-r--r--   0 runner    (1001) docker     (123)     2931 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/prediction_singletons.py
--rw-r--r--   0 runner    (1001) docker     (123)     5621 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/prediction_task_schema.py
--rw-r--r--   0 runner    (1001) docker     (123)     4261 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/prediction_task_schema_category.py
--rw-r--r--   0 runner    (1001) docker     (123)     2896 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/probabilities.py
--rw-r--r--   0 runner    (1001) docker     (123)     4219 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/questionnaire_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     2866 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/read_url.py
--rw-r--r--   0 runner    (1001) docker     (123)     2916 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/redirected_read_url.py
--rw-r--r--   0 runner    (1001) docker     (123)     3754 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/s3_region.py
--rw-r--r--   0 runner    (1001) docker     (123)     2971 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/s3_server_side_encryption_kms_key.py
--rw-r--r--   0 runner    (1001) docker     (123)     4931 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sama_task.py
--rw-r--r--   0 runner    (1001) docker     (123)     6775 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sama_task_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     2876 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sama_tasks.py
--rw-r--r--   0 runner    (1001) docker     (123)     8362 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sample_create_request.py
--rw-r--r--   0 runner    (1001) docker     (123)    12310 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sample_data.py
--rw-r--r--   0 runner    (1001) docker     (123)    12330 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sample_data_modes.py
--rw-r--r--   0 runner    (1001) docker     (123)     9695 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sample_meta_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     3019 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sample_partial_mode.py
--rw-r--r--   0 runner    (1001) docker     (123)     2968 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sample_sort_by.py
--rw-r--r--   0 runner    (1001) docker     (123)     2992 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sample_type.py
--rw-r--r--   0 runner    (1001) docker     (123)     6632 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sample_update_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     4365 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sample_write_urls.py
--rw-r--r--   0 runner    (1001) docker     (123)     3849 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sampling_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     5110 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sampling_config_stopping_condition.py
--rw-r--r--   0 runner    (1001) docker     (123)     8899 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sampling_create_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     3048 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sampling_method.py
--rw-r--r--   0 runner    (1001) docker     (123)     2856 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/score.py
--rw-r--r--   0 runner    (1001) docker     (123)     3400 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sector.py
--rw-r--r--   0 runner    (1001) docker     (123)     5382 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/selection_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     4655 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/selection_config_entry.py
--rw-r--r--   0 runner    (1001) docker     (123)     9474 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/selection_config_entry_input.py
--rw-r--r--   0 runner    (1001) docker     (123)     7337 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/selection_config_entry_strategy.py
--rw-r--r--   0 runner    (1001) docker     (123)     3101 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/selection_input_predictions_name.py
--rw-r--r--   0 runner    (1001) docker     (123)     3094 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/selection_input_type.py
--rw-r--r--   0 runner    (1001) docker     (123)     3163 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/selection_strategy_threshold_operation.py
--rw-r--r--   0 runner    (1001) docker     (123)     3111 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/selection_strategy_type.py
--rw-r--r--   0 runner    (1001) docker     (123)     4107 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/set_embeddings_is_processed_flag_by_id_body_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     6493 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/shared_access_config_create_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     9231 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/shared_access_config_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     2972 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/shared_access_type.py
--rw-r--r--   0 runner    (1001) docker     (123)     6421 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_active_learning_scores_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     3071 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_arithmetics_operation.py
--rw-r--r--   0 runner    (1001) docker     (123)     7673 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_arithmetics_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     2941 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_arithmetics_response.py
--rw-r--r--   0 runner    (1001) docker     (123)     2901 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_bit_mask_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     3872 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_bit_mask_response.py
--rw-r--r--   0 runner    (1001) docker     (123)     2896 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_change_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     5321 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_change_data_arithmetics.py
--rw-r--r--   0 runner    (1001) docker     (123)     3609 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_change_data_initial.py
--rw-r--r--   0 runner    (1001) docker     (123)     6996 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_change_data_metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     3082 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_change_data_operation_method.py
--rw-r--r--   0 runner    (1001) docker     (123)     4374 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_change_data_rename.py
--rw-r--r--   0 runner    (1001) docker     (123)     3712 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_change_data_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     6127 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_change_data_samples.py
--rw-r--r--   0 runner    (1001) docker     (123)     6836 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_change_data_scatterplot.py
--rw-r--r--   0 runner    (1001) docker     (123)     5060 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_change_data_upsize.py
--rw-r--r--   0 runner    (1001) docker     (123)     5976 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_change_entry.py
--rw-r--r--   0 runner    (1001) docker     (123)     9994 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_create_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     3260 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_creator.py
--rw-r--r--   0 runner    (1001) docker     (123)    12447 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     2911 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_filenames_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     2866 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_name.py
--rw-r--r--   0 runner    (1001) docker     (123)     5946 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_update_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     5573 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_upsize_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     2871 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/task_name.py
--rw-r--r--   0 runner    (1001) docker     (123)     3142 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/task_type.py
--rw-r--r--   0 runner    (1001) docker     (123)     5032 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/team_basic_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     3036 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/team_role.py
--rw-r--r--   0 runner    (1001) docker     (123)     2876 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/timestamp.py
--rw-r--r--   0 runner    (1001) docker     (123)     2911 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/timestamp_seconds.py
--rw-r--r--   0 runner    (1001) docker     (123)     4385 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/trigger2d_embedding_job_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     4775 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/update_docker_worker_registry_entry_request.py
--rw-r--r--   0 runner    (1001) docker     (123)     2896 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/version_number.py
--rw-r--r--   0 runner    (1001) docker     (123)     6053 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/video_frame_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     4761 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/models/write_csv_url_data.py
--rw-r--r--   0 runner    (1001) docker     (123)    13327 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/openapi_generated/swagger_client/rest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.152760 lightly-1.4.7/lightly/transforms/
--rw-r--r--   0 runner    (1001) docker     (123)     1468 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9167 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/dino_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     3973 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/fast_siam_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     2724 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/gaussian_blur.py
--rw-r--r--   0 runner    (1001) docker     (123)     1666 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/image_grid_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     2448 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/jigsaw.py
--rw-r--r--   0 runner    (1001) docker     (123)     1584 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/mae_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     5655 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/moco_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     6142 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/msn_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     2146 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/multi_crop_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)      775 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/multi_view_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     3315 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/pirl_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     7147 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/random_crop_and_flip_with_grid.py
--rw-r--r--   0 runner    (1001) docker     (123)     2951 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/rotation.py
--rw-r--r--   0 runner    (1001) docker     (123)     5973 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/simclr_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     5780 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/simsiam_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     5753 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/smog_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     1118 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/solarize.py
--rw-r--r--   0 runner    (1001) docker     (123)     5958 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/swav_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)       83 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     6022 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/vicreg_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     8414 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/transforms/vicregl_transform.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.156761 lightly-1.4.7/lightly/utils/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.156761 lightly-1.4.7/lightly/utils/benchmarking/
--rw-r--r--   0 runner    (1001) docker     (123)      426 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/benchmarking/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6475 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/benchmarking/benchmark_module.py
--rw-r--r--   0 runner    (1001) docker     (123)     2538 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/benchmarking/knn.py
--rw-r--r--   0 runner    (1001) docker     (123)     6260 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/benchmarking/knn_classifier.py
--rw-r--r--   0 runner    (1001) docker     (123)     5896 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/benchmarking/linear_classifier.py
--rw-r--r--   0 runner    (1001) docker     (123)     2432 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/benchmarking/metric_callback.py
--rw-r--r--   0 runner    (1001) docker     (123)     1811 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/benchmarking/online_linear_classifier.py
--rw-r--r--   0 runner    (1001) docker     (123)     1138 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/benchmarking/topk.py
--rw-r--r--   0 runner    (1001) docker     (123)     3469 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/bounding_box.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.156761 lightly-1.4.7/lightly/utils/cropping/
--rw-r--r--   0 runner    (1001) docker     (123)     4398 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/cropping/crop_image_by_bounding_boxes.py
--rw-r--r--   0 runner    (1001) docker     (123)     1269 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/cropping/read_yolo_label_file.py
--rw-r--r--   0 runner    (1001) docker     (123)     5552 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/debug.py
--rw-r--r--   0 runner    (1001) docker     (123)     2171 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/dist.py
--rw-r--r--   0 runner    (1001) docker     (123)     2604 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/embeddings_2d.py
--rw-r--r--   0 runner    (1001) docker     (123)      675 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/hipify.py
--rw-r--r--   0 runner    (1001) docker     (123)    11881 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/io.py
--rw-r--r--   0 runner    (1001) docker     (123)     5481 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/lars.py
--rw-r--r--   0 runner    (1001) docker     (123)     1178 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/reordering.py
--rw-r--r--   0 runner    (1001) docker     (123)     3212 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/scheduler.py
--rw-r--r--   0 runner    (1001) docker     (123)      885 2023-06-07 08:25:48.000000 lightly-1.4.7/lightly/utils/version_compare.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-07 08:25:57.124760 lightly-1.4.7/lightly.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)     2266 2023-06-07 08:25:57.000000 lightly-1.4.7/lightly.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    22189 2023-06-07 08:25:57.000000 lightly-1.4.7/lightly.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-07 08:25:57.000000 lightly-1.4.7/lightly.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      292 2023-06-07 08:25:57.000000 lightly-1.4.7/lightly.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (123)      866 2023-06-07 08:25:57.000000 lightly-1.4.7/lightly.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)        8 2023-06-07 08:25:57.000000 lightly-1.4.7/lightly.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)      135 2023-06-07 08:25:48.000000 lightly-1.4.7/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (123)       78 2023-06-07 08:25:57.156761 lightly-1.4.7/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     4901 2023-06-07 08:25:48.000000 lightly-1.4.7/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.681266 lightly-1.4.8/
+-rw-r--r--   0 runner    (1001) docker     (123)     1073 2023-06-13 17:21:06.000000 lightly-1.4.8/LICENSE.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      129 2023-06-13 17:21:06.000000 lightly-1.4.8/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)     2266 2023-06-13 17:21:23.681266 lightly-1.4.8/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    20812 2023-06-13 17:21:06.000000 lightly-1.4.8/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.649265 lightly-1.4.8/lightly/
+-rw-r--r--   0 runner    (1001) docker     (123)     3910 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.649265 lightly-1.4.8/lightly/active_learning/
+-rw-r--r--   0 runner    (1001) docker     (123)      391 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/active_learning/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.649265 lightly-1.4.8/lightly/active_learning/config/
+-rw-r--r--   0 runner    (1001) docker     (123)      200 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/active_learning/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2001 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/active_learning/config/selection_config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.653266 lightly-1.4.8/lightly/api/
+-rw-r--r--   0 runner    (1001) docker     (123)      555 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15450 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/api_workflow_artifacts.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10889 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/api_workflow_client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3132 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/api_workflow_collaboration.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24820 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/api_workflow_compute_worker.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16232 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/api_workflow_datasets.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32658 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/api_workflow_datasources.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11453 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/api_workflow_download_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13142 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/api_workflow_export.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6454 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/api_workflow_predictions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6982 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/api_workflow_selection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10007 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/api_workflow_tags.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12399 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/api_workflow_upload_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9895 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/api_workflow_upload_embeddings.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9094 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/api_workflow_upload_metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6802 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/bitmask.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19950 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/download.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2375 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/patch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2391 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/swagger_api_client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4271 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/swagger_rest_client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6424 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2528 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/api/version_checking.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.653266 lightly-1.4.8/lightly/cli/
+-rw-r--r--   0 runner    (1001) docker     (123)      444 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/cli/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1181 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/cli/_cli_simclr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7330 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/cli/_helpers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.653266 lightly-1.4.8/lightly/cli/config/
+-rw-r--r--   0 runner    (1001) docker     (123)      196 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/cli/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8870 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/cli/config/config.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      427 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/cli/config/get_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4468 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/cli/crop_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4847 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/cli/download_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3908 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/cli/embed_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1888 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/cli/lightly_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6372 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/cli/train_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)      677 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/cli/version_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7435 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/core.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.653266 lightly-1.4.8/lightly/data/
+-rw-r--r--   0 runner    (1001) docker     (123)      687 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4189 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/data/_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3811 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/data/_image.py
+-rw-r--r--   0 runner    (1001) docker     (123)      851 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/data/_image_loaders.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1203 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/data/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25874 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/data/_video.py
+-rw-r--r--   0 runner    (1001) docker     (123)    49619 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/data/collate.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13255 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/data/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2569 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/data/lightly_subset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3022 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/data/multi_view_collate.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.653266 lightly-1.4.8/lightly/embedding/
+-rw-r--r--   0 runner    (1001) docker     (123)      366 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/embedding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3778 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/embedding/_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2775 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/embedding/callbacks.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5426 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/embedding/embedding.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.657266 lightly-1.4.8/lightly/loss/
+-rw-r--r--   0 runner    (1001) docker     (123)      805 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2625 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/barlow_twins_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9091 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/dcl_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5289 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/dino_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3477 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/hypersphere_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3923 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/memory_bank.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8522 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/msn_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1344 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/negative_cosine_similarity.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7186 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/ntx_ent_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5790 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/pmsn_loss.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.657266 lightly-1.4.8/lightly/loss/regularizer/
+-rw-r--r--   0 runner    (1001) docker     (123)      230 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/regularizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5522 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/regularizer/co2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5741 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/swav_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2308 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/sym_neg_cos_sim_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3887 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/tico_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4817 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/vicreg_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17701 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/loss/vicregl_loss.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.657266 lightly-1.4.8/lightly/models/
+-rw-r--r--   0 runner    (1001) docker     (123)     1071 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3144 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/_momentum.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3976 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/barlowtwins.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2714 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/batchnorm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5349 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/byol.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4373 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/moco.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.657266 lightly-1.4.8/lightly/models/modules/
+-rw-r--r--   0 runner    (1001) docker     (123)      987 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23563 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/modules/heads.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14929 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/modules/masked_autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2108 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/modules/nn_memory_bank.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7095 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/nnclr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8383 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3399 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/simclr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4169 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/simsiam.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18480 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1573 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/models/zoo.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.657266 lightly-1.4.8/lightly/openapi_generated/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.657266 lightly-1.4.8/lightly/openapi_generated/swagger_client/
+-rw-r--r--   0 runner    (1001) docker     (123)    24822 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.661266 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/
+-rw-r--r--   0 runner    (1001) docker     (123)     1474 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22634 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/collaboration_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)    87539 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/datasets_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)   128887 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/datasources_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)   276374 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/docker_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24023 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/embeddings2d_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)    52407 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/embeddings_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13183 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/jobs_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8547 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/mappings_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30243 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/meta_data_configurations_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)    57379 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/predictions_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7024 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/quota_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)    80646 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/samples_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9319 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/samplings_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24340 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/scores_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)   160062 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/tags_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)    41554 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/teams_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13290 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api/versioning_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30347 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api_client.py
+-rw-r--r--   0 runner    (1001) docker     (123)      844 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/api_response.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17712 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5292 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/exceptions.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.677266 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/
+-rw-r--r--   0 runner    (1001) docker     (123)    22618 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3231 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/active_learning_score_create_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4160 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/active_learning_score_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4279 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/annotation_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2485 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/annotation_meta_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2723 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/annotation_offer_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1057 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/annotation_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5197 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/api_error_code.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3082 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/api_error_response.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2409 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/async_task_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2841 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/auth0_on_sign_up_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3161 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/auth0_on_sign_up_request_user.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3915 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/configuration_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3612 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/configuration_entry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3161 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/configuration_set_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2155 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/configuration_value_data_type.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2618 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/create_cf_bucket_activity_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3735 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/create_docker_worker_registry_entry_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2773 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/create_entity_response.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3477 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/create_sample_with_write_urls_response.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2707 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/create_team_membership_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1011 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/creator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4574 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/crop_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3948 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/dataset_create_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1046 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/dataset_creator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7328 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/dataset_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7426 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/dataset_data_enriched.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3553 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/dataset_embedding_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)      927 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/dataset_type.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2795 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/dataset_update_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13223 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3155 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_azure.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2822 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_azure_all_of.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5017 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3245 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_gcs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2912 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_gcs_all_of.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2760 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_lightly.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2742 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_local.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3850 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_obs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3575 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_obs_all_of.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4514 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_s3.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4181 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_s3_all_of.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5170 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_s3_delegated_access.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4837 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_s3_delegated_access_all_of.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3459 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_verify_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3018 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_verify_data_errors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2803 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_processed_until_timestamp_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2812 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_processed_until_timestamp_response.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1384 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_purpose.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3583 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_raw_samples_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2788 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_raw_samples_data_row.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3702 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_raw_samples_metadata_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2868 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_raw_samples_metadata_data_row.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3743 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_raw_samples_predictions_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2888 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_raw_samples_predictions_data_row.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1025 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/dimensionality_reduction_method.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3189 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_authorization_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2674 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_authorization_response.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2948 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_license_information.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3158 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_artifact_create_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2715 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_artifact_created_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3644 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_artifact_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1022 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_artifact_storage_location.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1207 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_artifact_type.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4405 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_create_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6058 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3215 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_log_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2964 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_log_entry_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1019 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_log_level.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3577 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_scheduled_create_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5358 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_scheduled_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1014 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_scheduled_priority.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1007 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_scheduled_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3151 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_scheduled_update_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1910 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2691 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_update_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4314 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_task_description.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3055 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_user_stats.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2662 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_authorization_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4169 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3128 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_create_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4044 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4143 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3155 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_create_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4075 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7094 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_docker.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4043 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_docker_object_level.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2932 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_docker_stopping_condition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5927 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_lightly.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5187 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_lightly_collate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3053 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_lightly_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3000 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_lightly_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4143 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3155 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_create_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4075 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6371 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_docker.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2860 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_docker_corruptness_check.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3015 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_docker_datasource.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3253 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_docker_training.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6811 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2821 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_checkpoint_callback.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5644 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_collate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2715 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_criterion.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3045 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_loader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3053 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2894 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3000 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4556 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_registry_entry_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)      977 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)      906 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_type.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3538 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/embedding2d_create_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5017 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/embedding2d_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3221 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/embedding_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1404 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/file_name_format.py
+-rw-r--r--   0 runner    (1001) docker     (123)      926 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/file_output_format.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2707 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/filename_and_read_url.py
+-rw-r--r--   0 runner    (1001) docker     (123)      917 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/image_type.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3817 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/initial_tag_create_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4002 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/internal_debug_latency.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2708 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/internal_debug_latency_mongodb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/job_result_type.py
+-rw-r--r--   0 runner    (1001) docker     (123)      968 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/job_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5317 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/job_status_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2922 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/job_status_data_result.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3014 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/job_status_meta.py
+-rw-r--r--   0 runner    (1001) docker     (123)      997 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/job_status_upload_method.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4075 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/jobs_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2764 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/label_box_data_row.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2938 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/label_box_v4_data_row.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2914 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/label_studio_task.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3438 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/label_studio_task_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)      994 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/lightly_docker_selection_method.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1052 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/lightly_model_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1052 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/lightly_model_v3.py
+-rw-r--r--   0 runner    (1001) docker     (123)      971 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/lightly_trainer_precision_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)      971 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/lightly_trainer_precision_v3.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12126 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5891 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3376 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton_classification.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2937 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton_classification_all_of.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3948 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton_instance_segmentation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3509 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton_instance_segmentation_all_of.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3692 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton_keypoint_detection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3253 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton_keypoint_detection_all_of.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3642 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton_object_detection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3203 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton_object_detection_all_of.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3691 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton_semantic_segmentation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3252 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton_semantic_segmentation_all_of.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4002 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_task_schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2936 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_task_schema_category.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3785 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/profile_basic_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4850 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/profile_me_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3261 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/profile_me_data_settings.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2642 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/questionnaire_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2289 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/s3_region.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2928 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sama_task.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3268 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sama_task_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4752 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sample_create_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6528 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sample_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6588 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sample_data_modes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4364 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sample_meta_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1022 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sample_partial_mode.py
+-rw-r--r--   0 runner    (1001) docker     (123)      903 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sample_sort_by.py
+-rw-r--r--   0 runner    (1001) docker     (123)      990 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sample_type.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3694 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sample_update_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2488 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sample_write_urls.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3014 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sampling_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3084 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sampling_config_stopping_condition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5504 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sampling_create_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)      984 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sampling_method.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1570 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/sector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3467 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/selection_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3386 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/selection_config_entry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5573 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/selection_config_entry_input.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3405 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/selection_config_entry_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1037 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/selection_input_predictions_name.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1030 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/selection_input_type.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1099 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/selection_strategy_threshold_operation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1047 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/selection_strategy_type.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2935 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/service_account_basic_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2804 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/set_embeddings_is_processed_flag_by_id_body_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3227 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/shared_access_config_create_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3890 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/shared_access_config_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)      908 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/shared_access_type.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3963 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_active_learning_scores_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1058 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_arithmetics_operation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4767 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_arithmetics_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5726 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_arithmetics_response.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2852 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_bit_mask_response.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6648 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2663 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data_arithmetics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2900 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data_initial.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3156 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data_metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1018 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data_operation_method.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2534 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data_rename.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2459 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3041 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data_samples.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3180 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data_scatterplot.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3132 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data_upsize.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3199 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_entry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6141 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_create_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1196 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_creator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7846 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4083 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_update_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3648 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_upsize_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1110 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/task_type.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2955 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/team_basic_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3129 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/team_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)      948 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/team_role.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2797 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/trigger2d_embedding_job_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2900 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/update_docker_worker_registry_entry_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2581 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/update_team_membership_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)      950 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/user_type.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3120 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/video_frame_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2603 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/models/write_csv_url_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12981 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/openapi_generated/swagger_client/rest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.681266 lightly-1.4.8/lightly/transforms/
+-rw-r--r--   0 runner    (1001) docker     (123)     1468 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9167 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/dino_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3973 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/fast_siam_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2724 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/gaussian_blur.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1666 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/image_grid_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2448 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/jigsaw.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1584 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/mae_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5655 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/moco_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6142 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/msn_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2146 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/multi_crop_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)      775 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/multi_view_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3315 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/pirl_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7147 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/random_crop_and_flip_with_grid.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2951 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/rotation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5973 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/simclr_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5780 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/simsiam_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5753 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/smog_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1118 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/solarize.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5958 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/swav_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)       83 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6022 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/vicreg_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8414 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/transforms/vicregl_transform.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.681266 lightly-1.4.8/lightly/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.681266 lightly-1.4.8/lightly/utils/benchmarking/
+-rw-r--r--   0 runner    (1001) docker     (123)      426 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/benchmarking/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6475 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/benchmarking/benchmark_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2538 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/benchmarking/knn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6214 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/benchmarking/knn_classifier.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5896 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/benchmarking/linear_classifier.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2432 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/benchmarking/metric_callback.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1811 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/benchmarking/online_linear_classifier.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1138 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/benchmarking/topk.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3469 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/bounding_box.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.681266 lightly-1.4.8/lightly/utils/cropping/
+-rw-r--r--   0 runner    (1001) docker     (123)     4398 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/cropping/crop_image_by_bounding_boxes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1269 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/cropping/read_yolo_label_file.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5552 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/debug.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2171 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/dist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2604 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/embeddings_2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      675 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/hipify.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11881 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/io.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5481 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/lars.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1178 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/reordering.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3212 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (123)      885 2023-06-13 17:21:06.000000 lightly-1.4.8/lightly/utils/version_compare.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:21:23.649265 lightly-1.4.8/lightly.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     2266 2023-06-13 17:21:23.000000 lightly-1.4.8/lightly.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    22568 2023-06-13 17:21:23.000000 lightly-1.4.8/lightly.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-13 17:21:23.000000 lightly-1.4.8/lightly.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      292 2023-06-13 17:21:23.000000 lightly-1.4.8/lightly.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      958 2023-06-13 17:21:23.000000 lightly-1.4.8/lightly.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        8 2023-06-13 17:21:23.000000 lightly-1.4.8/lightly.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      135 2023-06-13 17:21:06.000000 lightly-1.4.8/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)       78 2023-06-13 17:21:23.681266 lightly-1.4.8/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     4985 2023-06-13 17:21:06.000000 lightly-1.4.8/setup.py
```

### Comparing `lightly-1.4.7/LICENSE.txt` & `lightly-1.4.8/LICENSE.txt`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/PKG-INFO` & `lightly-1.4.8/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: lightly
-Version: 1.4.7
+Version: 1.4.8
 Summary: A deep learning package for self-supervised learning
 Author: Philipp Wirth & Igor Susmelj
 Author-email: philipp@lightly.ai
 License: MIT
 Project-URL: Homepage, https://www.lightly.ai
 Project-URL: Web-App, https://app.lightly.ai
 Project-URL: Documentation, https://docs.lightly.ai
```

### Comparing `lightly-1.4.7/README.md` & `lightly-1.4.8/README.md`

 * *Files 1% similar despite different names*

```diff
@@ -276,14 +276,21 @@
 Implemented models and their performance on various datasets. Hyperparameters are not
 tuned for maximum accuracy. For detailed results and more info about the benchmarks click
 [here](https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html).
 
 
 ### Imagenet
 
+> **Note**: Evaluation settings are based on these papers:
+> * Linear: [SimCLR](https://arxiv.org/abs/2002.05709)
+> * Finetune: [SimCLR](https://arxiv.org/abs/2002.05709)
+> * KNN: [InstDisc](https://arxiv.org/abs/1805.01978)
+> 
+> See the [benchmarking scripts](./benchmarks/imagenet/resnet50/) for details.
+
 | Model       | Backbone | Batch Size | Epochs | Linear Top1 | Finetune Top1 | KNN Top1 | Tensorboard | Checkpoint |
 |-------------|----------|------------|--------|-------------|---------------|----------|-------------|------------|
 | SimCLR      | Res50    |        256 |    100 |        63.2 |           N/A |     44.9 |      [link](https://tensorboard.dev/experiment/JwNs9E02TeeQkS7aljh8dA) |       [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_simclr_2023-05-04_09-02-54/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt) |
 | SwAV        | Res50    |        256 |    100 |        67.2 |          75.4 |     49.5 |      [link](https://tensorboard.dev/experiment/Ipx4Oxl5Qkqm5Sl5kWyKKg) |       [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_swav_2023-05-25_08-29-14/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)
 
 
 ### ImageNette
@@ -412,14 +419,15 @@
   By building, what we call a self-supervised active learning loop we help companies understand and work with their data more efficiently. 
   As the [Lightly Solution](https://docs.lightly.ai) is a freemium product, you can try it out for free. However, we will charge for some features.
   - In any case this framework will always be free to use, even for commercial purposes.
 
 
 ## Lightly in Research
 
+- [Reverse Engineering Self-Supervised Learning, 2023](https://arxiv.org/abs/2305.15614)
 - [Learning Visual Representations via Language-Guided Sampling, 2023](https://arxiv.org/pdf/2302.12248.pdf)
 - [Self-Supervised Learning Methods for Label-Efficient Dental Caries Classification, 2022](https://www.mdpi.com/2075-4418/12/5/1237)
 - [DPCL: Constrative Representation Learning with Differential Privacy, 2022](https://assets.researchsquare.com/files/rs-1516950/v1_covered.pdf?c=1654486158)
 - [Decoupled Contrastive Learning, 2021](https://arxiv.org/abs/2110.06848)
 - [solo-learn: A Library of Self-supervised Methods for Visual Representation Learning, 2021](https://www.jmlr.org/papers/volume23/21-1155/21-1155.pdf)
```

### Comparing `lightly-1.4.7/lightly/__init__.py` & `lightly-1.4.8/lightly/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -71,15 +71,15 @@
 
 """
 
 # Copyright (c) 2020. Lightly AG and its affiliates.
 # All Rights Reserved
 
 __name__ = "lightly"
-__version__ = "1.4.7"
+__version__ = "1.4.8"
 
 import os
 
 try:
     # See (https://github.com/PyTorchLightning/pytorch-lightning)
     # This variable is injected in the __builtins__ by the build
     # process. It used to enable importing subpackages of skimage when
```

### Comparing `lightly-1.4.7/lightly/active_learning/config/selection_config.py` & `lightly-1.4.8/lightly/active_learning/config/selection_config.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/api/__init__.py` & `lightly-1.4.8/lightly/api/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 """ The lightly.api module provides access to the Lightly API."""
 
 # Copyright (c) 2020. Lightly AG and its affiliates.
 # All Rights Reserved
 from lightly.api import patch as _patch
 from lightly.api.api_workflow_artifacts import ArtifactNotExist
 from lightly.api.api_workflow_client import ApiWorkflowClient
-from lightly.openapi_generated.swagger_client import Configuration as _Configuration
+from lightly.openapi_generated.swagger_client.api_client import (
+    Configuration as _Configuration,
+)
 
 # Make ApiWorkflowClient and swagger classes picklable.
 _patch.make_swagger_configuration_picklable(
     configuration_cls=_Configuration,
 )
```

### Comparing `lightly-1.4.7/lightly/api/api_workflow_artifacts.py` & `lightly-1.4.8/lightly/api/api_workflow_artifacts.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,16 +1,14 @@
 import os
 
 from lightly.api import download
-from lightly.openapi_generated.swagger_client import (
+from lightly.openapi_generated.swagger_client.models import (
     DockerRunArtifactData,
-    DockerRunData,
-)
-from lightly.openapi_generated.swagger_client.models.docker_run_artifact_type import (
     DockerRunArtifactType,
+    DockerRunData,
 )
 
 
 class ArtifactNotExist(Exception):
     pass
```

### Comparing `lightly-1.4.7/lightly/api/api_workflow_client.py` & `lightly-1.4.8/lightly/api/api_workflow_client.py`

 * *Files 0% similar despite different names*

```diff
@@ -27,33 +27,31 @@
     get_api_client_configuration,
     get_signed_url_destination,
 )
 from lightly.api.version_checking import (
     LightlyAPITimeoutException,
     is_compatible_version,
 )
-from lightly.openapi_generated.swagger_client import (
-    ApiClient,
+from lightly.openapi_generated.swagger_client.api import (
     CollaborationApi,
-    Creator,
-    DatasetData,
     DatasetsApi,
     DatasourcesApi,
     DockerApi,
     EmbeddingsApi,
     JobsApi,
     MappingsApi,
     MetaDataConfigurationsApi,
     PredictionsApi,
     QuotaApi,
     SamplesApi,
     SamplingsApi,
     ScoresApi,
     TagsApi,
 )
+from lightly.openapi_generated.swagger_client.models import Creator, DatasetData
 from lightly.openapi_generated.swagger_client.rest import ApiException
 from lightly.utils.reordering import sort_items_by_keys
 
 # Env variable for server side encryption on S3
 LIGHTLY_S3_SSE_KMS_KEY = "LIGHTLY_S3_SSE_KMS_KEY"
 
 
@@ -121,15 +119,15 @@
         ):
             pass
 
         configuration = get_api_client_configuration(token=token)
         self.api_client = LightlySwaggerApiClient(configuration=configuration)
         self.api_client.user_agent = f"Lightly/{__version__} ({platform.system()}/{platform.release()}; {platform.platform()}; {platform.processor()};) python/{platform.python_version()}"
 
-        self.token = configuration.api_key["token"]
+        self.token = configuration.api_key["ApiKeyAuth"]
         if dataset_id is not None:
             self._dataset_id = dataset_id
         if embedding_id is not None:
             self.embedding_id = embedding_id
         self._creator = creator
 
         self._collaboration_api = CollaborationApi(api_client=self.api_client)
```

### Comparing `lightly-1.4.7/lightly/api/api_workflow_collaboration.py` & `lightly-1.4.8/lightly/api/api_workflow_collaboration.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,16 +1,12 @@
 from typing import List
 
-from lightly.openapi_generated.swagger_client.models.shared_access_config_create_request import (
+from lightly.openapi_generated.swagger_client.models import (
     SharedAccessConfigCreateRequest,
-)
-from lightly.openapi_generated.swagger_client.models.shared_access_config_data import (
     SharedAccessConfigData,
-)
-from lightly.openapi_generated.swagger_client.models.shared_access_type import (
     SharedAccessType,
 )
 
 
 class _CollaborationMixin:
     def share_dataset_only_with(self, dataset_id: str, user_emails: List[str]) -> None:
         """Shares a dataset with a list of users.
@@ -41,15 +37,15 @@
           >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
           >>> client.share_dataset_only_with(dataset_id="MY_DATASET_ID", user_emails=[])
         """
         body = SharedAccessConfigCreateRequest(
             access_type=SharedAccessType.WRITE, users=user_emails, creator=self._creator
         )
         self._collaboration_api.create_or_update_shared_access_config_by_dataset_id(
-            body=body, dataset_id=dataset_id
+            shared_access_config_create_request=body, dataset_id=dataset_id
         )
 
     def get_shared_users(self, dataset_id: str) -> List[str]:
         """Fetches a list of users that have access to the dataset.
 
         Args:
             dataset_id:
```

### Comparing `lightly-1.4.7/lightly/api/api_workflow_compute_worker.py` & `lightly-1.4.8/lightly/api/api_workflow_compute_worker.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,16 +3,16 @@
 import difflib
 import time
 from functools import partial
 from typing import Any, Callable, Dict, Iterator, List, Optional, Type, TypeVar, Union
 
 from lightly.api import utils
 from lightly.api.utils import retry
-from lightly.openapi_generated.swagger_client import (
-    ApiClient,
+from lightly.openapi_generated.swagger_client.api_client import ApiClient
+from lightly.openapi_generated.swagger_client.models import (
     CreateDockerWorkerRegistryEntryRequest,
     DockerRunData,
     DockerRunScheduledCreateRequest,
     DockerRunScheduledData,
     DockerRunScheduledPriority,
     DockerRunScheduledState,
     DockerRunState,
@@ -304,15 +304,15 @@
         request = DockerRunScheduledCreateRequest(
             config_id=config_id,
             priority=priority,
             runs_on=runs_on,
             creator=self._creator,
         )
         response = self._compute_worker_api.create_docker_run_scheduled_by_dataset_id(
-            body=request,
+            docker_run_scheduled_create_request=request,
             dataset_id=self.dataset_id,
         )
         return response.id
 
     def get_compute_worker_runs_iter(
         self,
         dataset_id: Optional[str] = None,
@@ -669,32 +669,27 @@
     cfg: Optional[Dict[str, Any]],
     obj: Any,
 ) -> None:
     """Validates that all keys in cfg are legitimate configuration options.
 
     Recursively checks if the keys in the cfg dictionary match the attributes of
     the DockerWorkerConfigV2Docker/DockerWorkerConfigV2Lightly instances. If not,
-    suggests a best match based on the keys in 'swagger_types'.
+    suggests a best match.
 
     Raises:
-        TypeError: If obj is not of swagger type.
+        InvalidConfigurationError: If obj is not a valid config.
 
     """
 
     if cfg is None:
         return
 
-    if not hasattr(type(obj), "swagger_types"):
-        raise TypeError(
-            f"Type {type(obj)} of argument 'obj' has not attribute 'swagger_types'"
-        )
-
     for key, item in cfg.items():
         if not hasattr(obj, key):
-            possible_options = list(type(obj).swagger_types.keys())
+            possible_options = list(obj.__fields__.keys())
             closest_match = difflib.get_close_matches(
                 word=key, possibilities=possible_options, n=1, cutoff=0.0
             )[0]
             error_msg = (
                 f"Option '{key}' does not exist! Did you mean '{closest_match}'?"
             )
             raise InvalidConfigurationError(error_msg)
```

### Comparing `lightly-1.4.7/lightly/api/api_workflow_datasets.py` & `lightly-1.4.8/lightly/api/api_workflow_datasets.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import warnings
 from itertools import chain
 from typing import Iterator, List, Optional
 
 from lightly.api import utils
-from lightly.openapi_generated.swagger_client import (
+from lightly.openapi_generated.swagger_client.models import (
     CreateEntityResponse,
     DatasetCreateRequest,
     DatasetData,
     DatasetType,
 )
 from lightly.openapi_generated.swagger_client.rest import ApiException
 
@@ -288,15 +288,15 @@
                 `DatasetType.IMAGES` and `DatasetType.VIDEOS`.
 
         Raises:
             ValueError: If a dataset with dataset_name already exists.
 
         Examples:
             >>> from lightly.api import ApiWorkflowClient
-            >>> from lightly.openapi_generated.swagger_client.models.dataset_type import DatasetType
+            >>> from lightly.openapi_generated.swagger_client.models import DatasetType
             >>>
             >>> client = lightly.api.ApiWorkflowClient(token="YOUR_TOKEN")
             >>> client.create_dataset('your-dataset-name', dataset_type=DatasetType.IMAGES)
             >>>
             >>> # or to work with videos
             >>> client.create_dataset('your-dataset-name', dataset_type=DatasetType.VIDEOS)
             >>>
@@ -333,15 +333,17 @@
                 The type of the dataset. We recommend to use the API provided
                 constants `DatasetType.IMAGES` and `DatasetType.VIDEOS`.
 
         """
         body = DatasetCreateRequest(
             name=dataset_name, type=dataset_type, creator=self._creator
         )
-        response: CreateEntityResponse = self._datasets_api.create_dataset(body=body)
+        response: CreateEntityResponse = self._datasets_api.create_dataset(
+            dataset_create_request=body
+        )
         self._dataset_id = response.id
 
     def create_new_dataset_with_unique_name(
         self,
         dataset_basename: str,
         dataset_type: str = DatasetType.IMAGES,
     ) -> None:
```

### Comparing `lightly-1.4.7/lightly/api/api_workflow_datasources.py` & `lightly-1.4.8/lightly/api/api_workflow_datasources.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,27 +1,19 @@
 import time
 import warnings
 from typing import Dict, List, Optional, Tuple, Union
 
 import tqdm
 
-from lightly.openapi_generated.swagger_client import DatasourceConfigVerifyDataErrors
-from lightly.openapi_generated.swagger_client.models.datasource_config import (
+from lightly.openapi_generated.swagger_client.models import (
     DatasourceConfig,
-)
-from lightly.openapi_generated.swagger_client.models.datasource_processed_until_timestamp_request import (
+    DatasourceConfigVerifyDataErrors,
     DatasourceProcessedUntilTimestampRequest,
-)
-from lightly.openapi_generated.swagger_client.models.datasource_processed_until_timestamp_response import (
     DatasourceProcessedUntilTimestampResponse,
-)
-from lightly.openapi_generated.swagger_client.models.datasource_purpose import (
     DatasourcePurpose,
-)
-from lightly.openapi_generated.swagger_client.models.datasource_raw_samples_data import (
     DatasourceRawSamplesData,
 )
 
 
 class _DatasourcesMixin:
     def _download_raw_files(
         self,
@@ -43,15 +35,15 @@
             {"relevant_filenames_file_name": relevant_filenames_file_name}
             if relevant_filenames_file_name
             else dict()
         )
 
         response: DatasourceRawSamplesData = download_function(
             dataset_id=self.dataset_id,
-            _from=from_,
+            var_from=from_,
             to=to,
             use_redirected_read_url=use_redirected_read_url,
             **relevant_filenames_kwargs,
             **kwargs,
         )
         cursor = response.cursor
         samples = response.data
@@ -407,15 +399,16 @@
             >>> client.download_new_raw_samples()
             [('image-3.png', 'https://......'), ('image-4.png', 'https://......')]
         """
         body = DatasourceProcessedUntilTimestampRequest(
             processed_until_timestamp=timestamp
         )
         self._datasources_api.update_datasource_processed_until_timestamp_by_dataset_id(
-            dataset_id=self.dataset_id, body=body
+            dataset_id=self.dataset_id,
+            datasource_processed_until_timestamp_request=body,
         )
 
     def get_datasource(self) -> DatasourceConfig:
         """Returns the datasource of the current dataset.
 
         Returns:
             Datasource data of the datasource of the current dataset.
@@ -457,22 +450,24 @@
                 Datasource purpose, determines if datasource is read only (INPUT)
                 or can be written to as well (LIGHTLY, INPUT_OUTPUT).
                 The latter is required when Lightly extracts frames from input videos.
 
         """
         # TODO: Use DatasourceConfigAzure once we switch/update the api generator.
         self._datasources_api.update_datasource_by_dataset_id(
-            body={
-                "type": "AZURE",
-                "fullPath": container_name,
-                "thumbSuffix": thumbnail_suffix,
-                "accountName": account_name,
-                "accountKey": sas_token,
-                "purpose": purpose,
-            },
+            datasource_config=DatasourceConfig.from_dict(
+                {
+                    "type": "AZURE",
+                    "fullPath": container_name,
+                    "thumbSuffix": thumbnail_suffix,
+                    "accountName": account_name,
+                    "accountKey": sas_token,
+                    "purpose": purpose,
+                }
+            ),
             dataset_id=self.dataset_id,
         )
 
     def set_gcs_config(
         self,
         resource_path: str,
         project_id: str,
@@ -505,22 +500,24 @@
                 Datasource purpose, determines if datasource is read only (INPUT)
                 or can be written to as well (LIGHTLY, INPUT_OUTPUT).
                 The latter is required when Lightly extracts frames from input videos.
 
         """
         # TODO: Use DatasourceConfigGCS once we switch/update the api generator.
         self._datasources_api.update_datasource_by_dataset_id(
-            body={
-                "type": "GCS",
-                "fullPath": resource_path,
-                "thumbSuffix": thumbnail_suffix,
-                "gcsProjectId": project_id,
-                "gcsCredentials": credentials,
-                "purpose": purpose,
-            },
+            datasource_config=DatasourceConfig.from_dict(
+                {
+                    "type": "GCS",
+                    "fullPath": resource_path,
+                    "thumbSuffix": thumbnail_suffix,
+                    "gcsProjectId": project_id,
+                    "gcsCredentials": credentials,
+                    "purpose": purpose,
+                }
+            ),
             dataset_id=self.dataset_id,
         )
 
     def set_local_config(
         self,
         resource_path: str,
         thumbnail_suffix: Optional[
@@ -539,20 +536,22 @@
                 Where to save thumbnails of the images in the dataset, for
                 example ".lightly/thumbnails/[filename]_thumb.[extension]".
                 Set to None to disable thumbnails and use the full images from the
                 datasource instead.
         """
         # TODO: Use DatasourceConfigLocal once we switch/update the api generator.
         self._datasources_api.update_datasource_by_dataset_id(
-            body={
-                "type": "LOCAL",
-                "fullPath": resource_path,
-                "thumbSuffix": thumbnail_suffix,
-                "purpose": DatasourcePurpose.INPUT_OUTPUT,
-            },
+            datasource_config=DatasourceConfig.from_dict(
+                {
+                    "type": "LOCAL",
+                    "fullPath": resource_path,
+                    "thumbSuffix": thumbnail_suffix,
+                    "purpose": DatasourcePurpose.INPUT_OUTPUT,
+                }
+            ),
             dataset_id=self.dataset_id,
         )
 
     def set_s3_config(
         self,
         resource_path: str,
         region: str,
@@ -586,23 +585,25 @@
                 Datasource purpose, determines if datasource is read only (INPUT)
                 or can be written to as well (LIGHTLY, INPUT_OUTPUT).
                 The latter is required when Lightly extracts frames from input videos.
 
         """
         # TODO: Use DatasourceConfigS3 once we switch/update the api generator.
         self._datasources_api.update_datasource_by_dataset_id(
-            body={
-                "type": "S3",
-                "fullPath": resource_path,
-                "thumbSuffix": thumbnail_suffix,
-                "s3Region": region,
-                "s3AccessKeyId": access_key,
-                "s3SecretAccessKey": secret_access_key,
-                "purpose": purpose,
-            },
+            datasource_config=DatasourceConfig.from_dict(
+                {
+                    "type": "S3",
+                    "fullPath": resource_path,
+                    "thumbSuffix": thumbnail_suffix,
+                    "s3Region": region,
+                    "s3AccessKeyId": access_key,
+                    "s3SecretAccessKey": secret_access_key,
+                    "purpose": purpose,
+                }
+            ),
             dataset_id=self.dataset_id,
         )
 
     def set_s3_delegated_access_config(
         self,
         resource_path: str,
         region: str,
@@ -636,23 +637,25 @@
                 Datasource purpose, determines if datasource is read only (INPUT)
                 or can be written to as well (LIGHTLY, INPUT_OUTPUT).
                 The latter is required when Lightly extracts frames from input videos.
 
         """
         # TODO: Use DatasourceConfigS3 once we switch/update the api generator.
         self._datasources_api.update_datasource_by_dataset_id(
-            body={
-                "type": "S3DelegatedAccess",
-                "fullPath": resource_path,
-                "thumbSuffix": thumbnail_suffix,
-                "s3Region": region,
-                "s3ARN": role_arn,
-                "s3ExternalId": external_id,
-                "purpose": purpose,
-            },
+            datasource_config=DatasourceConfig.from_dict(
+                {
+                    "type": "S3DelegatedAccess",
+                    "fullPath": resource_path,
+                    "thumbSuffix": thumbnail_suffix,
+                    "s3Region": region,
+                    "s3ARN": role_arn,
+                    "s3ExternalId": external_id,
+                    "purpose": purpose,
+                }
+            ),
             dataset_id=self.dataset_id,
         )
 
     def set_obs_config(
         self,
         resource_path: str,
         obs_endpoint: str,
@@ -682,23 +685,25 @@
             purpose:
                 Datasource purpose, determines if datasource is read only (INPUT)
                 or can be written to as well (LIGHTLY, INPUT_OUTPUT).
                 The latter is required when Lightly extracts frames from input videos.
         """
         # TODO: Use DatasourceConfigOBS once we switch/update the api generator.
         self._datasources_api.update_datasource_by_dataset_id(
-            body={
-                "type": "OBS",
-                "fullPath": resource_path,
-                "thumbSuffix": thumbnail_suffix,
-                "obsEndpoint": obs_endpoint,
-                "obsAccessKeyId": obs_access_key_id,
-                "obsSecretAccessKey": obs_secret_access_key,
-                "purpose": purpose,
-            },
+            datasource_config=DatasourceConfig.from_dict(
+                {
+                    "type": "OBS",
+                    "fullPath": resource_path,
+                    "thumbSuffix": thumbnail_suffix,
+                    "obsEndpoint": obs_endpoint,
+                    "obsAccessKeyId": obs_access_key_id,
+                    "obsSecretAccessKey": obs_secret_access_key,
+                    "purpose": purpose,
+                }
+            ),
             dataset_id=self.dataset_id,
         )
 
     def get_prediction_read_url(
         self,
         filename: str,
     ):
@@ -710,16 +715,16 @@
 
         Returns:
             A read-url to the file. Note that a URL will be returned even if the file does not
             exist.
 
         """
         return self._datasources_api.get_prediction_file_read_url_from_datasource_by_dataset_id(
-            self.dataset_id,
-            filename,
+            dataset_id=self.dataset_id,
+            file_name=filename,
         )
 
     def get_metadata_read_url(
         self,
         filename: str,
     ):
         """Returns a read-url for .lightly/metadata/{filename}.
@@ -730,16 +735,16 @@
 
         Returns:
             A read-url to the file. Note that a URL will be returned even if the file does not
             exist.
 
         """
         return self._datasources_api.get_metadata_file_read_url_from_datasource_by_dataset_id(
-            self.dataset_id,
-            filename,
+            dataset_id=self.dataset_id,
+            file_name=filename,
         )
 
     def get_custom_embedding_read_url(
         self,
         filename: str,
     ) -> str:
         """Returns a read-url for .lightly/embeddings/{filename}.
@@ -750,16 +755,16 @@
 
         Returns:
             A read-url to the file. Note that a URL will be returned even if the file does not
             exist.
 
         """
         return self._datasources_api.get_custom_embedding_file_read_url_from_datasource_by_dataset_id(
-            self.dataset_id,
-            filename,
+            dataset_id=self.dataset_id,
+            file_name=filename,
         )
 
     def list_datasource_permissions(
         self,
     ) -> Dict[str, Union[bool, Optional[DatasourceConfigVerifyDataErrors]]]:
         """Lists granted access permissions for the datasource set up with a dataset.
```

### Comparing `lightly-1.4.7/lightly/api/api_workflow_download_dataset.py` & `lightly-1.4.8/lightly/api/api_workflow_download_dataset.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,18 +6,17 @@
 from urllib.request import Request, urlopen
 
 import tqdm
 from PIL import Image
 
 from lightly.api import download
 from lightly.api.bitmask import BitMask
-from lightly.api.utils import paginate_endpoint, retry
-from lightly.openapi_generated.swagger_client import (
+from lightly.api.utils import paginate_endpoint
+from lightly.openapi_generated.swagger_client.models import (
     DatasetEmbeddingData,
-    FileNameFormat,
     ImageType,
 )
 from lightly.utils.hipify import bcolors
 
 
 def _make_dir_and_save_image(output_dir: str, filename: str, img: Image):
     """Saves the images and creates necessary subdirectories."""
@@ -120,16 +119,16 @@
 
         # define lambda function for concurrent download
         def lambda_(i):
             sample_id, filename = i
             # try to download image
             try:
                 read_url = self._samples_api.get_sample_image_read_url_by_id(
-                    self.dataset_id,
-                    sample_id,
+                    dataset_id=self.dataset_id,
+                    sample_id=sample_id,
                     type="full",
                 )
                 img = _get_image_from_read_url(read_url)
                 _make_dir_and_save_image(output_dir, filename, img)
                 success = True
             except Exception as e:  # pylint: disable=broad-except
                 warnings.warn(f"Downloading of image {filename} failed with error {e}")
```

### Comparing `lightly-1.4.7/lightly/api/api_workflow_export.py` & `lightly-1.4.8/lightly/api/api_workflow_export.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,26 +1,17 @@
-import io
-import os
 import warnings
-from concurrent.futures.thread import ThreadPoolExecutor
-from typing import Dict, List, Optional
-from urllib.request import Request, urlopen
+from typing import Dict, List
 
-import tqdm
-from PIL import Image
-
-from lightly.api import download
-from lightly.api.bitmask import BitMask
 from lightly.api.utils import paginate_endpoint, retry
-from lightly.openapi_generated.swagger_client import (
-    DatasetEmbeddingData,
+from lightly.openapi_generated.swagger_client.models import (
     FileNameFormat,
-    ImageType,
+    LabelBoxDataRow,
+    LabelBoxV4DataRow,
+    LabelStudioTask,
 )
-from lightly.utils.hipify import bcolors
 
 
 class _ExportDatasetMixin:
     def export_label_studio_tasks_by_tag_id(
         self,
         tag_id: str,
     ) -> List[Dict]:
@@ -30,36 +21,36 @@
         https://labelstud.io/guide/tasks.html#Basic-Label-Studio-JSON-format
 
         More information:
         https://docs.lightly.ai/docs/labelstudio-integration
 
         Args:
             tag_id:
-                Id of the tag which should exported.
+                ID of the tag which should exported.
 
         Returns:
             A list of dictionaries in a format compatible with Label Studio.
 
         Examples:
             >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
             >>>
             >>> # Already created some Lightly Worker runs with this dataset
             >>> client.set_dataset_id_by_name("my-dataset")
             >>> client.export_label_studio_tasks_by_tag_id(tag_id="646f34608a5613b57d8b73cc")
             [{'id': 0, 'data': {'image': '...', ...}}]
         """
-        label_studio_tasks = list(
+        label_studio_tasks: List[LabelStudioTask] = list(
             paginate_endpoint(
                 self._tags_api.export_tag_to_label_studio_tasks,
                 page_size=20000,
                 dataset_id=self.dataset_id,
                 tag_id=tag_id,
             )
         )
-        return label_studio_tasks
+        return [task.to_dict(by_alias=True) for task in label_studio_tasks]
 
     def export_label_studio_tasks_by_tag_name(
         self,
         tag_name: str,
     ) -> List[Dict]:
         """Fetches samples in a format compatible with Label Studio.
 
@@ -118,23 +109,23 @@
         warnings.warn(
             DeprecationWarning(
                 "This method exports data in the deprecated Labelbox v3 format and "
                 "will be removed in the future. Use export_label_box_v4_data_rows_by_tag_id "
                 "to export data in the Labelbox v4 format instead."
             )
         )
-        label_box_data_rows = list(
+        label_box_data_rows: List[LabelBoxDataRow] = list(
             paginate_endpoint(
                 self._tags_api.export_tag_to_label_box_data_rows,
                 page_size=20000,
                 dataset_id=self.dataset_id,
                 tag_id=tag_id,
             )
         )
-        return label_box_data_rows
+        return [row.to_dict(by_alias=True) for row in label_box_data_rows]
 
     def export_label_box_data_rows_by_tag_name(
         self,
         tag_name: str,
     ) -> List[Dict]:
         """Fetches samples in a format compatible with Labelbox v3.
 
@@ -191,23 +182,23 @@
             >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
             >>>
             >>> # Already created some Lightly Worker runs with this dataset
             >>> client.set_dataset_id_by_name("my-dataset")
             >>> client.export_label_box_v4_data_rows_by_tag_id(tag_id="646f34608a5613b57d8b73cc")
             [{'row_data': '...', 'global_key': 'image-1.jpg', 'media_type': 'IMAGE'}
         """
-        label_box_data_rows = list(
+        label_box_data_rows: List[LabelBoxV4DataRow] = list(
             paginate_endpoint(
                 self._tags_api.export_tag_to_label_box_v4_data_rows,
                 page_size=20000,
                 dataset_id=self.dataset_id,
                 tag_id=tag_id,
             )
         )
-        return label_box_data_rows
+        return [row.to_dict() for row in label_box_data_rows]
 
     def export_label_box_v4_data_rows_by_tag_name(
         self,
         tag_name: str,
     ) -> List[Dict]:
         """Fetches samples in a format compatible with Labelbox.
```

### Comparing `lightly-1.4.7/lightly/api/api_workflow_predictions.py` & `lightly-1.4.8/lightly/api/api_workflow_predictions.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 from concurrent.futures import ThreadPoolExecutor
-from typing import List, Mapping, Optional, Sequence, Tuple
+from typing import Mapping, Optional, Sequence, Tuple
 
 import tqdm
 
-from lightly.api.prediction_singletons import PredictionSingletonRepr
-from lightly.openapi_generated.swagger_client import PredictionTaskSchema
+from lightly.openapi_generated.swagger_client.models import (
+    PredictionSingleton,
+    PredictionTaskSchema,
+)
 
 
 class _PredictionsMixin:
     def create_or_update_prediction_task_schema(
         self,
         schema: PredictionTaskSchema,
         prediction_version_id: int = -1,
@@ -21,15 +23,15 @@
             prediction_version_id:
                 A numerical ID (e.g., timestamp) to distinguish different predictions of different model versions.
                 Use the same ID if you don't require versioning or if you wish to overwrite the previous schema.
 
         Example:
           >>> import time
           >>> from lightly.api import ApiWorkflowClient
-          >>> from lightly.openapi_generated.swagger_client import (
+          >>> from lightly.openapi_generated.swagger_client.models import (
           >>>     PredictionTaskSchema,
           >>>     TaskType,
           >>>     PredictionTaskSchemaCategory,
           >>> )
           >>>
           >>> client = ApiWorkflowClient(
           >>>     token="MY_LIGHTLY_TOKEN", dataset_id="MY_DATASET_ID"
@@ -44,24 +46,22 @@
           >>>     ],
           >>> )
           >>> client.create_or_update_prediction_task_schema(schema=schema)
 
 
         """
         self._predictions_api.create_or_update_prediction_task_schema_by_dataset_id(
-            body=schema,
+            prediction_task_schema=schema,
             dataset_id=self.dataset_id,
             prediction_uuid_timestamp=prediction_version_id,
         )
 
     def create_or_update_predictions(
         self,
-        sample_id_to_prediction_singletons: Mapping[
-            str, Sequence[PredictionSingletonRepr]
-        ],
+        sample_id_to_prediction_singletons: Mapping[str, Sequence[PredictionSingleton]],
         prediction_version_id: int = -1,
         progress_bar: Optional[tqdm.tqdm] = None,
         max_workers: int = 8,
     ) -> None:
         """Creates or updates the predictions for specific samples.
 
         Args:
@@ -80,15 +80,15 @@
             max_workers:
                 Maximum number of workers uploading predictions in parallel.
 
         Example:
           >>> import time
           >>> from tqdm import tqdm
           >>> from lightly.api import ApiWorkflowClient
-          >>> from lightly.openapi_generated.swagger_client import (
+          >>> from lightly.openapi_generated.swagger_client.models import (
           >>>     PredictionTaskSchema,
           >>>     TaskType,
           >>>     PredictionTaskSchemaCategory,
           >>> )
           >>> from lightly.api.prediction_singletons import PredictionSingletonClassificationRepr
           >>>
           >>> client = ApiWorkflowClient(
@@ -110,15 +110,15 @@
 
         # handle the case where len(sample_id_to_prediction_singletons) < max_workers
         max_workers = min(len(sample_id_to_prediction_singletons), max_workers)
         max_workers = max(max_workers, 1)
 
         def upload_prediction(
             sample_id_prediction_singletons_tuple: Tuple[
-                str, Sequence[PredictionSingletonRepr]
+                str, Sequence[PredictionSingleton]
             ]
         ) -> None:
             (sample_id, prediction_singletons) = sample_id_prediction_singletons_tuple
             self.create_or_update_prediction(
                 sample_id=sample_id,
                 prediction_singletons=prediction_singletons,
                 prediction_version_id=prediction_version_id,
@@ -130,15 +130,15 @@
             ):
                 if progress_bar is not None:
                     progress_bar.update(1)
 
     def create_or_update_prediction(
         self,
         sample_id: str,
-        prediction_singletons: Sequence[PredictionSingletonRepr],
+        prediction_singletons: Sequence[PredictionSingleton],
         prediction_version_id: int = -1,
     ) -> None:
         """Creates or updates predictions for one specific sample.
 
         Args:
             sample_id
                 The ID of the sample.
@@ -147,16 +147,13 @@
                 A numerical ID (e.g., timestamp) to distinguish different predictions of different model versions.
                 Use the same id if you don't require versioning or if you wish to overwrite the previous schema.
                 This ID must match the ID of a prediction task schema.
 
             prediction_singletons:
                 Predictions to be uploaded for the designated sample.
         """
-        prediction_singletons_for_sending = [
-            singleton.to_dict() for singleton in prediction_singletons
-        ]
         self._predictions_api.create_or_update_prediction_by_sample_id(
-            body=prediction_singletons_for_sending,
+            prediction_singleton=prediction_singletons,
             dataset_id=self.dataset_id,
             sample_id=sample_id,
             prediction_uuid_timestamp=prediction_version_id,
         )
```

### Comparing `lightly-1.4.7/lightly/api/api_workflow_selection.py` & `lightly-1.4.8/lightly/api/api_workflow_selection.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,29 +2,23 @@
 import warnings
 from typing import Dict, List, Optional, Union
 
 import numpy as np
 from numpy.typing import NDArray
 
 from lightly.active_learning.config.selection_config import SelectionConfig
-from lightly.openapi_generated.swagger_client import ActiveLearningScoreCreateRequest
-from lightly.openapi_generated.swagger_client.models.job_state import JobState
-from lightly.openapi_generated.swagger_client.models.job_status_data import (
+from lightly.openapi_generated.swagger_client.models import (
+    ActiveLearningScoreCreateRequest,
+    JobState,
     JobStatusData,
-)
-from lightly.openapi_generated.swagger_client.models.sampling_config import (
     SamplingConfig,
-)
-from lightly.openapi_generated.swagger_client.models.sampling_config_stopping_condition import (
     SamplingConfigStoppingCondition,
-)
-from lightly.openapi_generated.swagger_client.models.sampling_create_request import (
     SamplingCreateRequest,
+    TagData,
 )
-from lightly.openapi_generated.swagger_client.models.tag_data import TagData
 
 
 def _parse_active_learning_scores(scores: Union[np.ndarray, List]):
     """Makes list/np.array of active learning scores serializable."""
     # the api only accepts float64s
     if isinstance(scores, np.ndarray):
         scores = scores.astype(np.float64)
@@ -49,15 +43,15 @@
         # iterate over all available score types and upload them
         for score_type, score_values in al_scores.items():
             body = ActiveLearningScoreCreateRequest(
                 score_type=score_type,
                 scores=_parse_active_learning_scores(score_values),
             )
             self._scores_api.create_or_update_active_learning_score_by_tag_id(
-                body,
+                active_learning_score_create_request=body,
                 dataset_id=self.dataset_id,
                 tag_id=query_tag_id,
             )
 
     def selection(
         self,
         selection_config: SelectionConfig,
@@ -111,15 +105,17 @@
 
         # trigger the selection
         payload = self._create_selection_create_request(
             selection_config, preselected_tag_id, query_tag_id
         )
         payload.row_count = self.get_all_tags()[0].tot_size
         response = self._selection_api.trigger_sampling_by_id(
-            payload, self.dataset_id, self.embedding_id
+            sampling_create_request=payload,
+            dataset_id=self.dataset_id,
+            embedding_id=self.embedding_id,
         )
         job_id = response.job_id
 
         # poll the job status till the job is not running anymore
         exception_counter = 0  # TODO; remove after solving https://github.com/lightly-ai/lightly-core/issues/156
         job_status_data = None
 
@@ -152,15 +148,15 @@
             )
 
         # get the new tag from the job status
         new_tag_id = job_status_data.result.data
         if new_tag_id is None:
             raise RuntimeError(f"TagId returned by job with job_id {job_id} is None.")
         new_tag_data = self._tags_api.get_tag_by_tag_id(
-            self.dataset_id, tag_id=new_tag_id
+            dataset_id=self.dataset_id, tag_id=new_tag_id
         )
 
         return new_tag_data
 
     def _create_selection_create_request(
         self,
         selection_config: SelectionConfig,
```

### Comparing `lightly-1.4.7/lightly/api/api_workflow_tags.py` & `lightly-1.4.8/lightly/api/api_workflow_tags.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 from typing import *
 
 from lightly.api.bitmask import BitMask
-from lightly.openapi_generated.swagger_client import (
+from lightly.openapi_generated.swagger_client.models import (
     TagArithmeticsOperation,
     TagArithmeticsRequest,
     TagBitMaskResponse,
+    TagCreateRequest,
     TagData,
 )
 
 
 class TagDoesNotExistError(ValueError):
     pass
 
@@ -56,15 +57,17 @@
              'dataset_id': '646b40a18355e2f54c6d2200',
              'id': '646b40d6c06aae1b91294a9e',
              'last_modified_at': 1684750550014,
              'name': 'cool-tag',
              'preselected_tag_id': None,
              ...}
         """
-        tag_data = self._tags_api.get_tag_by_tag_id(self.dataset_id, tag_id)
+        tag_data = self._tags_api.get_tag_by_tag_id(
+            dataset_id=self.dataset_id, tag_id=tag_id
+        )
         return tag_data
 
     def get_tag_by_name(self, tag_name: str) -> TagData:
         """Gets a tag from the current dataset by tag name.
 
         Args:
             tag_name:
@@ -128,15 +131,16 @@
             tag_arithmetics_request = TagArithmeticsRequest(
                 tag_id1=tag_data.id,
                 tag_id2=parent_tag_id,
                 operation=TagArithmeticsOperation.DIFFERENCE,
             )
             bit_mask_response: TagBitMaskResponse = (
                 self._tags_api.perform_tag_arithmetics_bitmask(
-                    body=tag_arithmetics_request, dataset_id=self.dataset_id
+                    tag_arithmetics_request=tag_arithmetics_request,
+                    dataset_id=self.dataset_id,
                 )
             )
             bit_mask_data = bit_mask_response.bit_mask_data
         else:
             bit_mask_data = tag_data.bit_mask_data
 
         if not filenames_on_server:
@@ -204,34 +208,34 @@
             if fname in fnames_new_tag:
                 bitmask.set_kth_bit(i)
 
         # quick sanity check
         num_selected_samples = len(bitmask.to_indices())
         if num_selected_samples != len(fnames_new_tag):
             raise RuntimeError(
-                f"An error occured when creating the new subset! "
+                "An error occured when creating the new subset! "
                 f"Out of the {len(fnames_new_tag)} filenames you provided "
                 f"to create a new tag, only {num_selected_samples} have been "
-                f"found on the server. "
-                f"Make sure you use the correct filenames. "
+                "found on the server. "
+                "Make sure you use the correct filenames. "
                 f"Valid filename example from the dataset: {fnames_server[0]}"
             )
 
         # create new tag
         tag_data_dict = {
             "name": new_tag_name,
             "prevTagId": parent_tag_id,
             "bitMaskData": bitmask.to_hex(),
             "totSize": tot_size,
             "creator": self._creator,
         }
 
         new_tag = self._tags_api.create_tag_by_dataset_id(
-            tag_data_dict,
-            self.dataset_id,
+            tag_create_request=TagCreateRequest.from_dict(tag_data_dict),
+            dataset_id=self.dataset_id,
         )
 
         return new_tag
 
     def delete_tag_by_id(self, tag_id: str) -> None:
         """Deletes a tag from the current dataset.
 
@@ -244,15 +248,15 @@
             >>>
             >>> # Already created some Lightly Worker runs with this dataset
             >>> client.set_dataset_id_by_name("my-dataset")
             >>> filenames = ['image-1.png', 'image-2.png']
             >>> tag_id = client.create_tag_from_filenames(fnames_new_tag=filenames, new_tag_name='new-tag')["id"]
             >>> client.delete_tag_by_id(tag_id=tag_id)
         """
-        self._tags_api.delete_tag_by_tag_id(self.dataset_id, tag_id)
+        self._tags_api.delete_tag_by_tag_id(dataset_id=self.dataset_id, tag_id=tag_id)
 
     def delete_tag_by_name(self, tag_name: str) -> None:
         """Deletes a tag from the current dataset.
 
         Args:
             tag_name:
                 The name of the tag to be deleted.
```

### Comparing `lightly-1.4.7/lightly/api/api_workflow_upload_dataset.py` & `lightly-1.4.8/lightly/api/api_workflow_upload_dataset.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,34 +12,22 @@
 
 from lightly.api.utils import (
     MAXIMUM_FILENAME_LENGTH,
     build_azure_signed_url_write_headers,
     check_filename,
     retry,
 )
-from lightly.openapi_generated.swagger_client import SampleWriteUrls
-from lightly.openapi_generated.swagger_client.models.datasource_config_base import (
+from lightly.openapi_generated.swagger_client.models import (
     DatasourceConfigBase,
-)
-from lightly.openapi_generated.swagger_client.models.initial_tag_create_request import (
     InitialTagCreateRequest,
-)
-from lightly.openapi_generated.swagger_client.models.job_status_meta import (
     JobStatusMeta,
-)
-from lightly.openapi_generated.swagger_client.models.job_status_upload_method import (
     JobStatusUploadMethod,
-)
-from lightly.openapi_generated.swagger_client.models.sample_create_request import (
     SampleCreateRequest,
-)
-from lightly.openapi_generated.swagger_client.models.sample_partial_mode import (
     SamplePartialMode,
-)
-from lightly.openapi_generated.swagger_client.models.tag_upsize_request import (
+    SampleWriteUrls,
     TagUpsizeRequest,
 )
 from lightly.openapi_generated.swagger_client.rest import ApiException
 from lightly.utils.hipify import bcolors
 
 try:
     from lightly.data import LightlyDataset
@@ -230,25 +218,25 @@
         if len(tags) == 0:
             # create initial tag
             initial_tag_create_request = InitialTagCreateRequest(
                 img_type=img_type,
                 creator=self._creator,
             )
             self._tags_api.create_initial_tag_by_dataset_id(
-                body=initial_tag_create_request,
+                initial_tag_create_request=initial_tag_create_request,
                 dataset_id=self.dataset_id,
             )
         else:
             # upsize existing tags
             upsize_tags_request = TagUpsizeRequest(
                 upsize_tag_name=datetime.now().strftime("%Y%m%d_%Hh%Mm%Ss"),
                 upsize_tag_creator=self._creator,
             )
             self._tags_api.upsize_tags_by_dataset_id(
-                body=upsize_tags_request,
+                tag_upsize_request=upsize_tags_request,
                 dataset_id=self.dataset_id,
             )
 
     def _upload_single_image(
         self,
         image,
         filename: str,
@@ -286,15 +274,15 @@
             thumb_name=thumbname,
             meta_data=metadata,
             exif=exifdata if exifdata is None else exifdata.to_dict(),
             custom_meta_data=custom_metadata,
         )
         sample_id = retry(
             self._samples_api.create_sample_by_dataset_id,
-            body=body,
+            sample_create_request=body,
             dataset_id=self.dataset_id,
         ).id
 
         if not metadata["is_corrupted"] and mode in ["thumbnails", "full"]:
 
             def upload_thumbnail(image, signed_url):
                 thumbnail = image_processing.Thumbnail(image)
```

### Comparing `lightly-1.4.7/lightly/api/api_workflow_upload_embeddings.py` & `lightly-1.4.8/lightly/api/api_workflow_upload_embeddings.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,24 +1,19 @@
 import csv
-import hashlib
 import io
 import tempfile
 from datetime import datetime
 from typing import List
 from urllib.request import Request, urlopen
 
 from lightly.api.utils import retry
-from lightly.openapi_generated.swagger_client import (
+from lightly.openapi_generated.swagger_client.models import (
+    DatasetEmbeddingData,
     DimensionalityReductionMethod,
     Trigger2dEmbeddingJobRequest,
-)
-from lightly.openapi_generated.swagger_client.models.dataset_embedding_data import (
-    DatasetEmbeddingData,
-)
-from lightly.openapi_generated.swagger_client.models.write_csv_url_data import (
     WriteCSVUrlData,
 )
 from lightly.utils.io import check_embeddings, check_filenames
 
 
 class EmbeddingDoesNotExistError(ValueError):
     pass
@@ -160,15 +155,17 @@
             DimensionalityReductionMethod.TSNE,
             DimensionalityReductionMethod.UMAP,
         ]:
             body = Trigger2dEmbeddingJobRequest(
                 dimensionality_reduction_method=dimensionality_reduction_method
             )
             self._embeddings_api.trigger2d_embeddings_job(
-                body=body, dataset_id=self.dataset_id, embedding_id=self.embedding_id
+                trigger2d_embedding_job_request=body,
+                dataset_id=self.dataset_id,
+                embedding_id=self.embedding_id,
             )
 
     def append_embeddings(self, path_to_embeddings_csv: str, embedding_id: str) -> None:
         """Concatenates embeddings from the Lightly Platform to the local ones.
 
         Loads the embedding csv file with the corresponding embedding ID in the current dataset
         and appends all of its rows to the local embeddings file located at
```

### Comparing `lightly-1.4.7/lightly/api/api_workflow_upload_metadata.py` & `lightly-1.4.8/lightly/api/api_workflow_upload_metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,25 +1,18 @@
-from bisect import bisect_left
 from concurrent.futures import ThreadPoolExecutor
 from typing import Any, Dict, List, Union
 
 from requests import Response
 from tqdm import tqdm
 
 from lightly.api.utils import retry
-from lightly.openapi_generated.swagger_client.models.configuration_entry import (
+from lightly.openapi_generated.swagger_client.models import (
     ConfigurationEntry,
-)
-from lightly.openapi_generated.swagger_client.models.configuration_set_request import (
     ConfigurationSetRequest,
-)
-from lightly.openapi_generated.swagger_client.models.sample_partial_mode import (
     SamplePartialMode,
-)
-from lightly.openapi_generated.swagger_client.models.sample_update_request import (
     SampleUpdateRequest,
 )
 from lightly.utils.hipify import print_as_warning
 from lightly.utils.io import COCO_ANNOTATION_KEYS
 
 
 class InvalidCustomMetadataWarning(Warning):
@@ -172,41 +165,41 @@
 
         upload_requests = []
         for metadata in custom_metadata[COCO_ANNOTATION_KEYS.custom_metadata]:
             image_id = metadata[COCO_ANNOTATION_KEYS.custom_metadata_image_id]
             filename = image_id_to_filename.get(image_id, None)
             if filename is None:
                 print_as_warning(
-                    f"No image found for custom metadata annotation "
+                    "No image found for custom metadata annotation "
                     f"with image_id {image_id}. "
-                    f"This custom metadata annotation is skipped. ",
+                    "This custom metadata annotation is skipped. ",
                     InvalidCustomMetadataWarning,
                 )
                 continue
             sample_id = filename_to_sample_id.get(filename, None)
             if sample_id is None:
                 print_as_warning(
-                    f"You tried to upload custom metadata for a sample with "
+                    "You tried to upload custom metadata for a sample with "
                     f"filename {{{filename}}}, "
-                    f"but a sample with this filename "
-                    f"does not exist on the server. "
-                    f"This custom metadata annotation is skipped. ",
+                    "but a sample with this filename "
+                    "does not exist on the server. "
+                    "This custom metadata annotation is skipped. ",
                     InvalidCustomMetadataWarning,
                 )
                 continue
             upload_request = (metadata, sample_id)
             upload_requests.append(upload_request)
 
         # retry upload if it times out
         def upload_sample_metadata(upload_request):
             metadata, sample_id = upload_request
             request = SampleUpdateRequest(custom_meta_data=metadata)
             return retry(
                 self._samples_api.update_sample_by_id,
-                request,
+                sample_update_request=request,
                 dataset_id=self.dataset_id,
                 sample_id=sample_id,
             )
 
         # Upload in parallel with a limit on the number of concurrent requests
         with ThreadPoolExecutor(max_workers=max_workers) as executor:
             # get iterator over results
@@ -244,11 +237,11 @@
             >>>     [entry],
             >>> )
 
 
         """
         config_set_request = ConfigurationSetRequest(name=name, configs=configs)
         resp = self._metadata_configurations_api.create_meta_data_configuration(
-            body=config_set_request,
+            configuration_set_request=config_set_request,
             dataset_id=self.dataset_id,
         )
         return resp
```

### Comparing `lightly-1.4.7/lightly/api/bitmask.py` & `lightly-1.4.8/lightly/api/bitmask.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/api/download.py` & `lightly-1.4.8/lightly/api/download.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/api/patch.py` & `lightly-1.4.8/lightly/api/patch.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/api/swagger_api_client.py` & `lightly-1.4.8/lightly/api/swagger_api_client.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from typing import Any, Dict, Tuple, Union
 
 from lightly.api.swagger_rest_client import LightlySwaggerRESTClientObject
-from lightly.openapi_generated.swagger_client import ApiClient, Configuration
+from lightly.openapi_generated.swagger_client.api_client import ApiClient, Configuration
 
 DEFAULT_API_TIMEOUT = 60 * 3  # seconds
 
 
 class PatchApiClientMixin:
     """Mixin that makes an ApiClient object picklable."""
```

### Comparing `lightly-1.4.7/lightly/api/swagger_rest_client.py` & `lightly-1.4.8/lightly/api/swagger_rest_client.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-from typing import Any, Dict, List, Tuple, Union
+from typing import Any, Dict, Tuple, Union
 
-from lightly.openapi_generated.swagger_client import Configuration
+from lightly.openapi_generated.swagger_client.api_client import Configuration
 from lightly.openapi_generated.swagger_client.rest import RESTClientObject
 
 
 class PatchRESTClientObjectMixin:
     """Mixin that adds patches to a RESTClientObject.
 
     * Adds default timeout to all requests
@@ -49,28 +49,26 @@
         query_params=None,
         headers=None,
         body=None,
         post_params=None,
         _preload_content=True,
         _request_timeout=None,
     ):
-        # Set default timeout. This is necessary because the swagger api client does not
+        # Set default timeout. This is necessary because the openapi client does not
         # respect timeouts configured by urllib3. Instead it expects a timeout to be
         # passed with every request. See code here:
         # https://github.com/lightly-ai/lightly/blob/ffbd32fe82f76b37c8ac497640355314474bfc3b/lightly/openapi_generated/swagger_client/rest.py#L141-L148
         if _request_timeout is None:
             _request_timeout = self.timeout
 
-        flat_query_params = _flatten_list_query_parameters(query_params=query_params)
-
         # Call RESTClientObject.request
         return super().request(
             method=method,
             url=url,
-            query_params=flat_query_params,
+            query_params=query_params,
             headers=headers,
             body=body,
             post_params=post_params,
             _preload_content=_preload_content,
             _request_timeout=_request_timeout,
         )
 
@@ -114,21 +112,7 @@
             Number of connection pools. Defaults to 4.
         maxsize:
             Maxsize is the number of requests to host that are allowed in parallel.
             Defaults to None.
     """
 
     pass
-
-
-def _flatten_list_query_parameters(
-    query_params: Union[None, List[Tuple[str, Any]]]
-) -> Union[None, List[Tuple[str, Any]]]:
-    if query_params is not None:
-        new_query_params = []
-        for name, value in query_params:
-            if isinstance(value, list):
-                new_query_params.extend([(name, val) for val in value])
-            else:
-                new_query_params.append((name, value))
-        query_params = new_query_params
-    return query_params
```

### Comparing `lightly-1.4.7/lightly/api/utils.py` & `lightly-1.4.8/lightly/api/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -224,12 +224,12 @@
         token = getenv("LIGHTLY_TOKEN", None)
     if token is None and raise_if_no_token_specified:
         raise ValueError(
             "Either provide a 'token' argument or export a LIGHTLY_TOKEN environment variable"
         )
 
     configuration = Configuration()
-    configuration.api_key = {"token": token}
+    configuration.api_key = {"ApiKeyAuth": token}
     configuration.ssl_ca_cert = ssl_ca_cert
     configuration.host = host
 
     return configuration
```

### Comparing `lightly-1.4.7/lightly/api/version_checking.py` & `lightly-1.4.8/lightly/api/version_checking.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import signal
 import warnings
 from typing import Tuple
 
 from lightly.api import utils
 from lightly.api.swagger_api_client import LightlySwaggerApiClient
-from lightly.openapi_generated.swagger_client import VersioningApi
+from lightly.openapi_generated.swagger_client.api import VersioningApi
 from lightly.utils.version_compare import version_compare
 
 
 class LightlyAPITimeoutException(Exception):
     pass
```

### Comparing `lightly-1.4.7/lightly/cli/_cli_simclr.py` & `lightly-1.4.8/lightly/cli/_cli_simclr.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/cli/_helpers.py` & `lightly-1.4.8/lightly/cli/_helpers.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/cli/config/config.yaml` & `lightly-1.4.8/lightly/cli/config/config.yaml`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/cli/crop_cli.py` & `lightly-1.4.8/lightly/cli/crop_cli.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/cli/download_cli.py` & `lightly-1.4.8/lightly/cli/download_cli.py`

 * *Files 5% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 import os
 
 import hydra
 
 import lightly.data as data
 from lightly.api.api_workflow_client import ApiWorkflowClient
 from lightly.cli._helpers import cpu_count, fix_hydra_arguments, fix_input_path
-from lightly.openapi_generated.swagger_client import Creator
+from lightly.openapi_generated.swagger_client.models import Creator
 from lightly.utils.hipify import bcolors, print_as_warning
 
 
 def _download_cli(cfg, is_cli_call=True):
     tag_name = str(cfg["tag_name"])
     dataset_id = str(cfg["dataset_id"])
     token = str(cfg["token"])
```

### Comparing `lightly-1.4.7/lightly/cli/embed_cli.py` & `lightly-1.4.8/lightly/cli/embed_cli.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/cli/lightly_cli.py` & `lightly-1.4.8/lightly/cli/lightly_cli.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/cli/train_cli.py` & `lightly-1.4.8/lightly/cli/train_cli.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/cli/version_cli.py` & `lightly-1.4.8/lightly/cli/version_cli.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/core.py` & `lightly-1.4.8/lightly/core.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/data/__init__.py` & `lightly-1.4.8/lightly/data/__init__.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/data/_helpers.py` & `lightly-1.4.8/lightly/data/_helpers.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/data/_image.py` & `lightly-1.4.8/lightly/data/_image.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/data/_image_loaders.py` & `lightly-1.4.8/lightly/data/_image_loaders.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/data/_utils.py` & `lightly-1.4.8/lightly/data/_utils.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/data/_video.py` & `lightly-1.4.8/lightly/data/_video.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/data/collate.py` & `lightly-1.4.8/lightly/data/collate.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/data/dataset.py` & `lightly-1.4.8/lightly/data/dataset.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/data/lightly_subset.py` & `lightly-1.4.8/lightly/data/lightly_subset.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/data/multi_view_collate.py` & `lightly-1.4.8/lightly/data/multi_view_collate.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/embedding/_base.py` & `lightly-1.4.8/lightly/embedding/_base.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/embedding/callbacks.py` & `lightly-1.4.8/lightly/embedding/callbacks.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/embedding/embedding.py` & `lightly-1.4.8/lightly/embedding/embedding.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/__init__.py` & `lightly-1.4.8/lightly/loss/__init__.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/barlow_twins_loss.py` & `lightly-1.4.8/lightly/loss/barlow_twins_loss.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/dcl_loss.py` & `lightly-1.4.8/lightly/loss/dcl_loss.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/dino_loss.py` & `lightly-1.4.8/lightly/loss/dino_loss.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/hypersphere_loss.py` & `lightly-1.4.8/lightly/loss/hypersphere_loss.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/memory_bank.py` & `lightly-1.4.8/lightly/loss/memory_bank.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/msn_loss.py` & `lightly-1.4.8/lightly/loss/msn_loss.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/negative_cosine_similarity.py` & `lightly-1.4.8/lightly/loss/negative_cosine_similarity.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/ntx_ent_loss.py` & `lightly-1.4.8/lightly/loss/ntx_ent_loss.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/pmsn_loss.py` & `lightly-1.4.8/lightly/loss/pmsn_loss.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/regularizer/co2.py` & `lightly-1.4.8/lightly/loss/regularizer/co2.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/swav_loss.py` & `lightly-1.4.8/lightly/loss/swav_loss.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/sym_neg_cos_sim_loss.py` & `lightly-1.4.8/lightly/loss/sym_neg_cos_sim_loss.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/tico_loss.py` & `lightly-1.4.8/lightly/loss/tico_loss.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/vicreg_loss.py` & `lightly-1.4.8/lightly/loss/vicreg_loss.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/loss/vicregl_loss.py` & `lightly-1.4.8/lightly/loss/vicregl_loss.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/__init__.py` & `lightly-1.4.8/lightly/models/__init__.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/_momentum.py` & `lightly-1.4.8/lightly/models/_momentum.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/barlowtwins.py` & `lightly-1.4.8/lightly/models/barlowtwins.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/batchnorm.py` & `lightly-1.4.8/lightly/models/batchnorm.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/byol.py` & `lightly-1.4.8/lightly/models/byol.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/moco.py` & `lightly-1.4.8/lightly/models/moco.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/modules/__init__.py` & `lightly-1.4.8/lightly/models/modules/__init__.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/modules/heads.py` & `lightly-1.4.8/lightly/models/modules/heads.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/modules/masked_autoencoder.py` & `lightly-1.4.8/lightly/models/modules/masked_autoencoder.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/modules/nn_memory_bank.py` & `lightly-1.4.8/lightly/models/modules/nn_memory_bank.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/nnclr.py` & `lightly-1.4.8/lightly/models/nnclr.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/resnet.py` & `lightly-1.4.8/lightly/models/resnet.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/simclr.py` & `lightly-1.4.8/lightly/models/simclr.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/simsiam.py` & `lightly-1.4.8/lightly/models/simsiam.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/utils.py` & `lightly-1.4.8/lightly/models/utils.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/models/zoo.py` & `lightly-1.4.8/lightly/models/zoo.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/__init__.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/__init__.py`

 * *Files 11% similar despite different names*

```diff
@@ -3,21 +3,23 @@
 # flake8: noqa
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
-from __future__ import absolute_import
+__version__ = "1.0.0"
 
 # import apis into sdk package
 from lightly.openapi_generated.swagger_client.api.collaboration_api import CollaborationApi
 from lightly.openapi_generated.swagger_client.api.datasets_api import DatasetsApi
 from lightly.openapi_generated.swagger_client.api.datasources_api import DatasourcesApi
 from lightly.openapi_generated.swagger_client.api.docker_api import DockerApi
 from lightly.openapi_generated.swagger_client.api.embeddings_api import EmbeddingsApi
@@ -31,56 +33,62 @@
 from lightly.openapi_generated.swagger_client.api.samplings_api import SamplingsApi
 from lightly.openapi_generated.swagger_client.api.scores_api import ScoresApi
 from lightly.openapi_generated.swagger_client.api.tags_api import TagsApi
 from lightly.openapi_generated.swagger_client.api.teams_api import TeamsApi
 from lightly.openapi_generated.swagger_client.api.versioning_api import VersioningApi
 
 # import ApiClient
+from lightly.openapi_generated.swagger_client.api_response import ApiResponse
 from lightly.openapi_generated.swagger_client.api_client import ApiClient
 from lightly.openapi_generated.swagger_client.configuration import Configuration
+from lightly.openapi_generated.swagger_client.exceptions import OpenApiException
+from lightly.openapi_generated.swagger_client.exceptions import ApiTypeError
+from lightly.openapi_generated.swagger_client.exceptions import ApiValueError
+from lightly.openapi_generated.swagger_client.exceptions import ApiKeyError
+from lightly.openapi_generated.swagger_client.exceptions import ApiAttributeError
+from lightly.openapi_generated.swagger_client.exceptions import ApiException
+
 # import models into sdk package
-from lightly.openapi_generated.swagger_client.models.access_role import AccessRole
 from lightly.openapi_generated.swagger_client.models.active_learning_score_create_request import ActiveLearningScoreCreateRequest
 from lightly.openapi_generated.swagger_client.models.active_learning_score_data import ActiveLearningScoreData
-from lightly.openapi_generated.swagger_client.models.active_learning_score_type import ActiveLearningScoreType
-from lightly.openapi_generated.swagger_client.models.active_learning_scores import ActiveLearningScores
 from lightly.openapi_generated.swagger_client.models.api_error_code import ApiErrorCode
 from lightly.openapi_generated.swagger_client.models.api_error_response import ApiErrorResponse
 from lightly.openapi_generated.swagger_client.models.async_task_data import AsyncTaskData
-from lightly.openapi_generated.swagger_client.models.bounding_box import BoundingBox
-from lightly.openapi_generated.swagger_client.models.category_id import CategoryId
-from lightly.openapi_generated.swagger_client.models.category_name import CategoryName
 from lightly.openapi_generated.swagger_client.models.configuration_data import ConfigurationData
 from lightly.openapi_generated.swagger_client.models.configuration_entry import ConfigurationEntry
 from lightly.openapi_generated.swagger_client.models.configuration_set_request import ConfigurationSetRequest
 from lightly.openapi_generated.swagger_client.models.configuration_value_data_type import ConfigurationValueDataType
+from lightly.openapi_generated.swagger_client.models.create_cf_bucket_activity_request import CreateCFBucketActivityRequest
 from lightly.openapi_generated.swagger_client.models.create_docker_worker_registry_entry_request import CreateDockerWorkerRegistryEntryRequest
 from lightly.openapi_generated.swagger_client.models.create_entity_response import CreateEntityResponse
 from lightly.openapi_generated.swagger_client.models.create_sample_with_write_urls_response import CreateSampleWithWriteUrlsResponse
+from lightly.openapi_generated.swagger_client.models.create_team_membership_request import CreateTeamMembershipRequest
 from lightly.openapi_generated.swagger_client.models.creator import Creator
 from lightly.openapi_generated.swagger_client.models.crop_data import CropData
-from lightly.openapi_generated.swagger_client.models.custom_sample_meta_data import CustomSampleMetaData
 from lightly.openapi_generated.swagger_client.models.dataset_create_request import DatasetCreateRequest
 from lightly.openapi_generated.swagger_client.models.dataset_creator import DatasetCreator
 from lightly.openapi_generated.swagger_client.models.dataset_data import DatasetData
 from lightly.openapi_generated.swagger_client.models.dataset_data_enriched import DatasetDataEnriched
 from lightly.openapi_generated.swagger_client.models.dataset_embedding_data import DatasetEmbeddingData
-from lightly.openapi_generated.swagger_client.models.dataset_name import DatasetName
-from lightly.openapi_generated.swagger_client.models.dataset_name_query import DatasetNameQuery
 from lightly.openapi_generated.swagger_client.models.dataset_type import DatasetType
 from lightly.openapi_generated.swagger_client.models.dataset_update_request import DatasetUpdateRequest
 from lightly.openapi_generated.swagger_client.models.datasource_config import DatasourceConfig
 from lightly.openapi_generated.swagger_client.models.datasource_config_azure import DatasourceConfigAzure
+from lightly.openapi_generated.swagger_client.models.datasource_config_azure_all_of import DatasourceConfigAzureAllOf
 from lightly.openapi_generated.swagger_client.models.datasource_config_base import DatasourceConfigBase
 from lightly.openapi_generated.swagger_client.models.datasource_config_gcs import DatasourceConfigGCS
+from lightly.openapi_generated.swagger_client.models.datasource_config_gcs_all_of import DatasourceConfigGCSAllOf
 from lightly.openapi_generated.swagger_client.models.datasource_config_lightly import DatasourceConfigLIGHTLY
 from lightly.openapi_generated.swagger_client.models.datasource_config_local import DatasourceConfigLOCAL
 from lightly.openapi_generated.swagger_client.models.datasource_config_obs import DatasourceConfigOBS
+from lightly.openapi_generated.swagger_client.models.datasource_config_obs_all_of import DatasourceConfigOBSAllOf
 from lightly.openapi_generated.swagger_client.models.datasource_config_s3 import DatasourceConfigS3
+from lightly.openapi_generated.swagger_client.models.datasource_config_s3_all_of import DatasourceConfigS3AllOf
 from lightly.openapi_generated.swagger_client.models.datasource_config_s3_delegated_access import DatasourceConfigS3DelegatedAccess
+from lightly.openapi_generated.swagger_client.models.datasource_config_s3_delegated_access_all_of import DatasourceConfigS3DelegatedAccessAllOf
 from lightly.openapi_generated.swagger_client.models.datasource_config_verify_data import DatasourceConfigVerifyData
 from lightly.openapi_generated.swagger_client.models.datasource_config_verify_data_errors import DatasourceConfigVerifyDataErrors
 from lightly.openapi_generated.swagger_client.models.datasource_processed_until_timestamp_request import DatasourceProcessedUntilTimestampRequest
 from lightly.openapi_generated.swagger_client.models.datasource_processed_until_timestamp_response import DatasourceProcessedUntilTimestampResponse
 from lightly.openapi_generated.swagger_client.models.datasource_purpose import DatasourcePurpose
 from lightly.openapi_generated.swagger_client.models.datasource_raw_samples_data import DatasourceRawSamplesData
 from lightly.openapi_generated.swagger_client.models.datasource_raw_samples_data_row import DatasourceRawSamplesDataRow
@@ -89,14 +97,15 @@
 from lightly.openapi_generated.swagger_client.models.datasource_raw_samples_predictions_data import DatasourceRawSamplesPredictionsData
 from lightly.openapi_generated.swagger_client.models.datasource_raw_samples_predictions_data_row import DatasourceRawSamplesPredictionsDataRow
 from lightly.openapi_generated.swagger_client.models.dimensionality_reduction_method import DimensionalityReductionMethod
 from lightly.openapi_generated.swagger_client.models.docker_license_information import DockerLicenseInformation
 from lightly.openapi_generated.swagger_client.models.docker_run_artifact_create_request import DockerRunArtifactCreateRequest
 from lightly.openapi_generated.swagger_client.models.docker_run_artifact_created_data import DockerRunArtifactCreatedData
 from lightly.openapi_generated.swagger_client.models.docker_run_artifact_data import DockerRunArtifactData
+from lightly.openapi_generated.swagger_client.models.docker_run_artifact_storage_location import DockerRunArtifactStorageLocation
 from lightly.openapi_generated.swagger_client.models.docker_run_artifact_type import DockerRunArtifactType
 from lightly.openapi_generated.swagger_client.models.docker_run_create_request import DockerRunCreateRequest
 from lightly.openapi_generated.swagger_client.models.docker_run_data import DockerRunData
 from lightly.openapi_generated.swagger_client.models.docker_run_log_data import DockerRunLogData
 from lightly.openapi_generated.swagger_client.models.docker_run_log_entry_data import DockerRunLogEntryData
 from lightly.openapi_generated.swagger_client.models.docker_run_log_level import DockerRunLogLevel
 from lightly.openapi_generated.swagger_client.models.docker_run_scheduled_create_request import DockerRunScheduledCreateRequest
@@ -132,102 +141,90 @@
 from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_checkpoint_callback import DockerWorkerConfigV3LightlyCheckpointCallback
 from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_collate import DockerWorkerConfigV3LightlyCollate
 from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_criterion import DockerWorkerConfigV3LightlyCriterion
 from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_loader import DockerWorkerConfigV3LightlyLoader
 from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_model import DockerWorkerConfigV3LightlyModel
 from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_optimizer import DockerWorkerConfigV3LightlyOptimizer
 from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_trainer import DockerWorkerConfigV3LightlyTrainer
-from lightly.openapi_generated.swagger_client.models.docker_worker_labels import DockerWorkerLabels
-from lightly.openapi_generated.swagger_client.models.docker_worker_name import DockerWorkerName
 from lightly.openapi_generated.swagger_client.models.docker_worker_registry_entry_data import DockerWorkerRegistryEntryData
 from lightly.openapi_generated.swagger_client.models.docker_worker_state import DockerWorkerState
 from lightly.openapi_generated.swagger_client.models.docker_worker_type import DockerWorkerType
-from lightly.openapi_generated.swagger_client.models.embedding2d_coordinates import Embedding2dCoordinates
 from lightly.openapi_generated.swagger_client.models.embedding2d_create_request import Embedding2dCreateRequest
 from lightly.openapi_generated.swagger_client.models.embedding2d_data import Embedding2dData
 from lightly.openapi_generated.swagger_client.models.embedding_data import EmbeddingData
 from lightly.openapi_generated.swagger_client.models.file_name_format import FileNameFormat
 from lightly.openapi_generated.swagger_client.models.file_output_format import FileOutputFormat
 from lightly.openapi_generated.swagger_client.models.filename_and_read_url import FilenameAndReadUrl
-from lightly.openapi_generated.swagger_client.models.filename_and_read_urls import FilenameAndReadUrls
-from lightly.openapi_generated.swagger_client.models.general_job_result import GeneralJobResult
 from lightly.openapi_generated.swagger_client.models.image_type import ImageType
 from lightly.openapi_generated.swagger_client.models.initial_tag_create_request import InitialTagCreateRequest
 from lightly.openapi_generated.swagger_client.models.job_result_type import JobResultType
 from lightly.openapi_generated.swagger_client.models.job_state import JobState
 from lightly.openapi_generated.swagger_client.models.job_status_data import JobStatusData
 from lightly.openapi_generated.swagger_client.models.job_status_data_result import JobStatusDataResult
 from lightly.openapi_generated.swagger_client.models.job_status_meta import JobStatusMeta
 from lightly.openapi_generated.swagger_client.models.job_status_upload_method import JobStatusUploadMethod
 from lightly.openapi_generated.swagger_client.models.jobs_data import JobsData
 from lightly.openapi_generated.swagger_client.models.label_box_data_row import LabelBoxDataRow
-from lightly.openapi_generated.swagger_client.models.label_box_data_rows import LabelBoxDataRows
 from lightly.openapi_generated.swagger_client.models.label_box_v4_data_row import LabelBoxV4DataRow
-from lightly.openapi_generated.swagger_client.models.label_box_v4_data_rows import LabelBoxV4DataRows
 from lightly.openapi_generated.swagger_client.models.label_studio_task import LabelStudioTask
 from lightly.openapi_generated.swagger_client.models.label_studio_task_data import LabelStudioTaskData
-from lightly.openapi_generated.swagger_client.models.label_studio_tasks import LabelStudioTasks
+from lightly.openapi_generated.swagger_client.models.lightly_docker_selection_method import LightlyDockerSelectionMethod
 from lightly.openapi_generated.swagger_client.models.lightly_model_v2 import LightlyModelV2
 from lightly.openapi_generated.swagger_client.models.lightly_model_v3 import LightlyModelV3
 from lightly.openapi_generated.swagger_client.models.lightly_trainer_precision_v2 import LightlyTrainerPrecisionV2
 from lightly.openapi_generated.swagger_client.models.lightly_trainer_precision_v3 import LightlyTrainerPrecisionV3
-from lightly.openapi_generated.swagger_client.models.mongo_object_id import MongoObjectID
-from lightly.openapi_generated.swagger_client.models.object_id import ObjectId
-from lightly.openapi_generated.swagger_client.models.path_safe_name import PathSafeName
 from lightly.openapi_generated.swagger_client.models.prediction_singleton import PredictionSingleton
 from lightly.openapi_generated.swagger_client.models.prediction_singleton_base import PredictionSingletonBase
 from lightly.openapi_generated.swagger_client.models.prediction_singleton_classification import PredictionSingletonClassification
+from lightly.openapi_generated.swagger_client.models.prediction_singleton_classification_all_of import PredictionSingletonClassificationAllOf
 from lightly.openapi_generated.swagger_client.models.prediction_singleton_instance_segmentation import PredictionSingletonInstanceSegmentation
+from lightly.openapi_generated.swagger_client.models.prediction_singleton_instance_segmentation_all_of import PredictionSingletonInstanceSegmentationAllOf
 from lightly.openapi_generated.swagger_client.models.prediction_singleton_keypoint_detection import PredictionSingletonKeypointDetection
+from lightly.openapi_generated.swagger_client.models.prediction_singleton_keypoint_detection_all_of import PredictionSingletonKeypointDetectionAllOf
 from lightly.openapi_generated.swagger_client.models.prediction_singleton_object_detection import PredictionSingletonObjectDetection
+from lightly.openapi_generated.swagger_client.models.prediction_singleton_object_detection_all_of import PredictionSingletonObjectDetectionAllOf
 from lightly.openapi_generated.swagger_client.models.prediction_singleton_semantic_segmentation import PredictionSingletonSemanticSegmentation
-from lightly.openapi_generated.swagger_client.models.prediction_singletons import PredictionSingletons
+from lightly.openapi_generated.swagger_client.models.prediction_singleton_semantic_segmentation_all_of import PredictionSingletonSemanticSegmentationAllOf
 from lightly.openapi_generated.swagger_client.models.prediction_task_schema import PredictionTaskSchema
 from lightly.openapi_generated.swagger_client.models.prediction_task_schema_category import PredictionTaskSchemaCategory
-from lightly.openapi_generated.swagger_client.models.probabilities import Probabilities
 from lightly.openapi_generated.swagger_client.models.questionnaire_data import QuestionnaireData
-from lightly.openapi_generated.swagger_client.models.read_url import ReadUrl
-from lightly.openapi_generated.swagger_client.models.redirected_read_url import RedirectedReadUrl
 from lightly.openapi_generated.swagger_client.models.s3_region import S3Region
-from lightly.openapi_generated.swagger_client.models.s3_server_side_encryption_kms_key import S3ServerSideEncryptionKMSKey
 from lightly.openapi_generated.swagger_client.models.sama_task import SamaTask
 from lightly.openapi_generated.swagger_client.models.sama_task_data import SamaTaskData
-from lightly.openapi_generated.swagger_client.models.sama_tasks import SamaTasks
 from lightly.openapi_generated.swagger_client.models.sample_create_request import SampleCreateRequest
 from lightly.openapi_generated.swagger_client.models.sample_data import SampleData
 from lightly.openapi_generated.swagger_client.models.sample_data_modes import SampleDataModes
 from lightly.openapi_generated.swagger_client.models.sample_meta_data import SampleMetaData
 from lightly.openapi_generated.swagger_client.models.sample_partial_mode import SamplePartialMode
 from lightly.openapi_generated.swagger_client.models.sample_sort_by import SampleSortBy
 from lightly.openapi_generated.swagger_client.models.sample_type import SampleType
 from lightly.openapi_generated.swagger_client.models.sample_update_request import SampleUpdateRequest
 from lightly.openapi_generated.swagger_client.models.sample_write_urls import SampleWriteUrls
 from lightly.openapi_generated.swagger_client.models.sampling_config import SamplingConfig
 from lightly.openapi_generated.swagger_client.models.sampling_config_stopping_condition import SamplingConfigStoppingCondition
 from lightly.openapi_generated.swagger_client.models.sampling_create_request import SamplingCreateRequest
 from lightly.openapi_generated.swagger_client.models.sampling_method import SamplingMethod
-from lightly.openapi_generated.swagger_client.models.score import Score
 from lightly.openapi_generated.swagger_client.models.sector import Sector
 from lightly.openapi_generated.swagger_client.models.selection_config import SelectionConfig
 from lightly.openapi_generated.swagger_client.models.selection_config_entry import SelectionConfigEntry
 from lightly.openapi_generated.swagger_client.models.selection_config_entry_input import SelectionConfigEntryInput
 from lightly.openapi_generated.swagger_client.models.selection_config_entry_strategy import SelectionConfigEntryStrategy
 from lightly.openapi_generated.swagger_client.models.selection_input_predictions_name import SelectionInputPredictionsName
 from lightly.openapi_generated.swagger_client.models.selection_input_type import SelectionInputType
 from lightly.openapi_generated.swagger_client.models.selection_strategy_threshold_operation import SelectionStrategyThresholdOperation
 from lightly.openapi_generated.swagger_client.models.selection_strategy_type import SelectionStrategyType
+from lightly.openapi_generated.swagger_client.models.service_account_basic_data import ServiceAccountBasicData
 from lightly.openapi_generated.swagger_client.models.set_embeddings_is_processed_flag_by_id_body_request import SetEmbeddingsIsProcessedFlagByIdBodyRequest
 from lightly.openapi_generated.swagger_client.models.shared_access_config_create_request import SharedAccessConfigCreateRequest
 from lightly.openapi_generated.swagger_client.models.shared_access_config_data import SharedAccessConfigData
 from lightly.openapi_generated.swagger_client.models.shared_access_type import SharedAccessType
 from lightly.openapi_generated.swagger_client.models.tag_active_learning_scores_data import TagActiveLearningScoresData
 from lightly.openapi_generated.swagger_client.models.tag_arithmetics_operation import TagArithmeticsOperation
 from lightly.openapi_generated.swagger_client.models.tag_arithmetics_request import TagArithmeticsRequest
 from lightly.openapi_generated.swagger_client.models.tag_arithmetics_response import TagArithmeticsResponse
-from lightly.openapi_generated.swagger_client.models.tag_bit_mask_data import TagBitMaskData
 from lightly.openapi_generated.swagger_client.models.tag_bit_mask_response import TagBitMaskResponse
 from lightly.openapi_generated.swagger_client.models.tag_change_data import TagChangeData
 from lightly.openapi_generated.swagger_client.models.tag_change_data_arithmetics import TagChangeDataArithmetics
 from lightly.openapi_generated.swagger_client.models.tag_change_data_initial import TagChangeDataInitial
 from lightly.openapi_generated.swagger_client.models.tag_change_data_metadata import TagChangeDataMetadata
 from lightly.openapi_generated.swagger_client.models.tag_change_data_operation_method import TagChangeDataOperationMethod
 from lightly.openapi_generated.swagger_client.models.tag_change_data_rename import TagChangeDataRename
@@ -235,22 +232,19 @@
 from lightly.openapi_generated.swagger_client.models.tag_change_data_samples import TagChangeDataSamples
 from lightly.openapi_generated.swagger_client.models.tag_change_data_scatterplot import TagChangeDataScatterplot
 from lightly.openapi_generated.swagger_client.models.tag_change_data_upsize import TagChangeDataUpsize
 from lightly.openapi_generated.swagger_client.models.tag_change_entry import TagChangeEntry
 from lightly.openapi_generated.swagger_client.models.tag_create_request import TagCreateRequest
 from lightly.openapi_generated.swagger_client.models.tag_creator import TagCreator
 from lightly.openapi_generated.swagger_client.models.tag_data import TagData
-from lightly.openapi_generated.swagger_client.models.tag_filenames_data import TagFilenamesData
-from lightly.openapi_generated.swagger_client.models.tag_name import TagName
 from lightly.openapi_generated.swagger_client.models.tag_update_request import TagUpdateRequest
 from lightly.openapi_generated.swagger_client.models.tag_upsize_request import TagUpsizeRequest
-from lightly.openapi_generated.swagger_client.models.task_name import TaskName
 from lightly.openapi_generated.swagger_client.models.task_type import TaskType
 from lightly.openapi_generated.swagger_client.models.team_basic_data import TeamBasicData
+from lightly.openapi_generated.swagger_client.models.team_data import TeamData
 from lightly.openapi_generated.swagger_client.models.team_role import TeamRole
-from lightly.openapi_generated.swagger_client.models.timestamp import Timestamp
-from lightly.openapi_generated.swagger_client.models.timestamp_seconds import TimestampSeconds
 from lightly.openapi_generated.swagger_client.models.trigger2d_embedding_job_request import Trigger2dEmbeddingJobRequest
 from lightly.openapi_generated.swagger_client.models.update_docker_worker_registry_entry_request import UpdateDockerWorkerRegistryEntryRequest
-from lightly.openapi_generated.swagger_client.models.version_number import VersionNumber
+from lightly.openapi_generated.swagger_client.models.update_team_membership_request import UpdateTeamMembershipRequest
+from lightly.openapi_generated.swagger_client.models.user_type import UserType
 from lightly.openapi_generated.swagger_client.models.video_frame_data import VideoFrameData
 from lightly.openapi_generated.swagger_client.models.write_csv_url_data import WriteCSVUrlData
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/api/__init__.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/api/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,9 +1,7 @@
-from __future__ import absolute_import
-
 # flake8: noqa
 
 # import apis into api package
 from lightly.openapi_generated.swagger_client.api.collaboration_api import CollaborationApi
 from lightly.openapi_generated.swagger_client.api.datasets_api import DatasetsApi
 from lightly.openapi_generated.swagger_client.api.datasources_api import DatasourcesApi
 from lightly.openapi_generated.swagger_client.api.docker_api import DockerApi
@@ -16,7 +14,8 @@
 from lightly.openapi_generated.swagger_client.api.quota_api import QuotaApi
 from lightly.openapi_generated.swagger_client.api.samples_api import SamplesApi
 from lightly.openapi_generated.swagger_client.api.samplings_api import SamplingsApi
 from lightly.openapi_generated.swagger_client.api.scores_api import ScoresApi
 from lightly.openapi_generated.swagger_client.api.tags_api import TagsApi
 from lightly.openapi_generated.swagger_client.api.teams_api import TeamsApi
 from lightly.openapi_generated.swagger_client.api.versioning_api import VersioningApi
+
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/api/docker_api.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/api/tags_api.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,3689 +1,3150 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
-"""
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
 
+    Do not edit the class manually.
+"""
 
-from __future__ import absolute_import
 
 import re  # noqa: F401
+import io
+import warnings
+
+from pydantic import validate_arguments, ValidationError
+from typing_extensions import Annotated
 
-# python 2 and python 3 compatibility library
-import six
+from pydantic import Field, StrictBool, StrictInt, StrictStr, conint, constr, validator
+
+from typing import List, Optional, Union
+
+from lightly.openapi_generated.swagger_client.models.create_entity_response import CreateEntityResponse
+from lightly.openapi_generated.swagger_client.models.file_name_format import FileNameFormat
+from lightly.openapi_generated.swagger_client.models.file_output_format import FileOutputFormat
+from lightly.openapi_generated.swagger_client.models.filename_and_read_url import FilenameAndReadUrl
+from lightly.openapi_generated.swagger_client.models.initial_tag_create_request import InitialTagCreateRequest
+from lightly.openapi_generated.swagger_client.models.label_box_data_row import LabelBoxDataRow
+from lightly.openapi_generated.swagger_client.models.label_box_v4_data_row import LabelBoxV4DataRow
+from lightly.openapi_generated.swagger_client.models.label_studio_task import LabelStudioTask
+from lightly.openapi_generated.swagger_client.models.sama_task import SamaTask
+from lightly.openapi_generated.swagger_client.models.tag_arithmetics_request import TagArithmeticsRequest
+from lightly.openapi_generated.swagger_client.models.tag_arithmetics_response import TagArithmeticsResponse
+from lightly.openapi_generated.swagger_client.models.tag_bit_mask_response import TagBitMaskResponse
+from lightly.openapi_generated.swagger_client.models.tag_create_request import TagCreateRequest
+from lightly.openapi_generated.swagger_client.models.tag_data import TagData
+from lightly.openapi_generated.swagger_client.models.tag_update_request import TagUpdateRequest
+from lightly.openapi_generated.swagger_client.models.tag_upsize_request import TagUpsizeRequest
 
 from lightly.openapi_generated.swagger_client.api_client import ApiClient
+from lightly.openapi_generated.swagger_client.api_response import ApiResponse
+from lightly.openapi_generated.swagger_client.exceptions import (  # noqa: F401
+    ApiTypeError,
+    ApiValueError
+)
 
 
-class DockerApi(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
+class TagsApi(object):
+    """NOTE: This class is auto generated by OpenAPI Generator
+    Ref: https://openapi-generator.tech
 
     Do not edit the class manually.
-    Ref: https://github.com/swagger-api/swagger-codegen
     """
 
     def __init__(self, api_client=None):
         if api_client is None:
-            api_client = ApiClient()
+            api_client = ApiClient.get_default()
         self.api_client = api_client
 
-    def cancel_scheduled_docker_run_state_by_id(self, dataset_id, scheduled_id, **kwargs):  # noqa: E501
-        """cancel_scheduled_docker_run_state_by_id  # noqa: E501
-
-        Cancel a scheduled run. This will fail if the state of the scheduled run is no longer OPEN (e.g when it is LOCKED)   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.cancel_scheduled_docker_run_state_by_id(dataset_id, scheduled_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID scheduled_id: ObjectId of the docker worker run (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.cancel_scheduled_docker_run_state_by_id_with_http_info(dataset_id, scheduled_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.cancel_scheduled_docker_run_state_by_id_with_http_info(dataset_id, scheduled_id, **kwargs)  # noqa: E501
-            return data
-
-    def cancel_scheduled_docker_run_state_by_id_with_http_info(self, dataset_id, scheduled_id, **kwargs):  # noqa: E501
-        """cancel_scheduled_docker_run_state_by_id  # noqa: E501
-
-        Cancel a scheduled run. This will fail if the state of the scheduled run is no longer OPEN (e.g when it is LOCKED)   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.cancel_scheduled_docker_run_state_by_id_with_http_info(dataset_id, scheduled_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID scheduled_id: ObjectId of the docker worker run (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['dataset_id', 'scheduled_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method cancel_scheduled_docker_run_state_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `cancel_scheduled_docker_run_state_by_id`")  # noqa: E501
-        # verify the required parameter 'scheduled_id' is set
-        if self.api_client.client_side_validation and ('scheduled_id' not in params or
-                                                       params['scheduled_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `scheduled_id` when calling `cancel_scheduled_docker_run_state_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'scheduled_id' in params:
-            path_params['scheduledId'] = params['scheduled_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/docker/worker/schedule/{scheduledId}', 'PUT',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type=None,  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def confirm_docker_run_artifact_creation(self, run_id, artifact_id, **kwargs):  # noqa: E501
-        """confirm_docker_run_artifact_creation  # noqa: E501
-
-        confirm that the docker run artifact has been uploaded and is available  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.confirm_docker_run_artifact_creation(run_id, artifact_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :param MongoObjectID artifact_id: ObjectId of the artifact of the docker run (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.confirm_docker_run_artifact_creation_with_http_info(run_id, artifact_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.confirm_docker_run_artifact_creation_with_http_info(run_id, artifact_id, **kwargs)  # noqa: E501
-            return data
-
-    def confirm_docker_run_artifact_creation_with_http_info(self, run_id, artifact_id, **kwargs):  # noqa: E501
-        """confirm_docker_run_artifact_creation  # noqa: E501
-
-        confirm that the docker run artifact has been uploaded and is available  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.confirm_docker_run_artifact_creation_with_http_info(run_id, artifact_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :param MongoObjectID artifact_id: ObjectId of the artifact of the docker run (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['run_id', 'artifact_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method confirm_docker_run_artifact_creation" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'run_id' is set
-        if self.api_client.client_side_validation and ('run_id' not in params or
-                                                       params['run_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `run_id` when calling `confirm_docker_run_artifact_creation`")  # noqa: E501
-        # verify the required parameter 'artifact_id' is set
-        if self.api_client.client_side_validation and ('artifact_id' not in params or
-                                                       params['artifact_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `artifact_id` when calling `confirm_docker_run_artifact_creation`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'run_id' in params:
-            path_params['runId'] = params['run_id']  # noqa: E501
-        if 'artifact_id' in params:
-            path_params['artifactId'] = params['artifact_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/runs/{runId}/artifacts/{artifactId}/confirmUpload', 'PUT',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type=None,  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def create_docker_run(self, body, **kwargs):  # noqa: E501
-        """create_docker_run  # noqa: E501
-
-        Creates a new docker run database entry.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_docker_run(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerRunCreateRequest body: (required)
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.create_docker_run_with_http_info(body, **kwargs)  # noqa: E501
-        else:
-            (data) = self.create_docker_run_with_http_info(body, **kwargs)  # noqa: E501
-            return data
-
-    def create_docker_run_with_http_info(self, body, **kwargs):  # noqa: E501
-        """create_docker_run  # noqa: E501
-
-        Creates a new docker run database entry.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_docker_run_with_http_info(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerRunCreateRequest body: (required)
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method create_docker_run" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `create_docker_run`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/runs', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='CreateEntityResponse',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def create_docker_run_artifact(self, body, run_id, **kwargs):  # noqa: E501
-        """create_docker_run_artifact  # noqa: E501
-
-        creates a docker run artifact and returns the writeUrl and artifactId to upload and confirm  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_docker_run_artifact(body, run_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerRunArtifactCreateRequest body: (required)
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :return: DockerRunArtifactCreatedData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.create_docker_run_artifact_with_http_info(body, run_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.create_docker_run_artifact_with_http_info(body, run_id, **kwargs)  # noqa: E501
-            return data
-
-    def create_docker_run_artifact_with_http_info(self, body, run_id, **kwargs):  # noqa: E501
-        """create_docker_run_artifact  # noqa: E501
-
-        creates a docker run artifact and returns the writeUrl and artifactId to upload and confirm  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_docker_run_artifact_with_http_info(body, run_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerRunArtifactCreateRequest body: (required)
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :return: DockerRunArtifactCreatedData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body', 'run_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method create_docker_run_artifact" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `create_docker_run_artifact`")  # noqa: E501
-        # verify the required parameter 'run_id' is set
-        if self.api_client.client_side_validation and ('run_id' not in params or
-                                                       params['run_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `run_id` when calling `create_docker_run_artifact`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'run_id' in params:
-            path_params['runId'] = params['run_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/runs/{runId}/artifacts', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='DockerRunArtifactCreatedData',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def create_docker_run_scheduled_by_dataset_id(self, body, dataset_id, **kwargs):  # noqa: E501
-        """create_docker_run_scheduled_by_dataset_id  # noqa: E501
-
-        Schedule a docker run by dataset id. With docker runs it's possible to process unlabeled images from a datasource and use active learning to select the most relevant samples for further processing and visualization in the web app   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_docker_run_scheduled_by_dataset_id(body, dataset_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerRunScheduledCreateRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param bool disable_config_validation: if set, disables the sanity check and validation where we check if the provided configuration can run on your datasource e.g     if predictions are used, we check that the bucket structure + tasks.json, schema.json are correct     if metadata is used, we check that the bucket structure + schema.json are correct     if relevantFilenamesFile is set, we check that the file exists 
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.create_docker_run_scheduled_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.create_docker_run_scheduled_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-            return data
-
-    def create_docker_run_scheduled_by_dataset_id_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501
-        """create_docker_run_scheduled_by_dataset_id  # noqa: E501
-
-        Schedule a docker run by dataset id. With docker runs it's possible to process unlabeled images from a datasource and use active learning to select the most relevant samples for further processing and visualization in the web app   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_docker_run_scheduled_by_dataset_id_with_http_info(body, dataset_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerRunScheduledCreateRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param bool disable_config_validation: if set, disables the sanity check and validation where we check if the provided configuration can run on your datasource e.g     if predictions are used, we check that the bucket structure + tasks.json, schema.json are correct     if metadata is used, we check that the bucket structure + schema.json are correct     if relevantFilenamesFile is set, we check that the file exists 
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body', 'dataset_id', 'disable_config_validation']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method create_docker_run_scheduled_by_dataset_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `create_docker_run_scheduled_by_dataset_id`")  # noqa: E501
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `create_docker_run_scheduled_by_dataset_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-
-        query_params = []
-        if 'disable_config_validation' in params:
-            query_params.append(('disableConfigValidation', params['disable_config_validation']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/docker/worker/schedule', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='CreateEntityResponse',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def create_docker_worker_config(self, body, **kwargs):  # noqa: E501
-        """create_docker_worker_config  # noqa: E501
-
-        Creates a docker worker configuration.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_docker_worker_config(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerWorkerConfigCreateRequest body: (required)
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.create_docker_worker_config_with_http_info(body, **kwargs)  # noqa: E501
-        else:
-            (data) = self.create_docker_worker_config_with_http_info(body, **kwargs)  # noqa: E501
-            return data
-
-    def create_docker_worker_config_with_http_info(self, body, **kwargs):  # noqa: E501
-        """create_docker_worker_config  # noqa: E501
-
-        Creates a docker worker configuration.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_docker_worker_config_with_http_info(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerWorkerConfigCreateRequest body: (required)
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method create_docker_worker_config" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `create_docker_worker_config`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker/config', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='CreateEntityResponse',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def create_docker_worker_config_v2(self, body, **kwargs):  # noqa: E501
-        """create_docker_worker_config_v2  # noqa: E501
-
-        Creates a docker worker v2 configuration.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_docker_worker_config_v2(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerWorkerConfigV2CreateRequest body: (required)
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.create_docker_worker_config_v2_with_http_info(body, **kwargs)  # noqa: E501
-        else:
-            (data) = self.create_docker_worker_config_v2_with_http_info(body, **kwargs)  # noqa: E501
-            return data
-
-    def create_docker_worker_config_v2_with_http_info(self, body, **kwargs):  # noqa: E501
-        """create_docker_worker_config_v2  # noqa: E501
-
-        Creates a docker worker v2 configuration.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_docker_worker_config_v2_with_http_info(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerWorkerConfigV2CreateRequest body: (required)
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method create_docker_worker_config_v2" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `create_docker_worker_config_v2`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker/config/v2', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='CreateEntityResponse',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def create_docker_worker_config_v3(self, body, **kwargs):  # noqa: E501
-        """create_docker_worker_config_v3  # noqa: E501
-
-        Creates a docker worker v3 configuration.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_docker_worker_config_v3(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerWorkerConfigV3CreateRequest body: (required)
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.create_docker_worker_config_v3_with_http_info(body, **kwargs)  # noqa: E501
-        else:
-            (data) = self.create_docker_worker_config_v3_with_http_info(body, **kwargs)  # noqa: E501
-            return data
-
-    def create_docker_worker_config_v3_with_http_info(self, body, **kwargs):  # noqa: E501
-        """create_docker_worker_config_v3  # noqa: E501
-
-        Creates a docker worker v3 configuration.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_docker_worker_config_v3_with_http_info(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerWorkerConfigV3CreateRequest body: (required)
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method create_docker_worker_config_v3" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `create_docker_worker_config_v3`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker/config/v3', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='CreateEntityResponse',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def delete_docker_worker_registry_entry_by_id(self, worker_id, **kwargs):  # noqa: E501
-        """delete_docker_worker_registry_entry_by_id  # noqa: E501
-
-        Deletes a worker registry entry by id.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.delete_docker_worker_registry_entry_by_id(worker_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID worker_id: ObjectId of the docker worker (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.delete_docker_worker_registry_entry_by_id_with_http_info(worker_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.delete_docker_worker_registry_entry_by_id_with_http_info(worker_id, **kwargs)  # noqa: E501
-            return data
-
-    def delete_docker_worker_registry_entry_by_id_with_http_info(self, worker_id, **kwargs):  # noqa: E501
-        """delete_docker_worker_registry_entry_by_id  # noqa: E501
-
-        Deletes a worker registry entry by id.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.delete_docker_worker_registry_entry_by_id_with_http_info(worker_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID worker_id: ObjectId of the docker worker (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['worker_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method delete_docker_worker_registry_entry_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'worker_id' is set
-        if self.api_client.client_side_validation and ('worker_id' not in params or
-                                                       params['worker_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `worker_id` when calling `delete_docker_worker_registry_entry_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'worker_id' in params:
-            path_params['workerId'] = params['worker_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker/{workerId}', 'DELETE',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type=None,  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_license_information(self, **kwargs):  # noqa: E501
-        """get_docker_license_information  # noqa: E501
-
-        Requests license information to run the container.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_license_information(async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :return: DockerLicenseInformation
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_license_information_with_http_info(**kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_license_information_with_http_info(**kwargs)  # noqa: E501
-            return data
-
-    def get_docker_license_information_with_http_info(self, **kwargs):  # noqa: E501
-        """get_docker_license_information  # noqa: E501
-
-        Requests license information to run the container.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_license_information_with_http_info(async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :return: DockerLicenseInformation
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = []  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_license_information" % key
-                )
-            params[key] = val
-        del params['kwargs']
-
-        collection_formats = {}
-
-        path_params = {}
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/licenseInformation', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='DockerLicenseInformation',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_run_artifact_read_url_by_id(self, run_id, artifact_id, **kwargs):  # noqa: E501
-        """get_docker_run_artifact_read_url_by_id  # noqa: E501
-
-        Get the url of a specific docker runs artifact  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_run_artifact_read_url_by_id(run_id, artifact_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :param MongoObjectID artifact_id: ObjectId of the artifact of the docker run (required)
-        :return: str
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_run_artifact_read_url_by_id_with_http_info(run_id, artifact_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_run_artifact_read_url_by_id_with_http_info(run_id, artifact_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_docker_run_artifact_read_url_by_id_with_http_info(self, run_id, artifact_id, **kwargs):  # noqa: E501
-        """get_docker_run_artifact_read_url_by_id  # noqa: E501
-
-        Get the url of a specific docker runs artifact  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_run_artifact_read_url_by_id_with_http_info(run_id, artifact_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :param MongoObjectID artifact_id: ObjectId of the artifact of the docker run (required)
-        :return: str
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['run_id', 'artifact_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_run_artifact_read_url_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'run_id' is set
-        if self.api_client.client_side_validation and ('run_id' not in params or
-                                                       params['run_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `run_id` when calling `get_docker_run_artifact_read_url_by_id`")  # noqa: E501
-        # verify the required parameter 'artifact_id' is set
-        if self.api_client.client_side_validation and ('artifact_id' not in params or
-                                                       params['artifact_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `artifact_id` when calling `get_docker_run_artifact_read_url_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'run_id' in params:
-            path_params['runId'] = params['run_id']  # noqa: E501
-        if 'artifact_id' in params:
-            path_params['artifactId'] = params['artifact_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/runs/{runId}/artifacts/{artifactId}/readurl', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='str',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_run_by_id(self, run_id, **kwargs):  # noqa: E501
-        """get_docker_run_by_id  # noqa: E501
-
-        Gets a docker run by docker run id.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_run_by_id(run_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :return: DockerRunData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_run_by_id_with_http_info(run_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_run_by_id_with_http_info(run_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_docker_run_by_id_with_http_info(self, run_id, **kwargs):  # noqa: E501
-        """get_docker_run_by_id  # noqa: E501
-
-        Gets a docker run by docker run id.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_run_by_id_with_http_info(run_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :return: DockerRunData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['run_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_run_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'run_id' is set
-        if self.api_client.client_side_validation and ('run_id' not in params or
-                                                       params['run_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `run_id` when calling `get_docker_run_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'run_id' in params:
-            path_params['runId'] = params['run_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/runs/{runId}', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='DockerRunData',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_run_by_scheduled_id(self, scheduled_id, **kwargs):  # noqa: E501
-        """get_docker_run_by_scheduled_id  # noqa: E501
-
-        Retrieves the associated docker run of a scheduled run; returns the docker run by the id of the scheduled run which caused this docker run. If a scheduled docker run has not yet started being processed by a worker, a 404 will be returned.   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_run_by_scheduled_id(scheduled_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID scheduled_id: ObjectId of the docker worker run (required)
-        :return: DockerRunData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_run_by_scheduled_id_with_http_info(scheduled_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_run_by_scheduled_id_with_http_info(scheduled_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_docker_run_by_scheduled_id_with_http_info(self, scheduled_id, **kwargs):  # noqa: E501
-        """get_docker_run_by_scheduled_id  # noqa: E501
-
-        Retrieves the associated docker run of a scheduled run; returns the docker run by the id of the scheduled run which caused this docker run. If a scheduled docker run has not yet started being processed by a worker, a 404 will be returned.   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_run_by_scheduled_id_with_http_info(scheduled_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID scheduled_id: ObjectId of the docker worker run (required)
-        :return: DockerRunData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['scheduled_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_run_by_scheduled_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'scheduled_id' is set
-        if self.api_client.client_side_validation and ('scheduled_id' not in params or
-                                                       params['scheduled_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `scheduled_id` when calling `get_docker_run_by_scheduled_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'scheduled_id' in params:
-            path_params['scheduledId'] = params['scheduled_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/runs/schedule/{scheduledId}', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='DockerRunData',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_run_logs_by_id(self, run_id, **kwargs):  # noqa: E501
-        """get_docker_run_logs_by_id  # noqa: E501
-
-        Gets the logs of a docker run by docker run id.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_run_logs_by_id(run_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :param int cursor: the cursor of where the logs last were
-        :return: DockerRunLogData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_run_logs_by_id_with_http_info(run_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_run_logs_by_id_with_http_info(run_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_docker_run_logs_by_id_with_http_info(self, run_id, **kwargs):  # noqa: E501
-        """get_docker_run_logs_by_id  # noqa: E501
-
-        Gets the logs of a docker run by docker run id.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_run_logs_by_id_with_http_info(run_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :param int cursor: the cursor of where the logs last were
-        :return: DockerRunLogData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['run_id', 'cursor']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_run_logs_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'run_id' is set
-        if self.api_client.client_side_validation and ('run_id' not in params or
-                                                       params['run_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `run_id` when calling `get_docker_run_logs_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'run_id' in params:
-            path_params['runId'] = params['run_id']  # noqa: E501
-
-        query_params = []
-        if 'cursor' in params:
-            query_params.append(('cursor', params['cursor']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/runs/{runId}/logs', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='DockerRunLogData',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_run_report_read_url_by_id(self, run_id, **kwargs):  # noqa: E501
-        """get_docker_run_report_read_url_by_id  # noqa: E501
-
-        Get the url of a specific docker runs report  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_run_report_read_url_by_id(run_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :return: str
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_run_report_read_url_by_id_with_http_info(run_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_run_report_read_url_by_id_with_http_info(run_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_docker_run_report_read_url_by_id_with_http_info(self, run_id, **kwargs):  # noqa: E501
-        """get_docker_run_report_read_url_by_id  # noqa: E501
-
-        Get the url of a specific docker runs report  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_run_report_read_url_by_id_with_http_info(run_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :return: str
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['run_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_run_report_read_url_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'run_id' is set
-        if self.api_client.client_side_validation and ('run_id' not in params or
-                                                       params['run_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `run_id` when calling `get_docker_run_report_read_url_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'run_id' in params:
-            path_params['runId'] = params['run_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/runs/{runId}/readReportUrl', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='str',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_run_report_write_url_by_id(self, run_id, **kwargs):  # noqa: E501
-        """get_docker_run_report_write_url_by_id  # noqa: E501
-
-        Get the signed url to upload a report of a docker run  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_run_report_write_url_by_id(run_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :return: str
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_run_report_write_url_by_id_with_http_info(run_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_run_report_write_url_by_id_with_http_info(run_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_docker_run_report_write_url_by_id_with_http_info(self, run_id, **kwargs):  # noqa: E501
-        """get_docker_run_report_write_url_by_id  # noqa: E501
-
-        Get the signed url to upload a report of a docker run  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_run_report_write_url_by_id_with_http_info(run_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :return: str
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['run_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_run_report_write_url_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'run_id' is set
-        if self.api_client.client_side_validation and ('run_id' not in params or
-                                                       params['run_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `run_id` when calling `get_docker_run_report_write_url_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'run_id' in params:
-            path_params['runId'] = params['run_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/runs/{runId}/writeReportUrl', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='str',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_run_tags(self, run_id, **kwargs):  # noqa: E501
-        """get_docker_run_tags  # noqa: E501
-
-        Gets all tags which were created from a docker run by docker run id.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_run_tags(run_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :return: list[TagData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_run_tags_with_http_info(run_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_run_tags_with_http_info(run_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_docker_run_tags_with_http_info(self, run_id, **kwargs):  # noqa: E501
-        """get_docker_run_tags  # noqa: E501
-
-        Gets all tags which were created from a docker run by docker run id.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_run_tags_with_http_info(run_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :return: list[TagData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['run_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_run_tags" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'run_id' is set
-        if self.api_client.client_side_validation and ('run_id' not in params or
-                                                       params['run_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `run_id` when calling `get_docker_run_tags`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'run_id' in params:
-            path_params['runId'] = params['run_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/runs/{runId}/tags', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='list[TagData]',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_runs(self, **kwargs):  # noqa: E501
-        """get_docker_runs  # noqa: E501
-
-        Gets all docker runs for a user.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_runs(async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: list[DockerRunData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_runs_with_http_info(**kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_runs_with_http_info(**kwargs)  # noqa: E501
-            return data
-
-    def get_docker_runs_with_http_info(self, **kwargs):  # noqa: E501
-        """get_docker_runs  # noqa: E501
-
-        Gets all docker runs for a user.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_runs_with_http_info(async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: list[DockerRunData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['page_size', 'page_offset']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_runs" % key
-                )
-            params[key] = val
-        del params['kwargs']
-
-        collection_formats = {}
-
-        path_params = {}
-
-        query_params = []
-        if 'page_size' in params:
-            query_params.append(('pageSize', params['page_size']))  # noqa: E501
-        if 'page_offset' in params:
-            query_params.append(('pageOffset', params['page_offset']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/runs', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='list[DockerRunData]',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_runs_count(self, **kwargs):  # noqa: E501
-        """get_docker_runs_count  # noqa: E501
-
-        Gets the total count of the amount of runs existing for a user  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_runs_count(async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :return: str
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_runs_count_with_http_info(**kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_runs_count_with_http_info(**kwargs)  # noqa: E501
-            return data
-
-    def get_docker_runs_count_with_http_info(self, **kwargs):  # noqa: E501
-        """get_docker_runs_count  # noqa: E501
-
-        Gets the total count of the amount of runs existing for a user  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_runs_count_with_http_info(async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :return: str
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = []  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_runs_count" % key
-                )
-            params[key] = val
-        del params['kwargs']
-
-        collection_formats = {}
-
-        path_params = {}
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
+    @validate_arguments
+    def create_initial_tag_by_dataset_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], initial_tag_create_request : InitialTagCreateRequest, **kwargs) -> CreateEntityResponse:  # noqa: E501
+        """create_initial_tag_by_dataset_id  # noqa: E501
+
+        create the intitial tag for a dataset which then locks the dataset  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.create_initial_tag_by_dataset_id(dataset_id, initial_tag_create_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param initial_tag_create_request: (required)
+        :type initial_tag_create_request: InitialTagCreateRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: CreateEntityResponse
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the create_initial_tag_by_dataset_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.create_initial_tag_by_dataset_id_with_http_info(dataset_id, initial_tag_create_request, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def create_initial_tag_by_dataset_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], initial_tag_create_request : InitialTagCreateRequest, **kwargs) -> ApiResponse:  # noqa: E501
+        """create_initial_tag_by_dataset_id  # noqa: E501
+
+        create the intitial tag for a dataset which then locks the dataset  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.create_initial_tag_by_dataset_id_with_http_info(dataset_id, initial_tag_create_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param initial_tag_create_request: (required)
+        :type initial_tag_create_request: InitialTagCreateRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(CreateEntityResponse, status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'initial_tag_create_request'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method create_initial_tag_by_dataset_id" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        if _params['initial_tag_create_request'] is not None:
+            _body_params = _params['initial_tag_create_request']
+
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
+
+        # set the HTTP header `Content-Type`
+        _content_types_list = _params.get('_content_type',
+            self.api_client.select_header_content_type(
+                ['application/json']))
+        if _content_types_list:
+                _header_params['Content-Type'] = _content_types_list
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {
+            '201': "CreateEntityResponse",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/initial', 'POST',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def create_tag_by_dataset_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_create_request : TagCreateRequest, **kwargs) -> CreateEntityResponse:  # noqa: E501
+        """create_tag_by_dataset_id  # noqa: E501
+
+        create new tag for dataset  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.create_tag_by_dataset_id(dataset_id, tag_create_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_create_request: (required)
+        :type tag_create_request: TagCreateRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: CreateEntityResponse
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the create_tag_by_dataset_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.create_tag_by_dataset_id_with_http_info(dataset_id, tag_create_request, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def create_tag_by_dataset_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_create_request : TagCreateRequest, **kwargs) -> ApiResponse:  # noqa: E501
+        """create_tag_by_dataset_id  # noqa: E501
+
+        create new tag for dataset  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.create_tag_by_dataset_id_with_http_info(dataset_id, tag_create_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_create_request: (required)
+        :type tag_create_request: TagCreateRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(CreateEntityResponse, status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_create_request'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method create_tag_by_dataset_id" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        if _params['tag_create_request'] is not None:
+            _body_params = _params['tag_create_request']
+
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
+
+        # set the HTTP header `Content-Type`
+        _content_types_list = _params.get('_content_type',
+            self.api_client.select_header_content_type(
+                ['application/json']))
+        if _content_types_list:
+                _header_params['Content-Type'] = _content_types_list
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {
+            '201': "CreateEntityResponse",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags', 'POST',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def delete_tag_by_tag_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], **kwargs) -> None:  # noqa: E501
+        """delete_tag_by_tag_id  # noqa: E501
+
+        delete a specific tag if its a leaf-tag (e.g is not a dependency of another tag)  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.delete_tag_by_tag_id(dataset_id, tag_id, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: None
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the delete_tag_by_tag_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.delete_tag_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def delete_tag_by_tag_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], **kwargs) -> ApiResponse:  # noqa: E501
+        """delete_tag_by_tag_id  # noqa: E501
+
+        delete a specific tag if its a leaf-tag (e.g is not a dependency of another tag)  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.delete_tag_by_tag_id_with_http_info(dataset_id, tag_id, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: None
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_id'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method delete_tag_by_tag_id" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['tag_id']:
+            _path_params['tagId'] = _params['tag_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {}
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/{tagId}', 'DELETE',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def download_zip_of_samples_by_tag_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], **kwargs) -> bytearray:  # noqa: E501
+        """download_zip_of_samples_by_tag_id  # noqa: E501
+
+        Download a zip file of the samples of a tag. Limited to 1000 images  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.download_zip_of_samples_by_tag_id(dataset_id, tag_id, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: bytearray
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the download_zip_of_samples_by_tag_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.download_zip_of_samples_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def download_zip_of_samples_by_tag_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], **kwargs) -> ApiResponse:  # noqa: E501
+        """download_zip_of_samples_by_tag_id  # noqa: E501
+
+        Download a zip file of the samples of a tag. Limited to 1000 images  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.download_zip_of_samples_by_tag_id_with_http_info(dataset_id, tag_id, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(bytearray, status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_id'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method download_zip_of_samples_by_tag_id" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['tag_id']:
+            _path_params['tagId'] = _params['tag_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/zip', 'application/json'])  # noqa: E501
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {
+            '200': "bytearray",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+            '413': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/{tagId}/export/zip', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def export_tag_to_basic_filenames(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], expires_in : Annotated[Optional[StrictInt], Field(description="If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. ")] = None, access_control : Annotated[Optional[StrictStr], Field(description="which access control name to be used")] = None, file_name_format : Optional[FileNameFormat] = None, include_meta_data : Annotated[Optional[StrictBool], Field(description="if true, will also include metadata")] = None, format : Optional[FileOutputFormat] = None, preview_example : Annotated[Optional[StrictBool], Field(description="if true, will generate a preview example of how the structure will look")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> str:  # noqa: E501
+        """export_tag_to_basic_filenames  # noqa: E501
+
+        Export the samples filenames of a specific tag   # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.export_tag_to_basic_filenames(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
+        :type expires_in: int
+        :param access_control: which access control name to be used
+        :type access_control: str
+        :param file_name_format:
+        :type file_name_format: FileNameFormat
+        :param include_meta_data: if true, will also include metadata
+        :type include_meta_data: bool
+        :param format:
+        :type format: FileOutputFormat
+        :param preview_example: if true, will generate a preview example of how the structure will look
+        :type preview_example: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: str
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the export_tag_to_basic_filenames_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.export_tag_to_basic_filenames_with_http_info(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def export_tag_to_basic_filenames_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], expires_in : Annotated[Optional[StrictInt], Field(description="If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. ")] = None, access_control : Annotated[Optional[StrictStr], Field(description="which access control name to be used")] = None, file_name_format : Optional[FileNameFormat] = None, include_meta_data : Annotated[Optional[StrictBool], Field(description="if true, will also include metadata")] = None, format : Optional[FileOutputFormat] = None, preview_example : Annotated[Optional[StrictBool], Field(description="if true, will generate a preview example of how the structure will look")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> ApiResponse:  # noqa: E501
+        """export_tag_to_basic_filenames  # noqa: E501
+
+        Export the samples filenames of a specific tag   # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.export_tag_to_basic_filenames_with_http_info(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
+        :type expires_in: int
+        :param access_control: which access control name to be used
+        :type access_control: str
+        :param file_name_format:
+        :type file_name_format: FileNameFormat
+        :param include_meta_data: if true, will also include metadata
+        :type include_meta_data: bool
+        :param format:
+        :type format: FileOutputFormat
+        :param preview_example: if true, will generate a preview example of how the structure will look
+        :type preview_example: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(str, status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_id',
+            'expires_in',
+            'access_control',
+            'file_name_format',
+            'include_meta_data',
+            'format',
+            'preview_example',
+            'page_size',
+            'page_offset'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method export_tag_to_basic_filenames" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['tag_id']:
+            _path_params['tagId'] = _params['tag_id']
+
+
+        # process the query parameters
+        _query_params = []
+        if _params.get('expires_in') is not None:  # noqa: E501
+            _query_params.append((
+                'expiresIn',
+                _params['expires_in'].value if hasattr(_params['expires_in'], 'value') else _params['expires_in']
+            ))
+
+        if _params.get('access_control') is not None:  # noqa: E501
+            _query_params.append((
+                'accessControl',
+                _params['access_control'].value if hasattr(_params['access_control'], 'value') else _params['access_control']
+            ))
+
+        if _params.get('file_name_format') is not None:  # noqa: E501
+            _query_params.append((
+                'fileNameFormat',
+                _params['file_name_format'].value if hasattr(_params['file_name_format'], 'value') else _params['file_name_format']
+            ))
+
+        if _params.get('include_meta_data') is not None:  # noqa: E501
+            _query_params.append((
+                'includeMetaData',
+                _params['include_meta_data'].value if hasattr(_params['include_meta_data'], 'value') else _params['include_meta_data']
+            ))
+
+        if _params.get('format') is not None:  # noqa: E501
+            _query_params.append((
+                'format',
+                _params['format'].value if hasattr(_params['format'], 'value') else _params['format']
+            ))
+
+        if _params.get('preview_example') is not None:  # noqa: E501
+            _query_params.append((
+                'previewExample',
+                _params['preview_example'].value if hasattr(_params['preview_example'], 'value') else _params['preview_example']
+            ))
+
+        if _params.get('page_size') is not None:  # noqa: E501
+            _query_params.append((
+                'pageSize',
+                _params['page_size'].value if hasattr(_params['page_size'], 'value') else _params['page_size']
+            ))
+
+        if _params.get('page_offset') is not None:  # noqa: E501
+            _query_params.append((
+                'pageOffset',
+                _params['page_offset'].value if hasattr(_params['page_offset'], 'value') else _params['page_offset']
+            ))
+
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['text/plain', 'application/json'])  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/runs/count', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='str',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_runs_query_by_dataset_id(self, dataset_id, **kwargs):  # noqa: E501
-        """get_docker_runs_query_by_dataset_id  # noqa: E501
-
-        Get all docker runs of a user by dataset id  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_runs_query_by_dataset_id(dataset_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: list[DockerRunData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_runs_query_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_runs_query_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_docker_runs_query_by_dataset_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501
-        """get_docker_runs_query_by_dataset_id  # noqa: E501
-
-        Get all docker runs of a user by dataset id  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_runs_query_by_dataset_id_with_http_info(dataset_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: list[DockerRunData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['dataset_id', 'page_size', 'page_offset']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_runs_query_by_dataset_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `get_docker_runs_query_by_dataset_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-
-        query_params = []
-        if 'page_size' in params:
-            query_params.append(('pageSize', params['page_size']))  # noqa: E501
-        if 'page_offset' in params:
-            query_params.append(('pageOffset', params['page_offset']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/runs/query/datasetId/{datasetId}', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='list[DockerRunData]',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_runs_scheduled_by_dataset_id(self, dataset_id, **kwargs):  # noqa: E501
-        """get_docker_runs_scheduled_by_dataset_id  # noqa: E501
-
-        Get all scheduled docker runs by dataset id. If no state is specified, returns runs which have not yet finished (neither DONE or CANCELED).  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_runs_scheduled_by_dataset_id(dataset_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param DockerRunScheduledState state:
-        :return: list[DockerRunScheduledData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_runs_scheduled_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_runs_scheduled_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_docker_runs_scheduled_by_dataset_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501
-        """get_docker_runs_scheduled_by_dataset_id  # noqa: E501
-
-        Get all scheduled docker runs by dataset id. If no state is specified, returns runs which have not yet finished (neither DONE or CANCELED).  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_runs_scheduled_by_dataset_id_with_http_info(dataset_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param DockerRunScheduledState state:
-        :return: list[DockerRunScheduledData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['dataset_id', 'state']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_runs_scheduled_by_dataset_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `get_docker_runs_scheduled_by_dataset_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-
-        query_params = []
-        if 'state' in params:
-            query_params.append(('state', params['state']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/docker/worker/schedule', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='list[DockerRunScheduledData]',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_runs_scheduled_by_state_and_labels(self, **kwargs):  # noqa: E501
-        """get_docker_runs_scheduled_by_state_and_labels  # noqa: E501
-
-        Get all scheduled docker runs of the user. Additionally, you can filter by state.  Furthermore, you can filter by only providing labels and only return scheduled runs whose runsOn labels are included in the provided labels. Runs are filtered by the provided version parameter. Version parameter set to * returns all configs   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_runs_scheduled_by_state_and_labels(async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerRunScheduledState state:
-        :param DockerWorkerLabels labels:
-        :param str version:
-        :return: list[DockerRunScheduledData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_runs_scheduled_by_state_and_labels_with_http_info(**kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_runs_scheduled_by_state_and_labels_with_http_info(**kwargs)  # noqa: E501
-            return data
-
-    def get_docker_runs_scheduled_by_state_and_labels_with_http_info(self, **kwargs):  # noqa: E501
-        """get_docker_runs_scheduled_by_state_and_labels  # noqa: E501
-
-        Get all scheduled docker runs of the user. Additionally, you can filter by state.  Furthermore, you can filter by only providing labels and only return scheduled runs whose runsOn labels are included in the provided labels. Runs are filtered by the provided version parameter. Version parameter set to * returns all configs   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_runs_scheduled_by_state_and_labels_with_http_info(async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerRunScheduledState state:
-        :param DockerWorkerLabels labels:
-        :param str version:
-        :return: list[DockerRunScheduledData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['state', 'labels', 'version']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_runs_scheduled_by_state_and_labels" % key
-                )
-            params[key] = val
-        del params['kwargs']
-
-        collection_formats = {}
-
-        path_params = {}
-
-        query_params = []
-        if 'state' in params:
-            query_params.append(('state', params['state']))  # noqa: E501
-        if 'labels' in params:
-            query_params.append(('labels', params['labels']))  # noqa: E501
-        if 'version' in params:
-            query_params.append(('version', params['version']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker/schedule', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='list[DockerRunScheduledData]',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_runs_scheduled_by_worker_id(self, worker_id, **kwargs):  # noqa: E501
-        """get_docker_runs_scheduled_by_worker_id  # noqa: E501
-
-        Get all scheduled runs that might be picked up by the worker with that workerId.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_runs_scheduled_by_worker_id(worker_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID worker_id: ObjectId of the docker worker (required)
-        :param DockerRunScheduledState state:
-        :return: list[DockerRunScheduledData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_runs_scheduled_by_worker_id_with_http_info(worker_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_runs_scheduled_by_worker_id_with_http_info(worker_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_docker_runs_scheduled_by_worker_id_with_http_info(self, worker_id, **kwargs):  # noqa: E501
-        """get_docker_runs_scheduled_by_worker_id  # noqa: E501
-
-        Get all scheduled runs that might be picked up by the worker with that workerId.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_runs_scheduled_by_worker_id_with_http_info(worker_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID worker_id: ObjectId of the docker worker (required)
-        :param DockerRunScheduledState state:
-        :return: list[DockerRunScheduledData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['worker_id', 'state']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_runs_scheduled_by_worker_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'worker_id' is set
-        if self.api_client.client_side_validation and ('worker_id' not in params or
-                                                       params['worker_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `worker_id` when calling `get_docker_runs_scheduled_by_worker_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'worker_id' in params:
-            path_params['workerId'] = params['worker_id']  # noqa: E501
-
-        query_params = []
-        if 'state' in params:
-            query_params.append(('state', params['state']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker/{workerId}/schedule', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='list[DockerRunScheduledData]',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_worker_config_by_id(self, config_id, **kwargs):  # noqa: E501
-        """get_docker_worker_config_by_id  # noqa: E501
-
-        Gets a docker worker configuration by id. It will try to return the config version but expects (and will fail if not) the config to be of v0   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_worker_config_by_id(config_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID config_id: ObjectId of the docker worker config (required)
-        :return: DockerWorkerConfigData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_worker_config_by_id_with_http_info(config_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_worker_config_by_id_with_http_info(config_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_docker_worker_config_by_id_with_http_info(self, config_id, **kwargs):  # noqa: E501
-        """get_docker_worker_config_by_id  # noqa: E501
-
-        Gets a docker worker configuration by id. It will try to return the config version but expects (and will fail if not) the config to be of v0   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_worker_config_by_id_with_http_info(config_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID config_id: ObjectId of the docker worker config (required)
-        :return: DockerWorkerConfigData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['config_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_worker_config_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'config_id' is set
-        if self.api_client.client_side_validation and ('config_id' not in params or
-                                                       params['config_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `config_id` when calling `get_docker_worker_config_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'config_id' in params:
-            path_params['configId'] = params['config_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker/config/{configId}', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='DockerWorkerConfigData',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_worker_config_v2_by_id(self, config_id, **kwargs):  # noqa: E501
-        """get_docker_worker_config_v2_by_id  # noqa: E501
-
-        Gets a docker worker configuration by id. It will try to return the config version but expects (and will fail if not) the config to be of v2   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_worker_config_v2_by_id(config_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID config_id: ObjectId of the docker worker config (required)
-        :return: DockerWorkerConfigV2Data
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_worker_config_v2_by_id_with_http_info(config_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_worker_config_v2_by_id_with_http_info(config_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_docker_worker_config_v2_by_id_with_http_info(self, config_id, **kwargs):  # noqa: E501
-        """get_docker_worker_config_v2_by_id  # noqa: E501
-
-        Gets a docker worker configuration by id. It will try to return the config version but expects (and will fail if not) the config to be of v2   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_worker_config_v2_by_id_with_http_info(config_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID config_id: ObjectId of the docker worker config (required)
-        :return: DockerWorkerConfigV2Data
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['config_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_worker_config_v2_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'config_id' is set
-        if self.api_client.client_side_validation and ('config_id' not in params or
-                                                       params['config_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `config_id` when calling `get_docker_worker_config_v2_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'config_id' in params:
-            path_params['configId'] = params['config_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker/config/v2/{configId}', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='DockerWorkerConfigV2Data',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_worker_config_v3_by_id(self, config_id, **kwargs):  # noqa: E501
-        """get_docker_worker_config_v3_by_id  # noqa: E501
-
-        Gets a docker worker configuration by id. It will try to return the config version but requires the config to be of v3.   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_worker_config_v3_by_id(config_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID config_id: ObjectId of the docker worker config (required)
-        :return: DockerWorkerConfigV3Data
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_worker_config_v3_by_id_with_http_info(config_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_worker_config_v3_by_id_with_http_info(config_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_docker_worker_config_v3_by_id_with_http_info(self, config_id, **kwargs):  # noqa: E501
-        """get_docker_worker_config_v3_by_id  # noqa: E501
-
-        Gets a docker worker configuration by id. It will try to return the config version but requires the config to be of v3.   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_worker_config_v3_by_id_with_http_info(config_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID config_id: ObjectId of the docker worker config (required)
-        :return: DockerWorkerConfigV3Data
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['config_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_worker_config_v3_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'config_id' is set
-        if self.api_client.client_side_validation and ('config_id' not in params or
-                                                       params['config_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `config_id` when calling `get_docker_worker_config_v3_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'config_id' in params:
-            path_params['configId'] = params['config_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker/config/v3/{configId}', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='DockerWorkerConfigV3Data',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_worker_configs(self, **kwargs):  # noqa: E501
-        """get_docker_worker_configs  # noqa: E501
-
-        Get docker worker configurations.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_worker_configs(async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :return: list[DockerWorkerConfigData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_worker_configs_with_http_info(**kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_worker_configs_with_http_info(**kwargs)  # noqa: E501
-            return data
-
-    def get_docker_worker_configs_with_http_info(self, **kwargs):  # noqa: E501
-        """get_docker_worker_configs  # noqa: E501
-
-        Get docker worker configurations.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_worker_configs_with_http_info(async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :return: list[DockerWorkerConfigData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = []  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_worker_configs" % key
-                )
-            params[key] = val
-        del params['kwargs']
-
-        collection_formats = {}
-
-        path_params = {}
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker/config', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='list[DockerWorkerConfigData]',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_worker_registry_entries(self, **kwargs):  # noqa: E501
-        """get_docker_worker_registry_entries  # noqa: E501
-
-        Returns all worker registry entries for a given user.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_worker_registry_entries(async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :return: list[DockerWorkerRegistryEntryData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_worker_registry_entries_with_http_info(**kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_worker_registry_entries_with_http_info(**kwargs)  # noqa: E501
-            return data
-
-    def get_docker_worker_registry_entries_with_http_info(self, **kwargs):  # noqa: E501
-        """get_docker_worker_registry_entries  # noqa: E501
-
-        Returns all worker registry entries for a given user.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_worker_registry_entries_with_http_info(async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :return: list[DockerWorkerRegistryEntryData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = []  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_worker_registry_entries" % key
-                )
-            params[key] = val
-        del params['kwargs']
-
-        collection_formats = {}
-
-        path_params = {}
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='list[DockerWorkerRegistryEntryData]',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_docker_worker_registry_entry_by_id(self, worker_id, **kwargs):  # noqa: E501
-        """get_docker_worker_registry_entry_by_id  # noqa: E501
-
-        Returns worker registry entry by id.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_worker_registry_entry_by_id(worker_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID worker_id: ObjectId of the docker worker (required)
-        :return: DockerWorkerRegistryEntryData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_docker_worker_registry_entry_by_id_with_http_info(worker_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_docker_worker_registry_entry_by_id_with_http_info(worker_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_docker_worker_registry_entry_by_id_with_http_info(self, worker_id, **kwargs):  # noqa: E501
-        """get_docker_worker_registry_entry_by_id  # noqa: E501
-
-        Returns worker registry entry by id.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_docker_worker_registry_entry_by_id_with_http_info(worker_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID worker_id: ObjectId of the docker worker (required)
-        :return: DockerWorkerRegistryEntryData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['worker_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_docker_worker_registry_entry_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'worker_id' is set
-        if self.api_client.client_side_validation and ('worker_id' not in params or
-                                                       params['worker_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `worker_id` when calling `get_docker_worker_registry_entry_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'worker_id' in params:
-            path_params['workerId'] = params['worker_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker/{workerId}', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='DockerWorkerRegistryEntryData',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def post_docker_authorization_request(self, body, **kwargs):  # noqa: E501
-        """post_docker_authorization_request  # noqa: E501
-
-        Performs an authorization to run the container.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.post_docker_authorization_request(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerAuthorizationRequest body: (required)
-        :return: DockerAuthorizationResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.post_docker_authorization_request_with_http_info(body, **kwargs)  # noqa: E501
-        else:
-            (data) = self.post_docker_authorization_request_with_http_info(body, **kwargs)  # noqa: E501
-            return data
-
-    def post_docker_authorization_request_with_http_info(self, body, **kwargs):  # noqa: E501
-        """post_docker_authorization_request  # noqa: E501
-
-        Performs an authorization to run the container.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.post_docker_authorization_request_with_http_info(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerAuthorizationRequest body: (required)
-        :return: DockerAuthorizationResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method post_docker_authorization_request" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `post_docker_authorization_request`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/authorization', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='DockerAuthorizationResponse',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def post_docker_usage_stats(self, body, **kwargs):  # noqa: E501
-        """post_docker_usage_stats  # noqa: E501
-
-        Adds a diagnostic entry of user stats.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.post_docker_usage_stats(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerUserStats body: (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.post_docker_usage_stats_with_http_info(body, **kwargs)  # noqa: E501
-        else:
-            (data) = self.post_docker_usage_stats_with_http_info(body, **kwargs)  # noqa: E501
-            return data
-
-    def post_docker_usage_stats_with_http_info(self, body, **kwargs):  # noqa: E501
-        """post_docker_usage_stats  # noqa: E501
-
-        Adds a diagnostic entry of user stats.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.post_docker_usage_stats_with_http_info(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerUserStats body: (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method post_docker_usage_stats" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `post_docker_usage_stats`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type=None,  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def post_docker_worker_authorization_request(self, body, **kwargs):  # noqa: E501
-        """post_docker_worker_authorization_request  # noqa: E501
-
-        Performs an authorization to run the Lightly Worker.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.post_docker_worker_authorization_request(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerWorkerAuthorizationRequest body: (required)
-        :return: str
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.post_docker_worker_authorization_request_with_http_info(body, **kwargs)  # noqa: E501
-        else:
-            (data) = self.post_docker_worker_authorization_request_with_http_info(body, **kwargs)  # noqa: E501
-            return data
-
-    def post_docker_worker_authorization_request_with_http_info(self, body, **kwargs):  # noqa: E501
-        """post_docker_worker_authorization_request  # noqa: E501
-
-        Performs an authorization to run the Lightly Worker.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.post_docker_worker_authorization_request_with_http_info(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerWorkerAuthorizationRequest body: (required)
-        :return: str
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method post_docker_worker_authorization_request" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `post_docker_worker_authorization_request`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
+        _response_types_map = {
+            '200': "str",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/{tagId}/export/basic/filenames', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def export_tag_to_basic_filenames_and_read_urls(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], format : Optional[FileOutputFormat] = None, preview_example : Annotated[Optional[StrictBool], Field(description="if true, will generate a preview example of how the structure will look")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> List[FilenameAndReadUrl]:  # noqa: E501
+        """export_tag_to_basic_filenames_and_read_urls  # noqa: E501
+
+        Export the samples filenames to map with their readURL.   # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.export_tag_to_basic_filenames_and_read_urls(dataset_id, tag_id, format, preview_example, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param format:
+        :type format: FileOutputFormat
+        :param preview_example: if true, will generate a preview example of how the structure will look
+        :type preview_example: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: List[FilenameAndReadUrl]
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the export_tag_to_basic_filenames_and_read_urls_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.export_tag_to_basic_filenames_and_read_urls_with_http_info(dataset_id, tag_id, format, preview_example, page_size, page_offset, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def export_tag_to_basic_filenames_and_read_urls_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], format : Optional[FileOutputFormat] = None, preview_example : Annotated[Optional[StrictBool], Field(description="if true, will generate a preview example of how the structure will look")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> ApiResponse:  # noqa: E501
+        """export_tag_to_basic_filenames_and_read_urls  # noqa: E501
+
+        Export the samples filenames to map with their readURL.   # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.export_tag_to_basic_filenames_and_read_urls_with_http_info(dataset_id, tag_id, format, preview_example, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param format:
+        :type format: FileOutputFormat
+        :param preview_example: if true, will generate a preview example of how the structure will look
+        :type preview_example: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(List[FilenameAndReadUrl], status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_id',
+            'format',
+            'preview_example',
+            'page_size',
+            'page_offset'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method export_tag_to_basic_filenames_and_read_urls" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['tag_id']:
+            _path_params['tagId'] = _params['tag_id']
+
+
+        # process the query parameters
+        _query_params = []
+        if _params.get('format') is not None:  # noqa: E501
+            _query_params.append((
+                'format',
+                _params['format'].value if hasattr(_params['format'], 'value') else _params['format']
+            ))
+
+        if _params.get('preview_example') is not None:  # noqa: E501
+            _query_params.append((
+                'previewExample',
+                _params['preview_example'].value if hasattr(_params['preview_example'], 'value') else _params['preview_example']
+            ))
+
+        if _params.get('page_size') is not None:  # noqa: E501
+            _query_params.append((
+                'pageSize',
+                _params['page_size'].value if hasattr(_params['page_size'], 'value') else _params['page_size']
+            ))
+
+        if _params.get('page_offset') is not None:  # noqa: E501
+            _query_params.append((
+                'pageOffset',
+                _params['page_offset'].value if hasattr(_params['page_offset'], 'value') else _params['page_offset']
+            ))
+
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {
+            '200': "List[FilenameAndReadUrl]",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/{tagId}/export/basic/filenamesAndReadUrls', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def export_tag_to_label_box_data_rows(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], expires_in : Annotated[Optional[StrictInt], Field(description="If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. ")] = None, access_control : Annotated[Optional[StrictStr], Field(description="which access control name to be used")] = None, file_name_format : Optional[FileNameFormat] = None, include_meta_data : Annotated[Optional[StrictBool], Field(description="if true, will also include metadata")] = None, format : Optional[FileOutputFormat] = None, preview_example : Annotated[Optional[StrictBool], Field(description="if true, will generate a preview example of how the structure will look")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> List[LabelBoxDataRow]:  # noqa: E501
+        """(Deprecated) export_tag_to_label_box_data_rows  # noqa: E501
+
+        Deprecated. Please use V4 unless there is a specific need to use the LabelBox V3 API. Export samples of a tag as a json for importing into LabelBox as outlined here; https://docs.labelbox.com/v3/reference/image ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.export_tag_to_label_box_data_rows(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
+        :type expires_in: int
+        :param access_control: which access control name to be used
+        :type access_control: str
+        :param file_name_format:
+        :type file_name_format: FileNameFormat
+        :param include_meta_data: if true, will also include metadata
+        :type include_meta_data: bool
+        :param format:
+        :type format: FileOutputFormat
+        :param preview_example: if true, will generate a preview example of how the structure will look
+        :type preview_example: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: List[LabelBoxDataRow]
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the export_tag_to_label_box_data_rows_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.export_tag_to_label_box_data_rows_with_http_info(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def export_tag_to_label_box_data_rows_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], expires_in : Annotated[Optional[StrictInt], Field(description="If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. ")] = None, access_control : Annotated[Optional[StrictStr], Field(description="which access control name to be used")] = None, file_name_format : Optional[FileNameFormat] = None, include_meta_data : Annotated[Optional[StrictBool], Field(description="if true, will also include metadata")] = None, format : Optional[FileOutputFormat] = None, preview_example : Annotated[Optional[StrictBool], Field(description="if true, will generate a preview example of how the structure will look")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> ApiResponse:  # noqa: E501
+        """(Deprecated) export_tag_to_label_box_data_rows  # noqa: E501
+
+        Deprecated. Please use V4 unless there is a specific need to use the LabelBox V3 API. Export samples of a tag as a json for importing into LabelBox as outlined here; https://docs.labelbox.com/v3/reference/image ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.export_tag_to_label_box_data_rows_with_http_info(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
+        :type expires_in: int
+        :param access_control: which access control name to be used
+        :type access_control: str
+        :param file_name_format:
+        :type file_name_format: FileNameFormat
+        :param include_meta_data: if true, will also include metadata
+        :type include_meta_data: bool
+        :param format:
+        :type format: FileOutputFormat
+        :param preview_example: if true, will generate a preview example of how the structure will look
+        :type preview_example: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(List[LabelBoxDataRow], status_code(int), headers(HTTPHeaderDict))
+        """
+
+        warnings.warn("GET /v1/datasets/{datasetId}/tags/{tagId}/export/LabelBox/datarows is deprecated.", DeprecationWarning)
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_id',
+            'expires_in',
+            'access_control',
+            'file_name_format',
+            'include_meta_data',
+            'format',
+            'preview_example',
+            'page_size',
+            'page_offset'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method export_tag_to_label_box_data_rows" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['tag_id']:
+            _path_params['tagId'] = _params['tag_id']
+
+
+        # process the query parameters
+        _query_params = []
+        if _params.get('expires_in') is not None:  # noqa: E501
+            _query_params.append((
+                'expiresIn',
+                _params['expires_in'].value if hasattr(_params['expires_in'], 'value') else _params['expires_in']
+            ))
+
+        if _params.get('access_control') is not None:  # noqa: E501
+            _query_params.append((
+                'accessControl',
+                _params['access_control'].value if hasattr(_params['access_control'], 'value') else _params['access_control']
+            ))
+
+        if _params.get('file_name_format') is not None:  # noqa: E501
+            _query_params.append((
+                'fileNameFormat',
+                _params['file_name_format'].value if hasattr(_params['file_name_format'], 'value') else _params['file_name_format']
+            ))
+
+        if _params.get('include_meta_data') is not None:  # noqa: E501
+            _query_params.append((
+                'includeMetaData',
+                _params['include_meta_data'].value if hasattr(_params['include_meta_data'], 'value') else _params['include_meta_data']
+            ))
+
+        if _params.get('format') is not None:  # noqa: E501
+            _query_params.append((
+                'format',
+                _params['format'].value if hasattr(_params['format'], 'value') else _params['format']
+            ))
+
+        if _params.get('preview_example') is not None:  # noqa: E501
+            _query_params.append((
+                'previewExample',
+                _params['preview_example'].value if hasattr(_params['preview_example'], 'value') else _params['preview_example']
+            ))
+
+        if _params.get('page_size') is not None:  # noqa: E501
+            _query_params.append((
+                'pageSize',
+                _params['page_size'].value if hasattr(_params['page_size'], 'value') else _params['page_size']
+            ))
+
+        if _params.get('page_offset') is not None:  # noqa: E501
+            _query_params.append((
+                'pageOffset',
+                _params['page_offset'].value if hasattr(_params['page_offset'], 'value') else _params['page_offset']
+            ))
+
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {
+            '200': "List[LabelBoxDataRow]",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/{tagId}/export/LabelBox/datarows', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def export_tag_to_label_box_v4_data_rows(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], expires_in : Annotated[Optional[StrictInt], Field(description="If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. ")] = None, access_control : Annotated[Optional[StrictStr], Field(description="which access control name to be used")] = None, file_name_format : Optional[FileNameFormat] = None, include_meta_data : Annotated[Optional[StrictBool], Field(description="if true, will also include metadata")] = None, format : Optional[FileOutputFormat] = None, preview_example : Annotated[Optional[StrictBool], Field(description="if true, will generate a preview example of how the structure will look")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> List[LabelBoxV4DataRow]:  # noqa: E501
+        """export_tag_to_label_box_v4_data_rows  # noqa: E501
+
+        Export samples of a tag as a json for importing into LabelBox as outlined here; https://docs.labelbox.com/v4/reference/image ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.export_tag_to_label_box_v4_data_rows(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
+        :type expires_in: int
+        :param access_control: which access control name to be used
+        :type access_control: str
+        :param file_name_format:
+        :type file_name_format: FileNameFormat
+        :param include_meta_data: if true, will also include metadata
+        :type include_meta_data: bool
+        :param format:
+        :type format: FileOutputFormat
+        :param preview_example: if true, will generate a preview example of how the structure will look
+        :type preview_example: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: List[LabelBoxV4DataRow]
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the export_tag_to_label_box_v4_data_rows_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.export_tag_to_label_box_v4_data_rows_with_http_info(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def export_tag_to_label_box_v4_data_rows_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], expires_in : Annotated[Optional[StrictInt], Field(description="If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. ")] = None, access_control : Annotated[Optional[StrictStr], Field(description="which access control name to be used")] = None, file_name_format : Optional[FileNameFormat] = None, include_meta_data : Annotated[Optional[StrictBool], Field(description="if true, will also include metadata")] = None, format : Optional[FileOutputFormat] = None, preview_example : Annotated[Optional[StrictBool], Field(description="if true, will generate a preview example of how the structure will look")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> ApiResponse:  # noqa: E501
+        """export_tag_to_label_box_v4_data_rows  # noqa: E501
+
+        Export samples of a tag as a json for importing into LabelBox as outlined here; https://docs.labelbox.com/v4/reference/image ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.export_tag_to_label_box_v4_data_rows_with_http_info(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
+        :type expires_in: int
+        :param access_control: which access control name to be used
+        :type access_control: str
+        :param file_name_format:
+        :type file_name_format: FileNameFormat
+        :param include_meta_data: if true, will also include metadata
+        :type include_meta_data: bool
+        :param format:
+        :type format: FileOutputFormat
+        :param preview_example: if true, will generate a preview example of how the structure will look
+        :type preview_example: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(List[LabelBoxV4DataRow], status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_id',
+            'expires_in',
+            'access_control',
+            'file_name_format',
+            'include_meta_data',
+            'format',
+            'preview_example',
+            'page_size',
+            'page_offset'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method export_tag_to_label_box_v4_data_rows" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['tag_id']:
+            _path_params['tagId'] = _params['tag_id']
+
+
+        # process the query parameters
+        _query_params = []
+        if _params.get('expires_in') is not None:  # noqa: E501
+            _query_params.append((
+                'expiresIn',
+                _params['expires_in'].value if hasattr(_params['expires_in'], 'value') else _params['expires_in']
+            ))
+
+        if _params.get('access_control') is not None:  # noqa: E501
+            _query_params.append((
+                'accessControl',
+                _params['access_control'].value if hasattr(_params['access_control'], 'value') else _params['access_control']
+            ))
+
+        if _params.get('file_name_format') is not None:  # noqa: E501
+            _query_params.append((
+                'fileNameFormat',
+                _params['file_name_format'].value if hasattr(_params['file_name_format'], 'value') else _params['file_name_format']
+            ))
+
+        if _params.get('include_meta_data') is not None:  # noqa: E501
+            _query_params.append((
+                'includeMetaData',
+                _params['include_meta_data'].value if hasattr(_params['include_meta_data'], 'value') else _params['include_meta_data']
+            ))
+
+        if _params.get('format') is not None:  # noqa: E501
+            _query_params.append((
+                'format',
+                _params['format'].value if hasattr(_params['format'], 'value') else _params['format']
+            ))
+
+        if _params.get('preview_example') is not None:  # noqa: E501
+            _query_params.append((
+                'previewExample',
+                _params['preview_example'].value if hasattr(_params['preview_example'], 'value') else _params['preview_example']
+            ))
+
+        if _params.get('page_size') is not None:  # noqa: E501
+            _query_params.append((
+                'pageSize',
+                _params['page_size'].value if hasattr(_params['page_size'], 'value') else _params['page_size']
+            ))
+
+        if _params.get('page_offset') is not None:  # noqa: E501
+            _query_params.append((
+                'pageOffset',
+                _params['page_offset'].value if hasattr(_params['page_offset'], 'value') else _params['page_offset']
+            ))
+
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {
+            '200': "List[LabelBoxV4DataRow]",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/{tagId}/export/LabelBoxV4/datarows', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def export_tag_to_label_studio_tasks(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], expires_in : Annotated[Optional[StrictInt], Field(description="If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. ")] = None, access_control : Annotated[Optional[StrictStr], Field(description="which access control name to be used")] = None, file_name_format : Optional[FileNameFormat] = None, include_meta_data : Annotated[Optional[StrictBool], Field(description="if true, will also include metadata")] = None, format : Optional[FileOutputFormat] = None, preview_example : Annotated[Optional[StrictBool], Field(description="if true, will generate a preview example of how the structure will look")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> List[LabelStudioTask]:  # noqa: E501
+        """export_tag_to_label_studio_tasks  # noqa: E501
+
+        Export samples of a tag as a json for importing into LabelStudio as outlined here; https://labelstud.io/guide/tasks.html#Basic-Label-Studio-JSON-format ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.export_tag_to_label_studio_tasks(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
+        :type expires_in: int
+        :param access_control: which access control name to be used
+        :type access_control: str
+        :param file_name_format:
+        :type file_name_format: FileNameFormat
+        :param include_meta_data: if true, will also include metadata
+        :type include_meta_data: bool
+        :param format:
+        :type format: FileOutputFormat
+        :param preview_example: if true, will generate a preview example of how the structure will look
+        :type preview_example: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: List[LabelStudioTask]
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the export_tag_to_label_studio_tasks_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.export_tag_to_label_studio_tasks_with_http_info(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def export_tag_to_label_studio_tasks_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], expires_in : Annotated[Optional[StrictInt], Field(description="If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. ")] = None, access_control : Annotated[Optional[StrictStr], Field(description="which access control name to be used")] = None, file_name_format : Optional[FileNameFormat] = None, include_meta_data : Annotated[Optional[StrictBool], Field(description="if true, will also include metadata")] = None, format : Optional[FileOutputFormat] = None, preview_example : Annotated[Optional[StrictBool], Field(description="if true, will generate a preview example of how the structure will look")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> ApiResponse:  # noqa: E501
+        """export_tag_to_label_studio_tasks  # noqa: E501
+
+        Export samples of a tag as a json for importing into LabelStudio as outlined here; https://labelstud.io/guide/tasks.html#Basic-Label-Studio-JSON-format ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.export_tag_to_label_studio_tasks_with_http_info(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
+        :type expires_in: int
+        :param access_control: which access control name to be used
+        :type access_control: str
+        :param file_name_format:
+        :type file_name_format: FileNameFormat
+        :param include_meta_data: if true, will also include metadata
+        :type include_meta_data: bool
+        :param format:
+        :type format: FileOutputFormat
+        :param preview_example: if true, will generate a preview example of how the structure will look
+        :type preview_example: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(List[LabelStudioTask], status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_id',
+            'expires_in',
+            'access_control',
+            'file_name_format',
+            'include_meta_data',
+            'format',
+            'preview_example',
+            'page_size',
+            'page_offset'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method export_tag_to_label_studio_tasks" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['tag_id']:
+            _path_params['tagId'] = _params['tag_id']
+
+
+        # process the query parameters
+        _query_params = []
+        if _params.get('expires_in') is not None:  # noqa: E501
+            _query_params.append((
+                'expiresIn',
+                _params['expires_in'].value if hasattr(_params['expires_in'], 'value') else _params['expires_in']
+            ))
+
+        if _params.get('access_control') is not None:  # noqa: E501
+            _query_params.append((
+                'accessControl',
+                _params['access_control'].value if hasattr(_params['access_control'], 'value') else _params['access_control']
+            ))
+
+        if _params.get('file_name_format') is not None:  # noqa: E501
+            _query_params.append((
+                'fileNameFormat',
+                _params['file_name_format'].value if hasattr(_params['file_name_format'], 'value') else _params['file_name_format']
+            ))
+
+        if _params.get('include_meta_data') is not None:  # noqa: E501
+            _query_params.append((
+                'includeMetaData',
+                _params['include_meta_data'].value if hasattr(_params['include_meta_data'], 'value') else _params['include_meta_data']
+            ))
+
+        if _params.get('format') is not None:  # noqa: E501
+            _query_params.append((
+                'format',
+                _params['format'].value if hasattr(_params['format'], 'value') else _params['format']
+            ))
+
+        if _params.get('preview_example') is not None:  # noqa: E501
+            _query_params.append((
+                'previewExample',
+                _params['preview_example'].value if hasattr(_params['preview_example'], 'value') else _params['preview_example']
+            ))
+
+        if _params.get('page_size') is not None:  # noqa: E501
+            _query_params.append((
+                'pageSize',
+                _params['page_size'].value if hasattr(_params['page_size'], 'value') else _params['page_size']
+            ))
+
+        if _params.get('page_offset') is not None:  # noqa: E501
+            _query_params.append((
+                'pageOffset',
+                _params['page_offset'].value if hasattr(_params['page_offset'], 'value') else _params['page_offset']
+            ))
+
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {
+            '200': "List[LabelStudioTask]",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/{tagId}/export/LabelStudio/tasks', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def export_tag_to_sama_tasks(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], expires_in : Annotated[Optional[StrictInt], Field(description="If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. ")] = None, access_control : Annotated[Optional[StrictStr], Field(description="which access control name to be used")] = None, file_name_format : Optional[FileNameFormat] = None, include_meta_data : Annotated[Optional[StrictBool], Field(description="if true, will also include metadata")] = None, format : Optional[FileOutputFormat] = None, preview_example : Annotated[Optional[StrictBool], Field(description="if true, will generate a preview example of how the structure will look")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> List[SamaTask]:  # noqa: E501
+        """export_tag_to_sama_tasks  # noqa: E501
+
+        Export samples of a tag as a json for importing into Sama as tasks with the upload form or via the API as outlined here; - https://docs.sama.com/reference/taskcreate - https://docs.sama.com/reference/createbatch  ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.export_tag_to_sama_tasks(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
+        :type expires_in: int
+        :param access_control: which access control name to be used
+        :type access_control: str
+        :param file_name_format:
+        :type file_name_format: FileNameFormat
+        :param include_meta_data: if true, will also include metadata
+        :type include_meta_data: bool
+        :param format:
+        :type format: FileOutputFormat
+        :param preview_example: if true, will generate a preview example of how the structure will look
+        :type preview_example: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: List[SamaTask]
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the export_tag_to_sama_tasks_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.export_tag_to_sama_tasks_with_http_info(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def export_tag_to_sama_tasks_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], expires_in : Annotated[Optional[StrictInt], Field(description="If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. ")] = None, access_control : Annotated[Optional[StrictStr], Field(description="which access control name to be used")] = None, file_name_format : Optional[FileNameFormat] = None, include_meta_data : Annotated[Optional[StrictBool], Field(description="if true, will also include metadata")] = None, format : Optional[FileOutputFormat] = None, preview_example : Annotated[Optional[StrictBool], Field(description="if true, will generate a preview example of how the structure will look")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> ApiResponse:  # noqa: E501
+        """export_tag_to_sama_tasks  # noqa: E501
+
+        Export samples of a tag as a json for importing into Sama as tasks with the upload form or via the API as outlined here; - https://docs.sama.com/reference/taskcreate - https://docs.sama.com/reference/createbatch  ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.export_tag_to_sama_tasks_with_http_info(dataset_id, tag_id, expires_in, access_control, file_name_format, include_meta_data, format, preview_example, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
+        :type expires_in: int
+        :param access_control: which access control name to be used
+        :type access_control: str
+        :param file_name_format:
+        :type file_name_format: FileNameFormat
+        :param include_meta_data: if true, will also include metadata
+        :type include_meta_data: bool
+        :param format:
+        :type format: FileOutputFormat
+        :param preview_example: if true, will generate a preview example of how the structure will look
+        :type preview_example: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(List[SamaTask], status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_id',
+            'expires_in',
+            'access_control',
+            'file_name_format',
+            'include_meta_data',
+            'format',
+            'preview_example',
+            'page_size',
+            'page_offset'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method export_tag_to_sama_tasks" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['tag_id']:
+            _path_params['tagId'] = _params['tag_id']
+
+
+        # process the query parameters
+        _query_params = []
+        if _params.get('expires_in') is not None:  # noqa: E501
+            _query_params.append((
+                'expiresIn',
+                _params['expires_in'].value if hasattr(_params['expires_in'], 'value') else _params['expires_in']
+            ))
+
+        if _params.get('access_control') is not None:  # noqa: E501
+            _query_params.append((
+                'accessControl',
+                _params['access_control'].value if hasattr(_params['access_control'], 'value') else _params['access_control']
+            ))
+
+        if _params.get('file_name_format') is not None:  # noqa: E501
+            _query_params.append((
+                'fileNameFormat',
+                _params['file_name_format'].value if hasattr(_params['file_name_format'], 'value') else _params['file_name_format']
+            ))
+
+        if _params.get('include_meta_data') is not None:  # noqa: E501
+            _query_params.append((
+                'includeMetaData',
+                _params['include_meta_data'].value if hasattr(_params['include_meta_data'], 'value') else _params['include_meta_data']
+            ))
+
+        if _params.get('format') is not None:  # noqa: E501
+            _query_params.append((
+                'format',
+                _params['format'].value if hasattr(_params['format'], 'value') else _params['format']
+            ))
+
+        if _params.get('preview_example') is not None:  # noqa: E501
+            _query_params.append((
+                'previewExample',
+                _params['preview_example'].value if hasattr(_params['preview_example'], 'value') else _params['preview_example']
+            ))
+
+        if _params.get('page_size') is not None:  # noqa: E501
+            _query_params.append((
+                'pageSize',
+                _params['page_size'].value if hasattr(_params['page_size'], 'value') else _params['page_size']
+            ))
+
+        if _params.get('page_offset') is not None:  # noqa: E501
+            _query_params.append((
+                'pageOffset',
+                _params['page_offset'].value if hasattr(_params['page_offset'], 'value') else _params['page_offset']
+            ))
+
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {
+            '200': "List[SamaTask]",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/{tagId}/export/Sama/tasks', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def get_filenames_by_tag_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], **kwargs) -> List[str]:  # noqa: E501
+        """(Deprecated) get_filenames_by_tag_id  # noqa: E501
+
+        Get list of filenames by tag. Deprecated, please use  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.get_filenames_by_tag_id(dataset_id, tag_id, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: List[str]
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the get_filenames_by_tag_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.get_filenames_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def get_filenames_by_tag_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], **kwargs) -> ApiResponse:  # noqa: E501
+        """(Deprecated) get_filenames_by_tag_id  # noqa: E501
+
+        Get list of filenames by tag. Deprecated, please use  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.get_filenames_by_tag_id_with_http_info(dataset_id, tag_id, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(List[str], status_code(int), headers(HTTPHeaderDict))
+        """
+
+        warnings.warn("GET /v1/datasets/{datasetId}/tags/{tagId}/filenames is deprecated.", DeprecationWarning)
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_id'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method get_filenames_by_tag_id" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['tag_id']:
+            _path_params['tagId'] = _params['tag_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['text/plain', 'application/json'])  # noqa: E501
 
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        return self.api_client.call_api(
-            '/v1/docker/workerAuthorization', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='str',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def register_docker_worker(self, body, **kwargs):  # noqa: E501
-        """register_docker_worker  # noqa: E501
-
-        Registers a worker for a user. If a worker with the same name is passed that already exists, the same workerId will be returned  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.register_docker_worker(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param CreateDockerWorkerRegistryEntryRequest body: (required)
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.register_docker_worker_with_http_info(body, **kwargs)  # noqa: E501
-        else:
-            (data) = self.register_docker_worker_with_http_info(body, **kwargs)  # noqa: E501
-            return data
-
-    def register_docker_worker_with_http_info(self, body, **kwargs):  # noqa: E501
-        """register_docker_worker  # noqa: E501
-
-        Registers a worker for a user. If a worker with the same name is passed that already exists, the same workerId will be returned  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.register_docker_worker_with_http_info(body, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param CreateDockerWorkerRegistryEntryRequest body: (required)
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method register_docker_worker" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `register_docker_worker`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='CreateEntityResponse',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def update_docker_run_by_id(self, body, run_id, **kwargs):  # noqa: E501
-        """update_docker_run_by_id  # noqa: E501
-
-        Updates a docker run database entry.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.update_docker_run_by_id(body, run_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerRunUpdateRequest body: (required)
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.update_docker_run_by_id_with_http_info(body, run_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.update_docker_run_by_id_with_http_info(body, run_id, **kwargs)  # noqa: E501
-            return data
-
-    def update_docker_run_by_id_with_http_info(self, body, run_id, **kwargs):  # noqa: E501
-        """update_docker_run_by_id  # noqa: E501
-
-        Updates a docker run database entry.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.update_docker_run_by_id_with_http_info(body, run_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerRunUpdateRequest body: (required)
-        :param MongoObjectID run_id: ObjectId of the docker run (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body', 'run_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method update_docker_run_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `update_docker_run_by_id`")  # noqa: E501
-        # verify the required parameter 'run_id' is set
-        if self.api_client.client_side_validation and ('run_id' not in params or
-                                                       params['run_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `run_id` when calling `update_docker_run_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'run_id' in params:
-            path_params['runId'] = params['run_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/runs/{runId}', 'PUT',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type=None,  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def update_docker_worker_config_by_id(self, body, config_id, **kwargs):  # noqa: E501
-        """update_docker_worker_config_by_id  # noqa: E501
-
-        DEPRECATED, DONT USE. Updates a docker worker configuration by id.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.update_docker_worker_config_by_id(body, config_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerWorkerConfigCreateRequest body: (required)
-        :param MongoObjectID config_id: ObjectId of the docker worker config (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.update_docker_worker_config_by_id_with_http_info(body, config_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.update_docker_worker_config_by_id_with_http_info(body, config_id, **kwargs)  # noqa: E501
-            return data
-
-    def update_docker_worker_config_by_id_with_http_info(self, body, config_id, **kwargs):  # noqa: E501
-        """update_docker_worker_config_by_id  # noqa: E501
-
-        DEPRECATED, DONT USE. Updates a docker worker configuration by id.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.update_docker_worker_config_by_id_with_http_info(body, config_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerWorkerConfigCreateRequest body: (required)
-        :param MongoObjectID config_id: ObjectId of the docker worker config (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body', 'config_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method update_docker_worker_config_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `update_docker_worker_config_by_id`")  # noqa: E501
-        # verify the required parameter 'config_id' is set
-        if self.api_client.client_side_validation and ('config_id' not in params or
-                                                       params['config_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `config_id` when calling `update_docker_worker_config_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'config_id' in params:
-            path_params['configId'] = params['config_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker/config/{configId}', 'PUT',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type=None,  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def update_docker_worker_registry_entry_by_id(self, body, worker_id, **kwargs):  # noqa: E501
-        """update_docker_worker_registry_entry_by_id  # noqa: E501
-
-        Updates the worker status by id.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.update_docker_worker_registry_entry_by_id(body, worker_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param UpdateDockerWorkerRegistryEntryRequest body: (required)
-        :param MongoObjectID worker_id: ObjectId of the docker worker (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.update_docker_worker_registry_entry_by_id_with_http_info(body, worker_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.update_docker_worker_registry_entry_by_id_with_http_info(body, worker_id, **kwargs)  # noqa: E501
-            return data
-
-    def update_docker_worker_registry_entry_by_id_with_http_info(self, body, worker_id, **kwargs):  # noqa: E501
-        """update_docker_worker_registry_entry_by_id  # noqa: E501
-
-        Updates the worker status by id.  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.update_docker_worker_registry_entry_by_id_with_http_info(body, worker_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param UpdateDockerWorkerRegistryEntryRequest body: (required)
-        :param MongoObjectID worker_id: ObjectId of the docker worker (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body', 'worker_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method update_docker_worker_registry_entry_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `update_docker_worker_registry_entry_by_id`")  # noqa: E501
-        # verify the required parameter 'worker_id' is set
-        if self.api_client.client_side_validation and ('worker_id' not in params or
-                                                       params['worker_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `worker_id` when calling `update_docker_worker_registry_entry_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'worker_id' in params:
-            path_params['workerId'] = params['worker_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/docker/worker/{workerId}', 'PUT',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type=None,  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def update_scheduled_docker_run_state_by_id(self, body, dataset_id, worker_id, scheduled_id, **kwargs):  # noqa: E501
-        """update_scheduled_docker_run_state_by_id  # noqa: E501
-
-        Update the state of a scheduled run. This will fail if the state of the scheduled run is LOCKED.   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.update_scheduled_docker_run_state_by_id(body, dataset_id, worker_id, scheduled_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerRunScheduledUpdateRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID worker_id: ObjectId of the docker worker (required)
-        :param MongoObjectID scheduled_id: ObjectId of the docker worker run (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.update_scheduled_docker_run_state_by_id_with_http_info(body, dataset_id, worker_id, scheduled_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.update_scheduled_docker_run_state_by_id_with_http_info(body, dataset_id, worker_id, scheduled_id, **kwargs)  # noqa: E501
-            return data
-
-    def update_scheduled_docker_run_state_by_id_with_http_info(self, body, dataset_id, worker_id, scheduled_id, **kwargs):  # noqa: E501
-        """update_scheduled_docker_run_state_by_id  # noqa: E501
-
-        Update the state of a scheduled run. This will fail if the state of the scheduled run is LOCKED.   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.update_scheduled_docker_run_state_by_id_with_http_info(body, dataset_id, worker_id, scheduled_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param DockerRunScheduledUpdateRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID worker_id: ObjectId of the docker worker (required)
-        :param MongoObjectID scheduled_id: ObjectId of the docker worker run (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body', 'dataset_id', 'worker_id', 'scheduled_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method update_scheduled_docker_run_state_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `update_scheduled_docker_run_state_by_id`")  # noqa: E501
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `update_scheduled_docker_run_state_by_id`")  # noqa: E501
-        # verify the required parameter 'worker_id' is set
-        if self.api_client.client_side_validation and ('worker_id' not in params or
-                                                       params['worker_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `worker_id` when calling `update_scheduled_docker_run_state_by_id`")  # noqa: E501
-        # verify the required parameter 'scheduled_id' is set
-        if self.api_client.client_side_validation and ('scheduled_id' not in params or
-                                                       params['scheduled_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `scheduled_id` when calling `update_scheduled_docker_run_state_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'worker_id' in params:
-            path_params['workerId'] = params['worker_id']  # noqa: E501
-        if 'scheduled_id' in params:
-            path_params['scheduledId'] = params['scheduled_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/docker/worker/{workerId}/schedule/{scheduledId}', 'PUT',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type=None,  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+        _response_types_map = {
+            '200': "List[str]",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/{tagId}/filenames', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def get_tag_by_tag_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], **kwargs) -> TagData:  # noqa: E501
+        """get_tag_by_tag_id  # noqa: E501
+
+        Get information about a specific tag  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.get_tag_by_tag_id(dataset_id, tag_id, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: TagData
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the get_tag_by_tag_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.get_tag_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def get_tag_by_tag_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], **kwargs) -> ApiResponse:  # noqa: E501
+        """get_tag_by_tag_id  # noqa: E501
+
+        Get information about a specific tag  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.get_tag_by_tag_id_with_http_info(dataset_id, tag_id, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(TagData, status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_id'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method get_tag_by_tag_id" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['tag_id']:
+            _path_params['tagId'] = _params['tag_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {
+            '200': "TagData",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/{tagId}', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def get_tags_by_dataset_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], **kwargs) -> List[TagData]:  # noqa: E501
+        """get_tags_by_dataset_id  # noqa: E501
+
+        Get all tags of a dataset  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.get_tags_by_dataset_id(dataset_id, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: List[TagData]
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the get_tags_by_dataset_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.get_tags_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def get_tags_by_dataset_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], **kwargs) -> ApiResponse:  # noqa: E501
+        """get_tags_by_dataset_id  # noqa: E501
+
+        Get all tags of a dataset  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.get_tags_by_dataset_id_with_http_info(dataset_id, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(List[TagData], status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method get_tags_by_dataset_id" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {
+            '200': "List[TagData]",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def perform_tag_arithmetics(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_arithmetics_request : TagArithmeticsRequest, **kwargs) -> TagArithmeticsResponse:  # noqa: E501
+        """perform_tag_arithmetics  # noqa: E501
+
+        performs tag arithmetics to compute a new bitmask out of two existing tags and optionally create a tag for it  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.perform_tag_arithmetics(dataset_id, tag_arithmetics_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_arithmetics_request: (required)
+        :type tag_arithmetics_request: TagArithmeticsRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: TagArithmeticsResponse
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the perform_tag_arithmetics_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.perform_tag_arithmetics_with_http_info(dataset_id, tag_arithmetics_request, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def perform_tag_arithmetics_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_arithmetics_request : TagArithmeticsRequest, **kwargs) -> ApiResponse:  # noqa: E501
+        """perform_tag_arithmetics  # noqa: E501
+
+        performs tag arithmetics to compute a new bitmask out of two existing tags and optionally create a tag for it  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.perform_tag_arithmetics_with_http_info(dataset_id, tag_arithmetics_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_arithmetics_request: (required)
+        :type tag_arithmetics_request: TagArithmeticsRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(TagArithmeticsResponse, status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_arithmetics_request'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method perform_tag_arithmetics" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        if _params['tag_arithmetics_request'] is not None:
+            _body_params = _params['tag_arithmetics_request']
+
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
+
+        # set the HTTP header `Content-Type`
+        _content_types_list = _params.get('_content_type',
+            self.api_client.select_header_content_type(
+                ['application/json']))
+        if _content_types_list:
+                _header_params['Content-Type'] = _content_types_list
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {
+            '200': "TagArithmeticsResponse",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/arithmetics', 'POST',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def perform_tag_arithmetics_bitmask(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_arithmetics_request : TagArithmeticsRequest, **kwargs) -> TagBitMaskResponse:  # noqa: E501
+        """(Deprecated) perform_tag_arithmetics_bitmask  # noqa: E501
+
+        Performs tag arithmetics to compute a new bitmask out of two existing tags. Does not create a new tag regardless if newTagName is provided  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.perform_tag_arithmetics_bitmask(dataset_id, tag_arithmetics_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_arithmetics_request: (required)
+        :type tag_arithmetics_request: TagArithmeticsRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: TagBitMaskResponse
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the perform_tag_arithmetics_bitmask_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.perform_tag_arithmetics_bitmask_with_http_info(dataset_id, tag_arithmetics_request, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def perform_tag_arithmetics_bitmask_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_arithmetics_request : TagArithmeticsRequest, **kwargs) -> ApiResponse:  # noqa: E501
+        """(Deprecated) perform_tag_arithmetics_bitmask  # noqa: E501
+
+        Performs tag arithmetics to compute a new bitmask out of two existing tags. Does not create a new tag regardless if newTagName is provided  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.perform_tag_arithmetics_bitmask_with_http_info(dataset_id, tag_arithmetics_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_arithmetics_request: (required)
+        :type tag_arithmetics_request: TagArithmeticsRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(TagBitMaskResponse, status_code(int), headers(HTTPHeaderDict))
+        """
+
+        warnings.warn("POST /v1/datasets/{datasetId}/tags/arithmetics/bitmask is deprecated.", DeprecationWarning)
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_arithmetics_request'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method perform_tag_arithmetics_bitmask" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        if _params['tag_arithmetics_request'] is not None:
+            _body_params = _params['tag_arithmetics_request']
+
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
+
+        # set the HTTP header `Content-Type`
+        _content_types_list = _params.get('_content_type',
+            self.api_client.select_header_content_type(
+                ['application/json']))
+        if _content_types_list:
+                _header_params['Content-Type'] = _content_types_list
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {
+            '200': "TagBitMaskResponse",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/arithmetics/bitmask', 'POST',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def update_tag_by_tag_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], tag_update_request : Annotated[TagUpdateRequest, Field(..., description="updated data for tag")], **kwargs) -> None:  # noqa: E501
+        """update_tag_by_tag_id  # noqa: E501
+
+        update information about a specific tag  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.update_tag_by_tag_id(dataset_id, tag_id, tag_update_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param tag_update_request: updated data for tag (required)
+        :type tag_update_request: TagUpdateRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: None
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the update_tag_by_tag_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.update_tag_by_tag_id_with_http_info(dataset_id, tag_id, tag_update_request, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def update_tag_by_tag_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the tag")], tag_update_request : Annotated[TagUpdateRequest, Field(..., description="updated data for tag")], **kwargs) -> ApiResponse:  # noqa: E501
+        """update_tag_by_tag_id  # noqa: E501
+
+        update information about a specific tag  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.update_tag_by_tag_id_with_http_info(dataset_id, tag_id, tag_update_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_id: ObjectId of the tag (required)
+        :type tag_id: str
+        :param tag_update_request: updated data for tag (required)
+        :type tag_update_request: TagUpdateRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: None
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_id',
+            'tag_update_request'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method update_tag_by_tag_id" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['tag_id']:
+            _path_params['tagId'] = _params['tag_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        if _params['tag_update_request'] is not None:
+            _body_params = _params['tag_update_request']
+
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
+
+        # set the HTTP header `Content-Type`
+        _content_types_list = _params.get('_content_type',
+            self.api_client.select_header_content_type(
+                ['application/json']))
+        if _content_types_list:
+                _header_params['Content-Type'] = _content_types_list
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {}
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/{tagId}', 'PUT',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def upsize_tags_by_dataset_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_upsize_request : TagUpsizeRequest, **kwargs) -> CreateEntityResponse:  # noqa: E501
+        """upsize_tags_by_dataset_id  # noqa: E501
+
+        Upsize all tags for the dataset to the current size of the dataset. Use this after adding more samples to a dataset with an initial-tag. | Creates a new tag holding all samples which are not yet in the initial-tag.   # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.upsize_tags_by_dataset_id(dataset_id, tag_upsize_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_upsize_request: (required)
+        :type tag_upsize_request: TagUpsizeRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: CreateEntityResponse
+        """
+        kwargs['_return_http_data_only'] = True
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the upsize_tags_by_dataset_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.upsize_tags_by_dataset_id_with_http_info(dataset_id, tag_upsize_request, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def upsize_tags_by_dataset_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], tag_upsize_request : TagUpsizeRequest, **kwargs) -> ApiResponse:  # noqa: E501
+        """upsize_tags_by_dataset_id  # noqa: E501
+
+        Upsize all tags for the dataset to the current size of the dataset. Use this after adding more samples to a dataset with an initial-tag. | Creates a new tag holding all samples which are not yet in the initial-tag.   # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.upsize_tags_by_dataset_id_with_http_info(dataset_id, tag_upsize_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param tag_upsize_request: (required)
+        :type tag_upsize_request: TagUpsizeRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(CreateEntityResponse, status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'tag_upsize_request'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method upsize_tags_by_dataset_id" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        if _params['tag_upsize_request'] is not None:
+            _body_params = _params['tag_upsize_request']
+
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
+
+        # set the HTTP header `Content-Type`
+        _content_types_list = _params.get('_content_type',
+            self.api_client.select_header_content_type(
+                ['application/json']))
+        if _content_types_list:
+                _header_params['Content-Type'] = _content_types_list
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {
+            '201': "CreateEntityResponse",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
+
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/tags/upsize', 'POST',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/api/samples_api.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/api/embeddings_api.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,1124 +1,1126 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
-"""
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
 
+    Do not edit the class manually.
+"""
 
-from __future__ import absolute_import
 
 import re  # noqa: F401
+import io
+import warnings
+
+from pydantic import validate_arguments, ValidationError
+from typing_extensions import Annotated
 
-# python 2 and python 3 compatibility library
-import six
+from pydantic import Field, StrictStr, constr, validator
+
+from typing import List, Optional
+
+from lightly.openapi_generated.swagger_client.models.dataset_embedding_data import DatasetEmbeddingData
+from lightly.openapi_generated.swagger_client.models.embedding_data import EmbeddingData
+from lightly.openapi_generated.swagger_client.models.set_embeddings_is_processed_flag_by_id_body_request import SetEmbeddingsIsProcessedFlagByIdBodyRequest
+from lightly.openapi_generated.swagger_client.models.trigger2d_embedding_job_request import Trigger2dEmbeddingJobRequest
+from lightly.openapi_generated.swagger_client.models.write_csv_url_data import WriteCSVUrlData
 
 from lightly.openapi_generated.swagger_client.api_client import ApiClient
+from lightly.openapi_generated.swagger_client.api_response import ApiResponse
+from lightly.openapi_generated.swagger_client.exceptions import (  # noqa: F401
+    ApiTypeError,
+    ApiValueError
+)
 
 
-class SamplesApi(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
+class EmbeddingsApi(object):
+    """NOTE: This class is auto generated by OpenAPI Generator
+    Ref: https://openapi-generator.tech
 
     Do not edit the class manually.
-    Ref: https://github.com/swagger-api/swagger-codegen
     """
 
     def __init__(self, api_client=None):
         if api_client is None:
-            api_client = ApiClient()
+            api_client = ApiClient.get_default()
         self.api_client = api_client
 
-    def create_sample_by_dataset_id(self, body, dataset_id, **kwargs):  # noqa: E501
-        """create_sample_by_dataset_id  # noqa: E501
+    @validate_arguments
+    def delete_embedding_by_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], embedding_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the embedding")], **kwargs) -> None:  # noqa: E501
+        """delete_embedding_by_id  # noqa: E501
 
-        Create a new sample in a dataset  # noqa: E501
+        Deletes a embedding entry by id.  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_sample_by_dataset_id(body, dataset_id, async_req=True)
+
+        >>> thread = api.delete_embedding_by_id(dataset_id, embedding_id, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param SampleCreateRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: CreateEntityResponse
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param embedding_id: ObjectId of the embedding (required)
+        :type embedding_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: None
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.create_sample_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.create_sample_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-            return data
-
-    def create_sample_by_dataset_id_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501
-        """create_sample_by_dataset_id  # noqa: E501
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the delete_embedding_by_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.delete_embedding_by_id_with_http_info(dataset_id, embedding_id, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def delete_embedding_by_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], embedding_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the embedding")], **kwargs) -> ApiResponse:  # noqa: E501
+        """delete_embedding_by_id  # noqa: E501
 
-        Create a new sample in a dataset  # noqa: E501
+        Deletes a embedding entry by id.  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_sample_by_dataset_id_with_http_info(body, dataset_id, async_req=True)
+
+        >>> thread = api.delete_embedding_by_id_with_http_info(dataset_id, embedding_id, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param SampleCreateRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: CreateEntityResponse
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param embedding_id: ObjectId of the embedding (required)
+        :type embedding_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: None
         """
 
-        all_params = ['body', 'dataset_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'embedding_id'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method create_sample_by_dataset_id" % key
+                    " to method delete_embedding_by_id" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `create_sample_by_dataset_id`")  # noqa: E501
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `create_sample_by_dataset_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
 
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['embedding_id']:
+            _path_params['embeddingId'] = _params['embedding_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/samples', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='CreateEntityResponse',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+        _response_types_map = {}
 
-    def create_sample_with_write_urls_by_dataset_id(self, body, dataset_id, **kwargs):  # noqa: E501
-        """create_sample_with_write_urls_by_dataset_id  # noqa: E501
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/embeddings/{embeddingId}', 'DELETE',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def get_embeddings_by_dataset_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], **kwargs) -> List[DatasetEmbeddingData]:  # noqa: E501
+        """get_embeddings_by_dataset_id  # noqa: E501
 
-        Create a sample and immediately receive write URLs (full image and thumbnail) to upload images  # noqa: E501
+        Get all annotations of a dataset  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_sample_with_write_urls_by_dataset_id(body, dataset_id, async_req=True)
+
+        >>> thread = api.get_embeddings_by_dataset_id(dataset_id, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param SampleCreateRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: CreateSampleWithWriteUrlsResponse
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: List[DatasetEmbeddingData]
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.create_sample_with_write_urls_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.create_sample_with_write_urls_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-            return data
-
-    def create_sample_with_write_urls_by_dataset_id_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501
-        """create_sample_with_write_urls_by_dataset_id  # noqa: E501
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the get_embeddings_by_dataset_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.get_embeddings_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def get_embeddings_by_dataset_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], **kwargs) -> ApiResponse:  # noqa: E501
+        """get_embeddings_by_dataset_id  # noqa: E501
 
-        Create a sample and immediately receive write URLs (full image and thumbnail) to upload images  # noqa: E501
+        Get all annotations of a dataset  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_sample_with_write_urls_by_dataset_id_with_http_info(body, dataset_id, async_req=True)
+
+        >>> thread = api.get_embeddings_by_dataset_id_with_http_info(dataset_id, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param SampleCreateRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: CreateSampleWithWriteUrlsResponse
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: tuple(List[DatasetEmbeddingData], status_code(int), headers(HTTPHeaderDict))
         """
 
-        all_params = ['body', 'dataset_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        _params = locals()
+
+        _all_params = [
+            'dataset_id'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method create_sample_with_write_urls_by_dataset_id" % key
+                    " to method get_embeddings_by_dataset_id" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `create_sample_with_write_urls_by_dataset_id`")  # noqa: E501
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `create_sample_with_write_urls_by_dataset_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
 
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/samples/withWriteUrls', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='CreateSampleWithWriteUrlsResponse',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+        _response_types_map = {
+            '200': "List[DatasetEmbeddingData]",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
 
-    def get_sample_by_id(self, dataset_id, sample_id, **kwargs):  # noqa: E501
-        """get_sample_by_id  # noqa: E501
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/embeddings', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def get_embeddings_by_sample_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], sample_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the sample")], mode : Annotated[Optional[StrictStr], Field(description="if we want everything (full) or just the summaries")] = None, **kwargs) -> List[EmbeddingData]:  # noqa: E501
+        """get_embeddings_by_sample_id  # noqa: E501
 
-        Get a specific sample of a dataset  # noqa: E501
+        Get all embeddings of a datasets sample  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_sample_by_id(dataset_id, sample_id, async_req=True)
+
+        >>> thread = api.get_embeddings_by_sample_id(dataset_id, sample_id, mode, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID sample_id: ObjectId of the sample (required)
-        :return: SampleData
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param sample_id: ObjectId of the sample (required)
+        :type sample_id: str
+        :param mode: if we want everything (full) or just the summaries
+        :type mode: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: List[EmbeddingData]
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_sample_by_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_sample_by_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_sample_by_id_with_http_info(self, dataset_id, sample_id, **kwargs):  # noqa: E501
-        """get_sample_by_id  # noqa: E501
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the get_embeddings_by_sample_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.get_embeddings_by_sample_id_with_http_info(dataset_id, sample_id, mode, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def get_embeddings_by_sample_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], sample_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the sample")], mode : Annotated[Optional[StrictStr], Field(description="if we want everything (full) or just the summaries")] = None, **kwargs) -> ApiResponse:  # noqa: E501
+        """get_embeddings_by_sample_id  # noqa: E501
 
-        Get a specific sample of a dataset  # noqa: E501
+        Get all embeddings of a datasets sample  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_sample_by_id_with_http_info(dataset_id, sample_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID sample_id: ObjectId of the sample (required)
-        :return: SampleData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
 
-        all_params = ['dataset_id', 'sample_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_sample_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `get_sample_by_id`")  # noqa: E501
-        # verify the required parameter 'sample_id' is set
-        if self.api_client.client_side_validation and ('sample_id' not in params or
-                                                       params['sample_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `sample_id` when calling `get_sample_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'sample_id' in params:
-            path_params['sampleId'] = params['sample_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/samples/{sampleId}', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='SampleData',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_sample_image_read_url_by_id(self, dataset_id, sample_id, **kwargs):  # noqa: E501
-        """get_sample_image_read_url_by_id  # noqa: E501
-
-        Get the image path of a specific sample of a dataset  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_sample_image_read_url_by_id(dataset_id, sample_id, async_req=True)
+        >>> thread = api.get_embeddings_by_sample_id_with_http_info(dataset_id, sample_id, mode, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID sample_id: ObjectId of the sample (required)
-        :param str type: if we want to get the full image or just the thumbnail
-        :return: str
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param sample_id: ObjectId of the sample (required)
+        :type sample_id: str
+        :param mode: if we want everything (full) or just the summaries
+        :type mode: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: tuple(List[EmbeddingData], status_code(int), headers(HTTPHeaderDict))
         """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_sample_image_read_url_by_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_sample_image_read_url_by_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501
-            return data
 
-    def get_sample_image_read_url_by_id_with_http_info(self, dataset_id, sample_id, **kwargs):  # noqa: E501
-        """get_sample_image_read_url_by_id  # noqa: E501
-
-        Get the image path of a specific sample of a dataset  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_sample_image_read_url_by_id_with_http_info(dataset_id, sample_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID sample_id: ObjectId of the sample (required)
-        :param str type: if we want to get the full image or just the thumbnail
-        :return: str
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
+        _params = locals()
 
-        all_params = ['dataset_id', 'sample_id', 'type']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        _all_params = [
+            'dataset_id',
+            'sample_id',
+            'mode'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method get_sample_image_read_url_by_id" % key
+                    " to method get_embeddings_by_sample_id" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `get_sample_image_read_url_by_id`")  # noqa: E501
-        # verify the required parameter 'sample_id' is set
-        if self.api_client.client_side_validation and ('sample_id' not in params or
-                                                       params['sample_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `sample_id` when calling `get_sample_image_read_url_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'sample_id' in params:
-            path_params['sampleId'] = params['sample_id']  # noqa: E501
-
-        query_params = []
-        if 'type' in params:
-            query_params.append(('type', params['type']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['sample_id']:
+            _path_params['sampleId'] = _params['sample_id']
+
+
+        # process the query parameters
+        _query_params = []
+        if _params.get('mode') is not None:  # noqa: E501
+            _query_params.append((
+                'mode',
+                _params['mode'].value if hasattr(_params['mode'], 'value') else _params['mode']
+            ))
+
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/samples/{sampleId}/readurl', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='str',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+        _response_types_map = {
+            '200': "List[EmbeddingData]",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
 
-    def get_sample_image_resource_redirect_by_id(self, dataset_id, sample_id, type, **kwargs):  # noqa: E501
-        """get_sample_image_resource_redirect_by_id  # noqa: E501
+        return self.api_client.call_api(
+            '/users/datasets/{datasetId}/samples/{sampleId}/embeddings', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def get_embeddings_csv_read_url_by_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], embedding_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the embedding")], **kwargs) -> str:  # noqa: E501
+        """get_embeddings_csv_read_url_by_id  # noqa: E501
 
-        This endpoint enables anyone given the correct credentials to access the actual image directly. By creating a readURL for the resource and redirecting to that URL, the client can use this endpoint to always have a way to access the resource as there is no expiration   # noqa: E501
+        Get the url of a specific embeddings CSV  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_sample_image_resource_redirect_by_id(dataset_id, sample_id, type, async_req=True)
+
+        >>> thread = api.get_embeddings_csv_read_url_by_id(dataset_id, embedding_id, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID sample_id: ObjectId of the sample (required)
-        :param str type: if we want to get the full image or just the thumbnail (required)
-        :return: None
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param embedding_id: ObjectId of the embedding (required)
+        :type embedding_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: str
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_sample_image_resource_redirect_by_id_with_http_info(dataset_id, sample_id, type, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_sample_image_resource_redirect_by_id_with_http_info(dataset_id, sample_id, type, **kwargs)  # noqa: E501
-            return data
-
-    def get_sample_image_resource_redirect_by_id_with_http_info(self, dataset_id, sample_id, type, **kwargs):  # noqa: E501
-        """get_sample_image_resource_redirect_by_id  # noqa: E501
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the get_embeddings_csv_read_url_by_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.get_embeddings_csv_read_url_by_id_with_http_info(dataset_id, embedding_id, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def get_embeddings_csv_read_url_by_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], embedding_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the embedding")], **kwargs) -> ApiResponse:  # noqa: E501
+        """get_embeddings_csv_read_url_by_id  # noqa: E501
 
-        This endpoint enables anyone given the correct credentials to access the actual image directly. By creating a readURL for the resource and redirecting to that URL, the client can use this endpoint to always have a way to access the resource as there is no expiration   # noqa: E501
+        Get the url of a specific embeddings CSV  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_sample_image_resource_redirect_by_id_with_http_info(dataset_id, sample_id, type, async_req=True)
+
+        >>> thread = api.get_embeddings_csv_read_url_by_id_with_http_info(dataset_id, embedding_id, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID sample_id: ObjectId of the sample (required)
-        :param str type: if we want to get the full image or just the thumbnail (required)
-        :return: None
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param embedding_id: ObjectId of the embedding (required)
+        :type embedding_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: tuple(str, status_code(int), headers(HTTPHeaderDict))
         """
 
-        all_params = ['dataset_id', 'sample_id', 'type']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'embedding_id'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method get_sample_image_resource_redirect_by_id" % key
+                    " to method get_embeddings_csv_read_url_by_id" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `get_sample_image_resource_redirect_by_id`")  # noqa: E501
-        # verify the required parameter 'sample_id' is set
-        if self.api_client.client_side_validation and ('sample_id' not in params or
-                                                       params['sample_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `sample_id` when calling `get_sample_image_resource_redirect_by_id`")  # noqa: E501
-        # verify the required parameter 'type' is set
-        if self.api_client.client_side_validation and ('type' not in params or
-                                                       params['type'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `type` when calling `get_sample_image_resource_redirect_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'sample_id' in params:
-            path_params['sampleId'] = params['sample_id']  # noqa: E501
-
-        query_params = []
-        if 'type' in params:
-            query_params.append(('type', params['type']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['embedding_id']:
+            _path_params['embeddingId'] = _params['embedding_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiPublicJWTAuth']  # noqa: E501
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/samples/{sampleId}/readurlRedirect', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type=None,  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+        _response_types_map = {
+            '200': "str",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
 
-    def get_sample_image_write_url_by_id(self, dataset_id, sample_id, is_thumbnail, **kwargs):  # noqa: E501
-        """get_sample_image_write_url_by_id  # noqa: E501
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/embeddings/{embeddingId}/readCSVUrl', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def get_embeddings_csv_write_url_by_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], name : Annotated[Optional[StrictStr], Field(description="the sampling requests name to create a signed url for")] = None, **kwargs) -> WriteCSVUrlData:  # noqa: E501
+        """get_embeddings_csv_write_url_by_id  # noqa: E501
 
-        Get the signed url to upload an image to for a specific sample of a dataset  # noqa: E501
+        Get the signed url to upload an CSVembedding to for a specific dataset  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_sample_image_write_url_by_id(dataset_id, sample_id, is_thumbnail, async_req=True)
+
+        >>> thread = api.get_embeddings_csv_write_url_by_id(dataset_id, name, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID sample_id: ObjectId of the sample (required)
-        :param bool is_thumbnail: Whether or not the image to upload is a thumbnail (required)
-        :return: str
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param name: the sampling requests name to create a signed url for
+        :type name: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: WriteCSVUrlData
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_sample_image_write_url_by_id_with_http_info(dataset_id, sample_id, is_thumbnail, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_sample_image_write_url_by_id_with_http_info(dataset_id, sample_id, is_thumbnail, **kwargs)  # noqa: E501
-            return data
-
-    def get_sample_image_write_url_by_id_with_http_info(self, dataset_id, sample_id, is_thumbnail, **kwargs):  # noqa: E501
-        """get_sample_image_write_url_by_id  # noqa: E501
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the get_embeddings_csv_write_url_by_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.get_embeddings_csv_write_url_by_id_with_http_info(dataset_id, name, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def get_embeddings_csv_write_url_by_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], name : Annotated[Optional[StrictStr], Field(description="the sampling requests name to create a signed url for")] = None, **kwargs) -> ApiResponse:  # noqa: E501
+        """get_embeddings_csv_write_url_by_id  # noqa: E501
 
-        Get the signed url to upload an image to for a specific sample of a dataset  # noqa: E501
+        Get the signed url to upload an CSVembedding to for a specific dataset  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_sample_image_write_url_by_id_with_http_info(dataset_id, sample_id, is_thumbnail, async_req=True)
+
+        >>> thread = api.get_embeddings_csv_write_url_by_id_with_http_info(dataset_id, name, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID sample_id: ObjectId of the sample (required)
-        :param bool is_thumbnail: Whether or not the image to upload is a thumbnail (required)
-        :return: str
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param name: the sampling requests name to create a signed url for
+        :type name: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: tuple(WriteCSVUrlData, status_code(int), headers(HTTPHeaderDict))
         """
 
-        all_params = ['dataset_id', 'sample_id', 'is_thumbnail']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'name'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method get_sample_image_write_url_by_id" % key
+                    " to method get_embeddings_csv_write_url_by_id" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `get_sample_image_write_url_by_id`")  # noqa: E501
-        # verify the required parameter 'sample_id' is set
-        if self.api_client.client_side_validation and ('sample_id' not in params or
-                                                       params['sample_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `sample_id` when calling `get_sample_image_write_url_by_id`")  # noqa: E501
-        # verify the required parameter 'is_thumbnail' is set
-        if self.api_client.client_side_validation and ('is_thumbnail' not in params or
-                                                       params['is_thumbnail'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `is_thumbnail` when calling `get_sample_image_write_url_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'sample_id' in params:
-            path_params['sampleId'] = params['sample_id']  # noqa: E501
-
-        query_params = []
-        if 'is_thumbnail' in params:
-            query_params.append(('isThumbnail', params['is_thumbnail']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+
+        # process the query parameters
+        _query_params = []
+        if _params.get('name') is not None:  # noqa: E501
+            _query_params.append((
+                'name',
+                _params['name'].value if hasattr(_params['name'], 'value') else _params['name']
+            ))
+
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/samples/{sampleId}/writeurl', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='str',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+        _response_types_map = {
+            '200': "WriteCSVUrlData",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
 
-    def get_sample_image_write_urls_by_id(self, dataset_id, sample_id, **kwargs):  # noqa: E501
-        """get_sample_image_write_urls_by_id  # noqa: E501
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/embeddings/writeCSVUrl', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def set_embeddings_is_processed_flag_by_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], embedding_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the embedding")], set_embeddings_is_processed_flag_by_id_body_request : SetEmbeddingsIsProcessedFlagByIdBodyRequest, **kwargs) -> None:  # noqa: E501
+        """set_embeddings_is_processed_flag_by_id  # noqa: E501
 
-        Get all signed write URLs to upload all images (full image and thumbnail) of a specific sample of a dataset  # noqa: E501
+        Sets the isProcessed flag of the specified embedding  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_sample_image_write_urls_by_id(dataset_id, sample_id, async_req=True)
+
+        >>> thread = api.set_embeddings_is_processed_flag_by_id(dataset_id, embedding_id, set_embeddings_is_processed_flag_by_id_body_request, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID sample_id: ObjectId of the sample (required)
-        :return: SampleWriteUrls
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param embedding_id: ObjectId of the embedding (required)
+        :type embedding_id: str
+        :param set_embeddings_is_processed_flag_by_id_body_request: (required)
+        :type set_embeddings_is_processed_flag_by_id_body_request: SetEmbeddingsIsProcessedFlagByIdBodyRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: None
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_sample_image_write_urls_by_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_sample_image_write_urls_by_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501
-            return data
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the set_embeddings_is_processed_flag_by_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.set_embeddings_is_processed_flag_by_id_with_http_info(dataset_id, embedding_id, set_embeddings_is_processed_flag_by_id_body_request, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def set_embeddings_is_processed_flag_by_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], embedding_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the embedding")], set_embeddings_is_processed_flag_by_id_body_request : SetEmbeddingsIsProcessedFlagByIdBodyRequest, **kwargs) -> ApiResponse:  # noqa: E501
+        """set_embeddings_is_processed_flag_by_id  # noqa: E501
 
-    def get_sample_image_write_urls_by_id_with_http_info(self, dataset_id, sample_id, **kwargs):  # noqa: E501
-        """get_sample_image_write_urls_by_id  # noqa: E501
-
-        Get all signed write URLs to upload all images (full image and thumbnail) of a specific sample of a dataset  # noqa: E501
+        Sets the isProcessed flag of the specified embedding  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_sample_image_write_urls_by_id_with_http_info(dataset_id, sample_id, async_req=True)
+
+        >>> thread = api.set_embeddings_is_processed_flag_by_id_with_http_info(dataset_id, embedding_id, set_embeddings_is_processed_flag_by_id_body_request, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID sample_id: ObjectId of the sample (required)
-        :return: SampleWriteUrls
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param embedding_id: ObjectId of the embedding (required)
+        :type embedding_id: str
+        :param set_embeddings_is_processed_flag_by_id_body_request: (required)
+        :type set_embeddings_is_processed_flag_by_id_body_request: SetEmbeddingsIsProcessedFlagByIdBodyRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: None
         """
 
-        all_params = ['dataset_id', 'sample_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'embedding_id',
+            'set_embeddings_is_processed_flag_by_id_body_request'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method get_sample_image_write_urls_by_id" % key
+                    " to method set_embeddings_is_processed_flag_by_id" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `get_sample_image_write_urls_by_id`")  # noqa: E501
-        # verify the required parameter 'sample_id' is set
-        if self.api_client.client_side_validation and ('sample_id' not in params or
-                                                       params['sample_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `sample_id` when calling `get_sample_image_write_urls_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'sample_id' in params:
-            path_params['sampleId'] = params['sample_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/samples/{sampleId}/writeurls', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='SampleWriteUrls',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+            _params[_key] = _val
+        del _params['kwargs']
 
-    def get_samples_by_dataset_id(self, dataset_id, **kwargs):  # noqa: E501
-        """get_samples_by_dataset_id  # noqa: E501
+        _collection_formats = {}
 
-        Get all samples of a dataset  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_samples_by_dataset_id(dataset_id, async_req=True)
-        >>> result = thread.get()
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['embedding_id']:
+            _path_params['embeddingId'] = _params['embedding_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        if _params['set_embeddings_is_processed_flag_by_id_body_request'] is not None:
+            _body_params = _params['set_embeddings_is_processed_flag_by_id_body_request']
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param str file_name: filter the samples by filename
-        :param SampleSortBy sort_by: sort the samples
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: list[SampleData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_samples_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_samples_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_samples_by_dataset_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501
-        """get_samples_by_dataset_id  # noqa: E501
-
-        Get all samples of a dataset  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_samples_by_dataset_id_with_http_info(dataset_id, async_req=True)
-        >>> result = thread.get()
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param str file_name: filter the samples by filename
-        :param SampleSortBy sort_by: sort the samples
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: list[SampleData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
+        # set the HTTP header `Content-Type`
+        _content_types_list = _params.get('_content_type',
+            self.api_client.select_header_content_type(
+                ['application/json']))
+        if _content_types_list:
+                _header_params['Content-Type'] = _content_types_list
 
-        all_params = ['dataset_id', 'file_name', 'sort_by', 'page_size', 'page_offset']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_samples_by_dataset_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `get_samples_by_dataset_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-
-        query_params = []
-        if 'file_name' in params:
-            query_params.append(('fileName', params['file_name']))  # noqa: E501
-        if 'sort_by' in params:
-            query_params.append(('sortBy', params['sort_by']))  # noqa: E501
-        if 'page_size' in params:
-            query_params.append(('pageSize', params['page_size']))  # noqa: E501
-        if 'page_offset' in params:
-            query_params.append(('pageOffset', params['page_offset']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        _response_types_map = {}
 
         return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/samples', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='list[SampleData]',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_samples_partial_by_dataset_id(self, dataset_id, **kwargs):  # noqa: E501
-        """get_samples_partial_by_dataset_id  # noqa: E501
+            '/v1/datasets/{datasetId}/embeddings/{embeddingId}/isProcessed', 'PUT',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def trigger2d_embeddings_job(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], embedding_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the embedding")], trigger2d_embedding_job_request : Trigger2dEmbeddingJobRequest, **kwargs) -> None:  # noqa: E501
+        """trigger2d_embeddings_job  # noqa: E501
 
-        Get partial information of all samples of a dataset  # noqa: E501
+        Trigger job to get 2d embeddings from embeddings  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_samples_partial_by_dataset_id(dataset_id, async_req=True)
+
+        >>> thread = api.trigger2d_embeddings_job(dataset_id, embedding_id, trigger2d_embedding_job_request, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param SamplePartialMode mode:
-        :param str file_name: filter the samples by filename
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: list[SampleDataModes]
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param embedding_id: ObjectId of the embedding (required)
+        :type embedding_id: str
+        :param trigger2d_embedding_job_request: (required)
+        :type trigger2d_embedding_job_request: Trigger2dEmbeddingJobRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: None
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_samples_partial_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_samples_partial_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_samples_partial_by_dataset_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501
-        """get_samples_partial_by_dataset_id  # noqa: E501
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the trigger2d_embeddings_job_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.trigger2d_embeddings_job_with_http_info(dataset_id, embedding_id, trigger2d_embedding_job_request, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def trigger2d_embeddings_job_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], embedding_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the embedding")], trigger2d_embedding_job_request : Trigger2dEmbeddingJobRequest, **kwargs) -> ApiResponse:  # noqa: E501
+        """trigger2d_embeddings_job  # noqa: E501
 
-        Get partial information of all samples of a dataset  # noqa: E501
+        Trigger job to get 2d embeddings from embeddings  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_samples_partial_by_dataset_id_with_http_info(dataset_id, async_req=True)
+
+        >>> thread = api.trigger2d_embeddings_job_with_http_info(dataset_id, embedding_id, trigger2d_embedding_job_request, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param SamplePartialMode mode:
-        :param str file_name: filter the samples by filename
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: list[SampleDataModes]
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param embedding_id: ObjectId of the embedding (required)
+        :type embedding_id: str
+        :param trigger2d_embedding_job_request: (required)
+        :type trigger2d_embedding_job_request: Trigger2dEmbeddingJobRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: None
         """
 
-        all_params = ['dataset_id', 'mode', 'file_name', 'page_size', 'page_offset']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'embedding_id',
+            'trigger2d_embedding_job_request'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method get_samples_partial_by_dataset_id" % key
+                    " to method trigger2d_embeddings_job" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `get_samples_partial_by_dataset_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-
-        query_params = []
-        if 'mode' in params:
-            query_params.append(('mode', params['mode']))  # noqa: E501
-        if 'file_name' in params:
-            query_params.append(('fileName', params['file_name']))  # noqa: E501
-        if 'page_size' in params:
-            query_params.append(('pageSize', params['page_size']))  # noqa: E501
-        if 'page_offset' in params:
-            query_params.append(('pageOffset', params['page_offset']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+            _params[_key] = _val
+        del _params['kwargs']
 
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/samples/partial', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='list[SampleDataModes]',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def update_sample_by_id(self, body, dataset_id, sample_id, **kwargs):  # noqa: E501
-        """update_sample_by_id  # noqa: E501
+        _collection_formats = {}
 
-        update a specific sample of a dataset  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.update_sample_by_id(body, dataset_id, sample_id, async_req=True)
-        >>> result = thread.get()
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+        if _params['embedding_id']:
+            _path_params['embeddingId'] = _params['embedding_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        if _params['trigger2d_embedding_job_request'] is not None:
+            _body_params = _params['trigger2d_embedding_job_request']
 
-        :param async_req bool
-        :param SampleUpdateRequest body: The updated sample to set (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID sample_id: ObjectId of the sample (required)
-        :param bool enable_dataset_update:
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.update_sample_by_id_with_http_info(body, dataset_id, sample_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.update_sample_by_id_with_http_info(body, dataset_id, sample_id, **kwargs)  # noqa: E501
-            return data
-
-    def update_sample_by_id_with_http_info(self, body, dataset_id, sample_id, **kwargs):  # noqa: E501
-        """update_sample_by_id  # noqa: E501
-
-        update a specific sample of a dataset  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.update_sample_by_id_with_http_info(body, dataset_id, sample_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param SampleUpdateRequest body: The updated sample to set (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID sample_id: ObjectId of the sample (required)
-        :param bool enable_dataset_update:
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body', 'dataset_id', 'sample_id', 'enable_dataset_update']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method update_sample_by_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `update_sample_by_id`")  # noqa: E501
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `update_sample_by_id`")  # noqa: E501
-        # verify the required parameter 'sample_id' is set
-        if self.api_client.client_side_validation and ('sample_id' not in params or
-                                                       params['sample_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `sample_id` when calling `update_sample_by_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'sample_id' in params:
-            path_params['sampleId'] = params['sample_id']  # noqa: E501
-
-        query_params = []
-        if 'enable_dataset_update' in params:
-            query_params.append(('enableDatasetUpdate', params['enable_dataset_update']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
+        # set the HTTP header `Content-Type`
+        _content_types_list = _params.get('_content_type',
+            self.api_client.select_header_content_type(
+                ['application/json']))
+        if _content_types_list:
+                _header_params['Content-Type'] = _content_types_list
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        _response_types_map = {}
 
         return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/samples/{sampleId}', 'PUT',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type=None,  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+            '/v1/datasets/{datasetId}/embeddings/{embeddingId}/trigger2dEmbeddingsJob', 'POST',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/api/tags_api.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/api/datasets_api.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,1985 +1,1749 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
-"""
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
 
+    Do not edit the class manually.
+"""
 
-from __future__ import absolute_import
 
 import re  # noqa: F401
+import io
+import warnings
+
+from pydantic import validate_arguments, ValidationError
+from typing_extensions import Annotated
+
+from pydantic import Field, StrictBool, StrictInt, conint, constr, validator
 
-# python 2 and python 3 compatibility library
-import six
+from typing import List, Optional
+
+from lightly.openapi_generated.swagger_client.models.create_entity_response import CreateEntityResponse
+from lightly.openapi_generated.swagger_client.models.dataset_create_request import DatasetCreateRequest
+from lightly.openapi_generated.swagger_client.models.dataset_data import DatasetData
+from lightly.openapi_generated.swagger_client.models.dataset_data_enriched import DatasetDataEnriched
+from lightly.openapi_generated.swagger_client.models.dataset_update_request import DatasetUpdateRequest
+from lightly.openapi_generated.swagger_client.models.job_status_meta import JobStatusMeta
 
 from lightly.openapi_generated.swagger_client.api_client import ApiClient
+from lightly.openapi_generated.swagger_client.api_response import ApiResponse
+from lightly.openapi_generated.swagger_client.exceptions import (  # noqa: F401
+    ApiTypeError,
+    ApiValueError
+)
 
 
-class TagsApi(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
+class DatasetsApi(object):
+    """NOTE: This class is auto generated by OpenAPI Generator
+    Ref: https://openapi-generator.tech
 
     Do not edit the class manually.
-    Ref: https://github.com/swagger-api/swagger-codegen
     """
 
     def __init__(self, api_client=None):
         if api_client is None:
-            api_client = ApiClient()
+            api_client = ApiClient.get_default()
         self.api_client = api_client
 
-    def create_initial_tag_by_dataset_id(self, body, dataset_id, **kwargs):  # noqa: E501
-        """create_initial_tag_by_dataset_id  # noqa: E501
+    @validate_arguments
+    def create_dataset(self, dataset_create_request : DatasetCreateRequest, **kwargs) -> CreateEntityResponse:  # noqa: E501
+        """create_dataset  # noqa: E501
 
-        create the intitial tag for a dataset which then locks the dataset  # noqa: E501
+        Creates a new dataset for a user  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_initial_tag_by_dataset_id(body, dataset_id, async_req=True)
+
+        >>> thread = api.create_dataset(dataset_create_request, async_req=True)
         >>> result = thread.get()
 
-        :param async_req bool
-        :param InitialTagCreateRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: CreateEntityResponse
+        :param dataset_create_request: (required)
+        :type dataset_create_request: DatasetCreateRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: CreateEntityResponse
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.create_initial_tag_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.create_initial_tag_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-            return data
-
-    def create_initial_tag_by_dataset_id_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501
-        """create_initial_tag_by_dataset_id  # noqa: E501
-
-        create the intitial tag for a dataset which then locks the dataset  # noqa: E501
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the create_dataset_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.create_dataset_with_http_info(dataset_create_request, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def create_dataset_with_http_info(self, dataset_create_request : DatasetCreateRequest, **kwargs) -> ApiResponse:  # noqa: E501
+        """create_dataset  # noqa: E501
+
+        Creates a new dataset for a user  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_initial_tag_by_dataset_id_with_http_info(body, dataset_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param InitialTagCreateRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
 
-        all_params = ['body', 'dataset_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        >>> thread = api.create_dataset_with_http_info(dataset_create_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_create_request: (required)
+        :type dataset_create_request: DatasetCreateRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(CreateEntityResponse, status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_create_request'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method create_initial_tag_by_dataset_id" % key
+                    " to method create_dataset" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `create_initial_tag_by_dataset_id`")  # noqa: E501
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `create_initial_tag_by_dataset_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+            _params[_key] = _val
+        del _params['kwargs']
 
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/initial', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='CreateEntityResponse',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+        _collection_formats = {}
 
-    def create_tag_by_dataset_id(self, body, dataset_id, **kwargs):  # noqa: E501
-        """create_tag_by_dataset_id  # noqa: E501
+        # process the path parameters
+        _path_params = {}
 
-        create new tag for dataset  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_tag_by_dataset_id(body, dataset_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param TagCreateRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.create_tag_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.create_tag_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-            return data
-
-    def create_tag_by_dataset_id_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501
-        """create_tag_by_dataset_id  # noqa: E501
-
-        create new tag for dataset  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.create_tag_by_dataset_id_with_http_info(body, dataset_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param TagCreateRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body', 'dataset_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method create_tag_by_dataset_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `create_tag_by_dataset_id`")  # noqa: E501
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `create_tag_by_dataset_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        if _params['dataset_create_request'] is not None:
+            _body_params = _params['dataset_create_request']
 
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        # set the HTTP header `Content-Type`
+        _content_types_list = _params.get('_content_type',
+            self.api_client.select_header_content_type(
+                ['application/json']))
+        if _content_types_list:
+                _header_params['Content-Type'] = _content_types_list
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
+
+        _response_types_map = {
+            '201': "CreateEntityResponse",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
 
         return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='CreateEntityResponse',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def delete_tag_by_tag_id(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """delete_tag_by_tag_id  # noqa: E501
-
-        delete a specific tag if its a leaf-tag (e.g is not a dependency of another tag)  # noqa: E501
+            '/v1/datasets', 'POST',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def delete_dataset_by_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], force : Optional[StrictBool] = None, **kwargs) -> None:  # noqa: E501
+        """delete_dataset_by_id  # noqa: E501
+
+        Delete a specific dataset  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.delete_tag_by_tag_id(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :return: None
+        >>> thread = api.delete_dataset_by_id(dataset_id, force, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param force:
+        :type force: bool
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: None
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.delete_tag_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.delete_tag_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-            return data
-
-    def delete_tag_by_tag_id_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """delete_tag_by_tag_id  # noqa: E501
-
-        delete a specific tag if its a leaf-tag (e.g is not a dependency of another tag)  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.delete_tag_by_tag_id_with_http_info(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['dataset_id', 'tag_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the delete_dataset_by_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.delete_dataset_by_id_with_http_info(dataset_id, force, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def delete_dataset_by_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], force : Optional[StrictBool] = None, **kwargs) -> ApiResponse:  # noqa: E501
+        """delete_dataset_by_id  # noqa: E501
+
+        Delete a specific dataset  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.delete_dataset_by_id_with_http_info(dataset_id, force, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param force:
+        :type force: bool
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: None
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'force'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method delete_tag_by_tag_id" % key
+                    " to method delete_dataset_by_id" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `delete_tag_by_tag_id`")  # noqa: E501
-        # verify the required parameter 'tag_id' is set
-        if self.api_client.client_side_validation and ('tag_id' not in params or
-                                                       params['tag_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `tag_id` when calling `delete_tag_by_tag_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'tag_id' in params:
-            path_params['tagId'] = params['tag_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/{tagId}', 'DELETE',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type=None,  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def download_zip_of_samples_by_tag_id(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """download_zip_of_samples_by_tag_id  # noqa: E501
-
-        Download a zip file of the samples of a tag. Limited to 1000 images  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.download_zip_of_samples_by_tag_id(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :return: str
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.download_zip_of_samples_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.download_zip_of_samples_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-            return data
+            _params[_key] = _val
+        del _params['kwargs']
 
-    def download_zip_of_samples_by_tag_id_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """download_zip_of_samples_by_tag_id  # noqa: E501
+        _collection_formats = {}
 
-        Download a zip file of the samples of a tag. Limited to 1000 images  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.download_zip_of_samples_by_tag_id_with_http_info(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :return: str
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+
+        # process the query parameters
+        _query_params = []
+        if _params.get('force') is not None:  # noqa: E501
+            _query_params.append((
+                'force',
+                _params['force'].value if hasattr(_params['force'], 'value') else _params['force']
+            ))
+
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
+            ['application/json'])  # noqa: E501
 
-        all_params = ['dataset_id', 'tag_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method download_zip_of_samples_by_tag_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `download_zip_of_samples_by_tag_id`")  # noqa: E501
-        # verify the required parameter 'tag_id' is set
-        if self.api_client.client_side_validation and ('tag_id' not in params or
-                                                       params['tag_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `tag_id` when calling `download_zip_of_samples_by_tag_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'tag_id' in params:
-            path_params['tagId'] = params['tag_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/zip', 'application/json'])  # noqa: E501
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        _response_types_map = {}
 
         return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/{tagId}/export/zip', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='str',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def export_tag_to_basic_filenames(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """export_tag_to_basic_filenames  # noqa: E501
-
-        Export the samples filenames of a specific tag   # noqa: E501
+            '/v1/datasets/{datasetId}', 'DELETE',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def get_children_of_dataset_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], **kwargs) -> List[DatasetData]:  # noqa: E501
+        """get_children_of_dataset_id  # noqa: E501
+
+        Get all datasets which are the children of a specific dataset (e.g crop datasets)  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.export_tag_to_basic_filenames(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :param int expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
-        :param str access_control: which access control name to be used
-        :param FileNameFormat file_name_format:
-        :param bool include_meta_data: if true, will also include metadata
-        :param FileOutputFormat format:
-        :param bool preview_example: if true, will generate a preview example of how the structure will look
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: str
+        >>> thread = api.get_children_of_dataset_id(dataset_id, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: List[DatasetData]
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.export_tag_to_basic_filenames_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.export_tag_to_basic_filenames_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-            return data
-
-    def export_tag_to_basic_filenames_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """export_tag_to_basic_filenames  # noqa: E501
-
-        Export the samples filenames of a specific tag   # noqa: E501
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the get_children_of_dataset_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.get_children_of_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def get_children_of_dataset_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], **kwargs) -> ApiResponse:  # noqa: E501
+        """get_children_of_dataset_id  # noqa: E501
+
+        Get all datasets which are the children of a specific dataset (e.g crop datasets)  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.export_tag_to_basic_filenames_with_http_info(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :param int expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
-        :param str access_control: which access control name to be used
-        :param FileNameFormat file_name_format:
-        :param bool include_meta_data: if true, will also include metadata
-        :param FileOutputFormat format:
-        :param bool preview_example: if true, will generate a preview example of how the structure will look
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: str
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['dataset_id', 'tag_id', 'expires_in', 'access_control', 'file_name_format', 'include_meta_data', 'format', 'preview_example', 'page_size', 'page_offset']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        >>> thread = api.get_children_of_dataset_id_with_http_info(dataset_id, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(List[DatasetData], status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method export_tag_to_basic_filenames" % key
+                    " to method get_children_of_dataset_id" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `export_tag_to_basic_filenames`")  # noqa: E501
-        # verify the required parameter 'tag_id' is set
-        if self.api_client.client_side_validation and ('tag_id' not in params or
-                                                       params['tag_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `tag_id` when calling `export_tag_to_basic_filenames`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'tag_id' in params:
-            path_params['tagId'] = params['tag_id']  # noqa: E501
-
-        query_params = []
-        if 'expires_in' in params:
-            query_params.append(('expiresIn', params['expires_in']))  # noqa: E501
-        if 'access_control' in params:
-            query_params.append(('accessControl', params['access_control']))  # noqa: E501
-        if 'file_name_format' in params:
-            query_params.append(('fileNameFormat', params['file_name_format']))  # noqa: E501
-        if 'include_meta_data' in params:
-            query_params.append(('includeMetaData', params['include_meta_data']))  # noqa: E501
-        if 'format' in params:
-            query_params.append(('format', params['format']))  # noqa: E501
-        if 'preview_example' in params:
-            query_params.append(('previewExample', params['preview_example']))  # noqa: E501
-        if 'page_size' in params:
-            query_params.append(('pageSize', params['page_size']))  # noqa: E501
-        if 'page_offset' in params:
-            query_params.append(('pageOffset', params['page_offset']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['text/plain', 'application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+            _params[_key] = _val
+        del _params['kwargs']
 
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/{tagId}/export/basic/filenames', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='str',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def export_tag_to_basic_filenames_and_read_urls(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """export_tag_to_basic_filenames_and_read_urls  # noqa: E501
-
-        Export the samples filenames to map with their readURL.   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.export_tag_to_basic_filenames_and_read_urls(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
+        _collection_formats = {}
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :param FileOutputFormat format:
-        :param bool preview_example: if true, will generate a preview example of how the structure will look
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: FilenameAndReadUrls
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.export_tag_to_basic_filenames_and_read_urls_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.export_tag_to_basic_filenames_and_read_urls_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-            return data
-
-    def export_tag_to_basic_filenames_and_read_urls_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """export_tag_to_basic_filenames_and_read_urls  # noqa: E501
-
-        Export the samples filenames to map with their readURL.   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.export_tag_to_basic_filenames_and_read_urls_with_http_info(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :param FileOutputFormat format:
-        :param bool preview_example: if true, will generate a preview example of how the structure will look
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: FilenameAndReadUrls
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['dataset_id', 'tag_id', 'format', 'preview_example', 'page_size', 'page_offset']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method export_tag_to_basic_filenames_and_read_urls" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `export_tag_to_basic_filenames_and_read_urls`")  # noqa: E501
-        # verify the required parameter 'tag_id' is set
-        if self.api_client.client_side_validation and ('tag_id' not in params or
-                                                       params['tag_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `tag_id` when calling `export_tag_to_basic_filenames_and_read_urls`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'tag_id' in params:
-            path_params['tagId'] = params['tag_id']  # noqa: E501
-
-        query_params = []
-        if 'format' in params:
-            query_params.append(('format', params['format']))  # noqa: E501
-        if 'preview_example' in params:
-            query_params.append(('previewExample', params['preview_example']))  # noqa: E501
-        if 'page_size' in params:
-            query_params.append(('pageSize', params['page_size']))  # noqa: E501
-        if 'page_offset' in params:
-            query_params.append(('pageOffset', params['page_offset']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/{tagId}/export/basic/filenamesAndReadUrls', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='FilenameAndReadUrls',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+        _response_types_map = {
+            '200': "List[DatasetData]",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
 
-    def export_tag_to_label_box_data_rows(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """export_tag_to_label_box_data_rows  # noqa: E501
-
-        Deprecated. Please use V4 unless there is a specific need to use the LabelBox V3 API. Export samples of a tag as a json for importing into LabelBox as outlined here; https://docs.labelbox.com/v3/reference/image ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/children', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def get_dataset_by_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], **kwargs) -> DatasetData:  # noqa: E501
+        """get_dataset_by_id  # noqa: E501
+
+        Get a specific dataset  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.export_tag_to_label_box_data_rows(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :param int expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
-        :param str access_control: which access control name to be used
-        :param FileNameFormat file_name_format:
-        :param bool include_meta_data: if true, will also include metadata
-        :param FileOutputFormat format:
-        :param bool preview_example: if true, will generate a preview example of how the structure will look
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: LabelBoxDataRows
+        >>> thread = api.get_dataset_by_id(dataset_id, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: DatasetData
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.export_tag_to_label_box_data_rows_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.export_tag_to_label_box_data_rows_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-            return data
-
-    def export_tag_to_label_box_data_rows_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """export_tag_to_label_box_data_rows  # noqa: E501
-
-        Deprecated. Please use V4 unless there is a specific need to use the LabelBox V3 API. Export samples of a tag as a json for importing into LabelBox as outlined here; https://docs.labelbox.com/v3/reference/image ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the get_dataset_by_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.get_dataset_by_id_with_http_info(dataset_id, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def get_dataset_by_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], **kwargs) -> ApiResponse:  # noqa: E501
+        """get_dataset_by_id  # noqa: E501
+
+        Get a specific dataset  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.export_tag_to_label_box_data_rows_with_http_info(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :param int expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
-        :param str access_control: which access control name to be used
-        :param FileNameFormat file_name_format:
-        :param bool include_meta_data: if true, will also include metadata
-        :param FileOutputFormat format:
-        :param bool preview_example: if true, will generate a preview example of how the structure will look
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: LabelBoxDataRows
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['dataset_id', 'tag_id', 'expires_in', 'access_control', 'file_name_format', 'include_meta_data', 'format', 'preview_example', 'page_size', 'page_offset']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        >>> thread = api.get_dataset_by_id_with_http_info(dataset_id, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(DatasetData, status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method export_tag_to_label_box_data_rows" % key
+                    " to method get_dataset_by_id" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `export_tag_to_label_box_data_rows`")  # noqa: E501
-        # verify the required parameter 'tag_id' is set
-        if self.api_client.client_side_validation and ('tag_id' not in params or
-                                                       params['tag_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `tag_id` when calling `export_tag_to_label_box_data_rows`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'tag_id' in params:
-            path_params['tagId'] = params['tag_id']  # noqa: E501
-
-        query_params = []
-        if 'expires_in' in params:
-            query_params.append(('expiresIn', params['expires_in']))  # noqa: E501
-        if 'access_control' in params:
-            query_params.append(('accessControl', params['access_control']))  # noqa: E501
-        if 'file_name_format' in params:
-            query_params.append(('fileNameFormat', params['file_name_format']))  # noqa: E501
-        if 'include_meta_data' in params:
-            query_params.append(('includeMetaData', params['include_meta_data']))  # noqa: E501
-        if 'format' in params:
-            query_params.append(('format', params['format']))  # noqa: E501
-        if 'preview_example' in params:
-            query_params.append(('previewExample', params['preview_example']))  # noqa: E501
-        if 'page_size' in params:
-            query_params.append(('pageSize', params['page_size']))  # noqa: E501
-        if 'page_offset' in params:
-            query_params.append(('pageOffset', params['page_offset']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/{tagId}/export/LabelBox/datarows', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='LabelBoxDataRows',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def export_tag_to_label_box_v4_data_rows(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """export_tag_to_label_box_v4_data_rows  # noqa: E501
-
-        Export samples of a tag as a json for importing into LabelBox as outlined here; https://docs.labelbox.com/v4/reference/image ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.export_tag_to_label_box_v4_data_rows(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :param int expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
-        :param str access_control: which access control name to be used
-        :param FileNameFormat file_name_format:
-        :param bool include_meta_data: if true, will also include metadata
-        :param FileOutputFormat format:
-        :param bool preview_example: if true, will generate a preview example of how the structure will look
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: LabelBoxV4DataRows
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.export_tag_to_label_box_v4_data_rows_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.export_tag_to_label_box_v4_data_rows_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-            return data
-
-    def export_tag_to_label_box_v4_data_rows_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """export_tag_to_label_box_v4_data_rows  # noqa: E501
+            _params[_key] = _val
+        del _params['kwargs']
 
-        Export samples of a tag as a json for importing into LabelBox as outlined here; https://docs.labelbox.com/v4/reference/image ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.export_tag_to_label_box_v4_data_rows_with_http_info(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :param int expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
-        :param str access_control: which access control name to be used
-        :param FileNameFormat file_name_format:
-        :param bool include_meta_data: if true, will also include metadata
-        :param FileOutputFormat format:
-        :param bool preview_example: if true, will generate a preview example of how the structure will look
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: LabelBoxV4DataRows
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
+        _collection_formats = {}
 
-        all_params = ['dataset_id', 'tag_id', 'expires_in', 'access_control', 'file_name_format', 'include_meta_data', 'format', 'preview_example', 'page_size', 'page_offset']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method export_tag_to_label_box_v4_data_rows" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `export_tag_to_label_box_v4_data_rows`")  # noqa: E501
-        # verify the required parameter 'tag_id' is set
-        if self.api_client.client_side_validation and ('tag_id' not in params or
-                                                       params['tag_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `tag_id` when calling `export_tag_to_label_box_v4_data_rows`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'tag_id' in params:
-            path_params['tagId'] = params['tag_id']  # noqa: E501
-
-        query_params = []
-        if 'expires_in' in params:
-            query_params.append(('expiresIn', params['expires_in']))  # noqa: E501
-        if 'access_control' in params:
-            query_params.append(('accessControl', params['access_control']))  # noqa: E501
-        if 'file_name_format' in params:
-            query_params.append(('fileNameFormat', params['file_name_format']))  # noqa: E501
-        if 'include_meta_data' in params:
-            query_params.append(('includeMetaData', params['include_meta_data']))  # noqa: E501
-        if 'format' in params:
-            query_params.append(('format', params['format']))  # noqa: E501
-        if 'preview_example' in params:
-            query_params.append(('previewExample', params['preview_example']))  # noqa: E501
-        if 'page_size' in params:
-            query_params.append(('pageSize', params['page_size']))  # noqa: E501
-        if 'page_offset' in params:
-            query_params.append(('pageOffset', params['page_offset']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/{tagId}/export/LabelBoxV4/datarows', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='LabelBoxV4DataRows',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+        _response_types_map = {
+            '200': "DatasetData",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
 
-    def export_tag_to_label_studio_tasks(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """export_tag_to_label_studio_tasks  # noqa: E501
-
-        Export samples of a tag as a json for importing into LabelStudio as outlined here; https://labelstud.io/guide/tasks.html#Basic-Label-Studio-JSON-format ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def get_datasets(self, shared : Annotated[Optional[StrictBool], Field(description="if set, only returns the datasets which have been shared with the user")] = None, get_assets_of_team : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user")] = None, get_assets_of_team_inclusive_self : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user including the assets of the user")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> List[DatasetData]:  # noqa: E501
+        """get_datasets  # noqa: E501
+
+        Get all datasets for a user  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.export_tag_to_label_studio_tasks(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :param int expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
-        :param str access_control: which access control name to be used
-        :param FileNameFormat file_name_format:
-        :param bool include_meta_data: if true, will also include metadata
-        :param FileOutputFormat format:
-        :param bool preview_example: if true, will generate a preview example of how the structure will look
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: LabelStudioTasks
+        >>> thread = api.get_datasets(shared, get_assets_of_team, get_assets_of_team_inclusive_self, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param shared: if set, only returns the datasets which have been shared with the user
+        :type shared: bool
+        :param get_assets_of_team: if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user
+        :type get_assets_of_team: bool
+        :param get_assets_of_team_inclusive_self: if this flag is true, we get the relevant asset of the team of the user including the assets of the user
+        :type get_assets_of_team_inclusive_self: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: List[DatasetData]
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.export_tag_to_label_studio_tasks_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.export_tag_to_label_studio_tasks_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-            return data
-
-    def export_tag_to_label_studio_tasks_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """export_tag_to_label_studio_tasks  # noqa: E501
-
-        Export samples of a tag as a json for importing into LabelStudio as outlined here; https://labelstud.io/guide/tasks.html#Basic-Label-Studio-JSON-format ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.export_tag_to_label_studio_tasks_with_http_info(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :param int expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
-        :param str access_control: which access control name to be used
-        :param FileNameFormat file_name_format:
-        :param bool include_meta_data: if true, will also include metadata
-        :param FileOutputFormat format:
-        :param bool preview_example: if true, will generate a preview example of how the structure will look
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: LabelStudioTasks
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['dataset_id', 'tag_id', 'expires_in', 'access_control', 'file_name_format', 'include_meta_data', 'format', 'preview_example', 'page_size', 'page_offset']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the get_datasets_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.get_datasets_with_http_info(shared, get_assets_of_team, get_assets_of_team_inclusive_self, page_size, page_offset, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def get_datasets_with_http_info(self, shared : Annotated[Optional[StrictBool], Field(description="if set, only returns the datasets which have been shared with the user")] = None, get_assets_of_team : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user")] = None, get_assets_of_team_inclusive_self : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user including the assets of the user")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> ApiResponse:  # noqa: E501
+        """get_datasets  # noqa: E501
+
+        Get all datasets for a user  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.get_datasets_with_http_info(shared, get_assets_of_team, get_assets_of_team_inclusive_self, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param shared: if set, only returns the datasets which have been shared with the user
+        :type shared: bool
+        :param get_assets_of_team: if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user
+        :type get_assets_of_team: bool
+        :param get_assets_of_team_inclusive_self: if this flag is true, we get the relevant asset of the team of the user including the assets of the user
+        :type get_assets_of_team_inclusive_self: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(List[DatasetData], status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'shared',
+            'get_assets_of_team',
+            'get_assets_of_team_inclusive_self',
+            'page_size',
+            'page_offset'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method export_tag_to_label_studio_tasks" % key
+                    " to method get_datasets" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `export_tag_to_label_studio_tasks`")  # noqa: E501
-        # verify the required parameter 'tag_id' is set
-        if self.api_client.client_side_validation and ('tag_id' not in params or
-                                                       params['tag_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `tag_id` when calling `export_tag_to_label_studio_tasks`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'tag_id' in params:
-            path_params['tagId'] = params['tag_id']  # noqa: E501
-
-        query_params = []
-        if 'expires_in' in params:
-            query_params.append(('expiresIn', params['expires_in']))  # noqa: E501
-        if 'access_control' in params:
-            query_params.append(('accessControl', params['access_control']))  # noqa: E501
-        if 'file_name_format' in params:
-            query_params.append(('fileNameFormat', params['file_name_format']))  # noqa: E501
-        if 'include_meta_data' in params:
-            query_params.append(('includeMetaData', params['include_meta_data']))  # noqa: E501
-        if 'format' in params:
-            query_params.append(('format', params['format']))  # noqa: E501
-        if 'preview_example' in params:
-            query_params.append(('previewExample', params['preview_example']))  # noqa: E501
-        if 'page_size' in params:
-            query_params.append(('pageSize', params['page_size']))  # noqa: E501
-        if 'page_offset' in params:
-            query_params.append(('pageOffset', params['page_offset']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/{tagId}/export/LabelStudio/tasks', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='LabelStudioTasks',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def export_tag_to_sama_tasks(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """export_tag_to_sama_tasks  # noqa: E501
-
-        Export samples of a tag as a json for importing into Sama as tasks with the upload form or via the API as outlined here; - https://docs.sama.com/reference/taskcreate - https://docs.sama.com/reference/createbatch  ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.export_tag_to_sama_tasks(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
+            _params[_key] = _val
+        del _params['kwargs']
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :param int expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
-        :param str access_control: which access control name to be used
-        :param FileNameFormat file_name_format:
-        :param bool include_meta_data: if true, will also include metadata
-        :param FileOutputFormat format:
-        :param bool preview_example: if true, will generate a preview example of how the structure will look
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: SamaTasks
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.export_tag_to_sama_tasks_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.export_tag_to_sama_tasks_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-            return data
+        _collection_formats = {}
 
-    def export_tag_to_sama_tasks_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """export_tag_to_sama_tasks  # noqa: E501
+        # process the path parameters
+        _path_params = {}
 
-        Export samples of a tag as a json for importing into Sama as tasks with the upload form or via the API as outlined here; - https://docs.sama.com/reference/taskcreate - https://docs.sama.com/reference/createbatch  ```openapi\\+warning The image URLs are special in that the resource can be accessed by anyone in possession of said URL for the time specified by the expiresIn query param ```   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.export_tag_to_sama_tasks_with_http_info(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :param int expires_in: If defined, the URLs provided will only be valid for amount of seconds from time of issuence. If not defined, the URls will be valid indefinitely. 
-        :param str access_control: which access control name to be used
-        :param FileNameFormat file_name_format:
-        :param bool include_meta_data: if true, will also include metadata
-        :param FileOutputFormat format:
-        :param bool preview_example: if true, will generate a preview example of how the structure will look
-        :param int page_size: pagination size/limit of the number of samples to return
-        :param int page_offset: pagination offset
-        :return: SamaTasks
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['dataset_id', 'tag_id', 'expires_in', 'access_control', 'file_name_format', 'include_meta_data', 'format', 'preview_example', 'page_size', 'page_offset']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method export_tag_to_sama_tasks" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `export_tag_to_sama_tasks`")  # noqa: E501
-        # verify the required parameter 'tag_id' is set
-        if self.api_client.client_side_validation and ('tag_id' not in params or
-                                                       params['tag_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `tag_id` when calling `export_tag_to_sama_tasks`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'tag_id' in params:
-            path_params['tagId'] = params['tag_id']  # noqa: E501
-
-        query_params = []
-        if 'expires_in' in params:
-            query_params.append(('expiresIn', params['expires_in']))  # noqa: E501
-        if 'access_control' in params:
-            query_params.append(('accessControl', params['access_control']))  # noqa: E501
-        if 'file_name_format' in params:
-            query_params.append(('fileNameFormat', params['file_name_format']))  # noqa: E501
-        if 'include_meta_data' in params:
-            query_params.append(('includeMetaData', params['include_meta_data']))  # noqa: E501
-        if 'format' in params:
-            query_params.append(('format', params['format']))  # noqa: E501
-        if 'preview_example' in params:
-            query_params.append(('previewExample', params['preview_example']))  # noqa: E501
-        if 'page_size' in params:
-            query_params.append(('pageSize', params['page_size']))  # noqa: E501
-        if 'page_offset' in params:
-            query_params.append(('pageOffset', params['page_offset']))  # noqa: E501
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
+        # process the query parameters
+        _query_params = []
+        if _params.get('shared') is not None:  # noqa: E501
+            _query_params.append((
+                'shared',
+                _params['shared'].value if hasattr(_params['shared'], 'value') else _params['shared']
+            ))
+
+        if _params.get('get_assets_of_team') is not None:  # noqa: E501
+            _query_params.append((
+                'getAssetsOfTeam',
+                _params['get_assets_of_team'].value if hasattr(_params['get_assets_of_team'], 'value') else _params['get_assets_of_team']
+            ))
+
+        if _params.get('get_assets_of_team_inclusive_self') is not None:  # noqa: E501
+            _query_params.append((
+                'getAssetsOfTeamInclusiveSelf',
+                _params['get_assets_of_team_inclusive_self'].value if hasattr(_params['get_assets_of_team_inclusive_self'], 'value') else _params['get_assets_of_team_inclusive_self']
+            ))
+
+        if _params.get('page_size') is not None:  # noqa: E501
+            _query_params.append((
+                'pageSize',
+                _params['page_size'].value if hasattr(_params['page_size'], 'value') else _params['page_size']
+            ))
+
+        if _params.get('page_offset') is not None:  # noqa: E501
+            _query_params.append((
+                'pageOffset',
+                _params['page_offset'].value if hasattr(_params['page_offset'], 'value') else _params['page_offset']
+            ))
+
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/{tagId}/export/Sama/tasks', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='SamaTasks',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+        _response_types_map = {
+            '200': "List[DatasetData]",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
 
-    def get_filenames_by_tag_id(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """get_filenames_by_tag_id  # noqa: E501
-
-        Get list of filenames by tag. Deprecated, please use  # noqa: E501
+        return self.api_client.call_api(
+            '/v1/datasets', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def get_datasets_enriched(self, shared : Annotated[Optional[StrictBool], Field(description="if set, only returns the datasets which have been shared with the user")] = None, limit : Annotated[Optional[StrictInt], Field(description="DEPRECATED, use pageSize instead. if set, only returns the newest up until limit")] = None, get_assets_of_team : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user")] = None, get_assets_of_team_inclusive_self : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user including the assets of the user")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> List[DatasetDataEnriched]:  # noqa: E501
+        """get_datasets_enriched  # noqa: E501
+
+        Get all datasets for a user but enriched with additional information as nTags, nEmbeddings, samples  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_filenames_by_tag_id(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :return: TagFilenamesData
+        >>> thread = api.get_datasets_enriched(shared, limit, get_assets_of_team, get_assets_of_team_inclusive_self, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param shared: if set, only returns the datasets which have been shared with the user
+        :type shared: bool
+        :param limit: DEPRECATED, use pageSize instead. if set, only returns the newest up until limit
+        :type limit: int
+        :param get_assets_of_team: if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user
+        :type get_assets_of_team: bool
+        :param get_assets_of_team_inclusive_self: if this flag is true, we get the relevant asset of the team of the user including the assets of the user
+        :type get_assets_of_team_inclusive_self: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: List[DatasetDataEnriched]
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_filenames_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_filenames_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_filenames_by_tag_id_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """get_filenames_by_tag_id  # noqa: E501
-
-        Get list of filenames by tag. Deprecated, please use  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_filenames_by_tag_id_with_http_info(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :return: TagFilenamesData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['dataset_id', 'tag_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the get_datasets_enriched_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.get_datasets_enriched_with_http_info(shared, limit, get_assets_of_team, get_assets_of_team_inclusive_self, page_size, page_offset, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def get_datasets_enriched_with_http_info(self, shared : Annotated[Optional[StrictBool], Field(description="if set, only returns the datasets which have been shared with the user")] = None, limit : Annotated[Optional[StrictInt], Field(description="DEPRECATED, use pageSize instead. if set, only returns the newest up until limit")] = None, get_assets_of_team : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user")] = None, get_assets_of_team_inclusive_self : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user including the assets of the user")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> ApiResponse:  # noqa: E501
+        """get_datasets_enriched  # noqa: E501
+
+        Get all datasets for a user but enriched with additional information as nTags, nEmbeddings, samples  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.get_datasets_enriched_with_http_info(shared, limit, get_assets_of_team, get_assets_of_team_inclusive_self, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param shared: if set, only returns the datasets which have been shared with the user
+        :type shared: bool
+        :param limit: DEPRECATED, use pageSize instead. if set, only returns the newest up until limit
+        :type limit: int
+        :param get_assets_of_team: if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user
+        :type get_assets_of_team: bool
+        :param get_assets_of_team_inclusive_self: if this flag is true, we get the relevant asset of the team of the user including the assets of the user
+        :type get_assets_of_team_inclusive_self: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(List[DatasetDataEnriched], status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'shared',
+            'limit',
+            'get_assets_of_team',
+            'get_assets_of_team_inclusive_self',
+            'page_size',
+            'page_offset'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method get_filenames_by_tag_id" % key
+                    " to method get_datasets_enriched" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `get_filenames_by_tag_id`")  # noqa: E501
-        # verify the required parameter 'tag_id' is set
-        if self.api_client.client_side_validation and ('tag_id' not in params or
-                                                       params['tag_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `tag_id` when calling `get_filenames_by_tag_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'tag_id' in params:
-            path_params['tagId'] = params['tag_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['text/plain', 'application/json'])  # noqa: E501
-
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/{tagId}/filenames', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='TagFilenamesData',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_tag_by_tag_id(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """get_tag_by_tag_id  # noqa: E501
-
-        Get information about a specific tag  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_tag_by_tag_id(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
+            _params[_key] = _val
+        del _params['kwargs']
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :return: TagData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_tag_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_tag_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_tag_by_tag_id_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """get_tag_by_tag_id  # noqa: E501
-
-        Get information about a specific tag  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_tag_by_tag_id_with_http_info(dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
+        _collection_formats = {}
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :return: TagData
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
+        # process the path parameters
+        _path_params = {}
 
-        all_params = ['dataset_id', 'tag_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method get_tag_by_tag_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `get_tag_by_tag_id`")  # noqa: E501
-        # verify the required parameter 'tag_id' is set
-        if self.api_client.client_side_validation and ('tag_id' not in params or
-                                                       params['tag_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `tag_id` when calling `get_tag_by_tag_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'tag_id' in params:
-            path_params['tagId'] = params['tag_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
+        # process the query parameters
+        _query_params = []
+        if _params.get('shared') is not None:  # noqa: E501
+            _query_params.append((
+                'shared',
+                _params['shared'].value if hasattr(_params['shared'], 'value') else _params['shared']
+            ))
+
+        if _params.get('limit') is not None:  # noqa: E501
+            _query_params.append((
+                'limit',
+                _params['limit'].value if hasattr(_params['limit'], 'value') else _params['limit']
+            ))
+
+        if _params.get('get_assets_of_team') is not None:  # noqa: E501
+            _query_params.append((
+                'getAssetsOfTeam',
+                _params['get_assets_of_team'].value if hasattr(_params['get_assets_of_team'], 'value') else _params['get_assets_of_team']
+            ))
+
+        if _params.get('get_assets_of_team_inclusive_self') is not None:  # noqa: E501
+            _query_params.append((
+                'getAssetsOfTeamInclusiveSelf',
+                _params['get_assets_of_team_inclusive_self'].value if hasattr(_params['get_assets_of_team_inclusive_self'], 'value') else _params['get_assets_of_team_inclusive_self']
+            ))
+
+        if _params.get('page_size') is not None:  # noqa: E501
+            _query_params.append((
+                'pageSize',
+                _params['page_size'].value if hasattr(_params['page_size'], 'value') else _params['page_size']
+            ))
+
+        if _params.get('page_offset') is not None:  # noqa: E501
+            _query_params.append((
+                'pageOffset',
+                _params['page_offset'].value if hasattr(_params['page_offset'], 'value') else _params['page_offset']
+            ))
+
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/{tagId}', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='TagData',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def get_tags_by_dataset_id(self, dataset_id, **kwargs):  # noqa: E501
-        """get_tags_by_dataset_id  # noqa: E501
+        _response_types_map = {
+            '200': "List[DatasetDataEnriched]",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
 
-        Get all tags of a dataset  # noqa: E501
+        return self.api_client.call_api(
+            '/v1/datasets/enriched', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def get_datasets_enriched_query_by_name(self, dataset_name : constr(strict=True, min_length=1), shared : Annotated[Optional[StrictBool], Field(description="if set, only returns the datasets which have been shared with the user")] = None, exact : Annotated[Optional[StrictBool], Field(description="if set, only returns the datasets which match the name exactly (not just by prefix)")] = None, get_assets_of_team : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user")] = None, get_assets_of_team_inclusive_self : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user including the assets of the user")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> List[DatasetDataEnriched]:  # noqa: E501
+        """get_datasets_enriched_query_by_name  # noqa: E501
+
+        Query for datasets  enriched with additional information by their name prefix unless exact flag is set  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_tags_by_dataset_id(dataset_id, async_req=True)
-        >>> result = thread.get()
 
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: list[TagData]
+        >>> thread = api.get_datasets_enriched_query_by_name(dataset_name, shared, exact, get_assets_of_team, get_assets_of_team_inclusive_self, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_name: (required)
+        :type dataset_name: str
+        :param shared: if set, only returns the datasets which have been shared with the user
+        :type shared: bool
+        :param exact: if set, only returns the datasets which match the name exactly (not just by prefix)
+        :type exact: bool
+        :param get_assets_of_team: if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user
+        :type get_assets_of_team: bool
+        :param get_assets_of_team_inclusive_self: if this flag is true, we get the relevant asset of the team of the user including the assets of the user
+        :type get_assets_of_team_inclusive_self: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: List[DatasetDataEnriched]
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.get_tags_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.get_tags_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501
-            return data
-
-    def get_tags_by_dataset_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501
-        """get_tags_by_dataset_id  # noqa: E501
-
-        Get all tags of a dataset  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.get_tags_by_dataset_id_with_http_info(dataset_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: list[TagData]
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['dataset_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the get_datasets_enriched_query_by_name_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.get_datasets_enriched_query_by_name_with_http_info(dataset_name, shared, exact, get_assets_of_team, get_assets_of_team_inclusive_self, page_size, page_offset, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def get_datasets_enriched_query_by_name_with_http_info(self, dataset_name : constr(strict=True, min_length=1), shared : Annotated[Optional[StrictBool], Field(description="if set, only returns the datasets which have been shared with the user")] = None, exact : Annotated[Optional[StrictBool], Field(description="if set, only returns the datasets which match the name exactly (not just by prefix)")] = None, get_assets_of_team : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user")] = None, get_assets_of_team_inclusive_self : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user including the assets of the user")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> ApiResponse:  # noqa: E501
+        """get_datasets_enriched_query_by_name  # noqa: E501
+
+        Query for datasets  enriched with additional information by their name prefix unless exact flag is set  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.get_datasets_enriched_query_by_name_with_http_info(dataset_name, shared, exact, get_assets_of_team, get_assets_of_team_inclusive_self, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_name: (required)
+        :type dataset_name: str
+        :param shared: if set, only returns the datasets which have been shared with the user
+        :type shared: bool
+        :param exact: if set, only returns the datasets which match the name exactly (not just by prefix)
+        :type exact: bool
+        :param get_assets_of_team: if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user
+        :type get_assets_of_team: bool
+        :param get_assets_of_team_inclusive_self: if this flag is true, we get the relevant asset of the team of the user including the assets of the user
+        :type get_assets_of_team_inclusive_self: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(List[DatasetDataEnriched], status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_name',
+            'shared',
+            'exact',
+            'get_assets_of_team',
+            'get_assets_of_team_inclusive_self',
+            'page_size',
+            'page_offset'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method get_tags_by_dataset_id" % key
+                    " to method get_datasets_enriched_query_by_name" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `get_tags_by_dataset_id`")  # noqa: E501
+            _params[_key] = _val
+        del _params['kwargs']
 
-        collection_formats = {}
+        _collection_formats = {}
 
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_name']:
+            _path_params['datasetName'] = _params['dataset_name']
+
+
+        # process the query parameters
+        _query_params = []
+        if _params.get('shared') is not None:  # noqa: E501
+            _query_params.append((
+                'shared',
+                _params['shared'].value if hasattr(_params['shared'], 'value') else _params['shared']
+            ))
+
+        if _params.get('exact') is not None:  # noqa: E501
+            _query_params.append((
+                'exact',
+                _params['exact'].value if hasattr(_params['exact'], 'value') else _params['exact']
+            ))
+
+        if _params.get('get_assets_of_team') is not None:  # noqa: E501
+            _query_params.append((
+                'getAssetsOfTeam',
+                _params['get_assets_of_team'].value if hasattr(_params['get_assets_of_team'], 'value') else _params['get_assets_of_team']
+            ))
+
+        if _params.get('get_assets_of_team_inclusive_self') is not None:  # noqa: E501
+            _query_params.append((
+                'getAssetsOfTeamInclusiveSelf',
+                _params['get_assets_of_team_inclusive_self'].value if hasattr(_params['get_assets_of_team_inclusive_self'], 'value') else _params['get_assets_of_team_inclusive_self']
+            ))
+
+        if _params.get('page_size') is not None:  # noqa: E501
+            _query_params.append((
+                'pageSize',
+                _params['page_size'].value if hasattr(_params['page_size'], 'value') else _params['page_size']
+            ))
+
+        if _params.get('page_offset') is not None:  # noqa: E501
+            _query_params.append((
+                'pageOffset',
+                _params['page_offset'].value if hasattr(_params['page_offset'], 'value') else _params['page_offset']
+            ))
+
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags', 'GET',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='list[TagData]',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-    def perform_tag_arithmetics(self, body, dataset_id, **kwargs):  # noqa: E501
-        """perform_tag_arithmetics  # noqa: E501
+        _response_types_map = {
+            '200': "List[DatasetDataEnriched]",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
 
-        performs tag arithmetics to compute a new bitmask out of two existing tags and optionally create a tag for it  # noqa: E501
+        return self.api_client.call_api(
+            '/v1/datasets/enriched/query/name/{datasetName}', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def get_datasets_query_by_name(self, dataset_name : constr(strict=True, min_length=1), shared : Annotated[Optional[StrictBool], Field(description="if set, only returns the datasets which have been shared with the user")] = None, exact : Annotated[Optional[StrictBool], Field(description="if set, only returns the datasets which match the name exactly (not just by prefix)")] = None, get_assets_of_team : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user")] = None, get_assets_of_team_inclusive_self : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user including the assets of the user")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> List[DatasetData]:  # noqa: E501
+        """get_datasets_query_by_name  # noqa: E501
+
+        Query for datasets by their name prefix unless exact flag is set  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.perform_tag_arithmetics(body, dataset_id, async_req=True)
-        >>> result = thread.get()
 
-        :param async_req bool
-        :param TagArithmeticsRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: TagArithmeticsResponse
+        >>> thread = api.get_datasets_query_by_name(dataset_name, shared, exact, get_assets_of_team, get_assets_of_team_inclusive_self, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_name: (required)
+        :type dataset_name: str
+        :param shared: if set, only returns the datasets which have been shared with the user
+        :type shared: bool
+        :param exact: if set, only returns the datasets which match the name exactly (not just by prefix)
+        :type exact: bool
+        :param get_assets_of_team: if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user
+        :type get_assets_of_team: bool
+        :param get_assets_of_team_inclusive_self: if this flag is true, we get the relevant asset of the team of the user including the assets of the user
+        :type get_assets_of_team_inclusive_self: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: List[DatasetData]
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.perform_tag_arithmetics_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.perform_tag_arithmetics_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-            return data
-
-    def perform_tag_arithmetics_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501
-        """perform_tag_arithmetics  # noqa: E501
-
-        performs tag arithmetics to compute a new bitmask out of two existing tags and optionally create a tag for it  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.perform_tag_arithmetics_with_http_info(body, dataset_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param TagArithmeticsRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: TagArithmeticsResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body', 'dataset_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the get_datasets_query_by_name_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.get_datasets_query_by_name_with_http_info(dataset_name, shared, exact, get_assets_of_team, get_assets_of_team_inclusive_self, page_size, page_offset, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def get_datasets_query_by_name_with_http_info(self, dataset_name : constr(strict=True, min_length=1), shared : Annotated[Optional[StrictBool], Field(description="if set, only returns the datasets which have been shared with the user")] = None, exact : Annotated[Optional[StrictBool], Field(description="if set, only returns the datasets which match the name exactly (not just by prefix)")] = None, get_assets_of_team : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user")] = None, get_assets_of_team_inclusive_self : Annotated[Optional[StrictBool], Field(description="if this flag is true, we get the relevant asset of the team of the user including the assets of the user")] = None, page_size : Annotated[Optional[conint(strict=True, ge=1)], Field(description="pagination size/limit of the number of samples to return")] = None, page_offset : Annotated[Optional[conint(strict=True, ge=0)], Field(description="pagination offset")] = None, **kwargs) -> ApiResponse:  # noqa: E501
+        """get_datasets_query_by_name  # noqa: E501
+
+        Query for datasets by their name prefix unless exact flag is set  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.get_datasets_query_by_name_with_http_info(dataset_name, shared, exact, get_assets_of_team, get_assets_of_team_inclusive_self, page_size, page_offset, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_name: (required)
+        :type dataset_name: str
+        :param shared: if set, only returns the datasets which have been shared with the user
+        :type shared: bool
+        :param exact: if set, only returns the datasets which match the name exactly (not just by prefix)
+        :type exact: bool
+        :param get_assets_of_team: if this flag is true, we get the relevant asset of the team of the user rather than the assets of the user
+        :type get_assets_of_team: bool
+        :param get_assets_of_team_inclusive_self: if this flag is true, we get the relevant asset of the team of the user including the assets of the user
+        :type get_assets_of_team_inclusive_self: bool
+        :param page_size: pagination size/limit of the number of samples to return
+        :type page_size: int
+        :param page_offset: pagination offset
+        :type page_offset: int
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: tuple(List[DatasetData], status_code(int), headers(HTTPHeaderDict))
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_name',
+            'shared',
+            'exact',
+            'get_assets_of_team',
+            'get_assets_of_team_inclusive_self',
+            'page_size',
+            'page_offset'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method perform_tag_arithmetics" % key
+                    " to method get_datasets_query_by_name" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `perform_tag_arithmetics`")  # noqa: E501
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `perform_tag_arithmetics`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
+            _params[_key] = _val
+        del _params['kwargs']
 
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
+        _collection_formats = {}
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
-
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/arithmetics', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='TagArithmeticsResponse',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def perform_tag_arithmetics_bitmask(self, body, dataset_id, **kwargs):  # noqa: E501
-        """perform_tag_arithmetics_bitmask  # noqa: E501
-
-        Performs tag arithmetics to compute a new bitmask out of two existing tags. Does not create a new tag regardless if newTagName is provided  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.perform_tag_arithmetics_bitmask(body, dataset_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param TagArithmeticsRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: TagBitMaskResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-        kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.perform_tag_arithmetics_bitmask_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.perform_tag_arithmetics_bitmask_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-            return data
-
-    def perform_tag_arithmetics_bitmask_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501
-        """perform_tag_arithmetics_bitmask  # noqa: E501
-
-        Performs tag arithmetics to compute a new bitmask out of two existing tags. Does not create a new tag regardless if newTagName is provided  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.perform_tag_arithmetics_bitmask_with_http_info(body, dataset_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param TagArithmeticsRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: TagBitMaskResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body', 'dataset_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method perform_tag_arithmetics_bitmask" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `perform_tag_arithmetics_bitmask`")  # noqa: E501
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `perform_tag_arithmetics_bitmask`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_name']:
+            _path_params['datasetName'] = _params['dataset_name']
+
+
+        # process the query parameters
+        _query_params = []
+        if _params.get('shared') is not None:  # noqa: E501
+            _query_params.append((
+                'shared',
+                _params['shared'].value if hasattr(_params['shared'], 'value') else _params['shared']
+            ))
+
+        if _params.get('exact') is not None:  # noqa: E501
+            _query_params.append((
+                'exact',
+                _params['exact'].value if hasattr(_params['exact'], 'value') else _params['exact']
+            ))
+
+        if _params.get('get_assets_of_team') is not None:  # noqa: E501
+            _query_params.append((
+                'getAssetsOfTeam',
+                _params['get_assets_of_team'].value if hasattr(_params['get_assets_of_team'], 'value') else _params['get_assets_of_team']
+            ))
+
+        if _params.get('get_assets_of_team_inclusive_self') is not None:  # noqa: E501
+            _query_params.append((
+                'getAssetsOfTeamInclusiveSelf',
+                _params['get_assets_of_team_inclusive_self'].value if hasattr(_params['get_assets_of_team_inclusive_self'], 'value') else _params['get_assets_of_team_inclusive_self']
+            ))
+
+        if _params.get('page_size') is not None:  # noqa: E501
+            _query_params.append((
+                'pageSize',
+                _params['page_size'].value if hasattr(_params['page_size'], 'value') else _params['page_size']
+            ))
+
+        if _params.get('page_offset') is not None:  # noqa: E501
+            _query_params.append((
+                'pageOffset',
+                _params['page_offset'].value if hasattr(_params['page_offset'], 'value') else _params['page_offset']
+            ))
+
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        _response_types_map = {
+            '200': "List[DatasetData]",
+            '400': "ApiErrorResponse",
+            '401': "ApiErrorResponse",
+            '403': "ApiErrorResponse",
+            '404': "ApiErrorResponse",
+        }
 
         return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/arithmetics/bitmask', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='TagBitMaskResponse',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
-
-    def update_tag_by_tag_id(self, body, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """update_tag_by_tag_id  # noqa: E501
-
-        update information about a specific tag  # noqa: E501
+            '/v1/datasets/query/name/{datasetName}', 'GET',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def register_dataset_upload_by_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], job_status_meta : JobStatusMeta, **kwargs) -> None:  # noqa: E501
+        """register_dataset_upload_by_id  # noqa: E501
+
+        Registers a job to track the dataset upload  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.update_tag_by_tag_id(body, dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
 
-        :param async_req bool
-        :param TagUpdateRequest body: updated data for tag (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :return: None
+        >>> thread = api.register_dataset_upload_by_id(dataset_id, job_status_meta, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param job_status_meta: (required)
+        :type job_status_meta: JobStatusMeta
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: None
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.update_tag_by_tag_id_with_http_info(body, dataset_id, tag_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.update_tag_by_tag_id_with_http_info(body, dataset_id, tag_id, **kwargs)  # noqa: E501
-            return data
-
-    def update_tag_by_tag_id_with_http_info(self, body, dataset_id, tag_id, **kwargs):  # noqa: E501
-        """update_tag_by_tag_id  # noqa: E501
-
-        update information about a specific tag  # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.update_tag_by_tag_id_with_http_info(body, dataset_id, tag_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param TagUpdateRequest body: updated data for tag (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :param MongoObjectID tag_id: ObjectId of the tag (required)
-        :return: None
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
-
-        all_params = ['body', 'dataset_id', 'tag_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the register_dataset_upload_by_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.register_dataset_upload_by_id_with_http_info(dataset_id, job_status_meta, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def register_dataset_upload_by_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], job_status_meta : JobStatusMeta, **kwargs) -> ApiResponse:  # noqa: E501
+        """register_dataset_upload_by_id  # noqa: E501
+
+        Registers a job to track the dataset upload  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.register_dataset_upload_by_id_with_http_info(dataset_id, job_status_meta, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param job_status_meta: (required)
+        :type job_status_meta: JobStatusMeta
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: None
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'job_status_meta'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
                     "Got an unexpected keyword argument '%s'"
-                    " to method update_tag_by_tag_id" % key
+                    " to method register_dataset_upload_by_id" % _key
                 )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `update_tag_by_tag_id`")  # noqa: E501
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `update_tag_by_tag_id`")  # noqa: E501
-        # verify the required parameter 'tag_id' is set
-        if self.api_client.client_side_validation and ('tag_id' not in params or
-                                                       params['tag_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `tag_id` when calling `update_tag_by_tag_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-        if 'tag_id' in params:
-            path_params['tagId'] = params['tag_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
-            ['application/json'])  # noqa: E501
+            _params[_key] = _val
+        del _params['kwargs']
+
+        _collection_formats = {}
+
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        if _params['job_status_meta'] is not None:
+            _body_params = _params['job_status_meta']
 
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        # set the HTTP header `Content-Type`
+        _content_types_list = _params.get('_content_type',
+            self.api_client.select_header_content_type(
+                ['application/json']))
+        if _content_types_list:
+                _header_params['Content-Type'] = _content_types_list
 
-        return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/{tagId}', 'PUT',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type=None,  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-    def upsize_tags_by_dataset_id(self, body, dataset_id, **kwargs):  # noqa: E501
-        """upsize_tags_by_dataset_id  # noqa: E501
+        _response_types_map = {}
 
-        Upsize all tags for the dataset to the current size of the dataset. Use this after adding more samples to a dataset with an initial-tag. | Creates a new tag holding all samples which are not yet in the initial-tag.   # noqa: E501
+        return self.api_client.call_api(
+            '/v1/datasets/{datasetId}/registerDatasetUpload', 'PUT',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
+
+    @validate_arguments
+    def update_dataset_by_id(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], dataset_update_request : Annotated[DatasetUpdateRequest, Field(..., description="updated data for dataset")], **kwargs) -> None:  # noqa: E501
+        """update_dataset_by_id  # noqa: E501
+
+        Update a specific dataset  # noqa: E501
         This method makes a synchronous HTTP request by default. To make an
         asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.upsize_tags_by_dataset_id(body, dataset_id, async_req=True)
-        >>> result = thread.get()
 
-        :param async_req bool
-        :param TagUpsizeRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: CreateEntityResponse
+        >>> thread = api.update_dataset_by_id(dataset_id, dataset_update_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param dataset_update_request: updated data for dataset (required)
+        :type dataset_update_request: DatasetUpdateRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :return: Returns the result object.
                  If the method is called asynchronously,
                  returns the request thread.
+        :rtype: None
         """
         kwargs['_return_http_data_only'] = True
-        if kwargs.get('async_req'):
-            return self.upsize_tags_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-        else:
-            (data) = self.upsize_tags_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501
-            return data
+        if '_preload_content' in kwargs:
+            raise ValueError("Error! Please call the update_dataset_by_id_with_http_info method with `_preload_content` instead and obtain raw data from ApiResponse.raw_data")
+        return self.update_dataset_by_id_with_http_info(dataset_id, dataset_update_request, **kwargs)  # noqa: E501
+
+    @validate_arguments
+    def update_dataset_by_id_with_http_info(self, dataset_id : Annotated[constr(strict=True), Field(..., description="ObjectId of the dataset")], dataset_update_request : Annotated[DatasetUpdateRequest, Field(..., description="updated data for dataset")], **kwargs) -> ApiResponse:  # noqa: E501
+        """update_dataset_by_id  # noqa: E501
+
+        Update a specific dataset  # noqa: E501
+        This method makes a synchronous HTTP request by default. To make an
+        asynchronous HTTP request, please pass async_req=True
+
+        >>> thread = api.update_dataset_by_id_with_http_info(dataset_id, dataset_update_request, async_req=True)
+        >>> result = thread.get()
+
+        :param dataset_id: ObjectId of the dataset (required)
+        :type dataset_id: str
+        :param dataset_update_request: updated data for dataset (required)
+        :type dataset_update_request: DatasetUpdateRequest
+        :param async_req: Whether to execute the request asynchronously.
+        :type async_req: bool, optional
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the 
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
+        :type _preload_content: bool, optional
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :type _return_http_data_only: bool, optional
+        :param _request_timeout: timeout setting for this request. If one
+                                 number provided, it will be total request
+                                 timeout. It can also be a pair (tuple) of
+                                 (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_auth: dict, optional
+        :type _content_type: string, optional: force content-type for the request
+        :return: Returns the result object.
+                 If the method is called asynchronously,
+                 returns the request thread.
+        :rtype: None
+        """
+
+        _params = locals()
+
+        _all_params = [
+            'dataset_id',
+            'dataset_update_request'
+        ]
+        _all_params.extend(
+            [
+                'async_req',
+                '_return_http_data_only',
+                '_preload_content',
+                '_request_timeout',
+                '_request_auth',
+                '_content_type',
+                '_headers'
+            ]
+        )
+
+        # validate the arguments
+        for _key, _val in _params['kwargs'].items():
+            if _key not in _all_params:
+                raise ApiTypeError(
+                    "Got an unexpected keyword argument '%s'"
+                    " to method update_dataset_by_id" % _key
+                )
+            _params[_key] = _val
+        del _params['kwargs']
 
-    def upsize_tags_by_dataset_id_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501
-        """upsize_tags_by_dataset_id  # noqa: E501
+        _collection_formats = {}
 
-        Upsize all tags for the dataset to the current size of the dataset. Use this after adding more samples to a dataset with an initial-tag. | Creates a new tag holding all samples which are not yet in the initial-tag.   # noqa: E501
-        This method makes a synchronous HTTP request by default. To make an
-        asynchronous HTTP request, please pass async_req=True
-        >>> thread = api.upsize_tags_by_dataset_id_with_http_info(body, dataset_id, async_req=True)
-        >>> result = thread.get()
-
-        :param async_req bool
-        :param TagUpsizeRequest body: (required)
-        :param MongoObjectID dataset_id: ObjectId of the dataset (required)
-        :return: CreateEntityResponse
-                 If the method is called asynchronously,
-                 returns the request thread.
-        """
+        # process the path parameters
+        _path_params = {}
+        if _params['dataset_id']:
+            _path_params['datasetId'] = _params['dataset_id']
+
+
+        # process the query parameters
+        _query_params = []
+        # process the header parameters
+        _header_params = dict(_params.get('_headers', {}))
+        # process the form parameters
+        _form_params = []
+        _files = {}
+        # process the body parameter
+        _body_params = None
+        if _params['dataset_update_request'] is not None:
+            _body_params = _params['dataset_update_request']
 
-        all_params = ['body', 'dataset_id']  # noqa: E501
-        all_params.append('async_req')
-        all_params.append('_return_http_data_only')
-        all_params.append('_preload_content')
-        all_params.append('_request_timeout')
-
-        params = locals()
-        for key, val in six.iteritems(params['kwargs']):
-            if key not in all_params:
-                raise TypeError(
-                    "Got an unexpected keyword argument '%s'"
-                    " to method upsize_tags_by_dataset_id" % key
-                )
-            params[key] = val
-        del params['kwargs']
-        # verify the required parameter 'body' is set
-        if self.api_client.client_side_validation and ('body' not in params or
-                                                       params['body'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `body` when calling `upsize_tags_by_dataset_id`")  # noqa: E501
-        # verify the required parameter 'dataset_id' is set
-        if self.api_client.client_side_validation and ('dataset_id' not in params or
-                                                       params['dataset_id'] is None):  # noqa: E501
-            raise ValueError("Missing the required parameter `dataset_id` when calling `upsize_tags_by_dataset_id`")  # noqa: E501
-
-        collection_formats = {}
-
-        path_params = {}
-        if 'dataset_id' in params:
-            path_params['datasetId'] = params['dataset_id']  # noqa: E501
-
-        query_params = []
-
-        header_params = {}
-
-        form_params = []
-        local_var_files = {}
-
-        body_params = None
-        if 'body' in params:
-            body_params = params['body']
-        # HTTP header `Accept`
-        header_params['Accept'] = self.api_client.select_header_accept(
+        # set the HTTP header `Accept`
+        _header_params['Accept'] = self.api_client.select_header_accept(
             ['application/json'])  # noqa: E501
 
-        # HTTP header `Content-Type`
-        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501
-            ['application/json'])  # noqa: E501
+        # set the HTTP header `Content-Type`
+        _content_types_list = _params.get('_content_type',
+            self.api_client.select_header_content_type(
+                ['application/json']))
+        if _content_types_list:
+                _header_params['Content-Type'] = _content_types_list
+
+        # authentication setting
+        _auth_settings = ['auth0Bearer', 'ApiKeyAuth']  # noqa: E501
 
-        # Authentication setting
-        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501
+        _response_types_map = {}
 
         return self.api_client.call_api(
-            '/v1/datasets/{datasetId}/tags/upsize', 'POST',
-            path_params,
-            query_params,
-            header_params,
-            body=body_params,
-            post_params=form_params,
-            files=local_var_files,
-            response_type='CreateEntityResponse',  # noqa: E501
-            auth_settings=auth_settings,
-            async_req=params.get('async_req'),
-            _return_http_data_only=params.get('_return_http_data_only'),
-            _preload_content=params.get('_preload_content', True),
-            _request_timeout=params.get('_request_timeout'),
-            collection_formats=collection_formats)
+            '/v1/datasets/{datasetId}', 'PUT',
+            _path_params,
+            _query_params,
+            _header_params,
+            body=_body_params,
+            post_params=_form_params,
+            files=_files,
+            response_types_map=_response_types_map,
+            auth_settings=_auth_settings,
+            async_req=_params.get('async_req'),
+            _return_http_data_only=_params.get('_return_http_data_only'),  # noqa: E501
+            _preload_content=_params.get('_preload_content', True),
+            _request_timeout=_params.get('_request_timeout'),
+            collection_formats=_collection_formats,
+            _request_auth=_params.get('_request_auth'))
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/api_client.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/api_client.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,115 +1,159 @@
 # coding: utf-8
+
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
-from __future__ import absolute_import
 
+import atexit
 import datetime
+from dateutil.parser import parse
 import json
 import mimetypes
 from multiprocessing.pool import ThreadPool
 import os
 import re
 import tempfile
 
-# python 2 and python 3 compatibility library
-import six
-from six.moves.urllib.parse import quote
+from urllib.parse import quote
 
 from lightly.openapi_generated.swagger_client.configuration import Configuration
+from lightly.openapi_generated.swagger_client.api_response import ApiResponse
 import lightly.openapi_generated.swagger_client.models
 from lightly.openapi_generated.swagger_client import rest
+from lightly.openapi_generated.swagger_client.exceptions import ApiValueError, ApiException
 
 
 class ApiClient(object):
-    """Generic API client for Swagger client library builds.
+    """Generic API client for OpenAPI client library builds.
 
-    Swagger generic API client. This client handles the client-
+    OpenAPI generic API client. This client handles the client-
     server communication, and is invariant across implementations. Specifics of
-    the methods and models for each application are generated from the Swagger
+    the methods and models for each application are generated from the OpenAPI
     templates.
 
-    NOTE: This class is auto generated by the swagger code generator program.
-    Ref: https://github.com/swagger-api/swagger-codegen
-    Do not edit the class manually.
-
     :param configuration: .Configuration object for this client
     :param header_name: a header to pass when making calls to the API.
     :param header_value: a header value to pass when making calls to
         the API.
     :param cookie: a cookie to include in the header when making calls
         to the API
+    :param pool_threads: The number of threads to use for async requests
+        to the API. More threads means more concurrent API requests.
     """
 
-    PRIMITIVE_TYPES = (float, bool, bytes, six.text_type) + six.integer_types
+    PRIMITIVE_TYPES = (float, bool, bytes, str, int)
     NATIVE_TYPES_MAPPING = {
         'int': int,
-        'long': int if six.PY3 else long,  # noqa: F821
+        'long': int, # TODO remove as only py3 is supported?
         'float': float,
         'str': str,
         'bool': bool,
         'date': datetime.date,
         'datetime': datetime.datetime,
         'object': object,
     }
+    _pool = None
 
     def __init__(self, configuration=None, header_name=None, header_value=None,
-                 cookie=None):
+                 cookie=None, pool_threads=1):
+        # use default configuration if none is provided
         if configuration is None:
-            configuration = Configuration()
+            configuration = Configuration.get_default()
         self.configuration = configuration
+        self.pool_threads = pool_threads
 
-        # Use the pool property to lazily initialize the ThreadPool.
-        self._pool = None
         self.rest_client = rest.RESTClientObject(configuration)
         self.default_headers = {}
         if header_name is not None:
             self.default_headers[header_name] = header_value
         self.cookie = cookie
         # Set default User-Agent.
-        self.user_agent = 'Swagger-Codegen/1.0.0/python'
+        self.user_agent = 'OpenAPI-Generator/1.0.0/python'
         self.client_side_validation = configuration.client_side_validation
 
-    def __del__(self):
-        if self._pool is not None:
+    def __enter__(self):
+        return self
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        self.close()
+
+    def close(self):
+        if self._pool:
             self._pool.close()
             self._pool.join()
+            self._pool = None
+            if hasattr(atexit, 'unregister'):
+                atexit.unregister(self.close)
 
     @property
     def pool(self):
+        """Create thread pool on first request
+         avoids instantiating unused threadpool for blocking clients.
+        """
         if self._pool is None:
-            self._pool = ThreadPool()
+            atexit.register(self.close)
+            self._pool = ThreadPool(self.pool_threads)
         return self._pool
 
     @property
     def user_agent(self):
         """User agent for this API client"""
         return self.default_headers['User-Agent']
 
     @user_agent.setter
     def user_agent(self, value):
         self.default_headers['User-Agent'] = value
 
     def set_default_header(self, header_name, header_value):
         self.default_headers[header_name] = header_value
 
+
+    _default = None
+
+    @classmethod
+    def get_default(cls):
+        """Return new instance of ApiClient.
+
+        This method returns newly created, based on default constructor,
+        object of ApiClient class or returns a copy of default
+        ApiClient.
+
+        :return: The ApiClient object.
+        """
+        if cls._default is None:
+            cls._default = ApiClient()
+        return cls._default
+
+    @classmethod
+    def set_default(cls, default):
+        """Set default instance of ApiClient.
+
+        It stores default ApiClient.
+
+        :param default: object of ApiClient.
+        """
+        cls._default = default
+
     def __call_api(
             self, resource_path, method, path_params=None,
             query_params=None, header_params=None, body=None, post_params=None,
-            files=None, response_type=None, auth_settings=None,
+            files=None, response_types_map=None, auth_settings=None,
             _return_http_data_only=None, collection_formats=None,
-            _preload_content=True, _request_timeout=None):
+            _preload_content=True, _request_timeout=None, _host=None,
+            _request_auth=None):
 
         config = self.configuration
 
         # header parameters
         header_params = header_params or {}
         header_params.update(self.default_headers)
         if self.cookie:
@@ -127,70 +171,103 @@
             for k, v in path_params:
                 # specified safe chars, encode everything
                 resource_path = resource_path.replace(
                     '{%s}' % k,
                     quote(str(v), safe=config.safe_chars_for_path_param)
                 )
 
-        # query parameters
-        if query_params:
-            query_params = self.sanitize_for_serialization(query_params)
-            query_params = self.parameters_to_tuples(query_params,
-                                                     collection_formats)
-
         # post parameters
         if post_params or files:
-            post_params = self.prepare_post_parameters(post_params, files)
+            post_params = post_params if post_params else []
             post_params = self.sanitize_for_serialization(post_params)
             post_params = self.parameters_to_tuples(post_params,
                                                     collection_formats)
+            post_params.extend(self.files_parameters(files))
 
         # auth setting
-        self.update_params_for_auth(header_params, query_params, auth_settings)
+        self.update_params_for_auth(
+            header_params, query_params, auth_settings,
+            resource_path, method, body,
+            request_auth=_request_auth)
 
         # body
         if body:
             body = self.sanitize_for_serialization(body)
 
         # request url
-        url = self.configuration.host + resource_path
+        if _host is None:
+            url = self.configuration.host + resource_path
+        else:
+            # use server/host defined in path or operation instead
+            url = _host + resource_path
 
-        # perform request and return response
-        response_data = self.request(
-            method, url, query_params=query_params, headers=header_params,
-            post_params=post_params, body=body,
-            _preload_content=_preload_content,
-            _request_timeout=_request_timeout)
+        # query parameters
+        if query_params:
+            query_params = self.sanitize_for_serialization(query_params)
+            url_query = self.parameters_to_url_query(query_params,
+                                                     collection_formats)
+            url += "?" + url_query
+
+        try:
+            # perform request and return response
+            response_data = self.request(
+                method, url,
+                query_params=query_params,
+                headers=header_params,
+                post_params=post_params, body=body,
+                _preload_content=_preload_content,
+                _request_timeout=_request_timeout)
+        except ApiException as e:
+            if e.body:
+                e.body = e.body.decode('utf-8')
+            raise e
 
         self.last_response = response_data
 
-        return_data = response_data
-        if _preload_content:
-            # deserialize response data
-            if response_type:
-                return_data = self.deserialize(response_data, response_type)
-            else:
-                return_data = None
+        return_data = None # assuming derialization is not needed
+        # data needs deserialization or returns HTTP data (deserialized) only
+        if _preload_content or _return_http_data_only:
+          response_type = response_types_map.get(str(response_data.status), None)
+
+          if response_type == "bytearray":
+              response_data.data = response_data.data
+          else:
+              match = None
+              content_type = response_data.getheader('content-type')
+              if content_type is not None:
+                  match = re.search(r"charset=([a-zA-Z\-\d]+)[\s;]?", content_type)
+              encoding = match.group(1) if match else "utf-8"
+              response_data.data = response_data.data.decode(encoding)
+
+          # deserialize response data
+          if response_type == "bytearray":
+              return_data = response_data.data
+          elif response_type:
+              return_data = self.deserialize(response_data, response_type)
+          else:
+              return_data = None
 
         if _return_http_data_only:
-            return (return_data)
+            return return_data
         else:
-            return (return_data, response_data.status,
-                    response_data.getheaders())
+            return ApiResponse(status_code = response_data.status,
+                           data = return_data,
+                           headers = response_data.getheaders(),
+                           raw_data = response_data.data)
 
     def sanitize_for_serialization(self, obj):
         """Builds a JSON POST object.
 
         If obj is None, return None.
         If obj is str, int, long, float, bool, return directly.
         If obj is datetime.datetime, datetime.date
             convert to string in iso8601 format.
         If obj is list, sanitize each element in the list.
         If obj is dict, return the dict.
-        If obj is swagger model, return the properties dict.
+        If obj is OpenAPI model, return the properties dict.
 
         :param obj: The data to serialize.
         :return: The serialized form of data.
         """
         if obj is None:
             return None
         elif isinstance(obj, self.PRIMITIVE_TYPES):
@@ -204,24 +281,22 @@
         elif isinstance(obj, (datetime.datetime, datetime.date)):
             return obj.isoformat()
 
         if isinstance(obj, dict):
             obj_dict = obj
         else:
             # Convert model obj to dict except
-            # attributes `swagger_types`, `attribute_map`
+            # attributes `openapi_types`, `attribute_map`
             # and attributes which value is not None.
             # Convert attribute name to json key in
             # model definition for request.
-            obj_dict = {obj.attribute_map[attr]: getattr(obj, attr)
-                        for attr, _ in six.iteritems(obj.swagger_types)
-                        if getattr(obj, attr) is not None}
+            obj_dict = obj.to_dict(by_alias=True)
 
         return {key: self.sanitize_for_serialization(val)
-                for key, val in six.iteritems(obj_dict)}
+                for key, val in obj_dict.items()}
 
     def deserialize(self, response, response_type):
         """Deserializes response into an object.
 
         :param response: RESTResponse object to be deserialized.
         :param response_type: class literal for
             deserialized object, or string of class name.
@@ -249,50 +324,51 @@
 
         :return: object.
         """
         if data is None:
             return None
 
         if type(klass) == str:
-            if klass.startswith('list['):
-                sub_kls = re.match(r'list\[(.*)\]', klass).group(1)
+            if klass.startswith('List['):
+                sub_kls = re.match(r'List\[(.*)]', klass).group(1)
                 return [self.__deserialize(sub_data, sub_kls)
                         for sub_data in data]
 
-            if klass.startswith('dict('):
-                sub_kls = re.match(r'dict\(([^,]*), (.*)\)', klass).group(2)
+            if klass.startswith('Dict['):
+                sub_kls = re.match(r'Dict\[([^,]*), (.*)]', klass).group(2)
                 return {k: self.__deserialize(v, sub_kls)
-                        for k, v in six.iteritems(data)}
+                        for k, v in data.items()}
 
             # convert str to class
             if klass in self.NATIVE_TYPES_MAPPING:
                 klass = self.NATIVE_TYPES_MAPPING[klass]
             else:
                 klass = getattr(lightly.openapi_generated.swagger_client.models, klass)
 
         if klass in self.PRIMITIVE_TYPES:
             return self.__deserialize_primitive(data, klass)
         elif klass == object:
             return self.__deserialize_object(data)
         elif klass == datetime.date:
             return self.__deserialize_date(data)
         elif klass == datetime.datetime:
-            return self.__deserialize_datatime(data)
+            return self.__deserialize_datetime(data)
         else:
             return self.__deserialize_model(data, klass)
 
     def call_api(self, resource_path, method,
                  path_params=None, query_params=None, header_params=None,
                  body=None, post_params=None, files=None,
-                 response_type=None, auth_settings=None, async_req=None,
-                 _return_http_data_only=None, collection_formats=None,
-                 _preload_content=True, _request_timeout=None):
+                 response_types_map=None, auth_settings=None,
+                 async_req=None, _return_http_data_only=None,
+                 collection_formats=None, _preload_content=True,
+                 _request_timeout=None, _host=None, _request_auth=None):
         """Makes the HTTP request (synchronous) and returns deserialized data.
 
-        To make an async request, set the async_req parameter.
+        To make an async_req request, set the async_req parameter.
 
         :param resource_path: Path to method endpoint.
         :param method: Method to call.
         :param path_params: Path parameters in the url.
         :param query_params: Query parameters in the url.
         :param header_params: Header parameters to be
             placed in the request header.
@@ -300,122 +376,129 @@
         :param post_params dict: Request post form parameters,
             for `application/x-www-form-urlencoded`, `multipart/form-data`.
         :param auth_settings list: Auth Settings names for the request.
         :param response: Response data type.
         :param files dict: key -> filename, value -> filepath,
             for `multipart/form-data`.
         :param async_req bool: execute request asynchronously
-        :param _return_http_data_only: response data without head status code
-                                       and headers
+        :param _return_http_data_only: response data instead of ApiResponse
+                                       object with status code, headers, etc
+        :param _preload_content: if False, the ApiResponse.data will
+                                 be set to none and raw_data will store the
+                                 HTTP response body without reading/decoding.
+                                 Default is True.
         :param collection_formats: dict of collection formats for path, query,
             header, and post parameters.
-        :param _preload_content: if False, the urllib3.HTTPResponse object will
-                                 be returned without reading/decoding response
-                                 data. Default is True.
         :param _request_timeout: timeout setting for this request. If one
                                  number provided, it will be total request
                                  timeout. It can also be a pair (tuple) of
                                  (connection, read) timeouts.
+        :param _request_auth: set to override the auth_settings for an a single
+                              request; this effectively ignores the authentication
+                              in the spec for a single request.
+        :type _request_token: dict, optional
         :return:
             If async_req parameter is True,
             the request will be called asynchronously.
             The method will return the request thread.
             If parameter async_req is False or missing,
             then the method will return the response directly.
         """
         if not async_req:
             return self.__call_api(resource_path, method,
                                    path_params, query_params, header_params,
                                    body, post_params, files,
-                                   response_type, auth_settings,
+                                   response_types_map, auth_settings,
                                    _return_http_data_only, collection_formats,
-                                   _preload_content, _request_timeout)
-        else:
-            thread = self.pool.apply_async(self.__call_api, (resource_path,
-                                           method, path_params, query_params,
-                                           header_params, body,
-                                           post_params, files,
-                                           response_type, auth_settings,
-                                           _return_http_data_only,
-                                           collection_formats,
-                                           _preload_content, _request_timeout))
-        return thread
+                                   _preload_content, _request_timeout, _host,
+                                   _request_auth)
+
+        return self.pool.apply_async(self.__call_api, (resource_path,
+                                                       method, path_params,
+                                                       query_params,
+                                                       header_params, body,
+                                                       post_params, files,
+                                                       response_types_map,
+                                                       auth_settings,
+                                                       _return_http_data_only,
+                                                       collection_formats,
+                                                       _preload_content,
+                                                       _request_timeout,
+                                                       _host, _request_auth))
 
     def request(self, method, url, query_params=None, headers=None,
                 post_params=None, body=None, _preload_content=True,
                 _request_timeout=None):
         """Makes the HTTP request using RESTClient."""
         if method == "GET":
-            return self.rest_client.GET(url,
+            return self.rest_client.get_request(url,
                                         query_params=query_params,
                                         _preload_content=_preload_content,
                                         _request_timeout=_request_timeout,
                                         headers=headers)
         elif method == "HEAD":
-            return self.rest_client.HEAD(url,
+            return self.rest_client.head_request(url,
                                          query_params=query_params,
                                          _preload_content=_preload_content,
                                          _request_timeout=_request_timeout,
                                          headers=headers)
         elif method == "OPTIONS":
-            return self.rest_client.OPTIONS(url,
+            return self.rest_client.options_request(url,
                                             query_params=query_params,
                                             headers=headers,
-                                            post_params=post_params,
                                             _preload_content=_preload_content,
-                                            _request_timeout=_request_timeout,
-                                            body=body)
+                                            _request_timeout=_request_timeout)
         elif method == "POST":
-            return self.rest_client.POST(url,
+            return self.rest_client.post_request(url,
                                          query_params=query_params,
                                          headers=headers,
                                          post_params=post_params,
                                          _preload_content=_preload_content,
                                          _request_timeout=_request_timeout,
                                          body=body)
         elif method == "PUT":
-            return self.rest_client.PUT(url,
+            return self.rest_client.put_request(url,
                                         query_params=query_params,
                                         headers=headers,
                                         post_params=post_params,
                                         _preload_content=_preload_content,
                                         _request_timeout=_request_timeout,
                                         body=body)
         elif method == "PATCH":
-            return self.rest_client.PATCH(url,
+            return self.rest_client.patch_request(url,
                                           query_params=query_params,
                                           headers=headers,
                                           post_params=post_params,
                                           _preload_content=_preload_content,
                                           _request_timeout=_request_timeout,
                                           body=body)
         elif method == "DELETE":
-            return self.rest_client.DELETE(url,
+            return self.rest_client.delete_request(url,
                                            query_params=query_params,
                                            headers=headers,
                                            _preload_content=_preload_content,
                                            _request_timeout=_request_timeout,
                                            body=body)
         else:
-            raise ValueError(
+            raise ApiValueError(
                 "http method must be `GET`, `HEAD`, `OPTIONS`,"
                 " `POST`, `PATCH`, `PUT` or `DELETE`."
             )
 
     def parameters_to_tuples(self, params, collection_formats):
         """Get parameters as list of tuples, formatting collections.
 
         :param params: Parameters as dict or list of two-tuples
         :param dict collection_formats: Parameter collection formats
         :return: Parameters as list of tuples, collections formatted
         """
         new_params = []
         if collection_formats is None:
             collection_formats = {}
-        for k, v in six.iteritems(params) if isinstance(params, dict) else params:  # noqa: E501
+        for k, v in params.items() if isinstance(params, dict) else params:  # noqa: E501
             if k in collection_formats:
                 collection_format = collection_formats[k]
                 if collection_format == 'multi':
                     new_params.extend((k, value) for value in v)
                 else:
                     if collection_format == 'ssv':
                         delimiter = ' '
@@ -427,28 +510,62 @@
                         delimiter = ','
                     new_params.append(
                         (k, delimiter.join(str(value) for value in v)))
             else:
                 new_params.append((k, v))
         return new_params
 
-    def prepare_post_parameters(self, post_params=None, files=None):
+    def parameters_to_url_query(self, params, collection_formats):
+        """Get parameters as list of tuples, formatting collections.
+
+        :param params: Parameters as dict or list of two-tuples
+        :param dict collection_formats: Parameter collection formats
+        :return: URL query string (e.g. a=Hello%20World&b=123)
+        """
+        new_params = []
+        if collection_formats is None:
+            collection_formats = {}
+        for k, v in params.items() if isinstance(params, dict) else params:  # noqa: E501
+            if isinstance(v, (int, float)):
+                v = str(v)
+            if isinstance(v, bool):
+                v = str(v).lower()
+            if isinstance(v, dict):
+                v = json.dumps(v)
+
+            if k in collection_formats:
+                collection_format = collection_formats[k]
+                if collection_format == 'multi':
+                    new_params.extend((k, value) for value in v)
+                else:
+                    if collection_format == 'ssv':
+                        delimiter = ' '
+                    elif collection_format == 'tsv':
+                        delimiter = '\t'
+                    elif collection_format == 'pipes':
+                        delimiter = '|'
+                    else:  # csv is the default
+                        delimiter = ','
+                    new_params.append(
+                        (k, delimiter.join(quote(str(value)) for value in v)))
+            else:
+                new_params.append((k, quote(str(v))))
+
+        return "&".join(["=".join(item) for item in new_params])
+
+    def files_parameters(self, files=None):
         """Builds form parameters.
 
-        :param post_params: Normal form parameters.
         :param files: File parameters.
         :return: Form parameters with files.
         """
         params = []
 
-        if post_params:
-            params = post_params
-
         if files:
-            for k, v in six.iteritems(files):
+            for k, v in files.items():
                 if not v:
                     continue
                 file_names = v if type(v) is list else [v]
                 for n in file_names:
                     with open(n, 'rb') as f:
                         filename = os.path.basename(f.name)
                         filedata = f.read()
@@ -464,60 +581,90 @@
 
         :param accepts: List of headers.
         :return: Accept (e.g. application/json).
         """
         if not accepts:
             return
 
-        accepts = [x.lower() for x in accepts]
+        for accept in accepts:
+            if re.search('json', accept, re.IGNORECASE):
+                return accept
 
-        if 'application/json' in accepts:
-            return 'application/json'
-        else:
-            return ', '.join(accepts)
+        return accepts[0]
 
     def select_header_content_type(self, content_types):
         """Returns `Content-Type` based on an array of content_types provided.
 
         :param content_types: List of content-types.
         :return: Content-Type (e.g. application/json).
         """
         if not content_types:
-            return 'application/json'
-
-        content_types = [x.lower() for x in content_types]
-
-        if 'application/json' in content_types or '*/*' in content_types:
-            return 'application/json'
-        else:
-            return content_types[0]
+            return None
 
-    def update_params_for_auth(self, headers, querys, auth_settings):
+        for content_type in content_types:
+            if re.search('json', content_type, re.IGNORECASE):
+                return content_type
+
+        return content_types[0]
+
+    def update_params_for_auth(self, headers, queries, auth_settings,
+                               resource_path, method, body,
+                               request_auth=None):
         """Updates header and query params based on authentication setting.
 
         :param headers: Header parameters dict to be updated.
-        :param querys: Query parameters tuple list to be updated.
+        :param queries: Query parameters tuple list to be updated.
         :param auth_settings: Authentication setting identifiers list.
+        :resource_path: A string representation of the HTTP request resource path.
+        :method: A string representation of the HTTP request method.
+        :body: A object representing the body of the HTTP request.
+        The object type is the return value of sanitize_for_serialization().
+        :param request_auth: if set, the provided settings will
+                             override the token in the configuration.
         """
         if not auth_settings:
             return
 
+        if request_auth:
+            self._apply_auth_params(headers, queries,
+                                    resource_path, method, body,
+                                    request_auth)
+            return
+
         for auth in auth_settings:
             auth_setting = self.configuration.auth_settings().get(auth)
             if auth_setting:
-                if not auth_setting['value']:
-                    continue
-                elif auth_setting['in'] == 'header':
-                    headers[auth_setting['key']] = auth_setting['value']
-                elif auth_setting['in'] == 'query':
-                    querys.append((auth_setting['key'], auth_setting['value']))
-                else:
-                    raise ValueError(
-                        'Authentication token must be in `query` or `header`'
-                    )
+                self._apply_auth_params(headers, queries,
+                                        resource_path, method, body,
+                                        auth_setting)
+
+    def _apply_auth_params(self, headers, queries,
+                           resource_path, method, body,
+                           auth_setting):
+        """Updates the request parameters based on a single auth_setting
+
+        :param headers: Header parameters dict to be updated.
+        :param queries: Query parameters tuple list to be updated.
+        :resource_path: A string representation of the HTTP request resource path.
+        :method: A string representation of the HTTP request method.
+        :body: A object representing the body of the HTTP request.
+        The object type is the return value of sanitize_for_serialization().
+        :param auth_setting: auth settings for the endpoint
+        """
+        if auth_setting['in'] == 'cookie':
+            headers['Cookie'] = auth_setting['value']
+        elif auth_setting['in'] == 'header':
+            if auth_setting['type'] != 'http-signature':
+                headers[auth_setting['key']] = auth_setting['value']
+        elif auth_setting['in'] == 'query':
+            queries.append((auth_setting['key'], auth_setting['value']))
+        else:
+            raise ApiValueError(
+                'Authentication token must be in `query` or `header`'
+            )
 
     def __deserialize_file(self, response):
         """Deserializes body to file
 
         Saves response body into a file in a temporary folder,
         using the filename from the `Content-Disposition` header if provided.
 
@@ -530,15 +677,15 @@
 
         content_disposition = response.getheader("Content-Disposition")
         if content_disposition:
             filename = re.search(r'filename=[\'"]?([^\'"\s]+)[\'"]?',
                                  content_disposition).group(1)
             path = os.path.join(os.path.dirname(path), filename)
 
-        with open(path, "w") as f:
+        with open(path, "wb") as f:
             f.write(response.data)
 
         return path
 
     def __deserialize_primitive(self, data, klass):
         """Deserializes string to primitive type.
 
@@ -546,94 +693,64 @@
         :param klass: class literal.
 
         :return: int, long, float, str, bool.
         """
         try:
             return klass(data)
         except UnicodeEncodeError:
-            return six.text_type(data)
+            return str(data)
         except TypeError:
             return data
 
     def __deserialize_object(self, value):
-        """Return a original value.
+        """Return an original value.
 
         :return: object.
         """
         return value
 
     def __deserialize_date(self, string):
         """Deserializes string to date.
 
         :param string: str.
         :return: date.
         """
         try:
-            from dateutil.parser import parse
             return parse(string).date()
         except ImportError:
             return string
         except ValueError:
             raise rest.ApiException(
                 status=0,
                 reason="Failed to parse `{0}` as date object".format(string)
             )
 
-    def __deserialize_datatime(self, string):
+    def __deserialize_datetime(self, string):
         """Deserializes string to datetime.
 
         The string should be in iso8601 datetime format.
 
         :param string: str.
         :return: datetime.
         """
         try:
-            from dateutil.parser import parse
             return parse(string)
         except ImportError:
             return string
         except ValueError:
             raise rest.ApiException(
                 status=0,
                 reason=(
                     "Failed to parse `{0}` as datetime object"
                     .format(string)
                 )
             )
 
-    def __hasattr(self, object, name):
-        return name in object.__class__.__dict__
-
     def __deserialize_model(self, data, klass):
         """Deserializes list or dict to model.
 
         :param data: dict, list.
         :param klass: class literal.
         :return: model object.
         """
 
-        if (not klass.swagger_types and
-                not self.__hasattr(klass, 'get_real_child_model')):
-            return data
-
-        kwargs = {}
-        if klass.swagger_types is not None:
-            for attr, attr_type in six.iteritems(klass.swagger_types):
-                if (data is not None and
-                        klass.attribute_map[attr] in data and
-                        isinstance(data, (list, dict))):
-                    value = data[klass.attribute_map[attr]]
-                    kwargs[attr] = self.__deserialize(value, attr_type)
-
-        instance = klass(**kwargs)
-
-        if (isinstance(instance, dict) and
-                klass.swagger_types is not None and
-                isinstance(data, dict)):
-            for key, value in data.items():
-                if key not in klass.swagger_types:
-                    instance[key] = value
-        if self.__hasattr(instance, 'get_real_child_model'):
-            klass_name = instance.get_real_child_model(data)
-            if klass_name:
-                instance = self.__deserialize(data, klass_name)
-        return instance
+        return klass.from_dict(data)
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/__init__.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -2,62 +2,60 @@
 
 # flake8: noqa
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
-"""
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
 
+    Do not edit the class manually.
+"""
 
-from __future__ import absolute_import
 
 # import models into model package
-from lightly.openapi_generated.swagger_client.models.access_role import AccessRole
 from lightly.openapi_generated.swagger_client.models.active_learning_score_create_request import ActiveLearningScoreCreateRequest
 from lightly.openapi_generated.swagger_client.models.active_learning_score_data import ActiveLearningScoreData
-from lightly.openapi_generated.swagger_client.models.active_learning_score_type import ActiveLearningScoreType
-from lightly.openapi_generated.swagger_client.models.active_learning_scores import ActiveLearningScores
 from lightly.openapi_generated.swagger_client.models.api_error_code import ApiErrorCode
 from lightly.openapi_generated.swagger_client.models.api_error_response import ApiErrorResponse
 from lightly.openapi_generated.swagger_client.models.async_task_data import AsyncTaskData
-from lightly.openapi_generated.swagger_client.models.bounding_box import BoundingBox
-from lightly.openapi_generated.swagger_client.models.category_id import CategoryId
-from lightly.openapi_generated.swagger_client.models.category_name import CategoryName
 from lightly.openapi_generated.swagger_client.models.configuration_data import ConfigurationData
 from lightly.openapi_generated.swagger_client.models.configuration_entry import ConfigurationEntry
 from lightly.openapi_generated.swagger_client.models.configuration_set_request import ConfigurationSetRequest
 from lightly.openapi_generated.swagger_client.models.configuration_value_data_type import ConfigurationValueDataType
+from lightly.openapi_generated.swagger_client.models.create_cf_bucket_activity_request import CreateCFBucketActivityRequest
 from lightly.openapi_generated.swagger_client.models.create_docker_worker_registry_entry_request import CreateDockerWorkerRegistryEntryRequest
 from lightly.openapi_generated.swagger_client.models.create_entity_response import CreateEntityResponse
 from lightly.openapi_generated.swagger_client.models.create_sample_with_write_urls_response import CreateSampleWithWriteUrlsResponse
+from lightly.openapi_generated.swagger_client.models.create_team_membership_request import CreateTeamMembershipRequest
 from lightly.openapi_generated.swagger_client.models.creator import Creator
 from lightly.openapi_generated.swagger_client.models.crop_data import CropData
-from lightly.openapi_generated.swagger_client.models.custom_sample_meta_data import CustomSampleMetaData
 from lightly.openapi_generated.swagger_client.models.dataset_create_request import DatasetCreateRequest
 from lightly.openapi_generated.swagger_client.models.dataset_creator import DatasetCreator
 from lightly.openapi_generated.swagger_client.models.dataset_data import DatasetData
 from lightly.openapi_generated.swagger_client.models.dataset_data_enriched import DatasetDataEnriched
 from lightly.openapi_generated.swagger_client.models.dataset_embedding_data import DatasetEmbeddingData
-from lightly.openapi_generated.swagger_client.models.dataset_name import DatasetName
-from lightly.openapi_generated.swagger_client.models.dataset_name_query import DatasetNameQuery
 from lightly.openapi_generated.swagger_client.models.dataset_type import DatasetType
 from lightly.openapi_generated.swagger_client.models.dataset_update_request import DatasetUpdateRequest
 from lightly.openapi_generated.swagger_client.models.datasource_config import DatasourceConfig
 from lightly.openapi_generated.swagger_client.models.datasource_config_azure import DatasourceConfigAzure
+from lightly.openapi_generated.swagger_client.models.datasource_config_azure_all_of import DatasourceConfigAzureAllOf
 from lightly.openapi_generated.swagger_client.models.datasource_config_base import DatasourceConfigBase
 from lightly.openapi_generated.swagger_client.models.datasource_config_gcs import DatasourceConfigGCS
+from lightly.openapi_generated.swagger_client.models.datasource_config_gcs_all_of import DatasourceConfigGCSAllOf
 from lightly.openapi_generated.swagger_client.models.datasource_config_lightly import DatasourceConfigLIGHTLY
 from lightly.openapi_generated.swagger_client.models.datasource_config_local import DatasourceConfigLOCAL
 from lightly.openapi_generated.swagger_client.models.datasource_config_obs import DatasourceConfigOBS
+from lightly.openapi_generated.swagger_client.models.datasource_config_obs_all_of import DatasourceConfigOBSAllOf
 from lightly.openapi_generated.swagger_client.models.datasource_config_s3 import DatasourceConfigS3
+from lightly.openapi_generated.swagger_client.models.datasource_config_s3_all_of import DatasourceConfigS3AllOf
 from lightly.openapi_generated.swagger_client.models.datasource_config_s3_delegated_access import DatasourceConfigS3DelegatedAccess
+from lightly.openapi_generated.swagger_client.models.datasource_config_s3_delegated_access_all_of import DatasourceConfigS3DelegatedAccessAllOf
 from lightly.openapi_generated.swagger_client.models.datasource_config_verify_data import DatasourceConfigVerifyData
 from lightly.openapi_generated.swagger_client.models.datasource_config_verify_data_errors import DatasourceConfigVerifyDataErrors
 from lightly.openapi_generated.swagger_client.models.datasource_processed_until_timestamp_request import DatasourceProcessedUntilTimestampRequest
 from lightly.openapi_generated.swagger_client.models.datasource_processed_until_timestamp_response import DatasourceProcessedUntilTimestampResponse
 from lightly.openapi_generated.swagger_client.models.datasource_purpose import DatasourcePurpose
 from lightly.openapi_generated.swagger_client.models.datasource_raw_samples_data import DatasourceRawSamplesData
 from lightly.openapi_generated.swagger_client.models.datasource_raw_samples_data_row import DatasourceRawSamplesDataRow
@@ -66,14 +64,15 @@
 from lightly.openapi_generated.swagger_client.models.datasource_raw_samples_predictions_data import DatasourceRawSamplesPredictionsData
 from lightly.openapi_generated.swagger_client.models.datasource_raw_samples_predictions_data_row import DatasourceRawSamplesPredictionsDataRow
 from lightly.openapi_generated.swagger_client.models.dimensionality_reduction_method import DimensionalityReductionMethod
 from lightly.openapi_generated.swagger_client.models.docker_license_information import DockerLicenseInformation
 from lightly.openapi_generated.swagger_client.models.docker_run_artifact_create_request import DockerRunArtifactCreateRequest
 from lightly.openapi_generated.swagger_client.models.docker_run_artifact_created_data import DockerRunArtifactCreatedData
 from lightly.openapi_generated.swagger_client.models.docker_run_artifact_data import DockerRunArtifactData
+from lightly.openapi_generated.swagger_client.models.docker_run_artifact_storage_location import DockerRunArtifactStorageLocation
 from lightly.openapi_generated.swagger_client.models.docker_run_artifact_type import DockerRunArtifactType
 from lightly.openapi_generated.swagger_client.models.docker_run_create_request import DockerRunCreateRequest
 from lightly.openapi_generated.swagger_client.models.docker_run_data import DockerRunData
 from lightly.openapi_generated.swagger_client.models.docker_run_log_data import DockerRunLogData
 from lightly.openapi_generated.swagger_client.models.docker_run_log_entry_data import DockerRunLogEntryData
 from lightly.openapi_generated.swagger_client.models.docker_run_log_level import DockerRunLogLevel
 from lightly.openapi_generated.swagger_client.models.docker_run_scheduled_create_request import DockerRunScheduledCreateRequest
@@ -109,102 +108,90 @@
 from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_checkpoint_callback import DockerWorkerConfigV3LightlyCheckpointCallback
 from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_collate import DockerWorkerConfigV3LightlyCollate
 from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_criterion import DockerWorkerConfigV3LightlyCriterion
 from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_loader import DockerWorkerConfigV3LightlyLoader
 from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_model import DockerWorkerConfigV3LightlyModel
 from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_optimizer import DockerWorkerConfigV3LightlyOptimizer
 from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_trainer import DockerWorkerConfigV3LightlyTrainer
-from lightly.openapi_generated.swagger_client.models.docker_worker_labels import DockerWorkerLabels
-from lightly.openapi_generated.swagger_client.models.docker_worker_name import DockerWorkerName
 from lightly.openapi_generated.swagger_client.models.docker_worker_registry_entry_data import DockerWorkerRegistryEntryData
 from lightly.openapi_generated.swagger_client.models.docker_worker_state import DockerWorkerState
 from lightly.openapi_generated.swagger_client.models.docker_worker_type import DockerWorkerType
-from lightly.openapi_generated.swagger_client.models.embedding2d_coordinates import Embedding2dCoordinates
 from lightly.openapi_generated.swagger_client.models.embedding2d_create_request import Embedding2dCreateRequest
 from lightly.openapi_generated.swagger_client.models.embedding2d_data import Embedding2dData
 from lightly.openapi_generated.swagger_client.models.embedding_data import EmbeddingData
 from lightly.openapi_generated.swagger_client.models.file_name_format import FileNameFormat
 from lightly.openapi_generated.swagger_client.models.file_output_format import FileOutputFormat
 from lightly.openapi_generated.swagger_client.models.filename_and_read_url import FilenameAndReadUrl
-from lightly.openapi_generated.swagger_client.models.filename_and_read_urls import FilenameAndReadUrls
-from lightly.openapi_generated.swagger_client.models.general_job_result import GeneralJobResult
 from lightly.openapi_generated.swagger_client.models.image_type import ImageType
 from lightly.openapi_generated.swagger_client.models.initial_tag_create_request import InitialTagCreateRequest
 from lightly.openapi_generated.swagger_client.models.job_result_type import JobResultType
 from lightly.openapi_generated.swagger_client.models.job_state import JobState
 from lightly.openapi_generated.swagger_client.models.job_status_data import JobStatusData
 from lightly.openapi_generated.swagger_client.models.job_status_data_result import JobStatusDataResult
 from lightly.openapi_generated.swagger_client.models.job_status_meta import JobStatusMeta
 from lightly.openapi_generated.swagger_client.models.job_status_upload_method import JobStatusUploadMethod
 from lightly.openapi_generated.swagger_client.models.jobs_data import JobsData
 from lightly.openapi_generated.swagger_client.models.label_box_data_row import LabelBoxDataRow
-from lightly.openapi_generated.swagger_client.models.label_box_data_rows import LabelBoxDataRows
 from lightly.openapi_generated.swagger_client.models.label_box_v4_data_row import LabelBoxV4DataRow
-from lightly.openapi_generated.swagger_client.models.label_box_v4_data_rows import LabelBoxV4DataRows
 from lightly.openapi_generated.swagger_client.models.label_studio_task import LabelStudioTask
 from lightly.openapi_generated.swagger_client.models.label_studio_task_data import LabelStudioTaskData
-from lightly.openapi_generated.swagger_client.models.label_studio_tasks import LabelStudioTasks
+from lightly.openapi_generated.swagger_client.models.lightly_docker_selection_method import LightlyDockerSelectionMethod
 from lightly.openapi_generated.swagger_client.models.lightly_model_v2 import LightlyModelV2
 from lightly.openapi_generated.swagger_client.models.lightly_model_v3 import LightlyModelV3
 from lightly.openapi_generated.swagger_client.models.lightly_trainer_precision_v2 import LightlyTrainerPrecisionV2
 from lightly.openapi_generated.swagger_client.models.lightly_trainer_precision_v3 import LightlyTrainerPrecisionV3
-from lightly.openapi_generated.swagger_client.models.mongo_object_id import MongoObjectID
-from lightly.openapi_generated.swagger_client.models.object_id import ObjectId
-from lightly.openapi_generated.swagger_client.models.path_safe_name import PathSafeName
 from lightly.openapi_generated.swagger_client.models.prediction_singleton import PredictionSingleton
 from lightly.openapi_generated.swagger_client.models.prediction_singleton_base import PredictionSingletonBase
 from lightly.openapi_generated.swagger_client.models.prediction_singleton_classification import PredictionSingletonClassification
+from lightly.openapi_generated.swagger_client.models.prediction_singleton_classification_all_of import PredictionSingletonClassificationAllOf
 from lightly.openapi_generated.swagger_client.models.prediction_singleton_instance_segmentation import PredictionSingletonInstanceSegmentation
+from lightly.openapi_generated.swagger_client.models.prediction_singleton_instance_segmentation_all_of import PredictionSingletonInstanceSegmentationAllOf
 from lightly.openapi_generated.swagger_client.models.prediction_singleton_keypoint_detection import PredictionSingletonKeypointDetection
+from lightly.openapi_generated.swagger_client.models.prediction_singleton_keypoint_detection_all_of import PredictionSingletonKeypointDetectionAllOf
 from lightly.openapi_generated.swagger_client.models.prediction_singleton_object_detection import PredictionSingletonObjectDetection
+from lightly.openapi_generated.swagger_client.models.prediction_singleton_object_detection_all_of import PredictionSingletonObjectDetectionAllOf
 from lightly.openapi_generated.swagger_client.models.prediction_singleton_semantic_segmentation import PredictionSingletonSemanticSegmentation
-from lightly.openapi_generated.swagger_client.models.prediction_singletons import PredictionSingletons
+from lightly.openapi_generated.swagger_client.models.prediction_singleton_semantic_segmentation_all_of import PredictionSingletonSemanticSegmentationAllOf
 from lightly.openapi_generated.swagger_client.models.prediction_task_schema import PredictionTaskSchema
 from lightly.openapi_generated.swagger_client.models.prediction_task_schema_category import PredictionTaskSchemaCategory
-from lightly.openapi_generated.swagger_client.models.probabilities import Probabilities
 from lightly.openapi_generated.swagger_client.models.questionnaire_data import QuestionnaireData
-from lightly.openapi_generated.swagger_client.models.read_url import ReadUrl
-from lightly.openapi_generated.swagger_client.models.redirected_read_url import RedirectedReadUrl
 from lightly.openapi_generated.swagger_client.models.s3_region import S3Region
-from lightly.openapi_generated.swagger_client.models.s3_server_side_encryption_kms_key import S3ServerSideEncryptionKMSKey
 from lightly.openapi_generated.swagger_client.models.sama_task import SamaTask
 from lightly.openapi_generated.swagger_client.models.sama_task_data import SamaTaskData
-from lightly.openapi_generated.swagger_client.models.sama_tasks import SamaTasks
 from lightly.openapi_generated.swagger_client.models.sample_create_request import SampleCreateRequest
 from lightly.openapi_generated.swagger_client.models.sample_data import SampleData
 from lightly.openapi_generated.swagger_client.models.sample_data_modes import SampleDataModes
 from lightly.openapi_generated.swagger_client.models.sample_meta_data import SampleMetaData
 from lightly.openapi_generated.swagger_client.models.sample_partial_mode import SamplePartialMode
 from lightly.openapi_generated.swagger_client.models.sample_sort_by import SampleSortBy
 from lightly.openapi_generated.swagger_client.models.sample_type import SampleType
 from lightly.openapi_generated.swagger_client.models.sample_update_request import SampleUpdateRequest
 from lightly.openapi_generated.swagger_client.models.sample_write_urls import SampleWriteUrls
 from lightly.openapi_generated.swagger_client.models.sampling_config import SamplingConfig
 from lightly.openapi_generated.swagger_client.models.sampling_config_stopping_condition import SamplingConfigStoppingCondition
 from lightly.openapi_generated.swagger_client.models.sampling_create_request import SamplingCreateRequest
 from lightly.openapi_generated.swagger_client.models.sampling_method import SamplingMethod
-from lightly.openapi_generated.swagger_client.models.score import Score
 from lightly.openapi_generated.swagger_client.models.sector import Sector
 from lightly.openapi_generated.swagger_client.models.selection_config import SelectionConfig
 from lightly.openapi_generated.swagger_client.models.selection_config_entry import SelectionConfigEntry
 from lightly.openapi_generated.swagger_client.models.selection_config_entry_input import SelectionConfigEntryInput
 from lightly.openapi_generated.swagger_client.models.selection_config_entry_strategy import SelectionConfigEntryStrategy
 from lightly.openapi_generated.swagger_client.models.selection_input_predictions_name import SelectionInputPredictionsName
 from lightly.openapi_generated.swagger_client.models.selection_input_type import SelectionInputType
 from lightly.openapi_generated.swagger_client.models.selection_strategy_threshold_operation import SelectionStrategyThresholdOperation
 from lightly.openapi_generated.swagger_client.models.selection_strategy_type import SelectionStrategyType
+from lightly.openapi_generated.swagger_client.models.service_account_basic_data import ServiceAccountBasicData
 from lightly.openapi_generated.swagger_client.models.set_embeddings_is_processed_flag_by_id_body_request import SetEmbeddingsIsProcessedFlagByIdBodyRequest
 from lightly.openapi_generated.swagger_client.models.shared_access_config_create_request import SharedAccessConfigCreateRequest
 from lightly.openapi_generated.swagger_client.models.shared_access_config_data import SharedAccessConfigData
 from lightly.openapi_generated.swagger_client.models.shared_access_type import SharedAccessType
 from lightly.openapi_generated.swagger_client.models.tag_active_learning_scores_data import TagActiveLearningScoresData
 from lightly.openapi_generated.swagger_client.models.tag_arithmetics_operation import TagArithmeticsOperation
 from lightly.openapi_generated.swagger_client.models.tag_arithmetics_request import TagArithmeticsRequest
 from lightly.openapi_generated.swagger_client.models.tag_arithmetics_response import TagArithmeticsResponse
-from lightly.openapi_generated.swagger_client.models.tag_bit_mask_data import TagBitMaskData
 from lightly.openapi_generated.swagger_client.models.tag_bit_mask_response import TagBitMaskResponse
 from lightly.openapi_generated.swagger_client.models.tag_change_data import TagChangeData
 from lightly.openapi_generated.swagger_client.models.tag_change_data_arithmetics import TagChangeDataArithmetics
 from lightly.openapi_generated.swagger_client.models.tag_change_data_initial import TagChangeDataInitial
 from lightly.openapi_generated.swagger_client.models.tag_change_data_metadata import TagChangeDataMetadata
 from lightly.openapi_generated.swagger_client.models.tag_change_data_operation_method import TagChangeDataOperationMethod
 from lightly.openapi_generated.swagger_client.models.tag_change_data_rename import TagChangeDataRename
@@ -212,22 +199,19 @@
 from lightly.openapi_generated.swagger_client.models.tag_change_data_samples import TagChangeDataSamples
 from lightly.openapi_generated.swagger_client.models.tag_change_data_scatterplot import TagChangeDataScatterplot
 from lightly.openapi_generated.swagger_client.models.tag_change_data_upsize import TagChangeDataUpsize
 from lightly.openapi_generated.swagger_client.models.tag_change_entry import TagChangeEntry
 from lightly.openapi_generated.swagger_client.models.tag_create_request import TagCreateRequest
 from lightly.openapi_generated.swagger_client.models.tag_creator import TagCreator
 from lightly.openapi_generated.swagger_client.models.tag_data import TagData
-from lightly.openapi_generated.swagger_client.models.tag_filenames_data import TagFilenamesData
-from lightly.openapi_generated.swagger_client.models.tag_name import TagName
 from lightly.openapi_generated.swagger_client.models.tag_update_request import TagUpdateRequest
 from lightly.openapi_generated.swagger_client.models.tag_upsize_request import TagUpsizeRequest
-from lightly.openapi_generated.swagger_client.models.task_name import TaskName
 from lightly.openapi_generated.swagger_client.models.task_type import TaskType
 from lightly.openapi_generated.swagger_client.models.team_basic_data import TeamBasicData
+from lightly.openapi_generated.swagger_client.models.team_data import TeamData
 from lightly.openapi_generated.swagger_client.models.team_role import TeamRole
-from lightly.openapi_generated.swagger_client.models.timestamp import Timestamp
-from lightly.openapi_generated.swagger_client.models.timestamp_seconds import TimestampSeconds
 from lightly.openapi_generated.swagger_client.models.trigger2d_embedding_job_request import Trigger2dEmbeddingJobRequest
 from lightly.openapi_generated.swagger_client.models.update_docker_worker_registry_entry_request import UpdateDockerWorkerRegistryEntryRequest
-from lightly.openapi_generated.swagger_client.models.version_number import VersionNumber
+from lightly.openapi_generated.swagger_client.models.update_team_membership_request import UpdateTeamMembershipRequest
+from lightly.openapi_generated.swagger_client.models.user_type import UserType
 from lightly.openapi_generated.swagger_client.models.video_frame_data import VideoFrameData
 from lightly.openapi_generated.swagger_client.models.write_csv_url_data import WriteCSVUrlData
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/access_role.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/service_account_basic_data.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,95 +1,84 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class AccessRole(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, StrictStr, conint, constr
 
+class ServiceAccountBasicData(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    ServiceAccountBasicData
     """
-    swagger_types = {
-    }
+    id: StrictStr = Field(...)
+    name: StrictStr = Field(...)
+    token: constr(strict=True, min_length=5) = Field(..., description="The user's token to be used for authentication via token querystring")
+    created_at: conint(strict=True, ge=0) = Field(..., alias="createdAt", description="unix timestamp in milliseconds")
+    __properties = ["id", "name", "token", "createdAt"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """AccessRole - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(AccessRole, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, AccessRole):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, AccessRole):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> ServiceAccountBasicData:
+        """Create an instance of ServiceAccountBasicData from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> ServiceAccountBasicData:
+        """Create an instance of ServiceAccountBasicData from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return ServiceAccountBasicData.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in ServiceAccountBasicData) in the input: " + str(obj))
+
+        _obj = ServiceAccountBasicData.parse_obj({
+            "id": obj.get("id"),
+            "name": obj.get("name"),
+            "token": obj.get("token"),
+            "created_at": obj.get("createdAt")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/active_learning_scores.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/create_cf_bucket_activity_request.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,95 +1,80 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class ActiveLearningScores(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, StrictStr
 
+class CreateCFBucketActivityRequest(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    CreateCFBucketActivityRequest
     """
-    swagger_types = {
-    }
+    name: StrictStr = Field(...)
+    bucket: StrictStr = Field(...)
+    __properties = ["name", "bucket"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """ActiveLearningScores - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(ActiveLearningScores, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, ActiveLearningScores):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, ActiveLearningScores):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> CreateCFBucketActivityRequest:
+        """Create an instance of CreateCFBucketActivityRequest from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> CreateCFBucketActivityRequest:
+        """Create an instance of CreateCFBucketActivityRequest from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return CreateCFBucketActivityRequest.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in CreateCFBucketActivityRequest) in the input: " + str(obj))
+
+        _obj = CreateCFBucketActivityRequest.parse_obj({
+            "name": obj.get("name"),
+            "bucket": obj.get("bucket")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/bounding_box.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/dataset_update_request.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,95 +1,85 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class BoundingBox(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, constr, validator
 
+class DatasetUpdateRequest(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DatasetUpdateRequest
     """
-    swagger_types = {
-    }
+    name: constr(strict=True, min_length=3) = Field(...)
+    __properties = ["name"]
 
-    attribute_map = {
-    }
+    @validator('name')
+    def name_validate_regular_expression(cls, value):
+        """Validates the regular expression"""
+        if not re.match(r"^[a-zA-Z0-9][a-zA-Z0-9 _-]+$", value):
+            raise ValueError(r"must validate the regular expression /^[a-zA-Z0-9][a-zA-Z0-9 _-]+$/")
+        return value
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def __init__(self, _configuration=None):  # noqa: E501
-        """BoundingBox - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(BoundingBox, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, BoundingBox):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, BoundingBox):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DatasetUpdateRequest:
+        """Create an instance of DatasetUpdateRequest from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DatasetUpdateRequest:
+        """Create an instance of DatasetUpdateRequest from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DatasetUpdateRequest.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DatasetUpdateRequest) in the input: " + str(obj))
+
+        _obj = DatasetUpdateRequest.parse_obj({
+            "name": obj.get("name")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/category_id.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/auth0_on_sign_up_request_user.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,95 +1,90 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class CategoryId(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from typing import Optional
+from pydantic import Extra,  BaseModel, Field, StrictStr
 
+class Auth0OnSignUpRequestUser(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    Auth0OnSignUpRequestUser
     """
-    swagger_types = {
-    }
+    user_id: StrictStr = Field(..., alias="userId")
+    email: Optional[StrictStr] = None
+    locale: Optional[StrictStr] = None
+    nickname: Optional[StrictStr] = None
+    name: Optional[StrictStr] = None
+    given_name: Optional[StrictStr] = Field(None, alias="givenName")
+    family_name: Optional[StrictStr] = Field(None, alias="familyName")
+    __properties = ["userId", "email", "locale", "nickname", "name", "givenName", "familyName"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """CategoryId - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(CategoryId, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, CategoryId):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, CategoryId):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> Auth0OnSignUpRequestUser:
+        """Create an instance of Auth0OnSignUpRequestUser from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> Auth0OnSignUpRequestUser:
+        """Create an instance of Auth0OnSignUpRequestUser from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return Auth0OnSignUpRequestUser.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in Auth0OnSignUpRequestUser) in the input: " + str(obj))
+
+        _obj = Auth0OnSignUpRequestUser.parse_obj({
+            "user_id": obj.get("userId"),
+            "email": obj.get("email"),
+            "locale": obj.get("locale"),
+            "nickname": obj.get("nickname"),
+            "name": obj.get("name"),
+            "given_name": obj.get("givenName"),
+            "family_name": obj.get("familyName")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/category_name.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_azure.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,95 +1,86 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class CategoryName(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, constr
+from lightly.openapi_generated.swagger_client.models.datasource_config_base import DatasourceConfigBase
 
+class DatasourceConfigAzure(DatasourceConfigBase):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DatasourceConfigAzure
     """
-    swagger_types = {
-    }
+    account_name: constr(strict=True, min_length=1) = Field(..., alias="accountName", description="name of the Azure Storage Account")
+    account_key: constr(strict=True, min_length=1) = Field(..., alias="accountKey", description="key of the Azure Storage Account")
+    __properties = ["id", "purpose", "type", "fullPath", "thumbSuffix", "accountName", "accountKey"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """CategoryName - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(CategoryName, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, CategoryName):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, CategoryName):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DatasourceConfigAzure:
+        """Create an instance of DatasourceConfigAzure from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DatasourceConfigAzure:
+        """Create an instance of DatasourceConfigAzure from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DatasourceConfigAzure.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DatasourceConfigAzure) in the input: " + str(obj))
+
+        _obj = DatasourceConfigAzure.parse_obj({
+            "id": obj.get("id"),
+            "purpose": obj.get("purpose"),
+            "type": obj.get("type"),
+            "full_path": obj.get("fullPath"),
+            "thumb_suffix": obj.get("thumbSuffix"),
+            "account_name": obj.get("accountName"),
+            "account_key": obj.get("accountKey")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/configuration_value_data_type.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/configuration_entry.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,107 +1,90 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
-
-
-class ConfigurationValueDataType(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
 
-    """
-    allowed enum values
-    """
-    NUMERIC_INT = "NUMERIC_INT"
-    NUMERIC_FLOAT = "NUMERIC_FLOAT"
-    CATEGORICAL_STRING = "CATEGORICAL_STRING"
-    CATEGORICAL_INT = "CATEGORICAL_INT"
-    CATEGORICAL_BOOLEAN = "CATEGORICAL_BOOLEAN"
-    CATEGORICAL_DATETIME = "CATEGORICAL_DATETIME"
-    CATEGORICAL_TIMESTAMP = "CATEGORICAL_TIMESTAMP"
-    OTHER_STRING = "OTHER_STRING"
+from typing import Any, Optional
+from pydantic import Extra,  BaseModel, Field, constr
+from lightly.openapi_generated.swagger_client.models.configuration_value_data_type import ConfigurationValueDataType
 
+class ConfigurationEntry(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    ConfigurationEntry
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """ConfigurationValueDataType - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(ConfigurationValueDataType, dict):
-            for key, value in self.items():
-                result[key] = value
+    name: constr(strict=True, min_length=1) = Field(..., description="the name of this entry which is displayed in the UI")
+    path: constr(strict=True, min_length=1) = Field(..., description="the path is the dotnotation which is used to easily access the customMetadata JSON structure of a sample e.g myArray[0].myObject.field")
+    default_value: Optional[Any] = Field(..., alias="defaultValue", description="the default value used if its not possible to extract the value using the path or if the value extracted is nullish")
+    value_data_type: ConfigurationValueDataType = Field(..., alias="valueDataType")
+    __properties = ["name", "path", "defaultValue", "valueDataType"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, ConfigurationValueDataType):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, ConfigurationValueDataType):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> ConfigurationEntry:
+        """Create an instance of ConfigurationEntry from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # set to None if default_value (nullable) is None
+        # and __fields_set__ contains the field
+        if self.default_value is None and "default_value" in self.__fields_set__:
+            _dict['defaultValue' if by_alias else 'default_value'] = None
+
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> ConfigurationEntry:
+        """Create an instance of ConfigurationEntry from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return ConfigurationEntry.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in ConfigurationEntry) in the input: " + str(obj))
+
+        _obj = ConfigurationEntry.parse_obj({
+            "name": obj.get("name"),
+            "path": obj.get("path"),
+            "default_value": obj.get("defaultValue"),
+            "value_data_type": obj.get("valueDataType")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/create_entity_response.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/shared_access_config_create_request.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,124 +1,86 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
+from typing import List, Optional
+from pydantic import Extra,  BaseModel, Field, StrictStr, conlist
+from lightly.openapi_generated.swagger_client.models.creator import Creator
+from lightly.openapi_generated.swagger_client.models.shared_access_type import SharedAccessType
 
-class CreateEntityResponse(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
+class SharedAccessConfigCreateRequest(BaseModel):
     """
-
+    SharedAccessConfigCreateRequest
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-        'id': 'MongoObjectID'
-    }
-
-    attribute_map = {
-        'id': 'id'
-    }
-
-    def __init__(self, id=None, _configuration=None):  # noqa: E501
-        """CreateEntityResponse - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-
-        self._id = None
-        self.discriminator = None
-
-        self.id = id
-
-    @property
-    def id(self):
-        """Gets the id of this CreateEntityResponse.  # noqa: E501
-
-
-        :return: The id of this CreateEntityResponse.  # noqa: E501
-        :rtype: MongoObjectID
-        """
-        return self._id
-
-    @id.setter
-    def id(self, id):
-        """Sets the id of this CreateEntityResponse.
-
-
-        :param id: The id of this CreateEntityResponse.  # noqa: E501
-        :type: MongoObjectID
-        """
-        if self._configuration.client_side_validation and id is None:
-            raise ValueError("Invalid value for `id`, must not be `None`")  # noqa: E501
-
-        self._id = id
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(CreateEntityResponse, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
+    access_type: SharedAccessType = Field(..., alias="accessType")
+    users: Optional[conlist(StrictStr)] = Field(None, description="List of users with access to the dataset.")
+    teams: Optional[conlist(StrictStr)] = Field(None, description="List of teams with access to the dataset.")
+    creator: Optional[Creator] = None
+    __properties = ["accessType", "users", "teams", "creator"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, CreateEntityResponse):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, CreateEntityResponse):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> SharedAccessConfigCreateRequest:
+        """Create an instance of SharedAccessConfigCreateRequest from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> SharedAccessConfigCreateRequest:
+        """Create an instance of SharedAccessConfigCreateRequest from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return SharedAccessConfigCreateRequest.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in SharedAccessConfigCreateRequest) in the input: " + str(obj))
+
+        _obj = SharedAccessConfigCreateRequest.parse_obj({
+            "access_type": obj.get("accessType"),
+            "users": obj.get("users"),
+            "teams": obj.get("teams"),
+            "creator": obj.get("creator")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/creator.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/job_status_data_result.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,104 +1,86 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
-
-
-class Creator(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
 
-    """
-    allowed enum values
-    """
-    UNKNOWN = "UNKNOWN"
-    USER_WEBAPP = "USER_WEBAPP"
-    USER_PIP = "USER_PIP"
-    USER_PIP_LIGHTLY_MAGIC = "USER_PIP_LIGHTLY_MAGIC"
-    USER_WORKER = "USER_WORKER"
+from typing import Any, Optional
+from pydantic import Extra,  BaseModel, Field
+from lightly.openapi_generated.swagger_client.models.job_result_type import JobResultType
 
+class JobStatusDataResult(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    JobStatusDataResult
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """Creator - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(Creator, dict):
-            for key, value in self.items():
-                result[key] = value
+    type: JobResultType = Field(...)
+    data: Optional[Any] = Field(None, description="Depending on the job type, this can be anything")
+    __properties = ["type", "data"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, Creator):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, Creator):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> JobStatusDataResult:
+        """Create an instance of JobStatusDataResult from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # set to None if data (nullable) is None
+        # and __fields_set__ contains the field
+        if self.data is None and "data" in self.__fields_set__:
+            _dict['data' if by_alias else 'data'] = None
+
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> JobStatusDataResult:
+        """Create an instance of JobStatusDataResult from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return JobStatusDataResult.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in JobStatusDataResult) in the input: " + str(obj))
+
+        _obj = JobStatusDataResult.parse_obj({
+            "type": obj.get("type"),
+            "data": obj.get("data")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/custom_sample_meta_data.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton_classification.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,95 +1,85 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
+from typing import List, Optional, Union
+from pydantic import Extra,  BaseModel, Field, confloat, conint, conlist
+from lightly.openapi_generated.swagger_client.models.prediction_singleton_base import PredictionSingletonBase
 
-class CustomSampleMetaData(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
-
+class PredictionSingletonClassification(PredictionSingletonBase):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    PredictionSingletonClassification
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """CustomSampleMetaData - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(CustomSampleMetaData, dict):
-            for key, value in self.items():
-                result[key] = value
+    probabilities: Optional[conlist(Union[confloat(le=1, ge=0, strict=True), conint(le=1, ge=0, strict=True)])] = Field(None, description="The probabilities of it being a certain category other than the one which was selected. The sum of all probabilities should equal 1.")
+    __properties = ["type", "taskName", "cropDatasetId", "cropSampleId", "categoryId", "score", "probabilities"]
 
-        return result
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, CustomSampleMetaData):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, CustomSampleMetaData):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> PredictionSingletonClassification:
+        """Create an instance of PredictionSingletonClassification from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> PredictionSingletonClassification:
+        """Create an instance of PredictionSingletonClassification from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return PredictionSingletonClassification.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in PredictionSingletonClassification) in the input: " + str(obj))
+
+        _obj = PredictionSingletonClassification.parse_obj({
+            "type": obj.get("type"),
+            "task_name": obj.get("taskName"),
+            "crop_dataset_id": obj.get("cropDatasetId"),
+            "crop_sample_id": obj.get("cropSampleId"),
+            "category_id": obj.get("categoryId"),
+            "score": obj.get("score"),
+            "probabilities": obj.get("probabilities")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dataset_creator.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data_rename.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,104 +1,80 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class DatasetCreator(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
 
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, StrictStr
 
+class TagChangeDataRename(BaseModel):
     """
-    allowed enum values
+    TagChangeDataRename
     """
-    UNKNOWN = "UNKNOWN"
-    USER_WEBAPP = "USER_WEBAPP"
-    USER_PIP = "USER_PIP"
-    USER_PIP_LIGHTLY_MAGIC = "USER_PIP_LIGHTLY_MAGIC"
-    USER_WORKER = "USER_WORKER"
+    var_from: StrictStr = Field(..., alias="from")
+    to: StrictStr = Field(...)
+    __properties = ["from", "to"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """DatasetCreator - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DatasetCreator, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DatasetCreator):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DatasetCreator):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> TagChangeDataRename:
+        """Create an instance of TagChangeDataRename from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> TagChangeDataRename:
+        """Create an instance of TagChangeDataRename from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return TagChangeDataRename.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in TagChangeDataRename) in the input: " + str(obj))
+
+        _obj = TagChangeDataRename.parse_obj({
+            "var_from": obj.get("from"),
+            "to": obj.get("to")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dataset_name.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_raw_samples_data_row.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,95 +1,80 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class DatasetName(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, StrictStr
 
+class DatasourceRawSamplesDataRow(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    Filename and corresponding read url for a sample in the datasource
     """
-    swagger_types = {
-    }
+    file_name: StrictStr = Field(..., alias="fileName")
+    read_url: StrictStr = Field(..., alias="readUrl", description="A URL which allows anyone in possession of said URL to access the resource")
+    __properties = ["fileName", "readUrl"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """DatasetName - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DatasetName, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DatasetName):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DatasetName):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DatasourceRawSamplesDataRow:
+        """Create an instance of DatasourceRawSamplesDataRow from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DatasourceRawSamplesDataRow:
+        """Create an instance of DatasourceRawSamplesDataRow from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DatasourceRawSamplesDataRow.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DatasourceRawSamplesDataRow) in the input: " + str(obj))
+
+        _obj = DatasourceRawSamplesDataRow.parse_obj({
+            "file_name": obj.get("fileName"),
+            "read_url": obj.get("readUrl")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dataset_name_query.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/auth0_on_sign_up_request.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,95 +1,82 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class DatasetNameQuery(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field
+from lightly.openapi_generated.swagger_client.models.auth0_on_sign_up_request_user import Auth0OnSignUpRequestUser
 
+class Auth0OnSignUpRequest(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    Auth0OnSignUpRequest
     """
-    swagger_types = {
-    }
+    user: Auth0OnSignUpRequestUser = Field(...)
+    __properties = ["user"]
 
-    attribute_map = {
-    }
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def __init__(self, _configuration=None):  # noqa: E501
-        """DatasetNameQuery - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DatasetNameQuery, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DatasetNameQuery):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DatasetNameQuery):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> Auth0OnSignUpRequest:
+        """Create an instance of Auth0OnSignUpRequest from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # override the default output from pydantic by calling `to_dict()` of user
+        if self.user:
+            _dict['user' if by_alias else 'user'] = self.user.to_dict(by_alias=by_alias)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> Auth0OnSignUpRequest:
+        """Create an instance of Auth0OnSignUpRequest from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return Auth0OnSignUpRequest.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in Auth0OnSignUpRequest) in the input: " + str(obj))
+
+        _obj = Auth0OnSignUpRequest.parse_obj({
+            "user": Auth0OnSignUpRequestUser.from_dict(obj.get("user")) if obj.get("user") is not None else None
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dataset_type.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_update_request.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,102 +1,81 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
-
-
-class DatasetType(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
 
-    """
-    allowed enum values
-    """
-    CROPS = "Crops"
-    IMAGES = "Images"
-    VIDEOS = "Videos"
+from typing import Optional
+from pydantic import Extra,  BaseModel, Field, StrictStr
+from lightly.openapi_generated.swagger_client.models.docker_run_state import DockerRunState
 
+class DockerRunUpdateRequest(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DockerRunUpdateRequest
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """DatasetType - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DatasetType, dict):
-            for key, value in self.items():
-                result[key] = value
+    state: DockerRunState = Field(...)
+    message: Optional[StrictStr] = None
+    __properties = ["state", "message"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DatasetType):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DatasetType):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerRunUpdateRequest:
+        """Create an instance of DockerRunUpdateRequest from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerRunUpdateRequest:
+        """Create an instance of DockerRunUpdateRequest from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerRunUpdateRequest.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerRunUpdateRequest) in the input: " + str(obj))
+
+        _obj = DockerRunUpdateRequest.parse_obj({
+            "state": obj.get("state"),
+            "message": obj.get("message")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dataset_update_request.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_docker_object_level.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,124 +1,102 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
+from typing import Optional, Union
+from pydantic import Extra,  BaseModel, Field, StrictFloat, StrictInt, constr, validator
 
-class DatasetUpdateRequest(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
+class DockerWorkerConfigV2DockerObjectLevel(BaseModel):
     """
-
+    DockerWorkerConfigV2DockerObjectLevel
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-        'name': 'DatasetName'
-    }
-
-    attribute_map = {
-        'name': 'name'
-    }
-
-    def __init__(self, name=None, _configuration=None):  # noqa: E501
-        """DatasetUpdateRequest - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-
-        self._name = None
-        self.discriminator = None
-
-        self.name = name
-
-    @property
-    def name(self):
-        """Gets the name of this DatasetUpdateRequest.  # noqa: E501
-
-
-        :return: The name of this DatasetUpdateRequest.  # noqa: E501
-        :rtype: DatasetName
-        """
-        return self._name
-
-    @name.setter
-    def name(self, name):
-        """Sets the name of this DatasetUpdateRequest.
-
-
-        :param name: The name of this DatasetUpdateRequest.  # noqa: E501
-        :type: DatasetName
-        """
-        if self._configuration.client_side_validation and name is None:
-            raise ValueError("Invalid value for `name`, must not be `None`")  # noqa: E501
-
-        self._name = name
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DatasetUpdateRequest, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
+    crop_dataset_name: Optional[constr(strict=True)] = Field(None, alias="cropDatasetName", description="Identical limitations than DatasetName however it can be empty")
+    padding: Optional[Union[StrictFloat, StrictInt]] = None
+    task_name: Optional[constr(strict=True)] = Field(None, alias="taskName", description="Since we sometimes stitch together SelectionInputTask+ActiveLearningScoreType, they need to follow the same specs of ActiveLearningScoreType. However, this can be an empty string due to internal logic. ")
+    __properties = ["cropDatasetName", "padding", "taskName"]
+
+    @validator('crop_dataset_name')
+    def crop_dataset_name_validate_regular_expression(cls, value):
+        """Validates the regular expression"""
+        if value is None:
+            return value
+
+        if not re.match(r"^[a-zA-Z0-9 _-]*$", value):
+            raise ValueError(r"must validate the regular expression /^[a-zA-Z0-9 _-]*$/")
+        return value
+
+    @validator('task_name')
+    def task_name_validate_regular_expression(cls, value):
+        """Validates the regular expression"""
+        if value is None:
+            return value
+
+        if not re.match(r"^[a-zA-Z0-9_+=,.@:\/-]*$", value):
+            raise ValueError(r"must validate the regular expression /^[a-zA-Z0-9_+=,.@:\/-]*$/")
+        return value
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DatasetUpdateRequest):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DatasetUpdateRequest):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerWorkerConfigV2DockerObjectLevel:
+        """Create an instance of DockerWorkerConfigV2DockerObjectLevel from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerWorkerConfigV2DockerObjectLevel:
+        """Create an instance of DockerWorkerConfigV2DockerObjectLevel from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerWorkerConfigV2DockerObjectLevel.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerWorkerConfigV2DockerObjectLevel) in the input: " + str(obj))
+
+        _obj = DockerWorkerConfigV2DockerObjectLevel.parse_obj({
+            "crop_dataset_name": obj.get("cropDatasetName"),
+            "padding": obj.get("padding"),
+            "task_name": obj.get("taskName")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_config_lightly.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/annotation_meta_data.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,95 +1,78 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
+from typing import Optional
+from pydantic import Extra,  BaseModel, StrictStr
 
-class DatasourceConfigLIGHTLY(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
-
+class AnnotationMetaData(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    AnnotationMetaData
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """DatasourceConfigLIGHTLY - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DatasourceConfigLIGHTLY, dict):
-            for key, value in self.items():
-                result[key] = value
+    description: Optional[StrictStr] = None
+    __properties = ["description"]
 
-        return result
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DatasourceConfigLIGHTLY):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DatasourceConfigLIGHTLY):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> AnnotationMetaData:
+        """Create an instance of AnnotationMetaData from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> AnnotationMetaData:
+        """Create an instance of AnnotationMetaData from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return AnnotationMetaData.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in AnnotationMetaData) in the input: " + str(obj))
+
+        _obj = AnnotationMetaData.parse_obj({
+            "description": obj.get("description")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_config_local.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_lightly.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,95 +1,82 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class DatasourceConfigLOCAL(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel
+from lightly.openapi_generated.swagger_client.models.datasource_config_base import DatasourceConfigBase
 
+class DatasourceConfigLIGHTLY(DatasourceConfigBase):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DatasourceConfigLIGHTLY
     """
-    swagger_types = {
-    }
+    __properties = ["id", "purpose", "type", "fullPath", "thumbSuffix"]
 
-    attribute_map = {
-    }
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def __init__(self, _configuration=None):  # noqa: E501
-        """DatasourceConfigLOCAL - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DatasourceConfigLOCAL, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DatasourceConfigLOCAL):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DatasourceConfigLOCAL):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DatasourceConfigLIGHTLY:
+        """Create an instance of DatasourceConfigLIGHTLY from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DatasourceConfigLIGHTLY:
+        """Create an instance of DatasourceConfigLIGHTLY from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DatasourceConfigLIGHTLY.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DatasourceConfigLIGHTLY) in the input: " + str(obj))
+
+        _obj = DatasourceConfigLIGHTLY.parse_obj({
+            "id": obj.get("id"),
+            "purpose": obj.get("purpose"),
+            "type": obj.get("type"),
+            "full_path": obj.get("fullPath"),
+            "thumb_suffix": obj.get("thumbSuffix")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/datasource_purpose.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_entry.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,102 +1,89 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class DatasourcePurpose(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
 
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, StrictStr, conint
+from lightly.openapi_generated.swagger_client.models.tag_change_data import TagChangeData
+from lightly.openapi_generated.swagger_client.models.tag_creator import TagCreator
 
+class TagChangeEntry(BaseModel):
     """
-    allowed enum values
+    TagChangeEntry
     """
-    INPUT_OUTPUT = "INPUT_OUTPUT"
-    INPUT = "INPUT"
-    LIGHTLY = "LIGHTLY"
+    user_id: StrictStr = Field(..., alias="userId")
+    creator: TagCreator = Field(...)
+    ts: conint(strict=True, ge=0) = Field(..., description="unix timestamp in milliseconds")
+    changes: TagChangeData = Field(...)
+    __properties = ["userId", "creator", "ts", "changes"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """DatasourcePurpose - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DatasourcePurpose, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DatasourcePurpose):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DatasourcePurpose):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> TagChangeEntry:
+        """Create an instance of TagChangeEntry from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # override the default output from pydantic by calling `to_dict()` of changes
+        if self.changes:
+            _dict['changes' if by_alias else 'changes'] = self.changes.to_dict(by_alias=by_alias)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> TagChangeEntry:
+        """Create an instance of TagChangeEntry from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return TagChangeEntry.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in TagChangeEntry) in the input: " + str(obj))
+
+        _obj = TagChangeEntry.parse_obj({
+            "user_id": obj.get("userId"),
+            "creator": obj.get("creator"),
+            "ts": obj.get("ts"),
+            "changes": TagChangeData.from_dict(obj.get("changes")) if obj.get("changes") is not None else None
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/dimensionality_reduction_method.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/active_learning_score_create_request.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,102 +1,87 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
-
-
-class DimensionalityReductionMethod(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
 
-    """
-    allowed enum values
-    """
-    PCA = "PCA"
-    TSNE = "TSNE"
-    UMAP = "UMAP"
+from typing import List, Union
+from pydantic import Extra,  BaseModel, Field, StrictFloat, StrictInt, conlist, constr, validator
 
+class ActiveLearningScoreCreateRequest(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    ActiveLearningScoreCreateRequest
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """DimensionalityReductionMethod - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DimensionalityReductionMethod, dict):
-            for key, value in self.items():
-                result[key] = value
+    score_type: constr(strict=True, min_length=1) = Field(..., alias="scoreType", description="Type of active learning score")
+    scores: conlist(Union[StrictFloat, StrictInt], min_items=1) = Field(..., description="Array of active learning scores")
+    __properties = ["scoreType", "scores"]
+
+    @validator('score_type')
+    def score_type_validate_regular_expression(cls, value):
+        """Validates the regular expression"""
+        if not re.match(r"^[a-zA-Z0-9_+=,.@:\/-]*$", value):
+            raise ValueError(r"must validate the regular expression /^[a-zA-Z0-9_+=,.@:\/-]*$/")
+        return value
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DimensionalityReductionMethod):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DimensionalityReductionMethod):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> ActiveLearningScoreCreateRequest:
+        """Create an instance of ActiveLearningScoreCreateRequest from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> ActiveLearningScoreCreateRequest:
+        """Create an instance of ActiveLearningScoreCreateRequest from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return ActiveLearningScoreCreateRequest.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in ActiveLearningScoreCreateRequest) in the input: " + str(obj))
+
+        _obj = ActiveLearningScoreCreateRequest.parse_obj({
+            "score_type": obj.get("scoreType"),
+            "scores": obj.get("scores")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_artifact_create_request.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/profile_me_data.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,153 +1,113 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
+from typing import List, Optional, Union
+from pydantic import Extra,  BaseModel, Field, StrictFloat, StrictInt, StrictStr, conint, conlist, constr
+from lightly.openapi_generated.swagger_client.models.profile_me_data_settings import ProfileMeDataSettings
+from lightly.openapi_generated.swagger_client.models.team_basic_data import TeamBasicData
+from lightly.openapi_generated.swagger_client.models.user_type import UserType
 
-class DockerRunArtifactCreateRequest(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
+class ProfileMeData(BaseModel):
     """
-
+    ProfileMeData
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-        'file_name': 'str',
-        'type': 'DockerRunArtifactType'
-    }
-
-    attribute_map = {
-        'file_name': 'fileName',
-        'type': 'type'
-    }
-
-    def __init__(self, file_name=None, type=None, _configuration=None):  # noqa: E501
-        """DockerRunArtifactCreateRequest - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-
-        self._file_name = None
-        self._type = None
-        self.discriminator = None
-
-        self.file_name = file_name
-        self.type = type
-
-    @property
-    def file_name(self):
-        """Gets the file_name of this DockerRunArtifactCreateRequest.  # noqa: E501
-
-        the fileName of the artifact  # noqa: E501
-
-        :return: The file_name of this DockerRunArtifactCreateRequest.  # noqa: E501
-        :rtype: str
-        """
-        return self._file_name
-
-    @file_name.setter
-    def file_name(self, file_name):
-        """Sets the file_name of this DockerRunArtifactCreateRequest.
-
-        the fileName of the artifact  # noqa: E501
-
-        :param file_name: The file_name of this DockerRunArtifactCreateRequest.  # noqa: E501
-        :type: str
-        """
-        if self._configuration.client_side_validation and file_name is None:
-            raise ValueError("Invalid value for `file_name`, must not be `None`")  # noqa: E501
-
-        self._file_name = file_name
-
-    @property
-    def type(self):
-        """Gets the type of this DockerRunArtifactCreateRequest.  # noqa: E501
-
-
-        :return: The type of this DockerRunArtifactCreateRequest.  # noqa: E501
-        :rtype: DockerRunArtifactType
-        """
-        return self._type
-
-    @type.setter
-    def type(self, type):
-        """Sets the type of this DockerRunArtifactCreateRequest.
-
-
-        :param type: The type of this DockerRunArtifactCreateRequest.  # noqa: E501
-        :type: DockerRunArtifactType
-        """
-        if self._configuration.client_side_validation and type is None:
-            raise ValueError("Invalid value for `type`, must not be `None`")  # noqa: E501
-
-        self._type = type
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DockerRunArtifactCreateRequest, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
+    id: StrictStr = Field(...)
+    user_type: UserType = Field(..., alias="userType")
+    email: StrictStr = Field(..., description="email of the user")
+    nickname: Optional[StrictStr] = None
+    name: Optional[StrictStr] = None
+    given_name: Optional[StrictStr] = Field(None, alias="givenName")
+    family_name: Optional[StrictStr] = Field(None, alias="familyName")
+    token: Optional[constr(strict=True, min_length=5)] = Field(None, description="The user's token to be used for authentication via token querystring")
+    created_at: conint(strict=True, ge=0) = Field(..., alias="createdAt", description="unix timestamp in milliseconds")
+    teams: Optional[conlist(TeamBasicData)] = None
+    settings: ProfileMeDataSettings = Field(...)
+    onboarding: Optional[Union[StrictFloat, StrictInt]] = None
+    __properties = ["id", "userType", "email", "nickname", "name", "givenName", "familyName", "token", "createdAt", "teams", "settings", "onboarding"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DockerRunArtifactCreateRequest):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DockerRunArtifactCreateRequest):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> ProfileMeData:
+        """Create an instance of ProfileMeData from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # override the default output from pydantic by calling `to_dict()` of each item in teams (list)
+        _items = []
+        if self.teams:
+            for _item in self.teams:
+                if _item:
+                    _items.append(_item.to_dict(by_alias=by_alias))
+            _dict['teams' if by_alias else 'teams'] = _items
+        # override the default output from pydantic by calling `to_dict()` of settings
+        if self.settings:
+            _dict['settings' if by_alias else 'settings'] = self.settings.to_dict(by_alias=by_alias)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> ProfileMeData:
+        """Create an instance of ProfileMeData from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return ProfileMeData.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in ProfileMeData) in the input: " + str(obj))
+
+        _obj = ProfileMeData.parse_obj({
+            "id": obj.get("id"),
+            "user_type": obj.get("userType"),
+            "email": obj.get("email"),
+            "nickname": obj.get("nickname"),
+            "name": obj.get("name"),
+            "given_name": obj.get("givenName"),
+            "family_name": obj.get("familyName"),
+            "token": obj.get("token"),
+            "created_at": obj.get("createdAt"),
+            "teams": [TeamBasicData.from_dict(_item) for _item in obj.get("teams")] if obj.get("teams") is not None else None,
+            "settings": ProfileMeDataSettings.from_dict(obj.get("settings")) if obj.get("settings") is not None else None,
+            "onboarding": obj.get("onboarding")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_artifact_type.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_artifact_create_request.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,106 +1,84 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
-
-
-class DockerRunArtifactType(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
 
-    """
-    allowed enum values
-    """
-    LOG = "LOG"
-    MEMLOG = "MEMLOG"
-    CHECKPOINT = "CHECKPOINT"
-    REPORT_PDF = "REPORT_PDF"
-    REPORT_JSON = "REPORT_JSON"
-    CORRUPTNESS_CHECK_INFORMATION = "CORRUPTNESS_CHECK_INFORMATION"
-    SEQUENCE_INFORMATION = "SEQUENCE_INFORMATION"
+from typing import Optional
+from pydantic import Extra,  BaseModel, Field, StrictStr
+from lightly.openapi_generated.swagger_client.models.docker_run_artifact_storage_location import DockerRunArtifactStorageLocation
+from lightly.openapi_generated.swagger_client.models.docker_run_artifact_type import DockerRunArtifactType
 
+class DockerRunArtifactCreateRequest(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DockerRunArtifactCreateRequest
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """DockerRunArtifactType - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DockerRunArtifactType, dict):
-            for key, value in self.items():
-                result[key] = value
+    file_name: StrictStr = Field(..., alias="fileName", description="the fileName of the artifact")
+    type: DockerRunArtifactType = Field(...)
+    storage_location: Optional[DockerRunArtifactStorageLocation] = Field(None, alias="storageLocation")
+    __properties = ["fileName", "type", "storageLocation"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DockerRunArtifactType):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DockerRunArtifactType):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerRunArtifactCreateRequest:
+        """Create an instance of DockerRunArtifactCreateRequest from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerRunArtifactCreateRequest:
+        """Create an instance of DockerRunArtifactCreateRequest from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerRunArtifactCreateRequest.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerRunArtifactCreateRequest) in the input: " + str(obj))
+
+        _obj = DockerRunArtifactCreateRequest.parse_obj({
+            "file_name": obj.get("fileName"),
+            "type": obj.get("type"),
+            "storage_location": obj.get("storageLocation")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_log_level.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_log_data.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,105 +1,88 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
-
-
-class DockerRunLogLevel(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
 
-    """
-    allowed enum values
-    """
-    VERBOSE = "VERBOSE"
-    DEBUG = "DEBUG"
-    INFO = "INFO"
-    WARN = "WARN"
-    ERROR = "ERROR"
-    CRITICAL = "CRITICAL"
+from typing import List, Optional
+from pydantic import Extra,  BaseModel, Field, conint, conlist
+from lightly.openapi_generated.swagger_client.models.docker_run_log_entry_data import DockerRunLogEntryData
 
+class DockerRunLogData(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DockerRunLogData
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """DockerRunLogLevel - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DockerRunLogLevel, dict):
-            for key, value in self.items():
-                result[key] = value
+    cursor: Optional[conint(strict=True, ge=0)] = Field(0, description="The cursor to use to fetch more logs.")
+    logs: conlist(DockerRunLogEntryData) = Field(...)
+    __properties = ["cursor", "logs"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DockerRunLogLevel):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DockerRunLogLevel):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerRunLogData:
+        """Create an instance of DockerRunLogData from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # override the default output from pydantic by calling `to_dict()` of each item in logs (list)
+        _items = []
+        if self.logs:
+            for _item in self.logs:
+                if _item:
+                    _items.append(_item.to_dict(by_alias=by_alias))
+            _dict['logs' if by_alias else 'logs'] = _items
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerRunLogData:
+        """Create an instance of DockerRunLogData from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerRunLogData.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerRunLogData) in the input: " + str(obj))
+
+        _obj = DockerRunLogData.parse_obj({
+            "cursor": obj.get("cursor") if obj.get("cursor") is not None else 0,
+            "logs": [DockerRunLogEntryData.from_dict(_item) for _item in obj.get("logs")] if obj.get("logs") is not None else None
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_run_scheduled_state.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_create_request.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,103 +1,85 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
-
-
-class DockerRunScheduledState(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
 
-    """
-    allowed enum values
-    """
-    OPEN = "OPEN"
-    LOCKED = "LOCKED"
-    DONE = "DONE"
-    CANCELED = "CANCELED"
+from typing import Optional
+from pydantic import Extra,  BaseModel, Field
+from lightly.openapi_generated.swagger_client.models.creator import Creator
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3 import DockerWorkerConfigV3
 
+class DockerWorkerConfigV3CreateRequest(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DockerWorkerConfigV3CreateRequest
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """DockerRunScheduledState - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DockerRunScheduledState, dict):
-            for key, value in self.items():
-                result[key] = value
+    config: DockerWorkerConfigV3 = Field(...)
+    creator: Optional[Creator] = None
+    __properties = ["config", "creator"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DockerRunScheduledState):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DockerRunScheduledState):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerWorkerConfigV3CreateRequest:
+        """Create an instance of DockerWorkerConfigV3CreateRequest from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # override the default output from pydantic by calling `to_dict()` of config
+        if self.config:
+            _dict['config' if by_alias else 'config'] = self.config.to_dict(by_alias=by_alias)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerWorkerConfigV3CreateRequest:
+        """Create an instance of DockerWorkerConfigV3CreateRequest from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerWorkerConfigV3CreateRequest.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerWorkerConfigV3CreateRequest) in the input: " + str(obj))
+
+        _obj = DockerWorkerConfigV3CreateRequest.parse_obj({
+            "config": DockerWorkerConfigV3.from_dict(obj.get("config")) if obj.get("config") is not None else None,
+            "creator": obj.get("creator")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,206 +1,120 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
+from typing import Optional
+from pydantic import Extra,  BaseModel, Field, StrictInt
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_checkpoint_callback import DockerWorkerConfigV3LightlyCheckpointCallback
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_collate import DockerWorkerConfigV3LightlyCollate
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_criterion import DockerWorkerConfigV3LightlyCriterion
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_loader import DockerWorkerConfigV3LightlyLoader
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_model import DockerWorkerConfigV3LightlyModel
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_optimizer import DockerWorkerConfigV3LightlyOptimizer
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_trainer import DockerWorkerConfigV3LightlyTrainer
 
-class DockerWorkerConfig(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
+class DockerWorkerConfigV3Lightly(BaseModel):
     """
-
+    Lightly configurations which are passed to a Lightly Worker run. For information about the options see https://docs.lightly.ai/docs/all-configuration-options#run-configuration. 
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-        'worker_type': 'DockerWorkerType',
-        'docker': 'dict(str, object)',
-        'lightly': 'dict(str, object)',
-        'selection': 'SelectionConfig'
-    }
-
-    attribute_map = {
-        'worker_type': 'workerType',
-        'docker': 'docker',
-        'lightly': 'lightly',
-        'selection': 'selection'
-    }
-
-    def __init__(self, worker_type=None, docker=None, lightly=None, selection=None, _configuration=None):  # noqa: E501
-        """DockerWorkerConfig - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-
-        self._worker_type = None
-        self._docker = None
-        self._lightly = None
-        self._selection = None
-        self.discriminator = None
-
-        self.worker_type = worker_type
-        if docker is not None:
-            self.docker = docker
-        if lightly is not None:
-            self.lightly = lightly
-        if selection is not None:
-            self.selection = selection
-
-    @property
-    def worker_type(self):
-        """Gets the worker_type of this DockerWorkerConfig.  # noqa: E501
-
-
-        :return: The worker_type of this DockerWorkerConfig.  # noqa: E501
-        :rtype: DockerWorkerType
-        """
-        return self._worker_type
-
-    @worker_type.setter
-    def worker_type(self, worker_type):
-        """Sets the worker_type of this DockerWorkerConfig.
-
-
-        :param worker_type: The worker_type of this DockerWorkerConfig.  # noqa: E501
-        :type: DockerWorkerType
-        """
-        if self._configuration.client_side_validation and worker_type is None:
-            raise ValueError("Invalid value for `worker_type`, must not be `None`")  # noqa: E501
-
-        self._worker_type = worker_type
-
-    @property
-    def docker(self):
-        """Gets the docker of this DockerWorkerConfig.  # noqa: E501
-
-        docker run configurations, keys should match the structure of https://github.com/lightly-ai/lightly-core/blob/develop/onprem-docker/lightly_worker/src/lightly_worker/resources/docker/docker.yaml   # noqa: E501
-
-        :return: The docker of this DockerWorkerConfig.  # noqa: E501
-        :rtype: dict(str, object)
-        """
-        return self._docker
-
-    @docker.setter
-    def docker(self, docker):
-        """Sets the docker of this DockerWorkerConfig.
-
-        docker run configurations, keys should match the structure of https://github.com/lightly-ai/lightly-core/blob/develop/onprem-docker/lightly_worker/src/lightly_worker/resources/docker/docker.yaml   # noqa: E501
-
-        :param docker: The docker of this DockerWorkerConfig.  # noqa: E501
-        :type: dict(str, object)
-        """
-
-        self._docker = docker
-
-    @property
-    def lightly(self):
-        """Gets the lightly of this DockerWorkerConfig.  # noqa: E501
-
-        lightly configurations which are passed to a docker run, keys should match structure of https://github.com/lightly-ai/lightly/blob/master/lightly/cli/config/config.yaml   # noqa: E501
-
-        :return: The lightly of this DockerWorkerConfig.  # noqa: E501
-        :rtype: dict(str, object)
-        """
-        return self._lightly
-
-    @lightly.setter
-    def lightly(self, lightly):
-        """Sets the lightly of this DockerWorkerConfig.
-
-        lightly configurations which are passed to a docker run, keys should match structure of https://github.com/lightly-ai/lightly/blob/master/lightly/cli/config/config.yaml   # noqa: E501
-
-        :param lightly: The lightly of this DockerWorkerConfig.  # noqa: E501
-        :type: dict(str, object)
-        """
-
-        self._lightly = lightly
-
-    @property
-    def selection(self):
-        """Gets the selection of this DockerWorkerConfig.  # noqa: E501
-
-
-        :return: The selection of this DockerWorkerConfig.  # noqa: E501
-        :rtype: SelectionConfig
-        """
-        return self._selection
-
-    @selection.setter
-    def selection(self, selection):
-        """Sets the selection of this DockerWorkerConfig.
-
-
-        :param selection: The selection of this DockerWorkerConfig.  # noqa: E501
-        :type: SelectionConfig
-        """
-
-        self._selection = selection
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DockerWorkerConfig, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
+    seed: Optional[StrictInt] = Field(None, description="Random seed.")
+    checkpoint_callback: Optional[DockerWorkerConfigV3LightlyCheckpointCallback] = Field(None, alias="checkpointCallback")
+    loader: Optional[DockerWorkerConfigV3LightlyLoader] = None
+    model: Optional[DockerWorkerConfigV3LightlyModel] = None
+    trainer: Optional[DockerWorkerConfigV3LightlyTrainer] = None
+    criterion: Optional[DockerWorkerConfigV3LightlyCriterion] = None
+    optimizer: Optional[DockerWorkerConfigV3LightlyOptimizer] = None
+    collate: Optional[DockerWorkerConfigV3LightlyCollate] = None
+    __properties = ["seed", "checkpointCallback", "loader", "model", "trainer", "criterion", "optimizer", "collate"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DockerWorkerConfig):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DockerWorkerConfig):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerWorkerConfigV3Lightly:
+        """Create an instance of DockerWorkerConfigV3Lightly from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # override the default output from pydantic by calling `to_dict()` of checkpoint_callback
+        if self.checkpoint_callback:
+            _dict['checkpointCallback' if by_alias else 'checkpoint_callback'] = self.checkpoint_callback.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of loader
+        if self.loader:
+            _dict['loader' if by_alias else 'loader'] = self.loader.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of model
+        if self.model:
+            _dict['model' if by_alias else 'model'] = self.model.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of trainer
+        if self.trainer:
+            _dict['trainer' if by_alias else 'trainer'] = self.trainer.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of criterion
+        if self.criterion:
+            _dict['criterion' if by_alias else 'criterion'] = self.criterion.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of optimizer
+        if self.optimizer:
+            _dict['optimizer' if by_alias else 'optimizer'] = self.optimizer.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of collate
+        if self.collate:
+            _dict['collate' if by_alias else 'collate'] = self.collate.to_dict(by_alias=by_alias)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerWorkerConfigV3Lightly:
+        """Create an instance of DockerWorkerConfigV3Lightly from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerWorkerConfigV3Lightly.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerWorkerConfigV3Lightly) in the input: " + str(obj))
+
+        _obj = DockerWorkerConfigV3Lightly.parse_obj({
+            "seed": obj.get("seed"),
+            "checkpoint_callback": DockerWorkerConfigV3LightlyCheckpointCallback.from_dict(obj.get("checkpointCallback")) if obj.get("checkpointCallback") is not None else None,
+            "loader": DockerWorkerConfigV3LightlyLoader.from_dict(obj.get("loader")) if obj.get("loader") is not None else None,
+            "model": DockerWorkerConfigV3LightlyModel.from_dict(obj.get("model")) if obj.get("model") is not None else None,
+            "trainer": DockerWorkerConfigV3LightlyTrainer.from_dict(obj.get("trainer")) if obj.get("trainer") is not None else None,
+            "criterion": DockerWorkerConfigV3LightlyCriterion.from_dict(obj.get("criterion")) if obj.get("criterion") is not None else None,
+            "optimizer": DockerWorkerConfigV3LightlyOptimizer.from_dict(obj.get("optimizer")) if obj.get("optimizer") is not None else None,
+            "collate": DockerWorkerConfigV3LightlyCollate.from_dict(obj.get("collate")) if obj.get("collate") is not None else None
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_create_request.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,150 +1,97 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
+from typing import Optional
+from pydantic import Extra,  BaseModel, Field
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v2_docker import DockerWorkerConfigV2Docker
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v2_lightly import DockerWorkerConfigV2Lightly
+from lightly.openapi_generated.swagger_client.models.docker_worker_type import DockerWorkerType
+from lightly.openapi_generated.swagger_client.models.selection_config import SelectionConfig
 
-class DockerWorkerConfigCreateRequest(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
+class DockerWorkerConfigV2(BaseModel):
     """
-
+    DockerWorkerConfigV2
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-        'config': 'DockerWorkerConfig',
-        'creator': 'Creator'
-    }
-
-    attribute_map = {
-        'config': 'config',
-        'creator': 'creator'
-    }
-
-    def __init__(self, config=None, creator=None, _configuration=None):  # noqa: E501
-        """DockerWorkerConfigCreateRequest - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-
-        self._config = None
-        self._creator = None
-        self.discriminator = None
-
-        self.config = config
-        if creator is not None:
-            self.creator = creator
-
-    @property
-    def config(self):
-        """Gets the config of this DockerWorkerConfigCreateRequest.  # noqa: E501
-
-
-        :return: The config of this DockerWorkerConfigCreateRequest.  # noqa: E501
-        :rtype: DockerWorkerConfig
-        """
-        return self._config
-
-    @config.setter
-    def config(self, config):
-        """Sets the config of this DockerWorkerConfigCreateRequest.
-
-
-        :param config: The config of this DockerWorkerConfigCreateRequest.  # noqa: E501
-        :type: DockerWorkerConfig
-        """
-        if self._configuration.client_side_validation and config is None:
-            raise ValueError("Invalid value for `config`, must not be `None`")  # noqa: E501
-
-        self._config = config
-
-    @property
-    def creator(self):
-        """Gets the creator of this DockerWorkerConfigCreateRequest.  # noqa: E501
-
-
-        :return: The creator of this DockerWorkerConfigCreateRequest.  # noqa: E501
-        :rtype: Creator
-        """
-        return self._creator
-
-    @creator.setter
-    def creator(self, creator):
-        """Sets the creator of this DockerWorkerConfigCreateRequest.
-
-
-        :param creator: The creator of this DockerWorkerConfigCreateRequest.  # noqa: E501
-        :type: Creator
-        """
-
-        self._creator = creator
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DockerWorkerConfigCreateRequest, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
+    worker_type: DockerWorkerType = Field(..., alias="workerType")
+    docker: Optional[DockerWorkerConfigV2Docker] = None
+    lightly: Optional[DockerWorkerConfigV2Lightly] = None
+    selection: Optional[SelectionConfig] = None
+    __properties = ["workerType", "docker", "lightly", "selection"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DockerWorkerConfigCreateRequest):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DockerWorkerConfigCreateRequest):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerWorkerConfigV2:
+        """Create an instance of DockerWorkerConfigV2 from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # override the default output from pydantic by calling `to_dict()` of docker
+        if self.docker:
+            _dict['docker' if by_alias else 'docker'] = self.docker.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of lightly
+        if self.lightly:
+            _dict['lightly' if by_alias else 'lightly'] = self.lightly.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of selection
+        if self.selection:
+            _dict['selection' if by_alias else 'selection'] = self.selection.to_dict(by_alias=by_alias)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerWorkerConfigV2:
+        """Create an instance of DockerWorkerConfigV2 from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerWorkerConfigV2.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerWorkerConfigV2) in the input: " + str(obj))
+
+        _obj = DockerWorkerConfigV2.parse_obj({
+            "worker_type": obj.get("workerType"),
+            "docker": DockerWorkerConfigV2Docker.from_dict(obj.get("docker")) if obj.get("docker") is not None else None,
+            "lightly": DockerWorkerConfigV2Lightly.from_dict(obj.get("lightly")) if obj.get("lightly") is not None else None,
+            "selection": SelectionConfig.from_dict(obj.get("selection")) if obj.get("selection") is not None else None
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_lightly.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,202 +1,112 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
+from typing import Optional
+from pydantic import Extra,  BaseModel
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v2_lightly_collate import DockerWorkerConfigV2LightlyCollate
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v2_lightly_model import DockerWorkerConfigV2LightlyModel
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v2_lightly_trainer import DockerWorkerConfigV2LightlyTrainer
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_criterion import DockerWorkerConfigV3LightlyCriterion
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_loader import DockerWorkerConfigV3LightlyLoader
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly_optimizer import DockerWorkerConfigV3LightlyOptimizer
 
-class DockerWorkerConfigV2(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
+class DockerWorkerConfigV2Lightly(BaseModel):
     """
-
+    Lightly configurations which are passed to a Lightly Worker run. For information about the options see https://docs.lightly.ai/docs/all-configuration-options#run-configuration. 
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-        'worker_type': 'DockerWorkerType',
-        'docker': 'DockerWorkerConfigV2Docker',
-        'lightly': 'DockerWorkerConfigV2Lightly',
-        'selection': 'SelectionConfig'
-    }
-
-    attribute_map = {
-        'worker_type': 'workerType',
-        'docker': 'docker',
-        'lightly': 'lightly',
-        'selection': 'selection'
-    }
-
-    def __init__(self, worker_type=None, docker=None, lightly=None, selection=None, _configuration=None):  # noqa: E501
-        """DockerWorkerConfigV2 - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-
-        self._worker_type = None
-        self._docker = None
-        self._lightly = None
-        self._selection = None
-        self.discriminator = None
-
-        self.worker_type = worker_type
-        if docker is not None:
-            self.docker = docker
-        if lightly is not None:
-            self.lightly = lightly
-        if selection is not None:
-            self.selection = selection
-
-    @property
-    def worker_type(self):
-        """Gets the worker_type of this DockerWorkerConfigV2.  # noqa: E501
-
-
-        :return: The worker_type of this DockerWorkerConfigV2.  # noqa: E501
-        :rtype: DockerWorkerType
-        """
-        return self._worker_type
-
-    @worker_type.setter
-    def worker_type(self, worker_type):
-        """Sets the worker_type of this DockerWorkerConfigV2.
-
-
-        :param worker_type: The worker_type of this DockerWorkerConfigV2.  # noqa: E501
-        :type: DockerWorkerType
-        """
-        if self._configuration.client_side_validation and worker_type is None:
-            raise ValueError("Invalid value for `worker_type`, must not be `None`")  # noqa: E501
-
-        self._worker_type = worker_type
-
-    @property
-    def docker(self):
-        """Gets the docker of this DockerWorkerConfigV2.  # noqa: E501
-
-
-        :return: The docker of this DockerWorkerConfigV2.  # noqa: E501
-        :rtype: DockerWorkerConfigV2Docker
-        """
-        return self._docker
-
-    @docker.setter
-    def docker(self, docker):
-        """Sets the docker of this DockerWorkerConfigV2.
-
-
-        :param docker: The docker of this DockerWorkerConfigV2.  # noqa: E501
-        :type: DockerWorkerConfigV2Docker
-        """
-
-        self._docker = docker
-
-    @property
-    def lightly(self):
-        """Gets the lightly of this DockerWorkerConfigV2.  # noqa: E501
-
-
-        :return: The lightly of this DockerWorkerConfigV2.  # noqa: E501
-        :rtype: DockerWorkerConfigV2Lightly
-        """
-        return self._lightly
-
-    @lightly.setter
-    def lightly(self, lightly):
-        """Sets the lightly of this DockerWorkerConfigV2.
-
-
-        :param lightly: The lightly of this DockerWorkerConfigV2.  # noqa: E501
-        :type: DockerWorkerConfigV2Lightly
-        """
-
-        self._lightly = lightly
-
-    @property
-    def selection(self):
-        """Gets the selection of this DockerWorkerConfigV2.  # noqa: E501
-
-
-        :return: The selection of this DockerWorkerConfigV2.  # noqa: E501
-        :rtype: SelectionConfig
-        """
-        return self._selection
-
-    @selection.setter
-    def selection(self, selection):
-        """Sets the selection of this DockerWorkerConfigV2.
-
-
-        :param selection: The selection of this DockerWorkerConfigV2.  # noqa: E501
-        :type: SelectionConfig
-        """
-
-        self._selection = selection
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DockerWorkerConfigV2, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
+    loader: Optional[DockerWorkerConfigV3LightlyLoader] = None
+    model: Optional[DockerWorkerConfigV2LightlyModel] = None
+    trainer: Optional[DockerWorkerConfigV2LightlyTrainer] = None
+    criterion: Optional[DockerWorkerConfigV3LightlyCriterion] = None
+    optimizer: Optional[DockerWorkerConfigV3LightlyOptimizer] = None
+    collate: Optional[DockerWorkerConfigV2LightlyCollate] = None
+    __properties = ["loader", "model", "trainer", "criterion", "optimizer", "collate"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DockerWorkerConfigV2):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DockerWorkerConfigV2):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerWorkerConfigV2Lightly:
+        """Create an instance of DockerWorkerConfigV2Lightly from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # override the default output from pydantic by calling `to_dict()` of loader
+        if self.loader:
+            _dict['loader' if by_alias else 'loader'] = self.loader.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of model
+        if self.model:
+            _dict['model' if by_alias else 'model'] = self.model.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of trainer
+        if self.trainer:
+            _dict['trainer' if by_alias else 'trainer'] = self.trainer.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of criterion
+        if self.criterion:
+            _dict['criterion' if by_alias else 'criterion'] = self.criterion.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of optimizer
+        if self.optimizer:
+            _dict['optimizer' if by_alias else 'optimizer'] = self.optimizer.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of collate
+        if self.collate:
+            _dict['collate' if by_alias else 'collate'] = self.collate.to_dict(by_alias=by_alias)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerWorkerConfigV2Lightly:
+        """Create an instance of DockerWorkerConfigV2Lightly from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerWorkerConfigV2Lightly.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerWorkerConfigV2Lightly) in the input: " + str(obj))
+
+        _obj = DockerWorkerConfigV2Lightly.parse_obj({
+            "loader": DockerWorkerConfigV3LightlyLoader.from_dict(obj.get("loader")) if obj.get("loader") is not None else None,
+            "model": DockerWorkerConfigV2LightlyModel.from_dict(obj.get("model")) if obj.get("model") is not None else None,
+            "trainer": DockerWorkerConfigV2LightlyTrainer.from_dict(obj.get("trainer")) if obj.get("trainer") is not None else None,
+            "criterion": DockerWorkerConfigV3LightlyCriterion.from_dict(obj.get("criterion")) if obj.get("criterion") is not None else None,
+            "optimizer": DockerWorkerConfigV3LightlyOptimizer.from_dict(obj.get("optimizer")) if obj.get("optimizer") is not None else None,
+            "collate": DockerWorkerConfigV2LightlyCollate.from_dict(obj.get("collate")) if obj.get("collate") is not None else None
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_create_request.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,150 +1,97 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
+from typing import Optional
+from pydantic import Extra,  BaseModel, Field
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_docker import DockerWorkerConfigV3Docker
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_lightly import DockerWorkerConfigV3Lightly
+from lightly.openapi_generated.swagger_client.models.docker_worker_type import DockerWorkerType
+from lightly.openapi_generated.swagger_client.models.selection_config import SelectionConfig
 
-class DockerWorkerConfigV2CreateRequest(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
+class DockerWorkerConfigV3(BaseModel):
     """
-
+    DockerWorkerConfigV3
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-        'config': 'DockerWorkerConfigV2',
-        'creator': 'Creator'
-    }
-
-    attribute_map = {
-        'config': 'config',
-        'creator': 'creator'
-    }
-
-    def __init__(self, config=None, creator=None, _configuration=None):  # noqa: E501
-        """DockerWorkerConfigV2CreateRequest - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-
-        self._config = None
-        self._creator = None
-        self.discriminator = None
-
-        self.config = config
-        if creator is not None:
-            self.creator = creator
-
-    @property
-    def config(self):
-        """Gets the config of this DockerWorkerConfigV2CreateRequest.  # noqa: E501
-
-
-        :return: The config of this DockerWorkerConfigV2CreateRequest.  # noqa: E501
-        :rtype: DockerWorkerConfigV2
-        """
-        return self._config
-
-    @config.setter
-    def config(self, config):
-        """Sets the config of this DockerWorkerConfigV2CreateRequest.
-
-
-        :param config: The config of this DockerWorkerConfigV2CreateRequest.  # noqa: E501
-        :type: DockerWorkerConfigV2
-        """
-        if self._configuration.client_side_validation and config is None:
-            raise ValueError("Invalid value for `config`, must not be `None`")  # noqa: E501
-
-        self._config = config
-
-    @property
-    def creator(self):
-        """Gets the creator of this DockerWorkerConfigV2CreateRequest.  # noqa: E501
-
-
-        :return: The creator of this DockerWorkerConfigV2CreateRequest.  # noqa: E501
-        :rtype: Creator
-        """
-        return self._creator
-
-    @creator.setter
-    def creator(self, creator):
-        """Sets the creator of this DockerWorkerConfigV2CreateRequest.
-
-
-        :param creator: The creator of this DockerWorkerConfigV2CreateRequest.  # noqa: E501
-        :type: Creator
-        """
-
-        self._creator = creator
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DockerWorkerConfigV2CreateRequest, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
+    worker_type: DockerWorkerType = Field(..., alias="workerType")
+    docker: Optional[DockerWorkerConfigV3Docker] = None
+    lightly: Optional[DockerWorkerConfigV3Lightly] = None
+    selection: Optional[SelectionConfig] = None
+    __properties = ["workerType", "docker", "lightly", "selection"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DockerWorkerConfigV2CreateRequest):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DockerWorkerConfigV2CreateRequest):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerWorkerConfigV3:
+        """Create an instance of DockerWorkerConfigV3 from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # override the default output from pydantic by calling `to_dict()` of docker
+        if self.docker:
+            _dict['docker' if by_alias else 'docker'] = self.docker.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of lightly
+        if self.lightly:
+            _dict['lightly' if by_alias else 'lightly'] = self.lightly.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of selection
+        if self.selection:
+            _dict['selection' if by_alias else 'selection'] = self.selection.to_dict(by_alias=by_alias)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerWorkerConfigV3:
+        """Create an instance of DockerWorkerConfigV3 from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerWorkerConfigV3.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerWorkerConfigV3) in the input: " + str(obj))
+
+        _obj = DockerWorkerConfigV3.parse_obj({
+            "worker_type": obj.get("workerType"),
+            "docker": DockerWorkerConfigV3Docker.from_dict(obj.get("docker")) if obj.get("docker") is not None else None,
+            "lightly": DockerWorkerConfigV3Lightly.from_dict(obj.get("lightly")) if obj.get("lightly") is not None else None,
+            "selection": SelectionConfig.from_dict(obj.get("selection")) if obj.get("selection") is not None else None
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_docker_datasource.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_docker.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,175 +1,118 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
+from typing import Optional
+from pydantic import Extra,  BaseModel, Field, StrictBool, StrictStr, conint
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_docker_corruptness_check import DockerWorkerConfigV3DockerCorruptnessCheck
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_docker_datasource import DockerWorkerConfigV3DockerDatasource
+from lightly.openapi_generated.swagger_client.models.docker_worker_config_v3_docker_training import DockerWorkerConfigV3DockerTraining
 
-class DockerWorkerConfigV3DockerDatasource(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
+class DockerWorkerConfigV3Docker(BaseModel):
     """
-
+    docker run configurations, keys should match the structure of https://github.com/lightly-ai/lightly-core/blob/develop/onprem-docker/lightly_worker/src/lightly_worker/resources/docker/docker.yaml 
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-        'bypass_verify': 'bool',
-        'enable_datapool_update': 'bool',
-        'process_all': 'bool'
-    }
-
-    attribute_map = {
-        'bypass_verify': 'bypassVerify',
-        'enable_datapool_update': 'enableDatapoolUpdate',
-        'process_all': 'processAll'
-    }
-
-    def __init__(self, bypass_verify=None, enable_datapool_update=None, process_all=None, _configuration=None):  # noqa: E501
-        """DockerWorkerConfigV3DockerDatasource - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-
-        self._bypass_verify = None
-        self._enable_datapool_update = None
-        self._process_all = None
-        self.discriminator = None
-
-        if bypass_verify is not None:
-            self.bypass_verify = bypass_verify
-        if enable_datapool_update is not None:
-            self.enable_datapool_update = enable_datapool_update
-        if process_all is not None:
-            self.process_all = process_all
-
-    @property
-    def bypass_verify(self):
-        """Gets the bypass_verify of this DockerWorkerConfigV3DockerDatasource.  # noqa: E501
-
-
-        :return: The bypass_verify of this DockerWorkerConfigV3DockerDatasource.  # noqa: E501
-        :rtype: bool
-        """
-        return self._bypass_verify
-
-    @bypass_verify.setter
-    def bypass_verify(self, bypass_verify):
-        """Sets the bypass_verify of this DockerWorkerConfigV3DockerDatasource.
-
-
-        :param bypass_verify: The bypass_verify of this DockerWorkerConfigV3DockerDatasource.  # noqa: E501
-        :type: bool
-        """
-
-        self._bypass_verify = bypass_verify
-
-    @property
-    def enable_datapool_update(self):
-        """Gets the enable_datapool_update of this DockerWorkerConfigV3DockerDatasource.  # noqa: E501
-
-
-        :return: The enable_datapool_update of this DockerWorkerConfigV3DockerDatasource.  # noqa: E501
-        :rtype: bool
-        """
-        return self._enable_datapool_update
-
-    @enable_datapool_update.setter
-    def enable_datapool_update(self, enable_datapool_update):
-        """Sets the enable_datapool_update of this DockerWorkerConfigV3DockerDatasource.
-
-
-        :param enable_datapool_update: The enable_datapool_update of this DockerWorkerConfigV3DockerDatasource.  # noqa: E501
-        :type: bool
-        """
-
-        self._enable_datapool_update = enable_datapool_update
-
-    @property
-    def process_all(self):
-        """Gets the process_all of this DockerWorkerConfigV3DockerDatasource.  # noqa: E501
-
-
-        :return: The process_all of this DockerWorkerConfigV3DockerDatasource.  # noqa: E501
-        :rtype: bool
-        """
-        return self._process_all
-
-    @process_all.setter
-    def process_all(self, process_all):
-        """Sets the process_all of this DockerWorkerConfigV3DockerDatasource.
-
-
-        :param process_all: The process_all of this DockerWorkerConfigV3DockerDatasource.  # noqa: E501
-        :type: bool
-        """
-
-        self._process_all = process_all
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DockerWorkerConfigV3DockerDatasource, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
+    checkpoint: Optional[StrictStr] = None
+    corruptness_check: Optional[DockerWorkerConfigV3DockerCorruptnessCheck] = Field(None, alias="corruptnessCheck")
+    datasource: Optional[DockerWorkerConfigV3DockerDatasource] = None
+    embeddings: Optional[StrictStr] = None
+    enable_training: Optional[StrictBool] = Field(None, alias="enableTraining")
+    training: Optional[DockerWorkerConfigV3DockerTraining] = None
+    normalize_embeddings: Optional[StrictBool] = Field(None, alias="normalizeEmbeddings")
+    num_processes: Optional[conint(strict=True, ge=-1)] = Field(None, alias="numProcesses")
+    num_threads: Optional[conint(strict=True, ge=-1)] = Field(None, alias="numThreads")
+    output_image_format: Optional[StrictStr] = Field(None, alias="outputImageFormat")
+    pretagging: Optional[StrictBool] = None
+    pretagging_upload: Optional[StrictBool] = Field(None, alias="pretaggingUpload")
+    relevant_filenames_file: Optional[StrictStr] = Field(None, alias="relevantFilenamesFile")
+    selected_sequence_length: Optional[conint(strict=True, ge=1)] = Field(None, alias="selectedSequenceLength")
+    upload_report: Optional[StrictBool] = Field(None, alias="uploadReport")
+    __properties = ["checkpoint", "corruptnessCheck", "datasource", "embeddings", "enableTraining", "training", "normalizeEmbeddings", "numProcesses", "numThreads", "outputImageFormat", "pretagging", "pretaggingUpload", "relevantFilenamesFile", "selectedSequenceLength", "uploadReport"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DockerWorkerConfigV3DockerDatasource):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DockerWorkerConfigV3DockerDatasource):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerWorkerConfigV3Docker:
+        """Create an instance of DockerWorkerConfigV3Docker from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # override the default output from pydantic by calling `to_dict()` of corruptness_check
+        if self.corruptness_check:
+            _dict['corruptnessCheck' if by_alias else 'corruptness_check'] = self.corruptness_check.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of datasource
+        if self.datasource:
+            _dict['datasource' if by_alias else 'datasource'] = self.datasource.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of training
+        if self.training:
+            _dict['training' if by_alias else 'training'] = self.training.to_dict(by_alias=by_alias)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerWorkerConfigV3Docker:
+        """Create an instance of DockerWorkerConfigV3Docker from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerWorkerConfigV3Docker.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerWorkerConfigV3Docker) in the input: " + str(obj))
+
+        _obj = DockerWorkerConfigV3Docker.parse_obj({
+            "checkpoint": obj.get("checkpoint"),
+            "corruptness_check": DockerWorkerConfigV3DockerCorruptnessCheck.from_dict(obj.get("corruptnessCheck")) if obj.get("corruptnessCheck") is not None else None,
+            "datasource": DockerWorkerConfigV3DockerDatasource.from_dict(obj.get("datasource")) if obj.get("datasource") is not None else None,
+            "embeddings": obj.get("embeddings"),
+            "enable_training": obj.get("enableTraining"),
+            "training": DockerWorkerConfigV3DockerTraining.from_dict(obj.get("training")) if obj.get("training") is not None else None,
+            "normalize_embeddings": obj.get("normalizeEmbeddings"),
+            "num_processes": obj.get("numProcesses"),
+            "num_threads": obj.get("numThreads"),
+            "output_image_format": obj.get("outputImageFormat"),
+            "pretagging": obj.get("pretagging"),
+            "pretagging_upload": obj.get("pretaggingUpload"),
+            "relevant_filenames_file": obj.get("relevantFilenamesFile"),
+            "selected_sequence_length": obj.get("selectedSequenceLength"),
+            "upload_report": obj.get("uploadReport")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_labels.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_checkpoint_callback.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,95 +1,78 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
+from typing import Optional
+from pydantic import Extra,  BaseModel, Field, StrictBool
 
-class DockerWorkerLabels(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
-
+class DockerWorkerConfigV3LightlyCheckpointCallback(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DockerWorkerConfigV3LightlyCheckpointCallback
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """DockerWorkerLabels - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DockerWorkerLabels, dict):
-            for key, value in self.items():
-                result[key] = value
+    save_last: Optional[StrictBool] = Field(None, alias="saveLast", description="If True, the checkpoint from the last epoch is saved.")
+    __properties = ["saveLast"]
 
-        return result
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DockerWorkerLabels):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DockerWorkerLabels):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerWorkerConfigV3LightlyCheckpointCallback:
+        """Create an instance of DockerWorkerConfigV3LightlyCheckpointCallback from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerWorkerConfigV3LightlyCheckpointCallback:
+        """Create an instance of DockerWorkerConfigV3LightlyCheckpointCallback from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerWorkerConfigV3LightlyCheckpointCallback.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerWorkerConfigV3LightlyCheckpointCallback) in the input: " + str(obj))
+
+        _obj = DockerWorkerConfigV3LightlyCheckpointCallback.parse_obj({
+            "save_last": obj.get("saveLast")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_state.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data_arithmetics.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,103 +1,82 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class DockerWorkerState(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
 
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, StrictStr
 
+class TagChangeDataArithmetics(BaseModel):
     """
-    allowed enum values
+    TagChangeDataArithmetics
     """
-    OFFLINE = "OFFLINE"
-    CRASHED = "CRASHED"
-    IDLE = "IDLE"
-    BUSY = "BUSY"
+    operation: StrictStr = Field(...)
+    tag1: StrictStr = Field(...)
+    tag2: StrictStr = Field(...)
+    __properties = ["operation", "tag1", "tag2"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """DockerWorkerState - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DockerWorkerState, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DockerWorkerState):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DockerWorkerState):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> TagChangeDataArithmetics:
+        """Create an instance of TagChangeDataArithmetics from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> TagChangeDataArithmetics:
+        """Create an instance of TagChangeDataArithmetics from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return TagChangeDataArithmetics.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in TagChangeDataArithmetics) in the input: " + str(obj))
+
+        _obj = TagChangeDataArithmetics.parse_obj({
+            "operation": obj.get("operation"),
+            "tag1": obj.get("tag1"),
+            "tag2": obj.get("tag2")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/docker_worker_type.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_license_information.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,100 +1,82 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class DockerWorkerType(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
 
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, StrictBool, StrictStr, conint
 
+class DockerLicenseInformation(BaseModel):
     """
-    allowed enum values
+    DockerLicenseInformation
     """
-    FULL = "FULL"
+    license_type: StrictStr = Field(..., alias="licenseType")
+    license_expiration_date: conint(strict=True, ge=0) = Field(..., alias="licenseExpirationDate", description="unix timestamp in milliseconds")
+    license_is_valid: StrictBool = Field(..., alias="licenseIsValid")
+    __properties = ["licenseType", "licenseExpirationDate", "licenseIsValid"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """DockerWorkerType - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(DockerWorkerType, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, DockerWorkerType):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, DockerWorkerType):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerLicenseInformation:
+        """Create an instance of DockerLicenseInformation from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerLicenseInformation:
+        """Create an instance of DockerLicenseInformation from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerLicenseInformation.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerLicenseInformation) in the input: " + str(obj))
+
+        _obj = DockerLicenseInformation.parse_obj({
+            "license_type": obj.get("licenseType"),
+            "license_expiration_date": obj.get("licenseExpirationDate"),
+            "license_is_valid": obj.get("licenseIsValid")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/embedding2d_coordinates.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_optimizer.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,95 +1,80 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class Embedding2dCoordinates(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from typing import Optional, Union
+from pydantic import Extra,  BaseModel, Field, confloat, conint
 
+class DockerWorkerConfigV3LightlyOptimizer(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DockerWorkerConfigV3LightlyOptimizer
     """
-    swagger_types = {
-    }
+    lr: Optional[Union[confloat(ge=0.0, strict=True), conint(ge=0, strict=True)]] = None
+    weight_decay: Optional[Union[confloat(le=1.0, ge=0.0, strict=True), conint(le=1, ge=0, strict=True)]] = Field(None, alias="weightDecay")
+    __properties = ["lr", "weightDecay"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """Embedding2dCoordinates - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(Embedding2dCoordinates, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, Embedding2dCoordinates):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, Embedding2dCoordinates):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerWorkerConfigV3LightlyOptimizer:
+        """Create an instance of DockerWorkerConfigV3LightlyOptimizer from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerWorkerConfigV3LightlyOptimizer:
+        """Create an instance of DockerWorkerConfigV3LightlyOptimizer from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerWorkerConfigV3LightlyOptimizer.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerWorkerConfigV3LightlyOptimizer) in the input: " + str(obj))
+
+        _obj = DockerWorkerConfigV3LightlyOptimizer.parse_obj({
+            "lr": obj.get("lr"),
+            "weight_decay": obj.get("weightDecay")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/filename_and_read_urls.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data_sampler.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,95 +1,78 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class FilenameAndReadUrls(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, StrictStr
 
+class TagChangeDataSampler(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    TagChangeDataSampler
     """
-    swagger_types = {
-    }
+    method: StrictStr = Field(...)
+    __properties = ["method"]
 
-    attribute_map = {
-    }
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def __init__(self, _configuration=None):  # noqa: E501
-        """FilenameAndReadUrls - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(FilenameAndReadUrls, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, FilenameAndReadUrls):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, FilenameAndReadUrls):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> TagChangeDataSampler:
+        """Create an instance of TagChangeDataSampler from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> TagChangeDataSampler:
+        """Create an instance of TagChangeDataSampler from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return TagChangeDataSampler.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in TagChangeDataSampler) in the input: " + str(obj))
+
+        _obj = TagChangeDataSampler.parse_obj({
+            "method": obj.get("method")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/general_job_result.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_trainer.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,95 +1,83 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class GeneralJobResult(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from typing import Optional
+from pydantic import Extra,  BaseModel, Field, conint
+from lightly.openapi_generated.swagger_client.models.lightly_trainer_precision_v3 import LightlyTrainerPrecisionV3
 
+class DockerWorkerConfigV3LightlyTrainer(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DockerWorkerConfigV3LightlyTrainer
     """
-    swagger_types = {
-    }
+    gpus: Optional[conint(strict=True, ge=0)] = None
+    max_epochs: Optional[conint(strict=True, ge=0)] = Field(None, alias="maxEpochs")
+    precision: Optional[LightlyTrainerPrecisionV3] = None
+    __properties = ["gpus", "maxEpochs", "precision"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """GeneralJobResult - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(GeneralJobResult, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, GeneralJobResult):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, GeneralJobResult):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerWorkerConfigV3LightlyTrainer:
+        """Create an instance of DockerWorkerConfigV3LightlyTrainer from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerWorkerConfigV3LightlyTrainer:
+        """Create an instance of DockerWorkerConfigV3LightlyTrainer from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerWorkerConfigV3LightlyTrainer.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerWorkerConfigV3LightlyTrainer) in the input: " + str(obj))
+
+        _obj = DockerWorkerConfigV3LightlyTrainer.parse_obj({
+            "gpus": obj.get("gpus"),
+            "max_epochs": obj.get("maxEpochs"),
+            "precision": obj.get("precision")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/image_type.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data_initial.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,102 +1,88 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class ImageType(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from typing import Optional
+from pydantic import Extra,  BaseModel, Field, constr, validator
 
+class TagChangeDataInitial(BaseModel):
     """
-    allowed enum values
+    TagChangeDataInitial
     """
-    FULL = "full"
-    THUMBNAIL = "thumbnail"
-    META = "meta"
+    run_id: Optional[constr(strict=True)] = Field(None, alias="runId", description="MongoDB ObjectId")
+    __properties = ["runId"]
 
-    """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """ImageType - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(ImageType, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
+    @validator('run_id')
+    def run_id_validate_regular_expression(cls, value):
+        """Validates the regular expression"""
+        if value is None:
+            return value
+
+        if not re.match(r"^[a-f0-9]{24}$", value):
+            raise ValueError(r"must validate the regular expression /^[a-f0-9]{24}$/")
+        return value
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, ImageType):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, ImageType):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> TagChangeDataInitial:
+        """Create an instance of TagChangeDataInitial from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> TagChangeDataInitial:
+        """Create an instance of TagChangeDataInitial from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return TagChangeDataInitial.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in TagChangeDataInitial) in the input: " + str(obj))
+
+        _obj = TagChangeDataInitial.parse_obj({
+            "run_id": obj.get("runId")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/job_status_upload_method.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/filename_and_read_url.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,102 +1,80 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class JobStatusUploadMethod(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
 
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, StrictStr
 
+class FilenameAndReadUrl(BaseModel):
     """
-    allowed enum values
+    Filename and corresponding read url for a sample in a tag
     """
-    USER_WEBAPP = "USER_WEBAPP"
-    USER_PIP = "USER_PIP"
-    INTERNAL = "INTERNAL"
+    file_name: StrictStr = Field(..., alias="fileName")
+    read_url: StrictStr = Field(..., alias="readUrl", description="A URL which allows anyone in possession of said URL to access the resource")
+    __properties = ["fileName", "readUrl"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """JobStatusUploadMethod - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(JobStatusUploadMethod, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, JobStatusUploadMethod):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, JobStatusUploadMethod):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> FilenameAndReadUrl:
+        """Create an instance of FilenameAndReadUrl from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> FilenameAndReadUrl:
+        """Create an instance of FilenameAndReadUrl from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return FilenameAndReadUrl.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in FilenameAndReadUrl) in the input: " + str(obj))
+
+        _obj = FilenameAndReadUrl.parse_obj({
+            "file_name": obj.get("fileName"),
+            "read_url": obj.get("readUrl")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/label_box_v4_data_rows.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/async_task_data.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,95 +1,78 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class LabelBoxV4DataRows(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, StrictStr
 
+class AsyncTaskData(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    AsyncTaskData
     """
-    swagger_types = {
-    }
+    job_id: StrictStr = Field(..., alias="jobId")
+    __properties = ["jobId"]
 
-    attribute_map = {
-    }
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def __init__(self, _configuration=None):  # noqa: E501
-        """LabelBoxV4DataRows - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(LabelBoxV4DataRows, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, LabelBoxV4DataRows):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, LabelBoxV4DataRows):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> AsyncTaskData:
+        """Create an instance of AsyncTaskData from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> AsyncTaskData:
+        """Create an instance of AsyncTaskData from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return AsyncTaskData.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in AsyncTaskData) in the input: " + str(obj))
+
+        _obj = AsyncTaskData.parse_obj({
+            "job_id": obj.get("jobId")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/label_studio_tasks.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/update_team_membership_request.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,95 +1,79 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class LabelStudioTasks(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field
+from lightly.openapi_generated.swagger_client.models.team_role import TeamRole
 
+class UpdateTeamMembershipRequest(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    UpdateTeamMembershipRequest
     """
-    swagger_types = {
-    }
+    role: TeamRole = Field(...)
+    __properties = ["role"]
 
-    attribute_map = {
-    }
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def __init__(self, _configuration=None):  # noqa: E501
-        """LabelStudioTasks - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(LabelStudioTasks, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, LabelStudioTasks):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, LabelStudioTasks):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> UpdateTeamMembershipRequest:
+        """Create an instance of UpdateTeamMembershipRequest from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> UpdateTeamMembershipRequest:
+        """Create an instance of UpdateTeamMembershipRequest from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return UpdateTeamMembershipRequest.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in UpdateTeamMembershipRequest) in the input: " + str(obj))
+
+        _obj = UpdateTeamMembershipRequest.parse_obj({
+            "role": obj.get("role")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/lightly_model_v2.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton_keypoint_detection_all_of.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,104 +1,80 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
-
-
-class LightlyModelV2(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
 
-    """
-    allowed enum values
-    """
-    _18 = "resnet-18"
-    _34 = "resnet-34"
-    _50 = "resnet-50"
-    _101 = "resnet-101"
-    _152 = "resnet-152"
+from typing import List, Optional, Union
+from pydantic import Extra,  BaseModel, Field, confloat, conint, conlist
 
+class PredictionSingletonKeypointDetectionAllOf(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    PredictionSingletonKeypointDetectionAllOf
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """LightlyModelV2 - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(LightlyModelV2, dict):
-            for key, value in self.items():
-                result[key] = value
+    keypoints: conlist(Union[confloat(ge=0, strict=True), conint(ge=0, strict=True)], min_items=3) = Field(..., description="[x1, y2, v1, ..., xk, yk, vk] as outlined by the coco format https://cocodataset.org/#format-results ")
+    probabilities: Optional[conlist(Union[confloat(le=1, ge=0, strict=True), conint(le=1, ge=0, strict=True)])] = Field(None, description="The probabilities of it being a certain category other than the one which was selected. The sum of all probabilities should equal 1.")
+    __properties = ["keypoints", "probabilities"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, LightlyModelV2):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, LightlyModelV2):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> PredictionSingletonKeypointDetectionAllOf:
+        """Create an instance of PredictionSingletonKeypointDetectionAllOf from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> PredictionSingletonKeypointDetectionAllOf:
+        """Create an instance of PredictionSingletonKeypointDetectionAllOf from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return PredictionSingletonKeypointDetectionAllOf.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in PredictionSingletonKeypointDetectionAllOf) in the input: " + str(obj))
+
+        _obj = PredictionSingletonKeypointDetectionAllOf.parse_obj({
+            "keypoints": obj.get("keypoints"),
+            "probabilities": obj.get("probabilities")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/lightly_trainer_precision_v3.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_local.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,101 +1,82 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class LightlyTrainerPrecisionV3(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
 
-    """
-    allowed enum values
-    """
-    _16 = "16"
-    _32 = "32"
+from pydantic import Extra,  BaseModel
+from lightly.openapi_generated.swagger_client.models.datasource_config_base import DatasourceConfigBase
 
+class DatasourceConfigLOCAL(DatasourceConfigBase):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DatasourceConfigLOCAL
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """LightlyTrainerPrecisionV3 - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(LightlyTrainerPrecisionV3, dict):
-            for key, value in self.items():
-                result[key] = value
+    __properties = ["id", "purpose", "type", "fullPath", "thumbSuffix"]
 
-        return result
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, LightlyTrainerPrecisionV3):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, LightlyTrainerPrecisionV3):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DatasourceConfigLOCAL:
+        """Create an instance of DatasourceConfigLOCAL from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DatasourceConfigLOCAL:
+        """Create an instance of DatasourceConfigLOCAL from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DatasourceConfigLOCAL.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DatasourceConfigLOCAL) in the input: " + str(obj))
+
+        _obj = DatasourceConfigLOCAL.parse_obj({
+            "id": obj.get("id"),
+            "purpose": obj.get("purpose"),
+            "type": obj.get("type"),
+            "full_path": obj.get("fullPath"),
+            "thumb_suffix": obj.get("thumbSuffix")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/mongo_object_id.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/profile_me_data_settings.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,95 +1,89 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
+from typing import Optional
+from pydantic import Extra,  BaseModel, Field, StrictStr
 
-class MongoObjectID(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
-
+class ProfileMeDataSettings(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    ProfileMeDataSettings
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """MongoObjectID - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(MongoObjectID, dict):
-            for key, value in self.items():
-                result[key] = value
+    locale: Optional[StrictStr] = Field('en', description="Which locale does the user prefer")
+    date_format: Optional[StrictStr] = Field(None, alias="dateFormat", description="Which format for dates does the user prefer")
+    number_format: Optional[StrictStr] = Field(None, alias="numberFormat", description="Which format for numbers does the user prefer")
+    additional_properties: Dict[str, Any] = {}
+    __properties = ["locale", "dateFormat", "numberFormat"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, MongoObjectID):
-            return False
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-        return self.to_dict() == other.to_dict()
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> ProfileMeDataSettings:
+        """Create an instance of ProfileMeDataSettings from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                            "additional_properties"
+                          },
+                          exclude_none=True)
+        # puts key-value pairs in additional_properties in the top level
+        if self.additional_properties is not None:
+            for _key, _value in self.additional_properties.items():
+                _dict[_key] = _value
+
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> ProfileMeDataSettings:
+        """Create an instance of ProfileMeDataSettings from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return ProfileMeDataSettings.parse_obj(obj)
+
+        _obj = ProfileMeDataSettings.parse_obj({
+            "locale": obj.get("locale") if obj.get("locale") is not None else 'en',
+            "date_format": obj.get("dateFormat"),
+            "number_format": obj.get("numberFormat")
+        })
+        # store additional fields in additional_properties
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                _obj.additional_properties[_key] = obj.get(_key)
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, MongoObjectID):
-            return True
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/prediction_singleton.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_raw_samples_predictions_data.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,104 +1,90 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class PredictionSingleton(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from typing import List
+from pydantic import Extra,  BaseModel, Field, StrictBool, StrictStr, conlist
+from lightly.openapi_generated.swagger_client.models.datasource_raw_samples_predictions_data_row import DatasourceRawSamplesPredictionsDataRow
 
+class DatasourceRawSamplesPredictionsData(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DatasourceRawSamplesPredictionsData
     """
-    swagger_types = {
-    }
+    has_more: StrictBool = Field(..., alias="hasMore", description="Set to `false` if end of list is reached. Otherwise `true`.")
+    cursor: StrictStr = Field(..., description="A cursor that indicates the current position in the list. Must be passed to future requests to continue reading from the same list. ")
+    data: conlist(DatasourceRawSamplesPredictionsDataRow) = Field(..., description="Array containing the raw samples prediction objects")
+    __properties = ["hasMore", "cursor", "data"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    attribute_map = {
-    }
-
-    discriminator_value_class_map = {
-        
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """PredictionSingleton - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = 'Discriminator{propertyName&#x3D;&#39;type&#39;, mapping&#x3D;{CLASSIFICATION&#x3D;#/components/schemas/PredictionSingletonClassification, OBJECT_DETECTION&#x3D;#/components/schemas/PredictionSingletonObjectDetection, SEMANTIC_SEGMENTATION&#x3D;#/components/schemas/PredictionSingletonSemanticSegmentation, INSTANCE_SEGMENTATION&#x3D;#/components/schemas/PredictionSingletonInstanceSegmentation, KEYPOINT_DETECTION&#x3D;#/components/schemas/PredictionSingletonKeypointDetection}, extensions&#x3D;null}'
-
-    def get_real_child_model(self, data):
-        """Returns the real base class specified by the discriminator"""
-        discriminator_value = data[self.discriminator].lower()
-        return self.discriminator_value_class_map.get(discriminator_value)
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(PredictionSingleton, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, PredictionSingleton):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, PredictionSingleton):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DatasourceRawSamplesPredictionsData:
+        """Create an instance of DatasourceRawSamplesPredictionsData from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # override the default output from pydantic by calling `to_dict()` of each item in data (list)
+        _items = []
+        if self.data:
+            for _item in self.data:
+                if _item:
+                    _items.append(_item.to_dict(by_alias=by_alias))
+            _dict['data' if by_alias else 'data'] = _items
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DatasourceRawSamplesPredictionsData:
+        """Create an instance of DatasourceRawSamplesPredictionsData from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DatasourceRawSamplesPredictionsData.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DatasourceRawSamplesPredictionsData) in the input: " + str(obj))
+
+        _obj = DatasourceRawSamplesPredictionsData.parse_obj({
+            "has_more": obj.get("hasMore"),
+            "cursor": obj.get("cursor"),
+            "data": [DatasourceRawSamplesPredictionsDataRow.from_dict(_item) for _item in obj.get("data")] if obj.get("data") is not None else None
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/prediction_singletons.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_azure_all_of.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,95 +1,80 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class PredictionSingletons(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, constr
 
+class DatasourceConfigAzureAllOf(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DatasourceConfigAzureAllOf
     """
-    swagger_types = {
-    }
+    account_name: constr(strict=True, min_length=1) = Field(..., alias="accountName", description="name of the Azure Storage Account")
+    account_key: constr(strict=True, min_length=1) = Field(..., alias="accountKey", description="key of the Azure Storage Account")
+    __properties = ["accountName", "accountKey"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """PredictionSingletons - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(PredictionSingletons, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, PredictionSingletons):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, PredictionSingletons):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DatasourceConfigAzureAllOf:
+        """Create an instance of DatasourceConfigAzureAllOf from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DatasourceConfigAzureAllOf:
+        """Create an instance of DatasourceConfigAzureAllOf from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DatasourceConfigAzureAllOf.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DatasourceConfigAzureAllOf) in the input: " + str(obj))
+
+        _obj = DatasourceConfigAzureAllOf.parse_obj({
+            "account_name": obj.get("accountName"),
+            "account_key": obj.get("accountKey")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/probabilities.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_gcs_all_of.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,95 +1,80 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class Probabilities(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, StrictStr, constr
 
+class DatasourceConfigGCSAllOf(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DatasourceConfigGCSAllOf
     """
-    swagger_types = {
-    }
+    gcs_project_id: constr(strict=True, min_length=1) = Field(..., alias="gcsProjectId", description="The projectId where you have your bucket configured")
+    gcs_credentials: StrictStr = Field(..., alias="gcsCredentials", description="this is the content of the credentials JSON file stringified which you downloaded from Google Cloud Platform")
+    __properties = ["gcsProjectId", "gcsCredentials"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """Probabilities - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(Probabilities, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, Probabilities):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, Probabilities):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DatasourceConfigGCSAllOf:
+        """Create an instance of DatasourceConfigGCSAllOf from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DatasourceConfigGCSAllOf:
+        """Create an instance of DatasourceConfigGCSAllOf from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DatasourceConfigGCSAllOf.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DatasourceConfigGCSAllOf) in the input: " + str(obj))
+
+        _obj = DatasourceConfigGCSAllOf.parse_obj({
+            "gcs_project_id": obj.get("gcsProjectId"),
+            "gcs_credentials": obj.get("gcsCredentials")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/questionnaire_data.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/configuration_data.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,149 +1,101 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
+from typing import List
+from pydantic import Extra,  BaseModel, Field, StrictStr, conint, conlist, constr, validator
+from lightly.openapi_generated.swagger_client.models.configuration_entry import ConfigurationEntry
 
-class QuestionnaireData(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
+class ConfigurationData(BaseModel):
     """
-
+    ConfigurationData
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-        'company': 'str',
-        'sector': 'Sector'
-    }
-
-    attribute_map = {
-        'company': 'company',
-        'sector': 'sector'
-    }
-
-    def __init__(self, company=None, sector=None, _configuration=None):  # noqa: E501
-        """QuestionnaireData - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-
-        self._company = None
-        self._sector = None
-        self.discriminator = None
-
-        if company is not None:
-            self.company = company
-        if sector is not None:
-            self.sector = sector
-
-    @property
-    def company(self):
-        """Gets the company of this QuestionnaireData.  # noqa: E501
-
-
-        :return: The company of this QuestionnaireData.  # noqa: E501
-        :rtype: str
-        """
-        return self._company
-
-    @company.setter
-    def company(self, company):
-        """Sets the company of this QuestionnaireData.
-
-
-        :param company: The company of this QuestionnaireData.  # noqa: E501
-        :type: str
-        """
-
-        self._company = company
-
-    @property
-    def sector(self):
-        """Gets the sector of this QuestionnaireData.  # noqa: E501
-
-
-        :return: The sector of this QuestionnaireData.  # noqa: E501
-        :rtype: Sector
-        """
-        return self._sector
-
-    @sector.setter
-    def sector(self, sector):
-        """Sets the sector of this QuestionnaireData.
-
-
-        :param sector: The sector of this QuestionnaireData.  # noqa: E501
-        :type: Sector
-        """
-
-        self._sector = sector
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(QuestionnaireData, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
+    id: constr(strict=True) = Field(..., description="MongoDB ObjectId")
+    name: StrictStr = Field(...)
+    configs: conlist(ConfigurationEntry) = Field(...)
+    created_at: conint(strict=True, ge=0) = Field(..., alias="createdAt", description="unix timestamp in milliseconds")
+    last_modified_at: conint(strict=True, ge=0) = Field(..., alias="lastModifiedAt", description="unix timestamp in milliseconds")
+    __properties = ["id", "name", "configs", "createdAt", "lastModifiedAt"]
+
+    @validator('id')
+    def id_validate_regular_expression(cls, value):
+        """Validates the regular expression"""
+        if not re.match(r"^[a-f0-9]{24}$", value):
+            raise ValueError(r"must validate the regular expression /^[a-f0-9]{24}$/")
+        return value
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, QuestionnaireData):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, QuestionnaireData):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> ConfigurationData:
+        """Create an instance of ConfigurationData from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # override the default output from pydantic by calling `to_dict()` of each item in configs (list)
+        _items = []
+        if self.configs:
+            for _item in self.configs:
+                if _item:
+                    _items.append(_item.to_dict(by_alias=by_alias))
+            _dict['configs' if by_alias else 'configs'] = _items
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> ConfigurationData:
+        """Create an instance of ConfigurationData from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return ConfigurationData.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in ConfigurationData) in the input: " + str(obj))
+
+        _obj = ConfigurationData.parse_obj({
+            "id": obj.get("id"),
+            "name": obj.get("name"),
+            "configs": [ConfigurationEntry.from_dict(_item) for _item in obj.get("configs")] if obj.get("configs") is not None else None,
+            "created_at": obj.get("createdAt"),
+            "last_modified_at": obj.get("lastModifiedAt")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/redirected_read_url.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_artifact_created_data.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,95 +1,80 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class RedirectedReadUrl(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, StrictStr
 
+class DockerRunArtifactCreatedData(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DockerRunArtifactCreatedData
     """
-    swagger_types = {
-    }
+    signed_write_url: StrictStr = Field(..., alias="signedWriteUrl")
+    artifact_id: StrictStr = Field(..., alias="artifactId")
+    __properties = ["signedWriteUrl", "artifactId"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """RedirectedReadUrl - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(RedirectedReadUrl, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, RedirectedReadUrl):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, RedirectedReadUrl):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerRunArtifactCreatedData:
+        """Create an instance of DockerRunArtifactCreatedData from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerRunArtifactCreatedData:
+        """Create an instance of DockerRunArtifactCreatedData from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerRunArtifactCreatedData.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerRunArtifactCreatedData) in the input: " + str(obj))
+
+        _obj = DockerRunArtifactCreatedData.parse_obj({
+            "signed_write_url": obj.get("signedWriteUrl"),
+            "artifact_id": obj.get("artifactId")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sample_partial_mode.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/questionnaire_data.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,102 +1,81 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
-
-
-class SamplePartialMode(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
 
-    """
-    allowed enum values
-    """
-    IDS = "ids"
-    FILENAMES = "fileNames"
-    FULL = "full"
+from typing import Optional
+from pydantic import Extra,  BaseModel, constr
+from lightly.openapi_generated.swagger_client.models.sector import Sector
 
+class QuestionnaireData(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    QuestionnaireData
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """SamplePartialMode - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(SamplePartialMode, dict):
-            for key, value in self.items():
-                result[key] = value
+    company: Optional[constr(strict=True, min_length=3)] = None
+    sector: Optional[Sector] = None
+    __properties = ["company", "sector"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, SamplePartialMode):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, SamplePartialMode):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> QuestionnaireData:
+        """Create an instance of QuestionnaireData from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> QuestionnaireData:
+        """Create an instance of QuestionnaireData from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return QuestionnaireData.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in QuestionnaireData) in the input: " + str(obj))
+
+        _obj = QuestionnaireData.parse_obj({
+            "company": obj.get("company"),
+            "sector": obj.get("sector")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sample_type.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_bit_mask_response.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,102 +1,85 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class SampleType(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
 
-    """
-    allowed enum values
-    """
-    CROP = "CROP"
-    IMAGE = "IMAGE"
-    VIDEO_FRAME = "VIDEO_FRAME"
+from pydantic import Extra,  BaseModel, Field, constr, validator
 
+class TagBitMaskResponse(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    TagBitMaskResponse
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """SampleType - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(SampleType, dict):
-            for key, value in self.items():
-                result[key] = value
+    bit_mask_data: constr(strict=True) = Field(..., alias="bitMaskData", description="BitMask as a base16 (hex) string")
+    __properties = ["bitMaskData"]
 
-        return result
+    @validator('bit_mask_data')
+    def bit_mask_data_validate_regular_expression(cls, value):
+        """Validates the regular expression"""
+        if not re.match(r"^0x[a-f0-9]+$", value):
+            raise ValueError(r"must validate the regular expression /^0x[a-f0-9]+$/")
+        return value
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, SampleType):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, SampleType):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> TagBitMaskResponse:
+        """Create an instance of TagBitMaskResponse from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> TagBitMaskResponse:
+        """Create an instance of TagBitMaskResponse from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return TagBitMaskResponse.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in TagBitMaskResponse) in the input: " + str(obj))
+
+        _obj = TagBitMaskResponse.parse_obj({
+            "bit_mask_data": obj.get("bitMaskData")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sampling_method.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_docker_corruptness_check.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,103 +1,78 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class SamplingMethod(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from typing import Optional, Union
+from pydantic import Extra,  BaseModel, Field, confloat, conint
 
+class DockerWorkerConfigV3DockerCorruptnessCheck(BaseModel):
     """
-    allowed enum values
+    DockerWorkerConfigV3DockerCorruptnessCheck
     """
-    ACTIVE_LEARNING = "ACTIVE_LEARNING"
-    CORAL = "CORAL"
-    CORESET = "CORESET"
-    RANDOM = "RANDOM"
+    corruption_threshold: Optional[Union[confloat(le=1.0, ge=0.0, strict=True), conint(le=1, ge=0, strict=True)]] = Field(None, alias="corruptionThreshold")
+    __properties = ["corruptionThreshold"]
 
-    """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """SamplingMethod - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(SamplingMethod, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, SamplingMethod):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, SamplingMethod):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerWorkerConfigV3DockerCorruptnessCheck:
+        """Create an instance of DockerWorkerConfigV3DockerCorruptnessCheck from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerWorkerConfigV3DockerCorruptnessCheck:
+        """Create an instance of DockerWorkerConfigV3DockerCorruptnessCheck from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerWorkerConfigV3DockerCorruptnessCheck.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerWorkerConfigV3DockerCorruptnessCheck) in the input: " + str(obj))
+
+        _obj = DockerWorkerConfigV3DockerCorruptnessCheck.parse_obj({
+            "corruption_threshold": obj.get("corruptionThreshold")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/score.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/selection_config.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,95 +1,90 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class Score(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from typing import List, Optional, Union
+from pydantic import Extra,  BaseModel, Field, confloat, conint, conlist
+from lightly.openapi_generated.swagger_client.models.selection_config_entry import SelectionConfigEntry
 
+class SelectionConfig(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    SelectionConfig
     """
-    swagger_types = {
-    }
+    n_samples: Optional[conint(strict=True, ge=-1)] = Field(None, alias="nSamples")
+    proportion_samples: Optional[Union[confloat(le=1.0, ge=0.0, strict=True), conint(le=1, ge=0, strict=True)]] = Field(None, alias="proportionSamples")
+    strategies: conlist(SelectionConfigEntry, min_items=1) = Field(...)
+    __properties = ["nSamples", "proportionSamples", "strategies"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """Score - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(Score, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, Score):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, Score):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> SelectionConfig:
+        """Create an instance of SelectionConfig from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # override the default output from pydantic by calling `to_dict()` of each item in strategies (list)
+        _items = []
+        if self.strategies:
+            for _item in self.strategies:
+                if _item:
+                    _items.append(_item.to_dict(by_alias=by_alias))
+            _dict['strategies' if by_alias else 'strategies'] = _items
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> SelectionConfig:
+        """Create an instance of SelectionConfig from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return SelectionConfig.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in SelectionConfig) in the input: " + str(obj))
+
+        _obj = SelectionConfig.parse_obj({
+            "n_samples": obj.get("nSamples"),
+            "proportion_samples": obj.get("proportionSamples"),
+            "strategies": [SelectionConfigEntry.from_dict(_item) for _item in obj.get("strategies")] if obj.get("strategies") is not None else None
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/sector.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/annotation_offer_data.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,115 +1,80 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
-
-
-class Sector(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
 
-    """
-    allowed enum values
-    """
-    AGRICULTURE = "AGRICULTURE"
-    AUTOMOTIVE = "AUTOMOTIVE"
-    ENTERTAINMENT = "ENTERTAINMENT"
-    FINANCE = "FINANCE"
-    FOOD = "FOOD"
-    HEALTH_CARE = "HEALTH_CARE"
-    MACHINE_LEARNING = "MACHINE_LEARNING"
-    MANUFACTURING = "MANUFACTURING"
-    MEDICINE = "MEDICINE"
-    RECYCLING = "RECYCLING"
-    ROBOTICS = "ROBOTICS"
-    SECURITY = "SECURITY"
-    SOFTWARE_DEVELOPMENT = "SOFTWARE_DEVELOPMENT"
-    SURVEILLANCE = "SURVEILLANCE"
-    TRANSPORTATION = "TRANSPORTATION"
-    OTHER = "OTHER"
+from typing import Optional, Union
+from pydantic import Extra,  BaseModel, Field, StrictFloat, StrictInt, conint
 
+class AnnotationOfferData(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    AnnotationOfferData
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """Sector - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(Sector, dict):
-            for key, value in self.items():
-                result[key] = value
+    cost: Optional[Union[StrictFloat, StrictInt]] = None
+    completed_by: Optional[conint(strict=True, ge=0)] = Field(None, alias="completedBy", description="unix timestamp in milliseconds")
+    __properties = ["cost", "completedBy"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, Sector):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, Sector):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> AnnotationOfferData:
+        """Create an instance of AnnotationOfferData from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> AnnotationOfferData:
+        """Create an instance of AnnotationOfferData from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return AnnotationOfferData.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in AnnotationOfferData) in the input: " + str(obj))
+
+        _obj = AnnotationOfferData.parse_obj({
+            "cost": obj.get("cost"),
+            "completed_by": obj.get("completedBy")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/selection_input_predictions_name.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/selection_config_entry.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,101 +1,88 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class SelectionInputPredictionsName(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
 
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field
+from lightly.openapi_generated.swagger_client.models.selection_config_entry_input import SelectionConfigEntryInput
+from lightly.openapi_generated.swagger_client.models.selection_config_entry_strategy import SelectionConfigEntryStrategy
 
+class SelectionConfigEntry(BaseModel):
     """
-    allowed enum values
+    SelectionConfigEntry
     """
-    CLASS_DISTRIBUTION = "CLASS_DISTRIBUTION"
-    CATEGORY_COUNT = "CATEGORY_COUNT"
+    input: SelectionConfigEntryInput = Field(...)
+    strategy: SelectionConfigEntryStrategy = Field(...)
+    __properties = ["input", "strategy"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """SelectionInputPredictionsName - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(SelectionInputPredictionsName, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, SelectionInputPredictionsName):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, SelectionInputPredictionsName):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> SelectionConfigEntry:
+        """Create an instance of SelectionConfigEntry from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        # override the default output from pydantic by calling `to_dict()` of input
+        if self.input:
+            _dict['input' if by_alias else 'input'] = self.input.to_dict(by_alias=by_alias)
+        # override the default output from pydantic by calling `to_dict()` of strategy
+        if self.strategy:
+            _dict['strategy' if by_alias else 'strategy'] = self.strategy.to_dict(by_alias=by_alias)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> SelectionConfigEntry:
+        """Create an instance of SelectionConfigEntry from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return SelectionConfigEntry.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in SelectionConfigEntry) in the input: " + str(obj))
+
+        _obj = SelectionConfigEntry.parse_obj({
+            "input": SelectionConfigEntryInput.from_dict(obj.get("input")) if obj.get("input") is not None else None,
+            "strategy": SelectionConfigEntryStrategy.from_dict(obj.get("strategy")) if obj.get("strategy") is not None else None
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/selection_strategy_threshold_operation.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/selection_config_entry_strategy.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,103 +1,88 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
-
-
-class SelectionStrategyThresholdOperation(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
 
-    """
-    allowed enum values
-    """
-    SMALLER = "SMALLER"
-    SMALLER_EQUAL = "SMALLER_EQUAL"
-    BIGGER = "BIGGER"
-    BIGGER_EQUAL = "BIGGER_EQUAL"
+from typing import Any, Dict, Optional, Union
+from pydantic import Extra,  BaseModel, Field, StrictFloat, StrictInt
+from lightly.openapi_generated.swagger_client.models.selection_strategy_threshold_operation import SelectionStrategyThresholdOperation
+from lightly.openapi_generated.swagger_client.models.selection_strategy_type import SelectionStrategyType
 
+class SelectionConfigEntryStrategy(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    SelectionConfigEntryStrategy
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """SelectionStrategyThresholdOperation - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(SelectionStrategyThresholdOperation, dict):
-            for key, value in self.items():
-                result[key] = value
+    type: SelectionStrategyType = Field(...)
+    stopping_condition_minimum_distance: Optional[Union[StrictFloat, StrictInt]] = None
+    threshold: Optional[Union[StrictFloat, StrictInt]] = None
+    operation: Optional[SelectionStrategyThresholdOperation] = None
+    target: Optional[Dict[str, Any]] = None
+    __properties = ["type", "stopping_condition_minimum_distance", "threshold", "operation", "target"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, SelectionStrategyThresholdOperation):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, SelectionStrategyThresholdOperation):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> SelectionConfigEntryStrategy:
+        """Create an instance of SelectionConfigEntryStrategy from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> SelectionConfigEntryStrategy:
+        """Create an instance of SelectionConfigEntryStrategy from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return SelectionConfigEntryStrategy.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in SelectionConfigEntryStrategy) in the input: " + str(obj))
+
+        _obj = SelectionConfigEntryStrategy.parse_obj({
+            "type": obj.get("type"),
+            "stopping_condition_minimum_distance": obj.get("stopping_condition_minimum_distance"),
+            "threshold": obj.get("threshold"),
+            "operation": obj.get("operation"),
+            "target": obj.get("target")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_arithmetics_operation.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/video_frame_data.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,102 +1,82 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
-
-
-class TagArithmeticsOperation(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
 
-    """
-    allowed enum values
-    """
-    UNION = "UNION"
-    INTERSECTION = "INTERSECTION"
-    DIFFERENCE = "DIFFERENCE"
+from typing import Optional, Union
+from pydantic import Extra,  BaseModel, Field, StrictFloat, StrictInt, StrictStr
 
+class VideoFrameData(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    VideoFrameData
     """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """TagArithmeticsOperation - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(TagArithmeticsOperation, dict):
-            for key, value in self.items():
-                result[key] = value
+    source_video: Optional[StrictStr] = Field(None, alias="sourceVideo", description="Name of the source video.")
+    source_video_frame_index: Optional[Union[StrictFloat, StrictInt]] = Field(None, alias="sourceVideoFrameIndex", description="Index of the frame in the source video.")
+    source_video_frame_timestamp: Optional[Union[StrictFloat, StrictInt]] = Field(None, alias="sourceVideoFrameTimestamp", description="Timestamp of the frame in the source video.")
+    __properties = ["sourceVideo", "sourceVideoFrameIndex", "sourceVideoFrameTimestamp"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, TagArithmeticsOperation):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, TagArithmeticsOperation):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> VideoFrameData:
+        """Create an instance of VideoFrameData from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> VideoFrameData:
+        """Create an instance of VideoFrameData from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return VideoFrameData.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in VideoFrameData) in the input: " + str(obj))
+
+        _obj = VideoFrameData.parse_obj({
+            "source_video": obj.get("sourceVideo"),
+            "source_video_frame_index": obj.get("sourceVideoFrameIndex"),
+            "source_video_frame_timestamp": obj.get("sourceVideoFrameTimestamp")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_change_data.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_run_log_entry_data.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,95 +1,86 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class TagChangeData(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, StrictStr, conint
+from lightly.openapi_generated.swagger_client.models.docker_run_log_level import DockerRunLogLevel
+from lightly.openapi_generated.swagger_client.models.docker_run_state import DockerRunState
 
+class DockerRunLogEntryData(BaseModel):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DockerRunLogEntryData
     """
-    swagger_types = {
-    }
+    msg: StrictStr = Field(...)
+    ts: conint(strict=True, ge=0) = Field(..., description="unix timestamp in milliseconds")
+    state: DockerRunState = Field(...)
+    level: DockerRunLogLevel = Field(...)
+    __properties = ["msg", "ts", "state", "level"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """TagChangeData - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(TagChangeData, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, TagChangeData):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, TagChangeData):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerRunLogEntryData:
+        """Create an instance of DockerRunLogEntryData from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerRunLogEntryData:
+        """Create an instance of DockerRunLogEntryData from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerRunLogEntryData.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerRunLogEntryData) in the input: " + str(obj))
+
+        _obj = DockerRunLogEntryData.parse_obj({
+            "msg": obj.get("msg"),
+            "ts": obj.get("ts"),
+            "state": obj.get("state"),
+            "level": obj.get("level")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_change_data_initial.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/tag_change_data_scatterplot.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,123 +1,87 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
+from typing import Optional, Union
+from pydantic import Extra,  BaseModel, Field, StrictStr, confloat, conint
+from lightly.openapi_generated.swagger_client.models.tag_change_data_operation_method import TagChangeDataOperationMethod
 
-class TagChangeDataInitial(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
+class TagChangeDataScatterplot(BaseModel):
     """
-
+    TagChangeDataScatterplot
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-        'run_id': 'MongoObjectID'
-    }
-
-    attribute_map = {
-        'run_id': 'runId'
-    }
-
-    def __init__(self, run_id=None, _configuration=None):  # noqa: E501
-        """TagChangeDataInitial - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-
-        self._run_id = None
-        self.discriminator = None
-
-        if run_id is not None:
-            self.run_id = run_id
-
-    @property
-    def run_id(self):
-        """Gets the run_id of this TagChangeDataInitial.  # noqa: E501
-
-
-        :return: The run_id of this TagChangeDataInitial.  # noqa: E501
-        :rtype: MongoObjectID
-        """
-        return self._run_id
-
-    @run_id.setter
-    def run_id(self, run_id):
-        """Sets the run_id of this TagChangeDataInitial.
-
-
-        :param run_id: The run_id of this TagChangeDataInitial.  # noqa: E501
-        :type: MongoObjectID
-        """
-
-        self._run_id = run_id
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(TagChangeDataInitial, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
+    method: TagChangeDataOperationMethod = Field(...)
+    view: Optional[StrictStr] = None
+    count: Union[confloat(ge=0, strict=True), conint(ge=0, strict=True)] = Field(...)
+    added: Union[confloat(ge=0, strict=True), conint(ge=0, strict=True)] = Field(...)
+    removed: Union[confloat(ge=0, strict=True), conint(ge=0, strict=True)] = Field(...)
+    __properties = ["method", "view", "count", "added", "removed"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, TagChangeDataInitial):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, TagChangeDataInitial):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> TagChangeDataScatterplot:
+        """Create an instance of TagChangeDataScatterplot from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> TagChangeDataScatterplot:
+        """Create an instance of TagChangeDataScatterplot from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return TagChangeDataScatterplot.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in TagChangeDataScatterplot) in the input: " + str(obj))
+
+        _obj = TagChangeDataScatterplot.parse_obj({
+            "method": obj.get("method"),
+            "view": obj.get("view"),
+            "count": obj.get("count"),
+            "added": obj.get("added"),
+            "removed": obj.get("removed")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_change_data_operation_method.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_criterion.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,102 +1,78 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class TagChangeDataOperationMethod(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from typing import Optional, Union
+from pydantic import Extra,  BaseModel, confloat, conint
 
+class DockerWorkerConfigV3LightlyCriterion(BaseModel):
     """
-    allowed enum values
+    DockerWorkerConfigV3LightlyCriterion
     """
-    SELECTED = "selected"
-    ADDED = "added"
-    REMOVED = "removed"
+    temperature: Optional[Union[confloat(gt=0.0, strict=True), conint(gt=0, strict=True)]] = None
+    __properties = ["temperature"]
 
-    """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """TagChangeDataOperationMethod - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(TagChangeDataOperationMethod, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, TagChangeDataOperationMethod):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, TagChangeDataOperationMethod):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DockerWorkerConfigV3LightlyCriterion:
+        """Create an instance of DockerWorkerConfigV3LightlyCriterion from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DockerWorkerConfigV3LightlyCriterion:
+        """Create an instance of DockerWorkerConfigV3LightlyCriterion from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DockerWorkerConfigV3LightlyCriterion.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DockerWorkerConfigV3LightlyCriterion) in the input: " + str(obj))
+
+        _obj = DockerWorkerConfigV3LightlyCriterion.parse_obj({
+            "temperature": obj.get("temperature")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_creator.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/prediction_singleton_classification_all_of.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,108 +1,78 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
-
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
+import json
 
 
-class TagCreator(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from typing import List, Optional, Union
+from pydantic import Extra,  BaseModel, Field, confloat, conint, conlist
 
+class PredictionSingletonClassificationAllOf(BaseModel):
     """
-    allowed enum values
+    PredictionSingletonClassificationAllOf
     """
-    UNKNOWN = "UNKNOWN"
-    USER_WEBAPP = "USER_WEBAPP"
-    USER_PIP = "USER_PIP"
-    USER_PIP_LIGHTLY_MAGIC = "USER_PIP_LIGHTLY_MAGIC"
-    USER_WORKER = "USER_WORKER"
-    SAMPLER_ACTIVE_LEARNING = "SAMPLER_ACTIVE_LEARNING"
-    SAMPLER_CORAL = "SAMPLER_CORAL"
-    SAMPLER_CORESET = "SAMPLER_CORESET"
-    SAMPLER_RANDOM = "SAMPLER_RANDOM"
+    probabilities: Optional[conlist(Union[confloat(le=1, ge=0, strict=True), conint(le=1, ge=0, strict=True)])] = Field(None, description="The probabilities of it being a certain category other than the one which was selected. The sum of all probabilities should equal 1.")
+    __properties = ["probabilities"]
 
-    """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
-    """
-    swagger_types = {
-    }
-
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """TagCreator - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(TagCreator, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, TagCreator):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, TagCreator):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> PredictionSingletonClassificationAllOf:
+        """Create an instance of PredictionSingletonClassificationAllOf from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> PredictionSingletonClassificationAllOf:
+        """Create an instance of PredictionSingletonClassificationAllOf from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return PredictionSingletonClassificationAllOf.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in PredictionSingletonClassificationAllOf) in the input: " + str(obj))
+
+        _obj = PredictionSingletonClassificationAllOf.parse_obj({
+            "probabilities": obj.get("probabilities")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/models/tag_filenames_data.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/models/datasource_config_gcs.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,95 +1,86 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
+
+    Do not edit the class manually.
 """
 
 
+from __future__ import annotations
 import pprint
 import re  # noqa: F401
+import json
 
-import six
-
-from lightly.openapi_generated.swagger_client.configuration import Configuration
 
 
-class TagFilenamesData(object):
-    """NOTE: This class is auto generated by the swagger code generator program.
-
-    Do not edit the class manually.
-    """
+from pydantic import Extra,  BaseModel, Field, StrictStr, constr
+from lightly.openapi_generated.swagger_client.models.datasource_config_base import DatasourceConfigBase
 
+class DatasourceConfigGCS(DatasourceConfigBase):
     """
-    Attributes:
-      swagger_types (dict): The key is attribute name
-                            and the value is attribute type.
-      attribute_map (dict): The key is attribute name
-                            and the value is json key in definition.
+    DatasourceConfigGCS
     """
-    swagger_types = {
-    }
+    gcs_project_id: constr(strict=True, min_length=1) = Field(..., alias="gcsProjectId", description="The projectId where you have your bucket configured")
+    gcs_credentials: StrictStr = Field(..., alias="gcsCredentials", description="this is the content of the credentials JSON file stringified which you downloaded from Google Cloud Platform")
+    __properties = ["id", "purpose", "type", "fullPath", "thumbSuffix", "gcsProjectId", "gcsCredentials"]
+
+    class Config:
+        """Pydantic configuration"""
+        allow_population_by_field_name = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = Extra.forbid
 
-    attribute_map = {
-    }
-
-    def __init__(self, _configuration=None):  # noqa: E501
-        """TagFilenamesData - a model defined in Swagger"""  # noqa: E501
-        if _configuration is None:
-            _configuration = Configuration()
-        self._configuration = _configuration
-        self.discriminator = None
-
-    def to_dict(self):
-        """Returns the model properties as a dict"""
-        result = {}
-
-        for attr, _ in six.iteritems(self.swagger_types):
-            value = getattr(self, attr)
-            if isinstance(value, list):
-                result[attr] = list(map(
-                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
-                    value
-                ))
-            elif hasattr(value, "to_dict"):
-                result[attr] = value.to_dict()
-            elif isinstance(value, dict):
-                result[attr] = dict(map(
-                    lambda item: (item[0], item[1].to_dict())
-                    if hasattr(item[1], "to_dict") else item,
-                    value.items()
-                ))
-            else:
-                result[attr] = value
-        if issubclass(TagFilenamesData, dict):
-            for key, value in self.items():
-                result[key] = value
-
-        return result
-
-    def to_str(self):
+    def to_str(self, by_alias: bool = False) -> str:
         """Returns the string representation of the model"""
-        return pprint.pformat(self.to_dict())
-
-    def __repr__(self):
-        """For `print` and `pprint`"""
-        return self.to_str()
-
-    def __eq__(self, other):
-        """Returns true if both objects are equal"""
-        if not isinstance(other, TagFilenamesData):
-            return False
-
-        return self.to_dict() == other.to_dict()
+        return pprint.pformat(self.dict(by_alias=by_alias))
 
-    def __ne__(self, other):
-        """Returns true if both objects are not equal"""
-        if not isinstance(other, TagFilenamesData):
-            return True
+    def to_json(self, by_alias: bool = False) -> str:
+        """Returns the JSON representation of the model"""
+        return json.dumps(self.to_dict(by_alias=by_alias))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> DatasourceConfigGCS:
+        """Create an instance of DatasourceConfigGCS from a JSON string"""
+        return cls.from_dict(json.loads(json_str))
+
+    def to_dict(self, by_alias: bool = False):
+        """Returns the dictionary representation of the model"""
+        _dict = self.dict(by_alias=by_alias,
+                          exclude={
+                          },
+                          exclude_none=True)
+        return _dict
+
+    @classmethod
+    def from_dict(cls, obj: dict) -> DatasourceConfigGCS:
+        """Create an instance of DatasourceConfigGCS from a dict"""
+        if obj is None:
+            return None
+
+        if not isinstance(obj, dict):
+            return DatasourceConfigGCS.parse_obj(obj)
+
+        # raise errors for additional fields in the input
+        for _key in obj.keys():
+            if _key not in cls.__properties:
+                raise ValueError("Error due to additional fields (not defined in DatasourceConfigGCS) in the input: " + str(obj))
+
+        _obj = DatasourceConfigGCS.parse_obj({
+            "id": obj.get("id"),
+            "purpose": obj.get("purpose"),
+            "type": obj.get("type"),
+            "full_path": obj.get("fullPath"),
+            "thumb_suffix": obj.get("thumbSuffix"),
+            "gcs_project_id": obj.get("gcsProjectId"),
+            "gcs_credentials": obj.get("gcsCredentials")
+        })
+        return _obj
 
-        return self.to_dict() != other.to_dict()
```

### Comparing `lightly-1.4.7/lightly/openapi_generated/swagger_client/rest.py` & `lightly-1.4.8/lightly/openapi_generated/swagger_client/rest.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,37 +1,32 @@
 # coding: utf-8
 
 """
     Lightly API
 
     Lightly.ai enables you to do self-supervised learning in an easy and intuitive way. The lightly.ai OpenAPI spec defines how one can interact with our REST API to unleash the full potential of lightly.ai  # noqa: E501
 
-    OpenAPI spec version: 1.0.0
+    The version of the OpenAPI document: 1.0.0
     Contact: support@lightly.ai
-    Generated by: https://github.com/swagger-api/swagger-codegen.git
-"""
+    Generated by OpenAPI Generator (https://openapi-generator.tech)
 
+    Do not edit the class manually.
+"""
 
-from __future__ import absolute_import
 
 import io
 import json
 import logging
 import re
 import ssl
 
-import certifi
-# python 2 and python 3 compatibility library
-import six
-from six.moves.urllib.parse import urlencode
-
-try:
-    import urllib3
-except ImportError:
-    raise ImportError('Swagger python client requires urllib3.')
+from urllib.parse import urlencode, quote_plus
+import urllib3
+
+from lightly.openapi_generated.swagger_client.exceptions import ApiException, UnauthorizedException, ForbiddenException, NotFoundException, ServiceException, ApiValueError
 
 
 logger = logging.getLogger(__name__)
 
 
 class RESTResponse(io.IOBase):
 
@@ -39,19 +34,19 @@
         self.urllib3_response = resp
         self.status = resp.status
         self.reason = resp.reason
         self.data = resp.data
 
     def getheaders(self):
         """Returns a dictionary of the response headers."""
-        return self.urllib3_response.getheaders()
+        return self.urllib3_response.headers
 
     def getheader(self, name, default=None):
         """Returns a given response header."""
-        return self.urllib3_response.getheader(name, default)
+        return self.urllib3_response.headers.get(name, default)
 
 
 class RESTClientObject(object):
 
     def __init__(self, configuration, pools_size=4, maxsize=None):
         # urllib3.PoolManager will pass all kw parameters to connectionpool
         # https://github.com/shazow/urllib3/blob/f9409436f83aeb79fbaf090181cd81b784f1b8ce/urllib3/poolmanager.py#L75  # noqa: E501
@@ -61,49 +56,53 @@
 
         # cert_reqs
         if configuration.verify_ssl:
             cert_reqs = ssl.CERT_REQUIRED
         else:
             cert_reqs = ssl.CERT_NONE
 
-        # ca_certs
-        if configuration.ssl_ca_cert:
-            ca_certs = configuration.ssl_ca_cert
-        else:
-            # if not set certificate file, use Mozilla's root certificates.
-            ca_certs = certifi.where()
-
         addition_pool_args = {}
         if configuration.assert_hostname is not None:
             addition_pool_args['assert_hostname'] = configuration.assert_hostname  # noqa: E501
 
+        if configuration.retries is not None:
+            addition_pool_args['retries'] = configuration.retries
+
+        if configuration.tls_server_name:
+            addition_pool_args['server_hostname'] = configuration.tls_server_name
+
+
+        if configuration.socket_options is not None:
+            addition_pool_args['socket_options'] = configuration.socket_options
+
         if maxsize is None:
             if configuration.connection_pool_maxsize is not None:
                 maxsize = configuration.connection_pool_maxsize
             else:
                 maxsize = 4
 
         # https pool manager
         if configuration.proxy:
             self.pool_manager = urllib3.ProxyManager(
                 num_pools=pools_size,
                 maxsize=maxsize,
                 cert_reqs=cert_reqs,
-                ca_certs=ca_certs,
+                ca_certs=configuration.ssl_ca_cert,
                 cert_file=configuration.cert_file,
                 key_file=configuration.key_file,
                 proxy_url=configuration.proxy,
+                proxy_headers=configuration.proxy_headers,
                 **addition_pool_args
             )
         else:
             self.pool_manager = urllib3.PoolManager(
                 num_pools=pools_size,
                 maxsize=maxsize,
                 cert_reqs=cert_reqs,
-                ca_certs=ca_certs,
+                ca_certs=configuration.ssl_ca_cert,
                 cert_file=configuration.cert_file,
                 key_file=configuration.key_file,
                 **addition_pool_args
             )
 
     def request(self, method, url, query_params=None, headers=None,
                 body=None, post_params=None, _preload_content=True,
@@ -127,42 +126,42 @@
                                  (connection, read) timeouts.
         """
         method = method.upper()
         assert method in ['GET', 'HEAD', 'DELETE', 'POST', 'PUT',
                           'PATCH', 'OPTIONS']
 
         if post_params and body:
-            raise ValueError(
+            raise ApiValueError(
                 "body parameter cannot be used with post_params parameter."
             )
 
         post_params = post_params or {}
         headers = headers or {}
+        # url already contains the URL query string
+        # so reset query_params to empty dict
+        query_params = {}
 
         timeout = None
         if _request_timeout:
-            if isinstance(_request_timeout, (int, ) if six.PY3 else (int, long)):  # noqa: E501,F821
+            if isinstance(_request_timeout, (int,float)):  # noqa: E501,F821
                 timeout = urllib3.Timeout(total=_request_timeout)
             elif (isinstance(_request_timeout, tuple) and
                   len(_request_timeout) == 2):
                 timeout = urllib3.Timeout(
                     connect=_request_timeout[0], read=_request_timeout[1])
 
-        if 'Content-Type' not in headers:
-            headers['Content-Type'] = 'application/json'
-
         try:
             # For `POST`, `PUT`, `PATCH`, `OPTIONS`, `DELETE`
             if method in ['POST', 'PUT', 'PATCH', 'OPTIONS', 'DELETE']:
-                if query_params:
-                    url += '?' + urlencode(query_params)
-                if re.search('json', headers['Content-Type'], re.IGNORECASE):
-                    request_body = '{}'
+
+                # no content type provided or payload is json
+                if not headers.get('Content-Type') or re.search('json', headers['Content-Type'], re.IGNORECASE):
+                    request_body = None
                     if body is not None:
-                        request_body = json.dumps(body)
+                        request_body = json.dumps(body, allow_nan=False)
                     r = self.pool_manager.request(
                         method, url,
                         body=request_body,
                         preload_content=_preload_content,
                         timeout=timeout,
                         headers=headers)
                 elif headers['Content-Type'] == 'application/x-www-form-urlencoded':  # noqa: E501
@@ -184,15 +183,15 @@
                         encode_multipart=True,
                         preload_content=_preload_content,
                         timeout=timeout,
                         headers=headers)
                 # Pass a `string` parameter directly in the body to support
                 # other content types than Json when `body` argument is
                 # provided in serialized form
-                elif isinstance(body, str):
+                elif isinstance(body, str) or isinstance(body, bytes):
                     request_body = body
                     r = self.pool_manager.request(
                         method, url,
                         body=request_body,
                         preload_content=_preload_content,
                         timeout=timeout,
                         headers=headers)
@@ -201,123 +200,102 @@
                     msg = """Cannot prepare a request message for provided
                              arguments. Please check that your arguments match
                              declared content type."""
                     raise ApiException(status=0, reason=msg)
             # For `GET`, `HEAD`
             else:
                 r = self.pool_manager.request(method, url,
-                                              fields=query_params,
+                                              fields={},
                                               preload_content=_preload_content,
                                               timeout=timeout,
                                               headers=headers)
         except urllib3.exceptions.SSLError as e:
             msg = "{0}\n{1}".format(type(e).__name__, str(e))
             raise ApiException(status=0, reason=msg)
 
         if _preload_content:
             r = RESTResponse(r)
 
-            # In the python 3, the response.data is bytes.
-            # we need to decode it to string.
-            if six.PY3:
-                r.data = r.data.decode('utf8')
-
             # log response body
             logger.debug("response body: %s", r.data)
 
         if not 200 <= r.status <= 299:
+            if r.status == 401:
+                raise UnauthorizedException(http_resp=r)
+
+            if r.status == 403:
+                raise ForbiddenException(http_resp=r)
+
+            if r.status == 404:
+                raise NotFoundException(http_resp=r)
+
+            if 500 <= r.status <= 599:
+                raise ServiceException(http_resp=r)
+
             raise ApiException(http_resp=r)
 
         return r
 
-    def GET(self, url, headers=None, query_params=None, _preload_content=True,
+    def get_request(self, url, headers=None, query_params=None, _preload_content=True,
             _request_timeout=None):
         return self.request("GET", url,
                             headers=headers,
                             _preload_content=_preload_content,
                             _request_timeout=_request_timeout,
                             query_params=query_params)
 
-    def HEAD(self, url, headers=None, query_params=None, _preload_content=True,
+    def head_request(self, url, headers=None, query_params=None, _preload_content=True,
              _request_timeout=None):
         return self.request("HEAD", url,
                             headers=headers,
                             _preload_content=_preload_content,
                             _request_timeout=_request_timeout,
                             query_params=query_params)
 
-    def OPTIONS(self, url, headers=None, query_params=None, post_params=None,
+    def options_request(self, url, headers=None, query_params=None, post_params=None,
                 body=None, _preload_content=True, _request_timeout=None):
         return self.request("OPTIONS", url,
                             headers=headers,
                             query_params=query_params,
                             post_params=post_params,
                             _preload_content=_preload_content,
                             _request_timeout=_request_timeout,
                             body=body)
 
-    def DELETE(self, url, headers=None, query_params=None, body=None,
+    def delete_request(self, url, headers=None, query_params=None, body=None,
                _preload_content=True, _request_timeout=None):
         return self.request("DELETE", url,
                             headers=headers,
                             query_params=query_params,
                             _preload_content=_preload_content,
                             _request_timeout=_request_timeout,
                             body=body)
 
-    def POST(self, url, headers=None, query_params=None, post_params=None,
+    def post_request(self, url, headers=None, query_params=None, post_params=None,
              body=None, _preload_content=True, _request_timeout=None):
         return self.request("POST", url,
                             headers=headers,
                             query_params=query_params,
                             post_params=post_params,
                             _preload_content=_preload_content,
                             _request_timeout=_request_timeout,
                             body=body)
 
-    def PUT(self, url, headers=None, query_params=None, post_params=None,
+    def put_request(self, url, headers=None, query_params=None, post_params=None,
             body=None, _preload_content=True, _request_timeout=None):
         return self.request("PUT", url,
                             headers=headers,
                             query_params=query_params,
                             post_params=post_params,
                             _preload_content=_preload_content,
                             _request_timeout=_request_timeout,
                             body=body)
 
-    def PATCH(self, url, headers=None, query_params=None, post_params=None,
+    def patch_request(self, url, headers=None, query_params=None, post_params=None,
               body=None, _preload_content=True, _request_timeout=None):
         return self.request("PATCH", url,
                             headers=headers,
                             query_params=query_params,
                             post_params=post_params,
                             _preload_content=_preload_content,
                             _request_timeout=_request_timeout,
                             body=body)
-
-
-class ApiException(Exception):
-
-    def __init__(self, status=None, reason=None, http_resp=None):
-        if http_resp:
-            self.status = http_resp.status
-            self.reason = http_resp.reason
-            self.body = http_resp.data
-            self.headers = http_resp.getheaders()
-        else:
-            self.status = status
-            self.reason = reason
-            self.body = None
-            self.headers = None
-
-    def __str__(self):
-        """Custom error messages for exception"""
-        error_message = "({0})\n"\
-                        "Reason: {1}\n".format(self.status, self.reason)
-        if self.headers:
-            error_message += "HTTP response headers: {0}\n".format(
-                self.headers)
-
-        if self.body:
-            error_message += "HTTP response body: {0}\n".format(self.body)
-
-        return error_message
```

### Comparing `lightly-1.4.7/lightly/transforms/__init__.py` & `lightly-1.4.8/lightly/transforms/__init__.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/dino_transform.py` & `lightly-1.4.8/lightly/transforms/dino_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/fast_siam_transform.py` & `lightly-1.4.8/lightly/transforms/fast_siam_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/gaussian_blur.py` & `lightly-1.4.8/lightly/transforms/gaussian_blur.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/image_grid_transform.py` & `lightly-1.4.8/lightly/transforms/image_grid_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/jigsaw.py` & `lightly-1.4.8/lightly/transforms/jigsaw.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/mae_transform.py` & `lightly-1.4.8/lightly/transforms/mae_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/moco_transform.py` & `lightly-1.4.8/lightly/transforms/moco_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/msn_transform.py` & `lightly-1.4.8/lightly/transforms/msn_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/multi_crop_transform.py` & `lightly-1.4.8/lightly/transforms/multi_crop_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/multi_view_transform.py` & `lightly-1.4.8/lightly/transforms/multi_view_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/pirl_transform.py` & `lightly-1.4.8/lightly/transforms/pirl_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/random_crop_and_flip_with_grid.py` & `lightly-1.4.8/lightly/transforms/random_crop_and_flip_with_grid.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/rotation.py` & `lightly-1.4.8/lightly/transforms/rotation.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/simclr_transform.py` & `lightly-1.4.8/lightly/transforms/simclr_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/simsiam_transform.py` & `lightly-1.4.8/lightly/transforms/simsiam_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/smog_transform.py` & `lightly-1.4.8/lightly/transforms/smog_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/solarize.py` & `lightly-1.4.8/lightly/transforms/solarize.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/swav_transform.py` & `lightly-1.4.8/lightly/transforms/swav_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/vicreg_transform.py` & `lightly-1.4.8/lightly/transforms/vicreg_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/transforms/vicregl_transform.py` & `lightly-1.4.8/lightly/transforms/vicregl_transform.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/benchmarking/benchmark_module.py` & `lightly-1.4.8/lightly/utils/benchmarking/benchmark_module.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/benchmarking/knn.py` & `lightly-1.4.8/lightly/utils/benchmarking/knn.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/benchmarking/knn_classifier.py` & `lightly-1.4.8/lightly/utils/benchmarking/knn_classifier.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,19 +19,18 @@
         knn_k: int = 200,
         knn_t: float = 0.1,
         topk: Tuple[int, ...] = (1, 5),
         feature_dtype: torch.dtype = torch.float32,
     ):
         """KNN classifier for benchmarking.
 
-        Settings based on "Unsupervised Feature Learning via Non-Parametric Instance
-        Discrimination" [0]. Code adapted from MoCo [1].
+        Settings based on InstDisc [0]. Code adapted from MoCo [1].
 
-        - [0]: https://arxiv.org/pdf/1805.01978v1.pdf
-        - [1]: https://github.com/facebookresearch/moco
+        - [0]: InstDisc, 2018, https://arxiv.org/pdf/1805.01978v1.pdf
+        - [1]: MoCo, 2019, https://github.com/facebookresearch/moco
 
         Args:
             model:
                 Model used for feature extraction. Must define a forward(images) method
                 that returns a feature tensor.
             num_classes:
                 Number of classes in the dataset.
```

### Comparing `lightly-1.4.7/lightly/utils/benchmarking/linear_classifier.py` & `lightly-1.4.8/lightly/utils/benchmarking/linear_classifier.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/benchmarking/metric_callback.py` & `lightly-1.4.8/lightly/utils/benchmarking/metric_callback.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/benchmarking/online_linear_classifier.py` & `lightly-1.4.8/lightly/utils/benchmarking/online_linear_classifier.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/benchmarking/topk.py` & `lightly-1.4.8/lightly/utils/benchmarking/topk.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/bounding_box.py` & `lightly-1.4.8/lightly/utils/bounding_box.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/cropping/crop_image_by_bounding_boxes.py` & `lightly-1.4.8/lightly/utils/cropping/crop_image_by_bounding_boxes.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/cropping/read_yolo_label_file.py` & `lightly-1.4.8/lightly/utils/cropping/read_yolo_label_file.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/debug.py` & `lightly-1.4.8/lightly/utils/debug.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/dist.py` & `lightly-1.4.8/lightly/utils/dist.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/embeddings_2d.py` & `lightly-1.4.8/lightly/utils/embeddings_2d.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/hipify.py` & `lightly-1.4.8/lightly/utils/hipify.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/io.py` & `lightly-1.4.8/lightly/utils/io.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/lars.py` & `lightly-1.4.8/lightly/utils/lars.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/reordering.py` & `lightly-1.4.8/lightly/utils/reordering.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/scheduler.py` & `lightly-1.4.8/lightly/utils/scheduler.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly/utils/version_compare.py` & `lightly-1.4.8/lightly/utils/version_compare.py`

 * *Files identical despite different names*

### Comparing `lightly-1.4.7/lightly.egg-info/PKG-INFO` & `lightly-1.4.8/lightly.egg-info/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: lightly
-Version: 1.4.7
+Version: 1.4.8
 Summary: A deep learning package for self-supervised learning
 Author: Philipp Wirth & Igor Susmelj
 Author-email: philipp@lightly.ai
 License: MIT
 Project-URL: Homepage, https://www.lightly.ai
 Project-URL: Web-App, https://app.lightly.ai
 Project-URL: Documentation, https://docs.lightly.ai
```

### Comparing `lightly-1.4.7/lightly.egg-info/SOURCES.txt` & `lightly-1.4.8/lightly.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -29,15 +29,14 @@
 lightly/api/api_workflow_tags.py
 lightly/api/api_workflow_upload_dataset.py
 lightly/api/api_workflow_upload_embeddings.py
 lightly/api/api_workflow_upload_metadata.py
 lightly/api/bitmask.py
 lightly/api/download.py
 lightly/api/patch.py
-lightly/api/prediction_singletons.py
 lightly/api/swagger_api_client.py
 lightly/api/swagger_rest_client.py
 lightly/api/utils.py
 lightly/api/version_checking.py
 lightly/cli/__init__.py
 lightly/cli/_cli_simclr.py
 lightly/cli/_helpers.py
@@ -93,17 +92,20 @@
 lightly/models/simsiam.py
 lightly/models/utils.py
 lightly/models/zoo.py
 lightly/models/modules/__init__.py
 lightly/models/modules/heads.py
 lightly/models/modules/masked_autoencoder.py
 lightly/models/modules/nn_memory_bank.py
+lightly/openapi_generated/__init__.py
 lightly/openapi_generated/swagger_client/__init__.py
 lightly/openapi_generated/swagger_client/api_client.py
+lightly/openapi_generated/swagger_client/api_response.py
 lightly/openapi_generated/swagger_client/configuration.py
+lightly/openapi_generated/swagger_client/exceptions.py
 lightly/openapi_generated/swagger_client/rest.py
 lightly/openapi_generated/swagger_client/api/__init__.py
 lightly/openapi_generated/swagger_client/api/collaboration_api.py
 lightly/openapi_generated/swagger_client/api/datasets_api.py
 lightly/openapi_generated/swagger_client/api/datasources_api.py
 lightly/openapi_generated/swagger_client/api/docker_api.py
 lightly/openapi_generated/swagger_client/api/embeddings2d_api.py
@@ -116,69 +118,76 @@
 lightly/openapi_generated/swagger_client/api/samples_api.py
 lightly/openapi_generated/swagger_client/api/samplings_api.py
 lightly/openapi_generated/swagger_client/api/scores_api.py
 lightly/openapi_generated/swagger_client/api/tags_api.py
 lightly/openapi_generated/swagger_client/api/teams_api.py
 lightly/openapi_generated/swagger_client/api/versioning_api.py
 lightly/openapi_generated/swagger_client/models/__init__.py
-lightly/openapi_generated/swagger_client/models/access_role.py
 lightly/openapi_generated/swagger_client/models/active_learning_score_create_request.py
 lightly/openapi_generated/swagger_client/models/active_learning_score_data.py
-lightly/openapi_generated/swagger_client/models/active_learning_score_type.py
-lightly/openapi_generated/swagger_client/models/active_learning_scores.py
+lightly/openapi_generated/swagger_client/models/annotation_data.py
+lightly/openapi_generated/swagger_client/models/annotation_meta_data.py
+lightly/openapi_generated/swagger_client/models/annotation_offer_data.py
+lightly/openapi_generated/swagger_client/models/annotation_state.py
 lightly/openapi_generated/swagger_client/models/api_error_code.py
 lightly/openapi_generated/swagger_client/models/api_error_response.py
 lightly/openapi_generated/swagger_client/models/async_task_data.py
-lightly/openapi_generated/swagger_client/models/bounding_box.py
-lightly/openapi_generated/swagger_client/models/category_id.py
-lightly/openapi_generated/swagger_client/models/category_name.py
+lightly/openapi_generated/swagger_client/models/auth0_on_sign_up_request.py
+lightly/openapi_generated/swagger_client/models/auth0_on_sign_up_request_user.py
 lightly/openapi_generated/swagger_client/models/configuration_data.py
 lightly/openapi_generated/swagger_client/models/configuration_entry.py
 lightly/openapi_generated/swagger_client/models/configuration_set_request.py
 lightly/openapi_generated/swagger_client/models/configuration_value_data_type.py
+lightly/openapi_generated/swagger_client/models/create_cf_bucket_activity_request.py
 lightly/openapi_generated/swagger_client/models/create_docker_worker_registry_entry_request.py
 lightly/openapi_generated/swagger_client/models/create_entity_response.py
 lightly/openapi_generated/swagger_client/models/create_sample_with_write_urls_response.py
+lightly/openapi_generated/swagger_client/models/create_team_membership_request.py
 lightly/openapi_generated/swagger_client/models/creator.py
 lightly/openapi_generated/swagger_client/models/crop_data.py
-lightly/openapi_generated/swagger_client/models/custom_sample_meta_data.py
 lightly/openapi_generated/swagger_client/models/dataset_create_request.py
 lightly/openapi_generated/swagger_client/models/dataset_creator.py
 lightly/openapi_generated/swagger_client/models/dataset_data.py
 lightly/openapi_generated/swagger_client/models/dataset_data_enriched.py
 lightly/openapi_generated/swagger_client/models/dataset_embedding_data.py
-lightly/openapi_generated/swagger_client/models/dataset_name.py
-lightly/openapi_generated/swagger_client/models/dataset_name_query.py
 lightly/openapi_generated/swagger_client/models/dataset_type.py
 lightly/openapi_generated/swagger_client/models/dataset_update_request.py
 lightly/openapi_generated/swagger_client/models/datasource_config.py
 lightly/openapi_generated/swagger_client/models/datasource_config_azure.py
+lightly/openapi_generated/swagger_client/models/datasource_config_azure_all_of.py
 lightly/openapi_generated/swagger_client/models/datasource_config_base.py
 lightly/openapi_generated/swagger_client/models/datasource_config_gcs.py
+lightly/openapi_generated/swagger_client/models/datasource_config_gcs_all_of.py
 lightly/openapi_generated/swagger_client/models/datasource_config_lightly.py
 lightly/openapi_generated/swagger_client/models/datasource_config_local.py
 lightly/openapi_generated/swagger_client/models/datasource_config_obs.py
+lightly/openapi_generated/swagger_client/models/datasource_config_obs_all_of.py
 lightly/openapi_generated/swagger_client/models/datasource_config_s3.py
+lightly/openapi_generated/swagger_client/models/datasource_config_s3_all_of.py
 lightly/openapi_generated/swagger_client/models/datasource_config_s3_delegated_access.py
+lightly/openapi_generated/swagger_client/models/datasource_config_s3_delegated_access_all_of.py
 lightly/openapi_generated/swagger_client/models/datasource_config_verify_data.py
 lightly/openapi_generated/swagger_client/models/datasource_config_verify_data_errors.py
 lightly/openapi_generated/swagger_client/models/datasource_processed_until_timestamp_request.py
 lightly/openapi_generated/swagger_client/models/datasource_processed_until_timestamp_response.py
 lightly/openapi_generated/swagger_client/models/datasource_purpose.py
 lightly/openapi_generated/swagger_client/models/datasource_raw_samples_data.py
 lightly/openapi_generated/swagger_client/models/datasource_raw_samples_data_row.py
 lightly/openapi_generated/swagger_client/models/datasource_raw_samples_metadata_data.py
 lightly/openapi_generated/swagger_client/models/datasource_raw_samples_metadata_data_row.py
 lightly/openapi_generated/swagger_client/models/datasource_raw_samples_predictions_data.py
 lightly/openapi_generated/swagger_client/models/datasource_raw_samples_predictions_data_row.py
 lightly/openapi_generated/swagger_client/models/dimensionality_reduction_method.py
+lightly/openapi_generated/swagger_client/models/docker_authorization_request.py
+lightly/openapi_generated/swagger_client/models/docker_authorization_response.py
 lightly/openapi_generated/swagger_client/models/docker_license_information.py
 lightly/openapi_generated/swagger_client/models/docker_run_artifact_create_request.py
 lightly/openapi_generated/swagger_client/models/docker_run_artifact_created_data.py
 lightly/openapi_generated/swagger_client/models/docker_run_artifact_data.py
+lightly/openapi_generated/swagger_client/models/docker_run_artifact_storage_location.py
 lightly/openapi_generated/swagger_client/models/docker_run_artifact_type.py
 lightly/openapi_generated/swagger_client/models/docker_run_create_request.py
 lightly/openapi_generated/swagger_client/models/docker_run_data.py
 lightly/openapi_generated/swagger_client/models/docker_run_log_data.py
 lightly/openapi_generated/swagger_client/models/docker_run_log_entry_data.py
 lightly/openapi_generated/swagger_client/models/docker_run_log_level.py
 lightly/openapi_generated/swagger_client/models/docker_run_scheduled_create_request.py
@@ -186,14 +195,15 @@
 lightly/openapi_generated/swagger_client/models/docker_run_scheduled_priority.py
 lightly/openapi_generated/swagger_client/models/docker_run_scheduled_state.py
 lightly/openapi_generated/swagger_client/models/docker_run_scheduled_update_request.py
 lightly/openapi_generated/swagger_client/models/docker_run_state.py
 lightly/openapi_generated/swagger_client/models/docker_run_update_request.py
 lightly/openapi_generated/swagger_client/models/docker_task_description.py
 lightly/openapi_generated/swagger_client/models/docker_user_stats.py
+lightly/openapi_generated/swagger_client/models/docker_worker_authorization_request.py
 lightly/openapi_generated/swagger_client/models/docker_worker_config.py
 lightly/openapi_generated/swagger_client/models/docker_worker_config_create_request.py
 lightly/openapi_generated/swagger_client/models/docker_worker_config_data.py
 lightly/openapi_generated/swagger_client/models/docker_worker_config_v2.py
 lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_create_request.py
 lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_data.py
 lightly/openapi_generated/swagger_client/models/docker_worker_config_v2_docker.py
@@ -214,102 +224,95 @@
 lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_checkpoint_callback.py
 lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_collate.py
 lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_criterion.py
 lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_loader.py
 lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_model.py
 lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_optimizer.py
 lightly/openapi_generated/swagger_client/models/docker_worker_config_v3_lightly_trainer.py
-lightly/openapi_generated/swagger_client/models/docker_worker_labels.py
-lightly/openapi_generated/swagger_client/models/docker_worker_name.py
 lightly/openapi_generated/swagger_client/models/docker_worker_registry_entry_data.py
 lightly/openapi_generated/swagger_client/models/docker_worker_state.py
 lightly/openapi_generated/swagger_client/models/docker_worker_type.py
-lightly/openapi_generated/swagger_client/models/embedding2d_coordinates.py
 lightly/openapi_generated/swagger_client/models/embedding2d_create_request.py
 lightly/openapi_generated/swagger_client/models/embedding2d_data.py
 lightly/openapi_generated/swagger_client/models/embedding_data.py
 lightly/openapi_generated/swagger_client/models/file_name_format.py
 lightly/openapi_generated/swagger_client/models/file_output_format.py
 lightly/openapi_generated/swagger_client/models/filename_and_read_url.py
-lightly/openapi_generated/swagger_client/models/filename_and_read_urls.py
-lightly/openapi_generated/swagger_client/models/general_job_result.py
 lightly/openapi_generated/swagger_client/models/image_type.py
 lightly/openapi_generated/swagger_client/models/initial_tag_create_request.py
+lightly/openapi_generated/swagger_client/models/internal_debug_latency.py
+lightly/openapi_generated/swagger_client/models/internal_debug_latency_mongodb.py
 lightly/openapi_generated/swagger_client/models/job_result_type.py
 lightly/openapi_generated/swagger_client/models/job_state.py
 lightly/openapi_generated/swagger_client/models/job_status_data.py
 lightly/openapi_generated/swagger_client/models/job_status_data_result.py
 lightly/openapi_generated/swagger_client/models/job_status_meta.py
 lightly/openapi_generated/swagger_client/models/job_status_upload_method.py
 lightly/openapi_generated/swagger_client/models/jobs_data.py
 lightly/openapi_generated/swagger_client/models/label_box_data_row.py
-lightly/openapi_generated/swagger_client/models/label_box_data_rows.py
 lightly/openapi_generated/swagger_client/models/label_box_v4_data_row.py
-lightly/openapi_generated/swagger_client/models/label_box_v4_data_rows.py
 lightly/openapi_generated/swagger_client/models/label_studio_task.py
 lightly/openapi_generated/swagger_client/models/label_studio_task_data.py
-lightly/openapi_generated/swagger_client/models/label_studio_tasks.py
+lightly/openapi_generated/swagger_client/models/lightly_docker_selection_method.py
 lightly/openapi_generated/swagger_client/models/lightly_model_v2.py
 lightly/openapi_generated/swagger_client/models/lightly_model_v3.py
 lightly/openapi_generated/swagger_client/models/lightly_trainer_precision_v2.py
 lightly/openapi_generated/swagger_client/models/lightly_trainer_precision_v3.py
-lightly/openapi_generated/swagger_client/models/mongo_object_id.py
-lightly/openapi_generated/swagger_client/models/object_id.py
-lightly/openapi_generated/swagger_client/models/path_safe_name.py
 lightly/openapi_generated/swagger_client/models/prediction_singleton.py
 lightly/openapi_generated/swagger_client/models/prediction_singleton_base.py
 lightly/openapi_generated/swagger_client/models/prediction_singleton_classification.py
+lightly/openapi_generated/swagger_client/models/prediction_singleton_classification_all_of.py
 lightly/openapi_generated/swagger_client/models/prediction_singleton_instance_segmentation.py
+lightly/openapi_generated/swagger_client/models/prediction_singleton_instance_segmentation_all_of.py
 lightly/openapi_generated/swagger_client/models/prediction_singleton_keypoint_detection.py
+lightly/openapi_generated/swagger_client/models/prediction_singleton_keypoint_detection_all_of.py
 lightly/openapi_generated/swagger_client/models/prediction_singleton_object_detection.py
+lightly/openapi_generated/swagger_client/models/prediction_singleton_object_detection_all_of.py
 lightly/openapi_generated/swagger_client/models/prediction_singleton_semantic_segmentation.py
-lightly/openapi_generated/swagger_client/models/prediction_singletons.py
+lightly/openapi_generated/swagger_client/models/prediction_singleton_semantic_segmentation_all_of.py
 lightly/openapi_generated/swagger_client/models/prediction_task_schema.py
 lightly/openapi_generated/swagger_client/models/prediction_task_schema_category.py
-lightly/openapi_generated/swagger_client/models/probabilities.py
+lightly/openapi_generated/swagger_client/models/profile_basic_data.py
+lightly/openapi_generated/swagger_client/models/profile_me_data.py
+lightly/openapi_generated/swagger_client/models/profile_me_data_settings.py
 lightly/openapi_generated/swagger_client/models/questionnaire_data.py
-lightly/openapi_generated/swagger_client/models/read_url.py
-lightly/openapi_generated/swagger_client/models/redirected_read_url.py
 lightly/openapi_generated/swagger_client/models/s3_region.py
-lightly/openapi_generated/swagger_client/models/s3_server_side_encryption_kms_key.py
 lightly/openapi_generated/swagger_client/models/sama_task.py
 lightly/openapi_generated/swagger_client/models/sama_task_data.py
-lightly/openapi_generated/swagger_client/models/sama_tasks.py
 lightly/openapi_generated/swagger_client/models/sample_create_request.py
 lightly/openapi_generated/swagger_client/models/sample_data.py
 lightly/openapi_generated/swagger_client/models/sample_data_modes.py
 lightly/openapi_generated/swagger_client/models/sample_meta_data.py
 lightly/openapi_generated/swagger_client/models/sample_partial_mode.py
 lightly/openapi_generated/swagger_client/models/sample_sort_by.py
 lightly/openapi_generated/swagger_client/models/sample_type.py
 lightly/openapi_generated/swagger_client/models/sample_update_request.py
 lightly/openapi_generated/swagger_client/models/sample_write_urls.py
 lightly/openapi_generated/swagger_client/models/sampling_config.py
 lightly/openapi_generated/swagger_client/models/sampling_config_stopping_condition.py
 lightly/openapi_generated/swagger_client/models/sampling_create_request.py
 lightly/openapi_generated/swagger_client/models/sampling_method.py
-lightly/openapi_generated/swagger_client/models/score.py
 lightly/openapi_generated/swagger_client/models/sector.py
 lightly/openapi_generated/swagger_client/models/selection_config.py
 lightly/openapi_generated/swagger_client/models/selection_config_entry.py
 lightly/openapi_generated/swagger_client/models/selection_config_entry_input.py
 lightly/openapi_generated/swagger_client/models/selection_config_entry_strategy.py
 lightly/openapi_generated/swagger_client/models/selection_input_predictions_name.py
 lightly/openapi_generated/swagger_client/models/selection_input_type.py
 lightly/openapi_generated/swagger_client/models/selection_strategy_threshold_operation.py
 lightly/openapi_generated/swagger_client/models/selection_strategy_type.py
+lightly/openapi_generated/swagger_client/models/service_account_basic_data.py
 lightly/openapi_generated/swagger_client/models/set_embeddings_is_processed_flag_by_id_body_request.py
 lightly/openapi_generated/swagger_client/models/shared_access_config_create_request.py
 lightly/openapi_generated/swagger_client/models/shared_access_config_data.py
 lightly/openapi_generated/swagger_client/models/shared_access_type.py
 lightly/openapi_generated/swagger_client/models/tag_active_learning_scores_data.py
 lightly/openapi_generated/swagger_client/models/tag_arithmetics_operation.py
 lightly/openapi_generated/swagger_client/models/tag_arithmetics_request.py
 lightly/openapi_generated/swagger_client/models/tag_arithmetics_response.py
-lightly/openapi_generated/swagger_client/models/tag_bit_mask_data.py
 lightly/openapi_generated/swagger_client/models/tag_bit_mask_response.py
 lightly/openapi_generated/swagger_client/models/tag_change_data.py
 lightly/openapi_generated/swagger_client/models/tag_change_data_arithmetics.py
 lightly/openapi_generated/swagger_client/models/tag_change_data_initial.py
 lightly/openapi_generated/swagger_client/models/tag_change_data_metadata.py
 lightly/openapi_generated/swagger_client/models/tag_change_data_operation_method.py
 lightly/openapi_generated/swagger_client/models/tag_change_data_rename.py
@@ -317,27 +320,24 @@
 lightly/openapi_generated/swagger_client/models/tag_change_data_samples.py
 lightly/openapi_generated/swagger_client/models/tag_change_data_scatterplot.py
 lightly/openapi_generated/swagger_client/models/tag_change_data_upsize.py
 lightly/openapi_generated/swagger_client/models/tag_change_entry.py
 lightly/openapi_generated/swagger_client/models/tag_create_request.py
 lightly/openapi_generated/swagger_client/models/tag_creator.py
 lightly/openapi_generated/swagger_client/models/tag_data.py
-lightly/openapi_generated/swagger_client/models/tag_filenames_data.py
-lightly/openapi_generated/swagger_client/models/tag_name.py
 lightly/openapi_generated/swagger_client/models/tag_update_request.py
 lightly/openapi_generated/swagger_client/models/tag_upsize_request.py
-lightly/openapi_generated/swagger_client/models/task_name.py
 lightly/openapi_generated/swagger_client/models/task_type.py
 lightly/openapi_generated/swagger_client/models/team_basic_data.py
+lightly/openapi_generated/swagger_client/models/team_data.py
 lightly/openapi_generated/swagger_client/models/team_role.py
-lightly/openapi_generated/swagger_client/models/timestamp.py
-lightly/openapi_generated/swagger_client/models/timestamp_seconds.py
 lightly/openapi_generated/swagger_client/models/trigger2d_embedding_job_request.py
 lightly/openapi_generated/swagger_client/models/update_docker_worker_registry_entry_request.py
-lightly/openapi_generated/swagger_client/models/version_number.py
+lightly/openapi_generated/swagger_client/models/update_team_membership_request.py
+lightly/openapi_generated/swagger_client/models/user_type.py
 lightly/openapi_generated/swagger_client/models/video_frame_data.py
 lightly/openapi_generated/swagger_client/models/write_csv_url_data.py
 lightly/transforms/__init__.py
 lightly/transforms/dino_transform.py
 lightly/transforms/fast_siam_transform.py
 lightly/transforms/gaussian_blur.py
 lightly/transforms/image_grid_transform.py
```

### Comparing `lightly-1.4.7/lightly.egg-info/requires.txt` & `lightly-1.4.8/lightly.egg-info/requires.txt`

 * *Files 14% similar despite different names*

```diff
@@ -3,14 +3,19 @@
 lightly_utils~=0.0.0
 numpy>=1.18.1
 python_dateutil>=2.5.3
 requests>=2.23.0
 six>=1.10
 tqdm>=4.44
 urllib3>=1.15.1
+python_dateutil>=2.5.3
+setuptools>=21.0.0
+urllib3>=1.25.3
+pydantic<2,>=1.10.5
+aenum>=3.1.11
 torch
 torchvision
 pytorch_lightning>=1.0.4
 
 [all]
 tox
 sphinx
```

### Comparing `lightly-1.4.7/setup.py` & `lightly-1.4.8/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -61,20 +61,21 @@
         ]
     }
 
     long_description = load_description()
 
     python_requires = ">=3.6"
     base_requires = load_requirements(filename="base.txt")
+    openapi_requires = load_requirements(filename="openapi.txt")
     torch_requires = load_requirements(filename="torch.txt")
     video_requires = load_requirements(filename="video.txt")
     dev_requires = load_requirements(filename="dev.txt")
 
     setup_requires = ["setuptools>=21"]
-    install_requires = base_requires + torch_requires
+    install_requires = base_requires + openapi_requires + torch_requires
     extras_require = {
         "video": video_requires,
         "dev": dev_requires,
         "all": dev_requires + video_requires,
     }
 
     packages = [
```

