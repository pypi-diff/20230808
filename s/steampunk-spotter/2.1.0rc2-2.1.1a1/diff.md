# Comparing `tmp/steampunk_spotter-2.1.0rc2-py3-none-any.whl.zip` & `tmp/steampunk_spotter-2.1.1a1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,43 +1,58 @@
-Zip file size: 65122 bytes, number of entries: 41
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-12 07:53 spotter/__init__.py
--rw-r--r--  2.0 unx    22727 b- defN 23-Jul-12 06:37 spotter/api.py
--rw-r--r--  2.0 unx     5923 b- defN 23-Jul-12 06:37 spotter/cli.py
--rw-r--r--  2.0 unx    21522 b- defN 23-Jul-12 06:37 spotter/environment.py
--rw-r--r--  2.0 unx     5697 b- defN 23-Jun-09 10:45 spotter/storage.py
--rw-r--r--  2.0 unx     1311 b- defN 23-Jul-12 06:37 spotter/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-12 07:53 spotter/commands/__init__.py
--rw-r--r--  2.0 unx     3910 b- defN 23-Jul-12 06:37 spotter/commands/clear_config.py
--rw-r--r--  2.0 unx     5466 b- defN 23-Jul-12 06:37 spotter/commands/clear_policies.py
--rw-r--r--  2.0 unx     4023 b- defN 23-Jul-12 06:37 spotter/commands/get_config.py
--rw-r--r--  2.0 unx     2446 b- defN 23-Jun-09 10:45 spotter/commands/login.py
--rw-r--r--  2.0 unx     2253 b- defN 23-Jun-09 10:45 spotter/commands/logout.py
--rw-r--r--  2.0 unx     1155 b- defN 23-Jul-12 06:37 spotter/commands/register.py
--rw-r--r--  2.0 unx    40746 b- defN 23-Jul-12 06:37 spotter/commands/scan.py
--rw-r--r--  2.0 unx     5255 b- defN 23-Jul-12 06:37 spotter/commands/set_config.py
--rw-r--r--  2.0 unx     6690 b- defN 23-Jul-12 06:37 spotter/commands/set_policies.py
--rw-r--r--  2.0 unx     3620 b- defN 23-Jun-09 10:45 spotter/commands/suggest.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-12 07:53 spotter/parsing/__init__.py
--rw-r--r--  2.0 unx     4964 b- defN 23-Jul-12 06:37 spotter/parsing/noqa_comments.py
--rw-r--r--  2.0 unx    25021 b- defN 23-Jul-12 06:37 spotter/parsing/parsing.py
--rw-r--r--  2.0 unx       60 b- defN 23-Mar-14 07:59 spotter/reporting/__init__.py
--rw-r--r--  2.0 unx     3461 b- defN 23-Jun-30 06:19 spotter/reporting/report.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-12 07:53 spotter/rewriting/__init__.py
--rw-r--r--  2.0 unx     7545 b- defN 23-Apr-11 07:05 spotter/rewriting/models.py
--rw-r--r--  2.0 unx     7588 b- defN 23-Jul-12 06:37 spotter/rewriting/processor.py
--rw-r--r--  2.0 unx     1937 b- defN 23-Apr-11 07:05 spotter/rewriting/rewrite_action_inline.py
--rw-r--r--  2.0 unx     2459 b- defN 23-Jul-06 06:23 spotter/rewriting/rewrite_action_object.py
--rw-r--r--  2.0 unx      989 b- defN 23-Apr-11 07:05 spotter/rewriting/rewrite_always_run.py
--rw-r--r--  2.0 unx      982 b- defN 23-Apr-11 07:05 spotter/rewriting/rewrite_fqcn.py
--rw-r--r--  2.0 unx     2124 b- defN 23-Jun-30 06:19 spotter/rewriting/rewrite_inline.py
--rw-r--r--  2.0 unx     2553 b- defN 23-Jun-30 06:19 spotter/rewriting/rewrite_local_action_inline.py
--rw-r--r--  2.0 unx     2559 b- defN 23-Jul-06 06:23 spotter/rewriting/rewrite_local_object.py
--rw-r--r--  2.0 unx      989 b- defN 23-Apr-11 07:05 spotter/rewriting/rewrite_module_inline.py
--rw-r--r--  2.0 unx      888 b- defN 23-Apr-11 07:05 spotter/rewriting/rewrite_module_object.py
--rw-rw-rw-  2.0 unx    11358 b- defN 23-Jul-12 07:54 steampunk_spotter-2.1.0rc2.dist-info/LICENSE
--rw-r--r--  2.0 unx     7288 b- defN 23-Jul-12 07:54 steampunk_spotter-2.1.0rc2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jul-12 07:54 steampunk_spotter-2.1.0rc2.dist-info/WHEEL
--rw-r--r--  2.0 unx       45 b- defN 23-Jul-12 07:54 steampunk_spotter-2.1.0rc2.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        8 b- defN 23-Jul-12 07:54 steampunk_spotter-2.1.0rc2.dist-info/top_level.txt
--rw-r--r--  2.0 unx        1 b- defN 23-Jul-12 07:54 steampunk_spotter-2.1.0rc2.dist-info/zip-safe
--rw-rw-r--  2.0 unx     3592 b- defN 23-Jul-12 07:54 steampunk_spotter-2.1.0rc2.dist-info/RECORD
-41 files, 219247 bytes uncompressed, 59320 bytes compressed:  72.9%
+Zip file size: 73745 bytes, number of entries: 56
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:06 spotter/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:06 spotter/client/__init__.py
+-rw-r--r--  2.0 unx     6030 b- defN 23-Aug-08 11:04 spotter/client/cli.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:06 spotter/client/commands/__init__.py
+-rw-r--r--  2.0 unx     3931 b- defN 23-Aug-08 06:15 spotter/client/commands/clear_config.py
+-rw-r--r--  2.0 unx     5569 b- defN 23-Aug-08 06:15 spotter/client/commands/clear_policies.py
+-rw-r--r--  2.0 unx     4068 b- defN 23-Aug-08 06:15 spotter/client/commands/get_config.py
+-rw-r--r--  2.0 unx     2480 b- defN 23-Aug-08 06:15 spotter/client/commands/login.py
+-rw-r--r--  2.0 unx     2273 b- defN 23-Aug-08 06:15 spotter/client/commands/logout.py
+-rw-r--r--  2.0 unx     1204 b- defN 23-Aug-08 06:15 spotter/client/commands/register.py
+-rw-r--r--  2.0 unx    16603 b- defN 23-Aug-08 09:19 spotter/client/commands/scan.py
+-rw-r--r--  2.0 unx     5325 b- defN 23-Aug-08 06:15 spotter/client/commands/set_config.py
+-rw-r--r--  2.0 unx     6866 b- defN 23-Aug-08 06:15 spotter/client/commands/set_policies.py
+-rw-r--r--  2.0 unx     3696 b- defN 23-Aug-08 06:15 spotter/client/commands/suggest.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:06 spotter/library/__init__.py
+-rw-r--r--  2.0 unx    23702 b- defN 23-Aug-08 06:15 spotter/library/api.py
+-rw-r--r--  2.0 unx    22719 b- defN 23-Aug-08 09:14 spotter/library/environment.py
+-rw-r--r--  2.0 unx     5697 b- defN 23-Aug-04 11:44 spotter/library/storage.py
+-rw-r--r--  2.0 unx     1787 b- defN 23-Aug-08 09:14 spotter/library/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:06 spotter/library/compat/__init__.py
+-rw-r--r--  2.0 unx      427 b- defN 23-Aug-08 06:15 spotter/library/compat/pydantic.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:06 spotter/library/parsing/__init__.py
+-rw-r--r--  2.0 unx     4978 b- defN 23-Aug-08 06:15 spotter/library/parsing/noqa_comments.py
+-rw-r--r--  2.0 unx    25889 b- defN 23-Aug-08 06:15 spotter/library/parsing/parsing.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:06 spotter/library/reporting/__init__.py
+-rw-r--r--  2.0 unx     4223 b- defN 23-Aug-08 06:15 spotter/library/reporting/report.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:06 spotter/library/rewriting/__init__.py
+-rw-r--r--  2.0 unx     9384 b- defN 23-Aug-08 06:15 spotter/library/rewriting/models.py
+-rw-r--r--  2.0 unx     5654 b- defN 23-Aug-08 06:15 spotter/library/rewriting/processor.py
+-rw-r--r--  2.0 unx     1951 b- defN 23-Aug-08 06:15 spotter/library/rewriting/rewrite_action_inline.py
+-rw-r--r--  2.0 unx     2500 b- defN 23-Aug-08 06:15 spotter/library/rewriting/rewrite_action_object.py
+-rw-r--r--  2.0 unx      997 b- defN 23-Jul-19 14:07 spotter/library/rewriting/rewrite_always_run.py
+-rw-r--r--  2.0 unx      989 b- defN 23-Jul-19 14:07 spotter/library/rewriting/rewrite_fqcn.py
+-rw-r--r--  2.0 unx     2132 b- defN 23-Jul-19 14:07 spotter/library/rewriting/rewrite_inline.py
+-rw-r--r--  2.0 unx     2569 b- defN 23-Aug-08 06:15 spotter/library/rewriting/rewrite_local_action_inline.py
+-rw-r--r--  2.0 unx     2577 b- defN 23-Aug-08 06:15 spotter/library/rewriting/rewrite_local_object.py
+-rw-r--r--  2.0 unx      997 b- defN 23-Jul-19 14:07 spotter/library/rewriting/rewrite_module_inline.py
+-rw-r--r--  2.0 unx      896 b- defN 23-Jul-19 14:07 spotter/library/rewriting/rewrite_module_object.py
+-rw-r--r--  2.0 unx     2520 b- defN 23-Jul-26 12:20 spotter/library/rewriting/rewrite_requirements.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:06 spotter/library/scanning/__init__.py
+-rw-r--r--  2.0 unx     1125 b- defN 23-Aug-08 06:15 spotter/library/scanning/check_catalog_info.py
+-rw-r--r--  2.0 unx     2854 b- defN 23-Aug-08 06:15 spotter/library/scanning/check_result.py
+-rw-r--r--  2.0 unx      956 b- defN 23-Aug-08 06:15 spotter/library/scanning/display_level.py
+-rw-r--r--  2.0 unx     1041 b- defN 23-Aug-08 06:15 spotter/library/scanning/item_metadata.py
+-rw-r--r--  2.0 unx      978 b- defN 23-Aug-08 09:19 spotter/library/scanning/output_format.py
+-rw-r--r--  2.0 unx     3226 b- defN 23-Aug-08 06:15 spotter/library/scanning/payload.py
+-rw-r--r--  2.0 unx      890 b- defN 23-Aug-08 06:15 spotter/library/scanning/profile.py
+-rw-r--r--  2.0 unx    16757 b- defN 23-Aug-08 09:19 spotter/library/scanning/result.py
+-rw-r--r--  2.0 unx      947 b- defN 23-Jul-19 14:07 spotter/library/scanning/summary.py
+-rw-rw-rw-  2.0 unx    11358 b- defN 23-Aug-08 11:07 steampunk_spotter-2.1.1a1.dist-info/LICENSE
+-rw-r--r--  2.0 unx     7352 b- defN 23-Aug-08 11:07 steampunk_spotter-2.1.1a1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Aug-08 11:07 steampunk_spotter-2.1.1a1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       52 b- defN 23-Aug-08 11:07 steampunk_spotter-2.1.1a1.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        8 b- defN 23-Aug-08 11:07 steampunk_spotter-2.1.1a1.dist-info/top_level.txt
+-rw-r--r--  2.0 unx        1 b- defN 23-Aug-08 11:07 steampunk_spotter-2.1.1a1.dist-info/zip-safe
+-rw-rw-r--  2.0 unx     5229 b- defN 23-Aug-08 11:07 steampunk_spotter-2.1.1a1.dist-info/RECORD
+56 files, 233499 bytes uncompressed, 65205 bytes compressed:  72.1%
```

## zipnote {}

```diff
@@ -1,124 +1,169 @@
 Filename: spotter/__init__.py
 Comment: 
 
-Filename: spotter/api.py
+Filename: spotter/client/__init__.py
 Comment: 
 
-Filename: spotter/cli.py
+Filename: spotter/client/cli.py
 Comment: 
 
-Filename: spotter/environment.py
+Filename: spotter/client/commands/__init__.py
 Comment: 
 
-Filename: spotter/storage.py
+Filename: spotter/client/commands/clear_config.py
 Comment: 
 
-Filename: spotter/utils.py
+Filename: spotter/client/commands/clear_policies.py
 Comment: 
 
-Filename: spotter/commands/__init__.py
+Filename: spotter/client/commands/get_config.py
 Comment: 
 
-Filename: spotter/commands/clear_config.py
+Filename: spotter/client/commands/login.py
 Comment: 
 
-Filename: spotter/commands/clear_policies.py
+Filename: spotter/client/commands/logout.py
 Comment: 
 
-Filename: spotter/commands/get_config.py
+Filename: spotter/client/commands/register.py
 Comment: 
 
-Filename: spotter/commands/login.py
+Filename: spotter/client/commands/scan.py
 Comment: 
 
-Filename: spotter/commands/logout.py
+Filename: spotter/client/commands/set_config.py
 Comment: 
 
-Filename: spotter/commands/register.py
+Filename: spotter/client/commands/set_policies.py
 Comment: 
 
-Filename: spotter/commands/scan.py
+Filename: spotter/client/commands/suggest.py
 Comment: 
 
-Filename: spotter/commands/set_config.py
+Filename: spotter/library/__init__.py
 Comment: 
 
-Filename: spotter/commands/set_policies.py
+Filename: spotter/library/api.py
 Comment: 
 
-Filename: spotter/commands/suggest.py
+Filename: spotter/library/environment.py
 Comment: 
 
-Filename: spotter/parsing/__init__.py
+Filename: spotter/library/storage.py
 Comment: 
 
-Filename: spotter/parsing/noqa_comments.py
+Filename: spotter/library/utils.py
 Comment: 
 
-Filename: spotter/parsing/parsing.py
+Filename: spotter/library/compat/__init__.py
 Comment: 
 
-Filename: spotter/reporting/__init__.py
+Filename: spotter/library/compat/pydantic.py
 Comment: 
 
-Filename: spotter/reporting/report.py
+Filename: spotter/library/parsing/__init__.py
 Comment: 
 
-Filename: spotter/rewriting/__init__.py
+Filename: spotter/library/parsing/noqa_comments.py
 Comment: 
 
-Filename: spotter/rewriting/models.py
+Filename: spotter/library/parsing/parsing.py
 Comment: 
 
-Filename: spotter/rewriting/processor.py
+Filename: spotter/library/reporting/__init__.py
 Comment: 
 
-Filename: spotter/rewriting/rewrite_action_inline.py
+Filename: spotter/library/reporting/report.py
 Comment: 
 
-Filename: spotter/rewriting/rewrite_action_object.py
+Filename: spotter/library/rewriting/__init__.py
 Comment: 
 
-Filename: spotter/rewriting/rewrite_always_run.py
+Filename: spotter/library/rewriting/models.py
 Comment: 
 
-Filename: spotter/rewriting/rewrite_fqcn.py
+Filename: spotter/library/rewriting/processor.py
 Comment: 
 
-Filename: spotter/rewriting/rewrite_inline.py
+Filename: spotter/library/rewriting/rewrite_action_inline.py
 Comment: 
 
-Filename: spotter/rewriting/rewrite_local_action_inline.py
+Filename: spotter/library/rewriting/rewrite_action_object.py
 Comment: 
 
-Filename: spotter/rewriting/rewrite_local_object.py
+Filename: spotter/library/rewriting/rewrite_always_run.py
 Comment: 
 
-Filename: spotter/rewriting/rewrite_module_inline.py
+Filename: spotter/library/rewriting/rewrite_fqcn.py
 Comment: 
 
-Filename: spotter/rewriting/rewrite_module_object.py
+Filename: spotter/library/rewriting/rewrite_inline.py
 Comment: 
 
-Filename: steampunk_spotter-2.1.0rc2.dist-info/LICENSE
+Filename: spotter/library/rewriting/rewrite_local_action_inline.py
 Comment: 
 
-Filename: steampunk_spotter-2.1.0rc2.dist-info/METADATA
+Filename: spotter/library/rewriting/rewrite_local_object.py
 Comment: 
 
-Filename: steampunk_spotter-2.1.0rc2.dist-info/WHEEL
+Filename: spotter/library/rewriting/rewrite_module_inline.py
 Comment: 
 
-Filename: steampunk_spotter-2.1.0rc2.dist-info/entry_points.txt
+Filename: spotter/library/rewriting/rewrite_module_object.py
 Comment: 
 
-Filename: steampunk_spotter-2.1.0rc2.dist-info/top_level.txt
+Filename: spotter/library/rewriting/rewrite_requirements.py
 Comment: 
 
-Filename: steampunk_spotter-2.1.0rc2.dist-info/zip-safe
+Filename: spotter/library/scanning/__init__.py
 Comment: 
 
-Filename: steampunk_spotter-2.1.0rc2.dist-info/RECORD
+Filename: spotter/library/scanning/check_catalog_info.py
+Comment: 
+
+Filename: spotter/library/scanning/check_result.py
+Comment: 
+
+Filename: spotter/library/scanning/display_level.py
+Comment: 
+
+Filename: spotter/library/scanning/item_metadata.py
+Comment: 
+
+Filename: spotter/library/scanning/output_format.py
+Comment: 
+
+Filename: spotter/library/scanning/payload.py
+Comment: 
+
+Filename: spotter/library/scanning/profile.py
+Comment: 
+
+Filename: spotter/library/scanning/result.py
+Comment: 
+
+Filename: spotter/library/scanning/summary.py
+Comment: 
+
+Filename: steampunk_spotter-2.1.1a1.dist-info/LICENSE
+Comment: 
+
+Filename: steampunk_spotter-2.1.1a1.dist-info/METADATA
+Comment: 
+
+Filename: steampunk_spotter-2.1.1a1.dist-info/WHEEL
+Comment: 
+
+Filename: steampunk_spotter-2.1.1a1.dist-info/entry_points.txt
+Comment: 
+
+Filename: steampunk_spotter-2.1.1a1.dist-info/top_level.txt
+Comment: 
+
+Filename: steampunk_spotter-2.1.1a1.dist-info/zip-safe
+Comment: 
+
+Filename: steampunk_spotter-2.1.1a1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## Comparing `spotter/api.py` & `spotter/library/api.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,30 +1,34 @@
 """Provide API client."""
 
 import sys
 from typing import Optional, Tuple, Dict, Any
 
 import requests
 
-from spotter.storage import Storage
-from spotter.utils import get_current_cli_version
+from spotter.library.storage import Storage
+from spotter.library.utils import get_current_cli_version
 
 
 class ApiClient:
     """A client interface for interacting with the API."""
 
     DEFAULT_ENDPOINT = "https://api.spotter.steampunk.si/api"
-    DEFAULT_HEADERS = {
-        "Accept": "application/json",
-        "User-Agent": f"steampunk-spotter/{get_current_cli_version()}"
-    }
+    DEFAULT_HEADERS = {"Accept": "application/json", "User-Agent": f"steampunk-spotter/{get_current_cli_version()}"}
     DEFAULT_TIMEOUT = 10
 
-    def __init__(self, base_url: str, storage: Storage, api_token: Optional[str], username: Optional[str],
-                 password: Optional[str], debug: bool = False):
+    def __init__(
+        self,
+        base_url: str,
+        storage: Storage,
+        api_token: Optional[str],
+        username: Optional[str],
+        password: Optional[str],
+        debug: bool = False,
+    ):
         """
         Construct ApiClient object.
 
         :param base_url: Base API endpoint url
         :param storage: Storage object, where tokens are stored
         :param username: Username
         :param password: Password
@@ -49,61 +53,75 @@
         :return: Full API endpoint URL based on the base path
         """
         return self._base_url + path
 
     def _check_auth_status(self) -> None:
         """Check if user is logged (if file with tokens exists in the storage)."""
         if not self._storage.exists(self._storage_tokens_path):
-            print("You are not logged in!\nTo log in, you should provide your API token or username and password:\n\n"
-                  "    - using spotter login command;\n"
-                  "    - via --api-token/-t optional argument;\n"
-                  "    - by setting SPOTTER_API_TOKEN environment variable;\n"
-                  "    - via --username/-u and --password/-p optional arguments;\n"
-                  "    - by setting SPOTTER_USERNAME and SPOTTER_PASSWORD environment variables.\n", file=sys.stderr)
+            print(
+                "Error: you are not logged in!\n"
+                "To log in, you should provide your API token or username and password:\n\n"
+                "    - using spotter login command;\n"
+                "    - via --api-token/-t optional argument;\n"
+                "    - by setting SPOTTER_API_TOKEN environment variable;\n"
+                "    - via --username/-u and --password/-p optional arguments;\n"
+                "    - by setting SPOTTER_USERNAME and SPOTTER_PASSWORD environment variables.\n",
+                file=sys.stderr,
+            )
             sys.exit(2)
 
     def _get_endpoint_tokens(self) -> Dict[str, str]:
         """
         Retrieve tokens for particular API endpoint from storage.
 
         :return: Dict with tokens
         """
         self._check_auth_status()
 
         tokens = self._storage.read_json(self._storage_tokens_path)
         endpoint_tokens = tokens.get(self._base_url, {})
 
         if not endpoint_tokens:
-            print(f"Error: No {self._base_url} endpoint in {self._storage.path / self._storage_tokens_path}.",
-                  file=sys.stderr)
+            print(
+                f"Error: no {self._base_url} endpoint in {self._storage.path / self._storage_tokens_path}.",
+                file=sys.stderr,
+            )
             sys.exit(2)
 
         if not isinstance(endpoint_tokens, dict):
-            print(f"Error: The {self._base_url} JSON entry from {self._storage.path / self._storage_tokens_path} "
-                  f"should be of type dict, but is '{type(endpoint_tokens)}'.", file=sys.stderr)
+            print(
+                f"Error: the {self._base_url} JSON entry from {self._storage.path / self._storage_tokens_path} "
+                f"should be of type dict, but is '{type(endpoint_tokens)}'.",
+                file=sys.stderr,
+            )
             sys.exit(2)
 
         return endpoint_tokens
 
     def _get_api_token(self) -> Optional[str]:
         """
         Retrieve API token from storage.
 
         :return: API token as string
         """
         endpoint_tokens = self._get_endpoint_tokens()
 
         if not endpoint_tokens:
-            print(f"Error: No {self._base_url} endpoint in {self._storage.path / self._storage_tokens_path}.",
-                  file=sys.stderr)
+            print(
+                f"Error: no {self._base_url} endpoint in {self._storage.path / self._storage_tokens_path}.",
+                file=sys.stderr,
+            )
             sys.exit(2)
 
         if not isinstance(endpoint_tokens, dict):
-            print(f"Error: The {self._base_url} JSON entry from {self._storage.path / self._storage_tokens_path} "
-                  f"should be of type dict, but is '{type(endpoint_tokens)}'.", file=sys.stderr)
+            print(
+                f"Error: the {self._base_url} JSON entry from {self._storage.path / self._storage_tokens_path} "
+                f"should be of type dict, but is '{type(endpoint_tokens)}'.",
+                file=sys.stderr,
+            )
             sys.exit(2)
 
         return endpoint_tokens.get("api_token", None)
 
     def _get_access_refresh_tokens(self) -> Tuple[str, str]:
         """
         Retrieve access and refresh token from storage.
@@ -112,18 +130,18 @@
         """
         endpoint_tokens = self._get_endpoint_tokens()
 
         access_token = endpoint_tokens.get("access", None)
         refresh_token = endpoint_tokens.get("refresh", None)
 
         if not access_token:
-            print(f"Error: No access token in {self._storage.path / self._storage_tokens_path}.", file=sys.stderr)
+            print(f"Error: no access token in {self._storage.path / self._storage_tokens_path}.", file=sys.stderr)
             sys.exit(2)
         if not refresh_token:
-            print(f"Error: No refresh token in {self._storage.path / self._storage_tokens_path}.", file=sys.stderr)
+            print(f"Error: no refresh token in {self._storage.path / self._storage_tokens_path}.", file=sys.stderr)
             sys.exit(2)
 
         return access_token, refresh_token
 
     def _old_tokens_fallback(self) -> None:
         """Execute a fallback mechanism to ensure that users with old tokens path have the same JSON tokens format."""
         tokens = self._storage.read_json(self._storage_tokens_path)
@@ -200,32 +218,29 @@
     def debug_print_me(self) -> None:
         """If in debug mode, fetch and print the information about myself."""
         if not self._debug:
             return
         self.debug_print(f"API endpoint: {self._base_url}")
         me = self._get_me()  # pylint: disable=invalid-name
         self.debug_print(
-            f"Logged in as ({me['id']}) {me['username']} - {me['first_name']}"
-            f" {me['last_name']} <{me['email']}>"
+            f"Logged in as ({me['id']}) {me['username']} - {me['first_name']}" f" {me['last_name']} <{me['email']}>"
         )
 
     def debug_organization(self, organization_id: str) -> None:
         """
         If in debug mode, fetch and print the information about this organization.
 
         :param organization_id: UUID of the organization to print the details of.
         """
         if not self._debug:
             return
         me = self._get_me()  # pylint: disable=invalid-name
         org_matches = [o for o in me["organizations"] if o["id"] == organization_id]
         if len(org_matches) < 1:
-            self.debug_print(
-                f"Organization id {organization_id} not found for any of my organizations"
-            )
+            self.debug_print(f"Organization id {organization_id} not found for any of my organizations")
         else:
             self._debug_print_organization("Target organization:", org_matches[0])
             self._debug_print_subscription(org_matches[0]["subscription"])
 
     def debug_my_default_organization(self) -> None:
         """If in debug mode, fetch and print the information about my default organization."""
         if not self._debug:
@@ -234,15 +249,15 @@
         try:
             default_organization_id = me["default_organization"]
             default_organization = [o for o in me["organizations"] if o["id"] == default_organization_id][0]
             self._debug_print_organization("Default organization", default_organization)
             self.debug_print(f"Projects: {default_organization['projects']}")
             self._debug_print_subscription(default_organization["subscription"])
         except Exception as e:  # pylint: disable=broad-exception-caught
-            print(f"Error obtaining default organization info: {e}", file=sys.stderr)
+            print(f"Error: obtaining default organization info failed: {e}", file=sys.stderr)
 
     def login(self, timeout: int = DEFAULT_TIMEOUT) -> None:
         """
         Login user to the API using API token or username and password, also verify and store auth tokens to storage.
 
         Note that we do not use self._request to prevent possible cyclic recursion errors.
 
@@ -252,16 +267,20 @@
         updated_auth_tokens = {}
 
         if self._api_token:
             request_headers.update({"Authorization": f"SPTKN {self._api_token}"})
             updated_auth_tokens.update({"api_token": self._api_token})
         else:
             # old login - generate access and refresh token
-            response = requests.post(self._url("/v2/token/"), headers=request_headers,
-                                     json={"username": self._username, "password": self._password}, timeout=timeout)
+            response = requests.post(
+                self._url("/v2/token/"),
+                headers=request_headers,
+                json={"username": self._username, "password": self._password},
+                timeout=timeout,
+            )
             if response.ok:
                 updated_auth_tokens.update(response.json())
                 access_token = response.json().get("access", None)
                 request_headers.update({"Authorization": f"Bearer {access_token}"})
             else:
                 print(self.format_api_error(response), file=sys.stderr)
                 sys.exit(2)
@@ -281,66 +300,78 @@
         Login user to the API using the tokens (access and refresh token) from storage.
 
         :param timeout: Request timeout
         """
         # get existing tokens and then refresh access token and save it to local storage
         # note that we do not use self._request to prevent possible cyclic recursion errors
         _, refresh_token = self._get_access_refresh_tokens()
-        response_token_refresh = requests.post(self._url("/v2/token/refresh/"), headers=self.DEFAULT_HEADERS.copy(),
-                                               json={"refresh": refresh_token}, timeout=timeout)
+        response_token_refresh = requests.post(
+            self._url("/v2/token/refresh/"),
+            headers=self.DEFAULT_HEADERS.copy(),
+            json={"refresh": refresh_token},
+            timeout=timeout,
+        )
         if response_token_refresh.ok:
             refreshed_access_token = response_token_refresh.json().get("access", None)
             if not refreshed_access_token:
-                print("Error: Refreshing access token failed.", file=sys.stderr)
+                print("Error: refreshing access token failed.", file=sys.stderr)
                 sys.exit(2)
 
             access_token = refreshed_access_token
-            self._storage.update_json({
-                self._base_url: {
-                    "access": access_token,
-                    "refresh": refresh_token
-                }
-            }, self._storage_tokens_path)
+            self._storage.update_json(
+                {self._base_url: {"access": access_token, "refresh": refresh_token}}, self._storage_tokens_path
+            )
         else:
             print(self.format_api_error(response_token_refresh), file=sys.stderr)
             sys.exit(2)
 
     def logout(self) -> None:
         """Logout user - remove tokens for the current API endpoint from storage."""
         self._check_auth_status()
 
         tokens = self._storage.read_json(self._storage_tokens_path)
         endpoint_tokens = tokens.pop(self._base_url, None)
         if endpoint_tokens:
             self._storage.write_json(tokens, self._storage_tokens_path)
         else:
-            print(f"You are already logged out because there is no {self._base_url} endpoint "
-                  f"in {self._storage.path / self._storage_tokens_path}.", file=sys.stderr)
+            print(
+                f"You are already logged out because there is no {self._base_url} endpoint "
+                f"in {self._storage.path / self._storage_tokens_path}.",
+                file=sys.stderr,
+            )
             sys.exit(0)
 
     # pylint: disable=too-many-arguments,too-many-branches,too-many-locals
-    def _request(self, method: str, path: str, authorize: Optional[bool] = True,
-                 headers: Optional[Dict[str, str]] = None, payload: Optional[Dict[str, Any]] = None,
-                 timeout: int = DEFAULT_TIMEOUT, allow_auth_retry: bool = True,
-                 ignore_response_status_codes: Optional[bool] = False) -> requests.Response:
+    def _request(
+        self,
+        method: str,
+        path: str,
+        authorize: Optional[bool] = True,
+        headers: Optional[Dict[str, str]] = None,
+        payload: Optional[Dict[str, Any]] = None,
+        timeout: int = DEFAULT_TIMEOUT,
+        allow_auth_retry: bool = True,
+        ignore_response_status_codes: Optional[bool] = False,
+    ) -> requests.Response:
         """
         Send HTTP request.
 
         :param path: API endpoint path
         :param authorize: Add Authorization header to authorize request (True/False)
         :param headers: Request headers (JSON payload dict)
         :param payload: Request payload (JSON payload dict)
         :param timeout: Request timeout
         :param allow_auth_retry: Whether to allow reauthenticating and retrying the request
         :param ignore_response_status_codes: Whether to ignore response status codes (even ones higher than 400)
         :return: Response object
         """
         # initiate login from start if API token or username and password have been provided and tokens do not exist yet
         if (self._api_token or (self._username and self._password)) and not self._storage.exists(
-                self._storage_tokens_path):
+            self._storage_tokens_path
+        ):
             self._storage.remove(self._storage_tokens_path)
             self.login()
 
         # initiate login from start if endpoint does not exist in tokens
         if self._storage.exists(self._storage_tokens_path):
             tokens = self._storage.read_json(self._storage_tokens_path)
             endpoint_tokens = tokens.get(self._base_url, None)
@@ -357,30 +388,41 @@
                 access_token, _ = self._get_access_refresh_tokens()
                 request_headers.update({"Authorization": f"Bearer {access_token}"})
         request_headers.update(headers if headers is not None else {})
 
         # try to make a request
         try:
             response = requests.request(
-                method, self._url(path),
-                headers=request_headers, json=payload if payload is not None else {}, timeout=timeout
+                method,
+                self._url(path),
+                headers=request_headers,
+                json=payload if payload is not None else {},
+                timeout=timeout,
             )
         except requests.exceptions.RequestException as e:
             print(f"API error: {str(e)}", file=sys.stderr)
             sys.exit(2)
 
         # if request fails for one time try to log in and make a request again
         if not self._api_token and response.status_code == 401:
             if allow_auth_retry:
                 self._refresh_login(timeout)
                 # retry, but don't allow any more auth retries
-                return self._request(method, path, authorize, headers, payload, timeout, allow_auth_retry=False,
-                                     ignore_response_status_codes=ignore_response_status_codes)
+                return self._request(
+                    method,
+                    path,
+                    authorize,
+                    headers,
+                    payload,
+                    timeout,
+                    allow_auth_retry=False,
+                    ignore_response_status_codes=ignore_response_status_codes,
+                )
 
-            print("Request error after reauthenticating.", file=sys.stderr)
+            print("Error: request failed after reauthenticating.", file=sys.stderr)
             sys.exit(2)
         else:
             # just return the response no matter what the response status code is
             if ignore_response_status_codes:
                 return response
             # check if response is ok and can be converted to JSON
             if response.ok:
@@ -390,95 +432,157 @@
                 except ValueError as e:
                     print(f"Error: {e}", file=sys.stderr)
                     sys.exit(2)
             else:
                 print(self.format_api_error(response), file=sys.stderr)
                 sys.exit(2)
 
-    def get(self, path: str, authorize: Optional[bool] = True, headers: Optional[Dict[str, str]] = None,
-            timeout: int = DEFAULT_TIMEOUT, ignore_response_status_codes: Optional[bool] = False) -> requests.Response:
+    def get(
+        self,
+        path: str,
+        authorize: Optional[bool] = True,
+        headers: Optional[Dict[str, str]] = None,
+        timeout: int = DEFAULT_TIMEOUT,
+        ignore_response_status_codes: Optional[bool] = False,
+    ) -> requests.Response:
         """
         Send GET request.
 
         :param path: API endpoint path
         :param authorize: Add Authorization header to authorize request (True/False)
         :param headers: Request headers (JSON payload dict)
         :param timeout: Request timeout
         :param ignore_response_status_codes: Whether to ignore response status codes (even ones higher than 400)
         :return: Response object
         """
-        return self._request("GET", path, authorize=authorize, headers=headers, timeout=timeout,
-                             ignore_response_status_codes=ignore_response_status_codes)
+        return self._request(
+            "GET",
+            path,
+            authorize=authorize,
+            headers=headers,
+            timeout=timeout,
+            ignore_response_status_codes=ignore_response_status_codes,
+        )
 
-    def post(self, path: str, authorize: Optional[bool] = True, headers: Optional[Dict[str, str]] = None,
-             payload: Optional[Dict[str, Any]] = None, timeout: int = DEFAULT_TIMEOUT,
-             ignore_response_status_codes: Optional[bool] = False) -> requests.Response:
+    def post(
+        self,
+        path: str,
+        authorize: Optional[bool] = True,
+        headers: Optional[Dict[str, str]] = None,
+        payload: Optional[Dict[str, Any]] = None,
+        timeout: int = DEFAULT_TIMEOUT,
+        ignore_response_status_codes: Optional[bool] = False,
+    ) -> requests.Response:
         """
         Send POST request.
 
         :param path: API endpoint path
         :param authorize: Add Authorization header to authorize request (True/False)
         :param headers: Request headers (JSON payload dict)
         :param payload: Request payload (JSON payload dict)
         :param timeout: Request timeout in seconds
         :param ignore_response_status_codes: Whether to ignore response status codes (even ones higher than 400)
         :return: Response object
         """
-        return self._request("POST", path, authorize=authorize, headers=headers, payload=payload, timeout=timeout,
-                             ignore_response_status_codes=ignore_response_status_codes)
+        return self._request(
+            "POST",
+            path,
+            authorize=authorize,
+            headers=headers,
+            payload=payload,
+            timeout=timeout,
+            ignore_response_status_codes=ignore_response_status_codes,
+        )
 
-    def patch(self, path: str, authorize: Optional[bool] = True, headers: Optional[Dict[str, str]] = None,
-              payload: Optional[Dict[str, Any]] = None, timeout: int = DEFAULT_TIMEOUT,
-              ignore_response_status_codes: Optional[bool] = False) -> requests.Response:
+    def patch(
+        self,
+        path: str,
+        authorize: Optional[bool] = True,
+        headers: Optional[Dict[str, str]] = None,
+        payload: Optional[Dict[str, Any]] = None,
+        timeout: int = DEFAULT_TIMEOUT,
+        ignore_response_status_codes: Optional[bool] = False,
+    ) -> requests.Response:
         """
         Send PATCH request.
 
         :param path: API endpoint path
         :param authorize: Add Authorization header to authorize request (True/False)
         :param headers: Request headers (JSON payload dict)
         :param payload: Request payload (JSON payload dict)
         :param timeout: Request timeout in seconds
         :param ignore_response_status_codes: Whether to ignore response status codes (even ones higher than 400)
         :return: Response object
         """
-        return self._request("PATCH", path, authorize=authorize, headers=headers, payload=payload, timeout=timeout,
-                             ignore_response_status_codes=ignore_response_status_codes)
+        return self._request(
+            "PATCH",
+            path,
+            authorize=authorize,
+            headers=headers,
+            payload=payload,
+            timeout=timeout,
+            ignore_response_status_codes=ignore_response_status_codes,
+        )
 
-    def put(self, path: str, authorize: Optional[bool] = True, headers: Optional[Dict[str, str]] = None,
-            payload: Optional[Dict[str, Any]] = None, timeout: int = DEFAULT_TIMEOUT,
-            ignore_response_status_codes: Optional[bool] = False) -> requests.Response:
+    def put(
+        self,
+        path: str,
+        authorize: Optional[bool] = True,
+        headers: Optional[Dict[str, str]] = None,
+        payload: Optional[Dict[str, Any]] = None,
+        timeout: int = DEFAULT_TIMEOUT,
+        ignore_response_status_codes: Optional[bool] = False,
+    ) -> requests.Response:
         """
         Send PUT request.
 
         :param path: API endpoint path
         :param authorize: Add Authorization header to authorize request (True/False)
         :param headers: Request headers (JSON payload dict)
         :param payload: Request payload (JSON payload dict)
         :param timeout: Request timeout in seconds
         :param ignore_response_status_codes: Whether to ignore response status codes (even ones higher than 400)
         :return: Response object
         """
-        return self._request("PUT", path, authorize=authorize, headers=headers, payload=payload, timeout=timeout,
-                             ignore_response_status_codes=ignore_response_status_codes)
+        return self._request(
+            "PUT",
+            path,
+            authorize=authorize,
+            headers=headers,
+            payload=payload,
+            timeout=timeout,
+            ignore_response_status_codes=ignore_response_status_codes,
+        )
 
-    def delete(self, path: str, authorize: Optional[bool] = True, headers: Optional[Dict[str, str]] = None,
-               timeout: int = DEFAULT_TIMEOUT,
-               ignore_response_status_codes: Optional[bool] = False) -> requests.Response:
+    def delete(
+        self,
+        path: str,
+        authorize: Optional[bool] = True,
+        headers: Optional[Dict[str, str]] = None,
+        timeout: int = DEFAULT_TIMEOUT,
+        ignore_response_status_codes: Optional[bool] = False,
+    ) -> requests.Response:
         """
         Send DELETE request.
 
         :param path: API endpoint path
         :param authorize: Add Authorization header to authorize request (True/False)
         :param headers: Request headers (JSON payload dict)
         :param timeout: Request timeout in seconds
         :param ignore_response_status_codes: Whether to ignore response status codes (even ones higher than 400)
         :return: Response object
         """
-        return self._request("DELETE", path, authorize=authorize, headers=headers, timeout=timeout,
-                             ignore_response_status_codes=ignore_response_status_codes)
+        return self._request(
+            "DELETE",
+            path,
+            authorize=authorize,
+            headers=headers,
+            timeout=timeout,
+            ignore_response_status_codes=ignore_response_status_codes,
+        )
 
     def format_api_error(self, response: requests.Response) -> str:
         """
         Format API error.
 
         :param response: Response object
         :return: Formatted API error as string
```

## Comparing `spotter/cli.py` & `spotter/client/cli.py`

 * *Files 11% similar despite different names*

```diff
@@ -3,31 +3,41 @@
 import argparse
 import sys
 from pathlib import Path
 from typing import Dict, Union, Sequence, Optional, Any, NoReturn
 
 import colorama
 
-from spotter.api import ApiClient
-from spotter.commands import login, logout, register, scan, suggest, set_policies, clear_policies, get_config, \
-    set_config, clear_config
-from spotter.storage import Storage
-from spotter.utils import get_current_cli_version, validate_url
+from spotter.client.commands import (
+    login,
+    logout,
+    register,
+    suggest,
+    set_policies,
+    clear_policies,
+    get_config,
+    set_config,
+    clear_config,
+)
+from spotter.client.commands import scan
+from spotter.library.api import ApiClient
+from spotter.library.storage import Storage
+from spotter.library.utils import get_current_cli_version, validate_url
 
 
 class ArgParser(argparse.ArgumentParser):
     """An argument parser that displays help on error."""
 
     def error(self, message: str) -> NoReturn:
         """
         Overridden the original error method.
 
         :param message: Error message
         """
-        print(f"error: {message}\n", file=sys.stderr)
+        print(f"Error: {message}\n", file=sys.stderr)
         self.print_help()
         sys.exit(2)
 
     def add_subparsers(self, **kwargs: Dict[str, Any]) -> argparse._SubParsersAction:  # type: ignore
         """Overridden the original add_subparsers method (workaround for http://bugs.python.org/issue9253)."""
         subparsers = super().add_subparsers()
         subparsers.required = True
@@ -42,84 +52,86 @@
 
     :return: Parser as argparse.ArgumentParser object
     """
     parser = ArgParser(
         description="Steampunk Spotter - Ansible Playbook Scanning Tool",
         formatter_class=argparse.RawDescriptionHelpFormatter,
         epilog="additional information:\n"
-               "  You will need Steampunk Spotter account to be able to use the CLI.\n"
-               "  Create one with spotter register command or at https://spotter.steampunk.si/.\n\n"
-               "  To log in to Steampunk Spotter, you should provide your API token or username and password:\n"
-               "    - using spotter login command;\n"
-               "    - via --api-token/-t optional argument;\n"
-               "    - by setting SPOTTER_API_TOKEN environment variable;\n"
-               "    - via --username/-u and --password/-p global optional arguments;\n"
-               "    - by setting SPOTTER_USERNAME and SPOTTER_PASSWORD environment variables.\n\n"
-               "  What do you think about Spotter? Share your thoughts at "
-               "https://spotter.steampunk.si/feedback.\n"
-               "  Need more help or having other questions? Contact us at https://steampunk.si/contact/."
+        "  You will need Steampunk Spotter account to be able to use the CLI.\n"
+        "  Create one with spotter register command or at https://spotter.steampunk.si/.\n\n"
+        "  To log in to Steampunk Spotter, you should provide your API token or username and password:\n"
+        "    - using spotter login command;\n"
+        "    - via --api-token/-t optional argument;\n"
+        "    - by setting SPOTTER_API_TOKEN environment variable;\n"
+        "    - via --username/-u and --password/-p global optional arguments;\n"
+        "    - by setting SPOTTER_USERNAME and SPOTTER_PASSWORD environment variables.\n\n"
+        "  What do you think about Spotter? Share your thoughts at "
+        "https://spotter.steampunk.si/feedback.\n"
+        "  Need more help or having other questions? Contact us at https://steampunk.si/contact/.",
     )
 
     parser.add_argument(
-        "--version", "-v", action=PrintCurrentVersionAction, nargs=0,
-        help="Display the version of Steampunk Spotter CLI"
-    )
-    parser.add_argument(
-        "--endpoint", "-e", type=validate_url,
-        help=f"Steampunk Spotter API endpoint (instead of default {ApiClient.DEFAULT_ENDPOINT})"
-    )
-    parser.add_argument(
-        "--storage-path", "-s", type=lambda p: Path(p).absolute(),
-        help=f"Storage folder location (instead of default {Storage.DEFAULT_PATH})"
-    )
-    parser.add_argument(
-        "--api-token", "-t", type=str, help="Steampunk Spotter API token"
-    )
-    parser.add_argument(
-        "--username", "-u", type=str, help="Steampunk Spotter username"
-    )
-    parser.add_argument(
-        "--password", "-p", type=str, help="Steampunk Spotter password"
-    )
-    parser.add_argument(
-        "--no-colors", action="store_true", help="Disable output colors"
-    )
-    parser.add_argument(
-        "--debug", "-d", action="store_true", help="Enable debug output"
-    )
+        "--version",
+        "-v",
+        action=PrintCurrentVersionAction,
+        nargs=0,
+        help="Display the version of Steampunk Spotter CLI",
+    )
+    parser.add_argument(
+        "--endpoint",
+        "-e",
+        type=validate_url,
+        help=f"Steampunk Spotter API endpoint (instead of default {ApiClient.DEFAULT_ENDPOINT})",
+    )
+    parser.add_argument(
+        "--storage-path",
+        "-s",
+        type=lambda p: Path(p).absolute(),
+        help=f"Storage folder location (instead of default {Storage.DEFAULT_PATH})",
+    )
+    parser.add_argument("--api-token", "-t", type=str, help="Steampunk Spotter API token")
+    parser.add_argument("--username", "-u", type=str, help="Steampunk Spotter username")
+    parser.add_argument("--password", "-p", type=str, help="Steampunk Spotter password")
+    parser.add_argument("--no-colors", action="store_true", help="Disable output colors")
+    parser.add_argument("--debug", "-d", action="store_true", help="Enable debug output")
 
     subparsers = parser.add_subparsers()
     subparsers_metavar = ""
     cmds = [
         (register.__name__.rsplit(".", maxsplit=1)[-1], register),
         (login.__name__.rsplit(".", maxsplit=1)[-1], login),
         (logout.__name__.rsplit(".", maxsplit=1)[-1], logout),
         (scan.__name__.rsplit(".", maxsplit=1)[-1], scan),
         (suggest.__name__.rsplit(".", maxsplit=1)[-1], suggest),
         (set_policies.__name__.rsplit(".", maxsplit=1)[-1], set_policies),
         (clear_policies.__name__.rsplit(".", maxsplit=1)[-1], clear_policies),
         (get_config.__name__.rsplit(".", maxsplit=1)[-1], get_config),
         (set_config.__name__.rsplit(".", maxsplit=1)[-1], set_config),
-        (clear_config.__name__.rsplit(".", maxsplit=1)[-1], clear_config)
+        (clear_config.__name__.rsplit(".", maxsplit=1)[-1], clear_config),
     ]
     for command_name, module in cmds:
         # FIXME: Remove this if we decide that suggest command can be used standalone
         if command_name != "suggest":
             subparsers_metavar += f"{command_name.replace('_', '-')},"
         module.add_parser(subparsers)
 
     subparsers.metavar = f"{{{subparsers_metavar.rstrip(',')}}}"
     return parser
 
 
 class PrintCurrentVersionAction(argparse.Action):
     """An argument parser action for displaying current Python package version."""
 
-    def __call__(self, parser: argparse.ArgumentParser, namespace: argparse.Namespace,
-                 values: Union[str, Sequence[str], None], option_string: Optional[str] = None) -> NoReturn:
+    def __call__(
+        self,
+        parser: argparse.ArgumentParser,
+        namespace: argparse.Namespace,
+        values: Union[str, Sequence[str], None],
+        option_string: Optional[str] = None,
+    ) -> NoReturn:
         """
         Overridden the original __call__ method for argparse.Action.
 
         :param parser: ArgumentParser object
         :param namespace: Namespace object
         :param values: Command-line arguments
         :param option_string: Option string used to invoke this action.
@@ -133,15 +145,18 @@
     colorama.init(autoreset=True)
     parser = create_parser()
     args = parser.parse_args()
     args_dict = vars(args)
     # check if any of the arguments is empty
     for k in vars(args):
         if args_dict[k] == "":
-            print(f"Error: --{k.replace('_', '-')} argument is empty. "
-                  f"Please set the non-empty value or omit the argument if it is not needed.")
+            print(
+                f"Error: --{k.replace('_', '-')} argument is empty. "
+                f"Please set the non-empty value or omit the argument if it is not needed.",
+                file=sys.stderr,
+            )
             sys.exit(2)
     args.func(args)
 
 
 if __name__ == "__main__":
     main()
```

## Comparing `spotter/environment.py` & `spotter/library/environment.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,29 +3,37 @@
 import json
 import os
 import platform
 import subprocess
 import sys
 from copy import deepcopy
 from pathlib import Path
-from typing import Optional, List, Dict, Any
+from typing import Optional, List, Dict, Any, Tuple
+from multiprocessing.pool import ThreadPool
 
-import pkg_resources
 import pydantic.dataclasses
 import ruamel.yaml as yaml
-from pydantic.json import pydantic_encoder
 
-from spotter.parsing.noqa_comments import SpotterNoqa
+
+from spotter.library.compat.pydantic import compat_to_jsonable_python
+from spotter.library.parsing.noqa_comments import SpotterNoqa
+from spotter.library.utils import get_package_version
+
+
+def path_func(args: Tuple[Optional[Path], Any]) -> Any:
+    arg, func = args
+    result = func(arg)
+    return result
 
 
 class _EnvironmentDataclassConfig:
     extra = "forbid"
     # all dataclass arguments are optional because this is a discovery process
     # but use
-    allow_mutation = False
+    frozen = True
 
 
 @pydantic.dataclasses.dataclass(config=_EnvironmentDataclassConfig)
 class EnvironmentAnsibleVersion:
     """Discovered Ansible versions (per edition, i.e. full, base, core)."""
 
     ansible_core: Optional[str] = None
@@ -57,30 +65,24 @@
     @staticmethod
     def _get_ansible_core_python_version() -> Optional[str]:
         """
         Get ansible-core python package version.
 
         :return: Version string
         """
-        try:
-            return pkg_resources.get_distribution("ansible-core").version
-        except pkg_resources.DistributionNotFound:
-            return None
+        return get_package_version("ansible-core", False)
 
     @staticmethod
     def _get_ansible_base_python_version() -> Optional[str]:
         """
         Get ansible-base python package version.
 
         :return: Version string
         """
-        try:
-            return pkg_resources.get_distribution("ansible-base").version
-        except pkg_resources.DistributionNotFound:
-            return None
+        return get_package_version("ansible-base", False)
 
     @staticmethod
     def _get_ansible_version() -> Optional[str]:
         """
         Get Ansible version.
 
         :return: Version string
@@ -96,17 +98,17 @@
         """
         Get installed Ansible collections.
 
         :return: Dict with Ansible collection names and their versions
         """
         installed_collections = []
         try:
-            output = subprocess.check_output(["ansible-galaxy", "collection", "list", "--format", "json"],
-                                             stderr=subprocess.DEVNULL).decode(
-                "utf-8")
+            output = subprocess.check_output(
+                ["ansible-galaxy", "collection", "list", "--format", "json"], stderr=subprocess.DEVNULL
+            ).decode("utf-8")
             for _, value in json.loads(output).items():
                 for fqcn, version in value.items():
                     installed_collections.append(
                         {
                             "fqcn": fqcn,
                             "version": version.get("version", None),
                         }
@@ -122,16 +124,17 @@
 
         :return: Dict with Ansible config current settings specified as key-value pairs
         """
         ansible_config = {}
         try:
             str_path = str(path) if path.is_dir() else str(path.parent)
             env = dict(os.environ, ANSIBLE_FORCE_COLOR="0")
-            output = subprocess.check_output(["ansible-config", "dump", "--only-changed"],
-                                             stderr=subprocess.DEVNULL, cwd=str_path, env=env).decode("utf-8")
+            output = subprocess.check_output(
+                ["ansible-config", "dump", "--only-changed"], stderr=subprocess.DEVNULL, cwd=str_path, env=env
+            ).decode("utf-8")
             for line in output.splitlines():
                 if line:
                     key, value = line.split("=", maxsplit=1)
                     ansible_config[key.strip()] = value.strip()
             return ansible_config
         except (subprocess.CalledProcessError, FileNotFoundError):
             return ansible_config
@@ -154,16 +157,17 @@
                     return parsed
                 except yaml.YAMLError:
                     return {}
         except OSError:
             return {}
 
     @staticmethod
-    def _validate_collection_requirements(parsed_collections: List[Any],
-                                          requirements_path: Path) -> List[Dict[str, Any]]:
+    def _validate_collection_requirements(
+        parsed_collections: List[Any], requirements_path: Path
+    ) -> List[Dict[str, Any]]:
         """
         Validate Ansible collection requirements from requirements.yml.
 
         The rules applied here have been taken from: https://docs.ansible.com/ansible/latest/galaxy/user_guide.html.
 
         :param requirements_path: Path to requirements file
         :return: List of Ansible collection requirements
@@ -174,45 +178,57 @@
         collection_requirements: List[Dict[str, Any]] = []
         for entry in parsed_collections:
             if isinstance(entry, str):
                 collection_requirements.append({"name": entry})
             elif isinstance(entry, dict):
                 extra_keys = entry.keys() - requirements_yml_collections_keys
                 if extra_keys:
-                    print(f"Invalid keys '{extra_keys}' in entry '{entry}' under 'collections' in the requirements "
-                          f"file '{requirements_path}'. Supported keys are '{requirements_yml_collections_keys}'. "
-                          f"Ignoring.")
+                    print(
+                        f"Invalid keys '{extra_keys}' in entry '{entry}' under 'collections' in the requirements "
+                        f"file '{requirements_path}'. Supported keys are '{requirements_yml_collections_keys}'. "
+                        f"Ignoring."
+                    )
                     return []
 
                 if "name" not in entry:
-                    print(f"Missing required 'name' key in entry '{entry}' under 'collections' in the requirements "
-                          f"file '{requirements_path}'. Ignoring.")
+                    print(
+                        f"Missing required 'name' key in entry '{entry}' under 'collections' in the requirements "
+                        f"file '{requirements_path}'. Ignoring."
+                    )
                     return []
 
                 for key in entry.keys():
                     if key == "signatures" and not isinstance(entry["signatures"], list):
-                        print(f"The 'signatures' key in entry '{entry}' under 'collections' in the requirements file "
-                              f"'{requirements_path}' should be of type list but is '{type(entry)}'. Ignoring.")
+                        print(
+                            f"The 'signatures' key in entry '{entry}' under 'collections' in the requirements file "
+                            f"'{requirements_path}' should be of type list but is '{type(entry)}'. Ignoring."
+                        )
                         return []
                     if key != "signatures" and not isinstance(entry[key], str):
-                        print(f"The '{key}' key in entry '{entry}' under 'collections' in the requirements file "
-                              f"'{requirements_path}' should be of type string but is '{type(entry[key])}'. Ignoring.")
+                        print(
+                            f"The '{key}' key in entry '{entry}' under 'collections' in the requirements file "
+                            f"'{requirements_path}' should be of type string but is '{type(entry[key])}'. Ignoring."
+                        )
                         return []
                     if key == "type":
                         extra_keys_type = {entry[key]} - type_key_allowed_values
                         if extra_keys_type:
-                            print(f"Invalid values '{extra_keys_type}' in for 'type' key in entry '{entry}' under "
-                                  f"'collections' in the requirements file '{requirements_path}'. Supported keys are "
-                                  f"'{type_key_allowed_values}'. Ignoring.")
+                            print(
+                                f"Invalid values '{extra_keys_type}' in for 'type' key in entry '{entry}' under "
+                                f"'collections' in the requirements file '{requirements_path}'. Supported keys are "
+                                f"'{type_key_allowed_values}'. Ignoring."
+                            )
                             return []
 
                 collection_requirements.append(entry)
             else:
-                print(f"The entry '{entry}' under 'collections' key in the requirements file '{requirements_path}' "
-                      f"should be of type string or dict but is '{type(entry)}'. Ignoring.")
+                print(
+                    f"The entry '{entry}' under 'collections' key in the requirements file '{requirements_path}' "
+                    f"should be of type string or dict but is '{type(entry)}'. Ignoring."
+                )
                 return []
 
         return collection_requirements
 
     @staticmethod
     def _validate_role_requirements(parsed_roles: List[Any], requirements_path: Path) -> List[Dict[str, Any]]:
         """
@@ -228,40 +244,50 @@
         role_requirements: List[Dict[str, Any]] = []
         for entry in parsed_roles:
             if isinstance(entry, str):
                 role_requirements.append({"src": entry})
             elif isinstance(entry, dict):
                 extra_keys = entry.keys() - requirements_yml_roles_keys
                 if extra_keys:
-                    print(f"Invalid keys '{extra_keys}' in entry '{entry}' under 'roles' in the requirements file "
-                          f"'{requirements_path}'. Supported keys are '{requirements_yml_roles_keys}'. Ignoring.")
+                    print(
+                        f"Invalid keys '{extra_keys}' in entry '{entry}' under 'roles' in the requirements file "
+                        f"'{requirements_path}'. Supported keys are '{requirements_yml_roles_keys}'. Ignoring."
+                    )
                     return []
 
                 if "src" not in entry and "name" not in entry:
-                    print(f"Missing required 'src' or 'name key in entry '{entry}' under 'roles' in the requirements "
-                          f"file '{requirements_path}'. Ignoring.")
+                    print(
+                        f"Missing required 'src' or 'name key in entry '{entry}' under 'roles' in the requirements "
+                        f"file '{requirements_path}'. Ignoring."
+                    )
                     return []
 
                 for key in entry.keys():
                     if not isinstance(entry[key], str):
-                        print(f"The '{key}' key in entry '{entry}' under 'roles' in the requirements file "
-                              f"'{requirements_path}' should be of type string but is '{type(entry[key])}'. Ignoring.")
+                        print(
+                            f"The '{key}' key in entry '{entry}' under 'roles' in the requirements file "
+                            f"'{requirements_path}' should be of type string but is '{type(entry[key])}'. Ignoring."
+                        )
                         return []
                     if key == "scm":
                         extra_keys_scm = {entry[key]} - scm_key_allowed_values
                         if extra_keys_scm:
-                            print(f"Invalid values '{extra_keys}' in for 'scm' key in entry '{entry}' under 'roles' in "
-                                  f"the requirements file {requirements_path}'. Supported keys are "
-                                  f"'{scm_key_allowed_values}'. Ignoring.")
+                            print(
+                                f"Invalid values '{extra_keys}' in for 'scm' key in entry '{entry}' under 'roles' in "
+                                f"the requirements file {requirements_path}'. Supported keys are "
+                                f"'{scm_key_allowed_values}'. Ignoring."
+                            )
                             return []
 
                 role_requirements.append(entry)
             else:
-                print(f"The entry '{entry}' under 'roles' key in the requirements file '{requirements_path}' should be "
-                      f"of type string or dict but is '{type(entry)}'. Ignoring.")
+                print(
+                    f"The entry '{entry}' under 'roles' key in the requirements file '{requirements_path}' should be "
+                    f"of type string or dict but is '{type(entry)}'. Ignoring."
+                )
                 return []
 
         return role_requirements
 
     @staticmethod
     def _get_requirements(path: Path) -> Dict[str, List[Dict[str, Any]]]:
         """
@@ -273,61 +299,71 @@
         # pylint: disable=too-many-branches,too-many-return-statements
         try:
             # TODO: Update discovery as requirements.yml files can be anywhere
             search_path = path
             if path.is_file():
                 search_path = path.parent
 
-            possible_requirements_yml_paths = (search_path / "requirements.yml",
-                                               search_path / "requirements.yaml",
-                                               search_path / "collections" / "requirements.yml",
-                                               search_path / "collections" / "requirements.yaml",
-                                               search_path / "roles" / "requirements.yml",
-                                               search_path / "roles" / "requirements.yaml")
+            possible_requirements_yml_paths = (
+                search_path / "requirements.yml",
+                search_path / "requirements.yaml",
+                search_path / "collections" / "requirements.yml",
+                search_path / "collections" / "requirements.yaml",
+                search_path / "roles" / "requirements.yml",
+                search_path / "roles" / "requirements.yaml",
+            )
 
             requirements_yml_path = None
             for possible_requirements_yml_path in possible_requirements_yml_paths:
                 if possible_requirements_yml_path.exists():
                     requirements_yml_path = possible_requirements_yml_path
                     break
 
             if requirements_yml_path:
                 with requirements_yml_path.open("r", encoding="utf-8") as stream:
                     try:
                         parsed = yaml.safe_load(stream)
                         if isinstance(parsed, dict) and all(isinstance(k, str) for k in parsed.keys()):
                             if not ("collections" in parsed or "roles" in parsed):
-                                print(f"Missing 'collections' or 'roles' key in the requirements file "
-                                      f"'{requirements_yml_path}'. Ignoring.")
+                                print(
+                                    f"Missing 'collections' or 'roles' key in the requirements file "
+                                    f"'{requirements_yml_path}'. Ignoring."
+                                )
                                 return {}
 
                             if "collections" in parsed:
                                 if not isinstance(parsed["collections"], list):
-                                    print(f"The 'collections' key in the requirements file '{requirements_yml_path}' "
-                                          f"is not of list type. Ignoring.")
+                                    print(
+                                        f"The 'collections' key in the requirements file '{requirements_yml_path}' "
+                                        f"is not of list type. Ignoring."
+                                    )
                                     return {}
 
                                 parsed["collections"] = Environment._validate_collection_requirements(
-                                    parsed["collections"],
-                                    requirements_yml_path
+                                    parsed["collections"], requirements_yml_path
                                 )
 
                             if "roles" in parsed:
                                 if not isinstance(parsed["roles"], list):
-                                    print(f"The 'roles' key in the requirements file '{requirements_yml_path}' is not "
-                                          f"of list type. Ignoring.")
+                                    print(
+                                        f"The 'roles' key in the requirements file '{requirements_yml_path}' is not "
+                                        f"of list type. Ignoring."
+                                    )
                                     return {}
 
-                                parsed["roles"] = Environment._validate_role_requirements(parsed["roles"],
-                                                                                          requirements_yml_path)
+                                parsed["roles"] = Environment._validate_role_requirements(
+                                    parsed["roles"], requirements_yml_path
+                                )
                         elif isinstance(parsed, list):
                             parsed = {"roles": Environment._validate_role_requirements(parsed, requirements_yml_path)}
                         else:
-                            print(f"Failed basic format checking for the requirements file '{requirements_yml_path}'. "
-                                  f"Ignoring.")
+                            print(
+                                f"Failed basic format checking for the requirements file '{requirements_yml_path}'. "
+                                f"Ignoring."
+                            )
                             return {}
 
                         return parsed  # type: ignore
                     except yaml.YAMLError:
                         return {}
             else:
                 return {}
@@ -338,25 +374,36 @@
     def from_local_discovery(cls, paths: List[Path]) -> "Environment":
         """Set workspace variables discovered locally on user's system.
 
         :param paths: List of paths to directory where to look for local files
         :return: Environment object
         """
         # TODO: Add support to combine multiple galaxy.yml and requirements.yml, right now we use just the first one
+
+        active_path = paths[0] if paths else None
+        functions = [
+            (active_path, lambda x: cls._get_installed_ansible_collections()),
+            (active_path, cls._get_ansible_config if active_path else lambda x: {}),
+            (active_path, lambda x: cls._get_ansible_version()),
+        ]
+
+        pool = ThreadPool()
+        results = pool.map(path_func, functions)
+
         return cls(
             python_version=cls._get_python_version(),
             ansible_version=EnvironmentAnsibleVersion(
                 ansible_core=cls._get_ansible_core_python_version(),
                 ansible_base=cls._get_ansible_base_python_version(),
-                ansible=cls._get_ansible_version(),
+                ansible=results[2],
             ),
-            installed_collections=cls._get_installed_ansible_collections(),
-            ansible_config=cls._get_ansible_config(paths[0]) if paths else {},
+            installed_collections=results[0],
+            ansible_config=results[1],
             galaxy_yml=cls._get_galaxy_yml(paths[0]) if paths else {},
-            collection_requirements=cls._get_requirements(paths[0]) if paths else {}
+            collection_requirements=cls._get_requirements(paths[0]) if paths else {},
         )
 
     @classmethod
     def from_config_file(cls, config_path: Path) -> "Environment":
         """
         Set workspace variables from config file.
 
@@ -372,86 +419,97 @@
                 config = yaml.safe_load(config_file)
 
                 if config is None:
                     print(f"Warning: empty configuration file '{config_path}'. Ignoring.")
                     return cls()
 
                 if not isinstance(config, dict) and all(isinstance(k, str) for k in config.keys()):
-                    print(f"The content of configuration file '{config_path}' should be of type dict but is "
-                          f"'{type(config)}'.")
+                    print(
+                        f"Error: the content of configuration file '{config_path}' should be of type dict but is "
+                        f"'{type(config)}'.",
+                        file=sys.stderr,
+                    )
                     sys.exit(2)
 
-                valid_config_entries = {
-                    "ansible_version": str,
-                    "skip_checks": list,
-                    "enforce_checks": list
-                }
+                valid_config_entries = {"ansible_version": str, "skip_checks": list, "enforce_checks": list}
                 extra_keys = config.keys() - valid_config_entries.keys()
                 if extra_keys:
-                    print(f"Invalid keys '{extra_keys}' in configuration file '{config_path}'. "
-                          f"Supported keys are '{valid_config_entries.keys()}'.")
+                    print(
+                        f"Error: invalid keys '{extra_keys}' in configuration file '{config_path}'. "
+                        f"Supported keys are '{valid_config_entries.keys()}'.",
+                        file=sys.stderr,
+                    )
                     sys.exit(2)
 
                 for key, typ in valid_config_entries.items():
                     entry = config.get(key, None)
                     if entry and not isinstance(entry, typ):
-                        print(f"The '{key}' key in the configuration file '{config_path}' should be of type {typ} but "
-                              f"is '{type(entry)}'.")
+                        print(
+                            f"Error: the '{key}' key in the configuration file '{config_path}' should be of type "
+                            f"{typ} but is '{type(entry)}'.",
+                            file=sys.stderr,
+                        )
                         sys.exit(2)
 
                 environment = cls()
                 ansible_version = config.get("ansible_version", None)
                 if ansible_version:
-                    environment.ansible_version = EnvironmentAnsibleVersion(
-                        ansible_core=config.get("ansible_version")
-                    )
+                    environment.ansible_version = EnvironmentAnsibleVersion(ansible_core=config.get("ansible_version"))
 
                 skip_checks = config.get("skip_checks", [])
                 if isinstance(skip_checks, list) and all(isinstance(e, str) for e in skip_checks):
                     skip_checks = [SpotterNoqa(event=e) for e in skip_checks]
                 else:
-                    skip_checks = [SpotterNoqa(
-                        event=e.get("event", None),
-                        subevent_code=e.get("subevent_code", None),
-                        fqcn=e.get("fqcn", None)
-                    ) for e in skip_checks]
+                    skip_checks = [
+                        SpotterNoqa(
+                            event=e.get("event", None),
+                            subevent_code=e.get("subevent_code", None),
+                            fqcn=e.get("fqcn", None),
+                        )
+                        for e in skip_checks
+                    ]
 
                 enforce_checks = config.get("enforce_checks", [])
                 if isinstance(enforce_checks, list) and all(isinstance(e, str) for e in enforce_checks):
                     enforce_checks = [SpotterNoqa(event=e) for e in enforce_checks]
                 else:
-                    enforce_checks = [SpotterNoqa(
-                        event=e.get("event", None),
-                        subevent_code=e.get("subevent_code", None),
-                        fqcn=e.get("fqcn", None)
-                    ) for e in enforce_checks]
-
-                environment.cli_scan_args = {
-                    "skip_checks": skip_checks,
-                    "enforce_checks": enforce_checks
-                }
+                    enforce_checks = [
+                        SpotterNoqa(
+                            event=e.get("event", None),
+                            subevent_code=e.get("subevent_code", None),
+                            fqcn=e.get("fqcn", None),
+                        )
+                        for e in enforce_checks
+                    ]
+
+                environment.cli_scan_args = {"skip_checks": skip_checks, "enforce_checks": enforce_checks}
 
                 return environment
         except (yaml.YAMLError, AttributeError) as e:
-            print(f"Invalid configuration file: {e}", file=sys.stderr)
+            print(f"Error: invalid configuration file: {e}", file=sys.stderr)
             sys.exit(2)
 
     @classmethod
     def from_project_configuration_file(cls) -> "Environment":
         """Set workspace variables from project-level configuration file.
 
         :return: Configuration object
         """
-        possible_project_config_paths = (Path.cwd() / ".spotter.json",
-                                         Path.cwd() / ".spotter.yml",
-                                         Path.cwd() / ".spotter.yaml")
+        possible_project_config_paths = (
+            Path.cwd() / ".spotter.json",
+            Path.cwd() / ".spotter.yml",
+            Path.cwd() / ".spotter.yaml",
+        )
         project_config_paths = [p for p in possible_project_config_paths if p.exists()]
         if len(project_config_paths) > 1:
-            print(f"There should be exactly one Spotter configuration file in the '{Path.cwd()}' project. Found "
-                  f"{len(project_config_paths)} files: {[p.name for p in project_config_paths]}.", file=sys.stderr)
+            print(
+                f"Error: there should be exactly one Spotter configuration file in the '{Path.cwd()}' project. Found "
+                f"{len(project_config_paths)} files: {[p.name for p in project_config_paths]}.",
+                file=sys.stderr,
+            )
             sys.exit(2)
 
         if project_config_paths:
             return cls.from_config_file(project_config_paths[0])
         return cls()
 
     def combine(self, other: "Environment") -> "Environment":
@@ -459,12 +517,12 @@
         Combine two dataclasses into one, overriding with values from `other`.
 
         Null values in `other` do not override original values.
 
         :param other: Environment to combine with
         :return: Environment object
         """
-        original_dict_copy = deepcopy(pydantic_encoder(self))
-        other_dict_copy = deepcopy(pydantic_encoder(other))
+        original_dict_copy = deepcopy(compat_to_jsonable_python(self))
+        other_dict_copy = deepcopy(compat_to_jsonable_python(other))
         other_dict_without_nulls = {k: v for k, v in other_dict_copy.items() if v is not None}
         original_dict_copy.update(other_dict_without_nulls)
         return self.__class__(**original_dict_copy)
```

## Comparing `spotter/storage.py` & `spotter/library/storage.py`

 * *Files identical despite different names*

## Comparing `spotter/commands/clear_config.py` & `spotter/client/commands/clear_config.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,32 +2,34 @@
 
 import argparse
 import os
 import sys
 from pathlib import Path
 from typing import Optional
 
-from spotter.api import ApiClient
-from spotter.storage import Storage
+from spotter.library.api import ApiClient
+from spotter.library.storage import Storage
 
 
 def add_parser(subparsers: "argparse._SubParsersAction[argparse.ArgumentParser]") -> None:
     """
     Add a new parser for clear-config command to subparsers.
 
     :param subparsers: Subparsers action
     """
     parser = subparsers.add_parser(
-        "clear-config", help="Clear configuration from organization",
-        description="Clear organization-level file with configuration (e.g., for enforcing and skipping checks)"
+        "clear-config",
+        help="Clear configuration from organization",
+        description="Clear organization-level file with configuration (e.g., for enforcing and skipping checks)",
     )
     parser.add_argument(
-        "--organization-id", type=str,
+        "--organization-id",
+        type=str,
         help="UUID of an existing Steampunk Spotter organization to clear configuration from "
-             "(default organization will be used if not specified)"
+        "(default organization will be used if not specified)",
     )
     parser.set_defaults(func=_parser_callback)
 
 
 def _parser_callback(args: argparse.Namespace) -> None:
     """
     Execute callback for clear-config command.
@@ -50,16 +52,23 @@
         api_client.debug_organization(organization_id)
     else:
         api_client.debug_print("Clearing configuration for default organization")
         api_client.debug_my_default_organization()
 
 
 # pylint: disable=too-many-arguments,too-many-locals,too-many-branches
-def clear_config(api_endpoint: Optional[str], storage_path: Path, api_token: Optional[str], username: Optional[str],
-                 password: Optional[str], organization_id: Optional[str], debug: bool = False) -> None:
+def clear_config(
+    api_endpoint: Optional[str],
+    storage_path: Path,
+    api_token: Optional[str],
+    username: Optional[str],
+    password: Optional[str],
+    organization_id: Optional[str],
+    debug: bool = False,
+) -> None:
     """
     Clear configuration file for organization.
 
     By default, this will clear configuration from the default organization.
 
     :param api_endpoint: Steampunk Spotter API endpoint
     :param storage_path: Path to storage
@@ -81,17 +90,15 @@
     else:
         endpoint = api_endpoint
 
     api_client = ApiClient(endpoint, storage, api_token, username, password, debug=debug)
     api_client.debug_print_me()
     _debug_print_project_and_org(api_client, organization_id)
     if organization_id:
-        response = api_client.patch(
-            f"/v3/configuration/?organization={organization_id}", payload={"spotter_noqa": []}
-        )
+        response = api_client.patch(f"/v3/configuration/?organization={organization_id}", payload={"spotter_noqa": []})
     else:
         response = api_client.patch("/v3/configuration/", payload={"spotter_noqa": []})
     if not response.ok:
         print(api_client.format_api_error(response), file=sys.stderr)
         sys.exit(2)
 
     print("Configuration successfully cleared.")
```

## Comparing `spotter/commands/clear_policies.py` & `spotter/client/commands/clear_policies.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,37 +2,41 @@
 
 import argparse
 import os
 import sys
 from pathlib import Path
 from typing import Optional, Dict, Any
 
-from spotter.api import ApiClient
-from spotter.storage import Storage
+from spotter.library.api import ApiClient
+from spotter.library.storage import Storage
 
 
 def add_parser(subparsers: "argparse._SubParsersAction[argparse.ArgumentParser]") -> None:
     """
     Add a new parser for clear-policies command to subparsers.
 
     :param subparsers: Subparsers action
     """
     parser = subparsers.add_parser(
-        "clear-policies", help="Clear custom policies (enterprise feature)",
-        description="Clear OPA policies for custom Spotter checks (enterprise feature)"
+        "clear-policies",
+        help="Clear custom policies (enterprise feature)",
+        description="Clear OPA policies for custom Spotter checks (enterprise feature)",
     )
     project_organization_group = parser.add_mutually_exclusive_group()
     project_organization_group.add_argument(
-        "--project-id", "-p", type=str,
+        "--project-id",
+        "-p",
+        type=str,
         help="UUID of an existing Steampunk Spotter project to clear custom policies from "
-             "(default project from the default organization will be used if not specified)"
+        "(default project from the default organization will be used if not specified)",
     )
     project_organization_group.add_argument(
-        "--organization-id", type=str, help="UUID of an existing Steampunk Spotter organization to clear custom "
-                                            "policies from"
+        "--organization-id",
+        type=str,
+        help="UUID of an existing Steampunk Spotter organization to clear custom " "policies from",
     )
     parser.set_defaults(func=_parser_callback)
 
 
 def _parser_callback(args: argparse.Namespace) -> None:
     """
     Execute callback for clear-policies command.
@@ -42,35 +46,44 @@
     api_endpoint = args.endpoint or os.environ.get("SPOTTER_ENDPOINT", None)
     storage_path = args.storage_path or Storage.DEFAULT_PATH
     api_token = args.api_token or os.environ.get("SPOTTER_API_TOKEN")
     username = args.username or os.environ.get("SPOTTER_USERNAME")
     password = args.password or os.environ.get("SPOTTER_PASSWORD")
     debug = args.debug
 
-    clear_policies(api_endpoint, storage_path, api_token, username, password, args.project_id, args.organization_id,
-                   debug=debug)
+    clear_policies(
+        api_endpoint, storage_path, api_token, username, password, args.project_id, args.organization_id, debug=debug
+    )
 
 
-def _debug_print_project_and_org(api_client: ApiClient, project_id: Optional[str],
-                                 organization_id: Optional[str]) -> None:
+def _debug_print_project_and_org(
+    api_client: ApiClient, project_id: Optional[str], organization_id: Optional[str]
+) -> None:
     if project_id is not None:
         api_client.debug_print("Clearing polices for project id {}", project_id)
         api_client.debug_project(project_id)
     elif organization_id is not None:
         api_client.debug_print("Clearing polices for organization id {}", organization_id)
         api_client.debug_organization(organization_id)
     else:
         api_client.debug_print("Clearing polices for default organization")
         api_client.debug_my_default_organization()
 
 
 # pylint: disable=too-many-arguments,too-many-locals,too-many-branches
-def clear_policies(api_endpoint: Optional[str], storage_path: Path, api_token: Optional[str], username: Optional[str],
-                   password: Optional[str], project_id: Optional[str], organization_id: Optional[str],
-                   debug: bool = False) -> None:
+def clear_policies(
+    api_endpoint: Optional[str],
+    storage_path: Path,
+    api_token: Optional[str],
+    username: Optional[str],
+    password: Optional[str],
+    project_id: Optional[str],
+    organization_id: Optional[str],
+    debug: bool = False,
+) -> None:
     """
     Clear custom OPA policies.
 
     By default, this will clear policies that belong to the default project from default organization.
 
     :param api_endpoint: Steampunk Spotter API endpoint
     :param storage_path: Path to storage
@@ -89,38 +102,39 @@
             storage_configuration_json = storage.read_json("spotter.json")
             endpoint = storage_configuration_json.get("endpoint", ApiClient.DEFAULT_ENDPOINT)
         else:
             endpoint = ApiClient.DEFAULT_ENDPOINT
     else:
         endpoint = api_endpoint
 
-    payload: Dict[str, Any] = {
-        "policies": [],
-        "project_id": project_id,
-        "organization_id": organization_id
-    }
+    payload: Dict[str, Any] = {"policies": [], "project_id": project_id, "organization_id": organization_id}
 
     api_client = ApiClient(endpoint, storage, api_token, username, password, debug=debug)
     api_client.debug_print_me()
     _debug_print_project_and_org(api_client, project_id, organization_id)
     response = api_client.put("/v2/opa/", payload=payload, ignore_response_status_codes=True)
     if response.status_code == 402:
-        print("The use of custom policies is only available in Spotter's ENTERPRISE plan. "
-              "Please upgrade your plan to use this functionality.")
+        print(
+            "Error: the use of custom policies is only available in Spotter's ENTERPRISE plan. "
+            "Please upgrade your plan to use this functionality."
+        )
         sys.exit(2)
     if response.status_code == 403 and project_id:
-        print("The user is not a member of the organization that includes the project with the given ID.")
+        print(
+            "Error: the user is not a member of the organization that includes the project with the given ID.",
+            file=sys.stderr,
+        )
         sys.exit(2)
     if response.status_code == 403 and organization_id:
-        print("The user is not a member of the organization with the given ID.")
+        print("Error: the user is not a member of the organization with the given ID.", file=sys.stderr)
         sys.exit(2)
     if response.status_code == 404 and project_id:
-        print("The project with the given ID was not found.")
+        print("Error: the project with the given ID was not found.", file=sys.stderr)
         sys.exit(2)
     if response.status_code == 404 and organization_id:
-        print("The organization with the given ID was not found.")
+        print("Error: the organization with the given ID was not found.", file=sys.stderr)
         sys.exit(2)
     if not response.ok:
         print(api_client.format_api_error(response), file=sys.stderr)
         sys.exit(2)
 
     print("Custom policies successfully cleared.")
```

## Comparing `spotter/commands/get_config.py` & `spotter/client/commands/get_config.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,32 +3,34 @@
 import argparse
 import json
 import os
 import sys
 from pathlib import Path
 from typing import Optional
 
-from spotter.api import ApiClient
-from spotter.storage import Storage
+from spotter.library.api import ApiClient
+from spotter.library.storage import Storage
 
 
 def add_parser(subparsers: "argparse._SubParsersAction[argparse.ArgumentParser]") -> None:
     """
     Add a new parser for get-config command to subparsers.
 
     :param subparsers: Subparsers action
     """
     parser = subparsers.add_parser(
-        "get-config", help="Get configuration from organization",
-        description="Print organization-level file with configuration (e.g., for enforcing and skipping checks)"
+        "get-config",
+        help="Get configuration from organization",
+        description="Print organization-level file with configuration (e.g., for enforcing and skipping checks)",
     )
     parser.add_argument(
-        "--organization-id", type=str,
+        "--organization-id",
+        type=str,
         help="UUID of an existing Steampunk Spotter organization to get configuration from "
-             "(default organization will be used if not specified)"
+        "(default organization will be used if not specified)",
     )
     parser.set_defaults(func=_parser_callback)
 
 
 def _parser_callback(args: argparse.Namespace) -> None:
     """
     Execute callback for get-config command.
@@ -51,16 +53,23 @@
         api_client.debug_organization(organization_id)
     else:
         api_client.debug_print("Getting configuration for default organization")
         api_client.debug_my_default_organization()
 
 
 # pylint: disable=too-many-arguments,too-many-locals,too-many-branches
-def get_config(api_endpoint: Optional[str], storage_path: Path, api_token: Optional[str], username: Optional[str],
-               password: Optional[str], organization_id: Optional[str], debug: bool = False) -> None:
+def get_config(
+    api_endpoint: Optional[str],
+    storage_path: Path,
+    api_token: Optional[str],
+    username: Optional[str],
+    password: Optional[str],
+    organization_id: Optional[str],
+    debug: bool = False,
+) -> None:
     """
     Get configuration file for organization.
 
     By default, this will print configuration from the default organization.
 
     :param api_endpoint: Steampunk Spotter API endpoint
     :param storage_path: Path to storage
```

## Comparing `spotter/commands/login.py` & `spotter/client/commands/login.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 """Provide login CLI command."""
 
 import argparse
 import os
 from getpass import getpass
 from pathlib import Path
 from typing import Optional
-from spotter.api import ApiClient
-from spotter.storage import Storage
+
+from spotter.library.api import ApiClient
+from spotter.library.storage import Storage
 
 
 def add_parser(subparsers: "argparse._SubParsersAction[argparse.ArgumentParser]") -> None:
     """
     Add a new parser for login command to subparsers.
 
     :param subparsers: Subparsers action
@@ -37,16 +38,22 @@
     if not api_token and not password:
         password = getpass()
 
     login(api_endpoint, storage_path, api_token, username, password, debug=args.debug)
     print("Login successful!")
 
 
-def login(api_endpoint: str, storage_path: Path, api_token: Optional[str], username: Optional[str],
-          password: Optional[str], debug: bool = False) -> None:
+def login(
+    api_endpoint: str,
+    storage_path: Path,
+    api_token: Optional[str],
+    username: Optional[str],
+    password: Optional[str],
+    debug: bool = False,
+) -> None:
     """
     Do user login.
 
     :param api_endpoint: Steampunk Spotter API endpoint
     :param storage_path: Path to storage
     :param api_token: Steampunk Spotter API token
     :param username: Steampunk Spotter username
```

## Comparing `spotter/commands/logout.py` & `spotter/client/commands/logout.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,27 +1,28 @@
 """Provide logout CLI command."""
 
 import argparse
 import os
 from pathlib import Path
 from typing import Optional
 
-from spotter.api import ApiClient
-from spotter.storage import Storage
+from spotter.library.api import ApiClient
+from spotter.library.storage import Storage
 
 
 def add_parser(subparsers: "argparse._SubParsersAction[argparse.ArgumentParser]") -> None:
     """
     Add a new parser for logout command to subparsers.
 
     :param subparsers: Subparsers action
     """
     parser = subparsers.add_parser(
-        "logout", help="Log out from Steampunk Spotter user account",
-        description="Log out from Steampunk Spotter user account"
+        "logout",
+        help="Log out from Steampunk Spotter user account",
+        description="Log out from Steampunk Spotter user account",
     )
     parser.set_defaults(func=_parser_callback)
 
 
 def _parser_callback(args: argparse.Namespace) -> None:
     """
     Execute callback for logout command.
@@ -34,16 +35,17 @@
     username = args.username or os.environ.get("SPOTTER_USERNAME")
     password = args.password or os.environ.get("SPOTTER_PASSWORD")
 
     logout(api_endpoint, storage_path, api_token, username, password)
     print("Logout successful!")
 
 
-def logout(api_endpoint: str, storage_path: Path, api_token: Optional[str], username: Optional[str],
-           password: Optional[str]) -> None:
+def logout(
+    api_endpoint: str, storage_path: Path, api_token: Optional[str], username: Optional[str], password: Optional[str]
+) -> None:
     """
     Do user logout.
 
     This will remove storage folder with auth tokens.
 
     :param api_endpoint: Steampunk Spotter API endpoint
     :param storage_path: Path to storage
```

## Comparing `spotter/commands/register.py` & `spotter/client/commands/register.py`

 * *Files 11% similar despite different names*

```diff
@@ -9,16 +9,17 @@
 def add_parser(subparsers: "argparse._SubParsersAction[argparse.ArgumentParser]") -> None:
     """
     Add a new parser for register command to subparsers.
 
     :param subparsers: Subparsers action
     """
     parser = subparsers.add_parser(
-        "register", help="Register for a new Steampunk Spotter user account",
-        description="Register for a new Steampunk Spotter user account"
+        "register",
+        help="Register for a new Steampunk Spotter user account",
+        description="Register for a new Steampunk Spotter user account",
     )
     parser.set_defaults(func=_parser_callback)
 
 
 def _parser_callback(args: argparse.Namespace) -> None:  # pylint: disable=unused-argument
     """
     Execute callback for register command.
@@ -30,10 +31,13 @@
 
 def register() -> None:
     """Open the browser at the registration form."""
     registration_url = "https://spotter.steampunk.si/register/team-plan"
     try:
         webbrowser.open(registration_url)
     except webbrowser.Error as e:
-        print(f"Cannot open a browser to display the registration form: {e}.\n"
-              f"Please visit {registration_url} in your browser.", file=sys.stderr)
+        print(
+            f"Error: cannot open a browser to display the registration form: {e}.\n"
+            f"Please visit {registration_url} in your browser.",
+            file=sys.stderr,
+        )
         sys.exit(2)
```

## Comparing `spotter/commands/set_config.py` & `spotter/client/commands/set_config.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,39 +2,39 @@
 
 import argparse
 import os
 import sys
 from pathlib import Path
 from typing import Optional
 
-from pydantic.json import pydantic_encoder
-
-from spotter.api import ApiClient
-from spotter.environment import Environment
-from spotter.storage import Storage
+from spotter.library.api import ApiClient
+from spotter.library.compat.pydantic import compat_to_jsonable_python
+from spotter.library.environment import Environment
+from spotter.library.storage import Storage
 
 
 def add_parser(subparsers: "argparse._SubParsersAction[argparse.ArgumentParser]") -> None:
     """
     Add a new parser for set-config command to subparsers.
 
     :param subparsers: Subparsers action
     """
     parser = subparsers.add_parser(
-        "set-config", help="Set configuration file for organization",
-        description="Set organization-level file with configuration (e.g., for enforcing and skipping checks)"
+        "set-config",
+        help="Set configuration file for organization",
+        description="Set organization-level file with configuration (e.g., for enforcing and skipping checks)",
     )
     parser.add_argument(
-        "--organization-id", type=str,
+        "--organization-id",
+        type=str,
         help="UUID of an existing Steampunk Spotter organization to set configuration for "
-             "(default organization will be used if not specified)"
+        "(default organization will be used if not specified)",
     )
     parser.add_argument(
-        "config_path", type=lambda p: Path(p).absolute(),
-        help="Path to the configuration file (JSON/YAML)"
+        "config_path", type=lambda p: Path(p).absolute(), help="Path to the configuration file (JSON/YAML)"
     )
     parser.set_defaults(func=_parser_callback)
 
 
 def _parser_callback(args: argparse.Namespace) -> None:
     """
     Execute callback for set-config command.
@@ -46,39 +46,45 @@
     api_token = args.api_token or os.environ.get("SPOTTER_API_TOKEN")
     username = args.username or os.environ.get("SPOTTER_USERNAME")
     password = args.password or os.environ.get("SPOTTER_PASSWORD")
     debug = args.debug
 
     config_path: Path = args.config_path
     if not config_path.exists():
-        print(f"Error: Path at {config_path} does not exist.", file=sys.stderr)
+        print(f"Error: path at {config_path} does not exist.", file=sys.stderr)
         sys.exit(2)
     if not config_path.is_file():
-        print(f"Error: Path at {config_path} is not a valid file.", file=sys.stderr)
+        print(f"Error: path at {config_path} is not a valid file.", file=sys.stderr)
         sys.exit(2)
 
     set_config(
-        api_endpoint, storage_path, api_token, username, password, args.organization_id, config_path,
-        debug=debug
+        api_endpoint, storage_path, api_token, username, password, args.organization_id, config_path, debug=debug
     )
 
 
 def _debug_print_project_and_org(api_client: ApiClient, organization_id: Optional[str]) -> None:
     if organization_id is not None:
         api_client.debug_print("Setting configuration for organization id {}", organization_id)
         api_client.debug_organization(organization_id)
     else:
         api_client.debug_print("Setting configuration for default organization")
         api_client.debug_my_default_organization()
 
 
 # pylint: disable=too-many-arguments,too-many-locals,too-many-branches
-def set_config(api_endpoint: Optional[str], storage_path: Path, api_token: Optional[str], username: Optional[str],
-               password: Optional[str], organization_id: Optional[str],
-               config_path: Path, debug: bool = False) -> None:
+def set_config(
+    api_endpoint: Optional[str],
+    storage_path: Path,
+    api_token: Optional[str],
+    username: Optional[str],
+    password: Optional[str],
+    organization_id: Optional[str],
+    config_path: Path,
+    debug: bool = False,
+) -> None:
     """
     Set configuration file for organization.
 
     By default, this will set configuration for the default organization.
 
     :param api_endpoint: Steampunk Spotter API endpoint
     :param storage_path: Path to storage
@@ -102,21 +108,21 @@
         endpoint = api_endpoint
 
     spotter_noqa = []
     environment = Environment.from_config_file(config_path)
     if environment.cli_scan_args:
         skip_checks = environment.cli_scan_args.get("skip_checks", [])
         for skip_check in skip_checks:
-            skip_check_dict = pydantic_encoder(skip_check)
+            skip_check_dict = compat_to_jsonable_python(skip_check)
             skip_check_dict["type"] = "skip"
             spotter_noqa.append(skip_check_dict)
 
         enforce_checks = environment.cli_scan_args.get("enforce_checks", [])
         for enforce_check in enforce_checks:
-            enforce_check_dict = pydantic_encoder(enforce_check)
+            enforce_check_dict = compat_to_jsonable_python(enforce_check)
             enforce_check_dict["type"] = "enforce"
             spotter_noqa.append(enforce_check_dict)
 
     api_client = ApiClient(endpoint, storage, api_token, username, password, debug=debug)
     api_client.debug_print_me()
     _debug_print_project_and_org(api_client, organization_id)
     if organization_id:
```

## Comparing `spotter/commands/set_policies.py` & `spotter/client/commands/set_policies.py`

 * *Files 8% similar despite different names*

```diff
@@ -2,41 +2,46 @@
 
 import argparse
 import os
 import sys
 from pathlib import Path
 from typing import List, Dict, Any, Optional
 
-from spotter.api import ApiClient
-from spotter.storage import Storage
+from spotter.library.api import ApiClient
+from spotter.library.storage import Storage
 
 
 def add_parser(subparsers: "argparse._SubParsersAction[argparse.ArgumentParser]") -> None:
     """
     Add a new parser for set-policies command to subparsers.
 
     :param subparsers: Subparsers action
     """
     parser = subparsers.add_parser(
-        "set-policies", help="Set custom policies (enterprise feature)",
-        description="Set OPA policies (written in Rego Language) for custom Spotter checks (enterprise feature)"
+        "set-policies",
+        help="Set custom policies (enterprise feature)",
+        description="Set OPA policies (written in Rego Language) for custom Spotter checks (enterprise feature)",
     )
     project_organization_group = parser.add_mutually_exclusive_group()
     project_organization_group.add_argument(
-        "--project-id", "-p", type=str,
+        "--project-id",
+        "-p",
+        type=str,
         help="UUID of an existing Steampunk Spotter project to set custom policies for "
-             "(default project from the default organization will be used if not specified)"
+        "(default project from the default organization will be used if not specified)",
     )
     project_organization_group.add_argument(
-        "--organization-id", type=str, help="UUID of an existing Steampunk Spotter organization to set custom "
-                                            "policies for"
+        "--organization-id",
+        type=str,
+        help="UUID of an existing Steampunk Spotter organization to set custom " "policies for",
     )
     parser.add_argument(
-        "path", type=lambda p: Path(p).absolute(),
-        help="Path to the file or folder with custom OPA policies (written in Rego Language)"
+        "path",
+        type=lambda p: Path(p).absolute(),
+        help="Path to the file or folder with custom OPA policies (written in Rego Language)",
     )
     parser.set_defaults(func=_parser_callback)
 
 
 def _parser_callback(args: argparse.Namespace) -> None:
     """
     Execute callback for include-policies command.
@@ -48,43 +53,59 @@
     api_token = args.api_token or os.environ.get("SPOTTER_API_TOKEN")
     username = args.username or os.environ.get("SPOTTER_USERNAME")
     password = args.password or os.environ.get("SPOTTER_PASSWORD")
     debug = args.debug
 
     path: Path = args.path
     if not path.exists():
-        print(f"Error: Path at {path} provided for scanning does not exist.", file=sys.stderr)
+        print(f"Error: path at {path} provided for scanning does not exist.", file=sys.stderr)
         sys.exit(2)
     if not path.is_file() and not path.is_dir():
-        print(f"Error: Path at {path} is not a file or directory.", file=sys.stderr)
+        print(f"Error: path at {path} is not a file or directory.", file=sys.stderr)
         sys.exit(2)
 
     include_policies(
-        api_endpoint, storage_path, api_token, username, password, args.project_id, args.organization_id, path,
-        debug=debug
+        api_endpoint,
+        storage_path,
+        api_token,
+        username,
+        password,
+        args.project_id,
+        args.organization_id,
+        path,
+        debug=debug,
     )
 
 
-def _debug_print_project_and_org(api_client: ApiClient, project_id: Optional[str],
-                                 organization_id: Optional[str]) -> None:
+def _debug_print_project_and_org(
+    api_client: ApiClient, project_id: Optional[str], organization_id: Optional[str]
+) -> None:
     if project_id is not None:
         api_client.debug_print("Setting polices for project id {}", project_id)
         api_client.debug_project(project_id)
     elif organization_id is not None:
         api_client.debug_print("Setting polices for organization id {}", organization_id)
         api_client.debug_organization(organization_id)
     else:
         api_client.debug_print("Setting polices for default organization")
         api_client.debug_my_default_organization()
 
 
 # pylint: disable=too-many-arguments,too-many-locals,too-many-branches
-def include_policies(api_endpoint: Optional[str], storage_path: Path, api_token: Optional[str], username: Optional[str],
-                     password: Optional[str], project_id: Optional[str], organization_id: Optional[str],
-                     path: Path, debug: bool = False) -> None:
+def include_policies(
+    api_endpoint: Optional[str],
+    storage_path: Path,
+    api_token: Optional[str],
+    username: Optional[str],
+    password: Optional[str],
+    project_id: Optional[str],
+    organization_id: Optional[str],
+    path: Path,
+    debug: bool = False,
+) -> None:
     """
     Set custom OPA policies.
 
     By default, this will set policies for the default project from default organization.
 
     :param api_endpoint: Steampunk Spotter API endpoint
     :param storage_path: Path to storage
@@ -111,52 +132,53 @@
     policies: List[Dict[str, Any]] = []
     if path.is_file():
         item = {
             "policy_name": path.name,
             "policy_rego": path.read_text(),
             "severity": "",
             "description": "",
-            "type": "CUSTOM"
+            "type": "CUSTOM",
         }
         policies.append(item)
     else:
         for task_file in path.rglob("*.rego"):
             item = {
                 "policy_name": task_file.name,
                 "policy_rego": task_file.read_text(),
                 "severity": "",
                 "description": "",
-                "type": "CUSTOM"
+                "type": "CUSTOM",
             }
             policies.append(item)
 
-    payload = {
-        "policies": policies,
-        "project_id": project_id,
-        "organization_id": organization_id
-    }
+    payload = {"policies": policies, "project_id": project_id, "organization_id": organization_id}
 
     api_client = ApiClient(endpoint, storage, api_token, username, password, debug=debug)
     api_client.debug_print_me()
     _debug_print_project_and_org(api_client, project_id, organization_id)
     response = api_client.put("/v2/opa/", payload=payload, ignore_response_status_codes=True)
     if response.status_code == 402:
-        print("The use of custom policies is only available in Spotter's ENTERPRISE plan. "
-              "Please upgrade your plan to use this functionality.")
+        print(
+            "Error: the use of custom policies is only available in Spotter's ENTERPRISE plan. "
+            "Please upgrade your plan to use this functionality."
+        )
         sys.exit(2)
     if response.status_code == 403 and project_id:
-        print("The user is not a member of the organization that includes the project with the given ID.")
+        print(
+            "Error: the user is not a member of the organization that includes the project with the given ID.",
+            file=sys.stderr,
+        )
         sys.exit(2)
     if response.status_code == 403 and organization_id:
-        print("The user is not a member of the organization with the given ID.")
+        print("Error: the user is not a member of the organization with the given ID.", file=sys.stderr)
         sys.exit(2)
     if response.status_code == 404 and project_id:
-        print("The project with the given ID was not found.")
+        print("Error: the project with the given ID was not found.", file=sys.stderr)
         sys.exit(2)
     if response.status_code == 404 and organization_id:
-        print("The organization with the given ID was not found.")
+        print("Error: the organization with the given ID was not found.", file=sys.stderr)
         sys.exit(2)
     if not response.ok:
         print(api_client.format_api_error(response), file=sys.stderr)
         sys.exit(2)
 
     print("Custom policies successfully set.")
```

## Comparing `spotter/commands/suggest.py` & `spotter/client/commands/suggest.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,30 +4,35 @@
 import json
 import os
 import sys
 import urllib.parse
 from pathlib import Path
 from typing import List, Dict, Any, Optional
 
-from spotter.api import ApiClient
-from spotter.storage import Storage
+from spotter.library.api import ApiClient
+from spotter.library.storage import Storage
 
 
 def add_parser(subparsers: "argparse._SubParsersAction[argparse.ArgumentParser]") -> None:
     """
     Add a new parser for suggest command to subparsers.
 
     :param subparsers: Subparsers action
     """
     parser = subparsers.add_parser(
         "suggest", argument_default=argparse.SUPPRESS, description="Get suggestions from Spotter's AI component"
     )
     parser.add_argument(
-        "--num-results", "-n", type=int, default=5, choices=range(1, 51), metavar="[1, 50]",
-        help="Number of expected suggestions"
+        "--num-results",
+        "-n",
+        type=int,
+        default=5,
+        choices=range(1, 51),
+        metavar="[1, 50]",
+        help="Number of expected suggestions",
     )
     parser.add_argument(
         "query", type=str, help="Query that will be used to produce a suggestion from Spotter's AI component"
     )
     parser.set_defaults(func=_parser_callback)
 
 
@@ -49,16 +54,23 @@
         print(json.dumps(suggestions, indent=2))
     except TypeError as e:
         print(f"Error: unable to serialize the object to JSON: {str(e)}", file=sys.stderr)
         sys.exit(2)
 
 
 # pylint: disable=too-many-locals
-def suggest(api_endpoint: Optional[str], storage_path: Path, api_token: Optional[str], username: Optional[str],
-            password: Optional[str], query: str, num_results: int) -> List[Dict[str, Any]]:
+def suggest(
+    api_endpoint: Optional[str],
+    storage_path: Path,
+    api_token: Optional[str],
+    username: Optional[str],
+    password: Optional[str],
+    query: str,
+    num_results: int,
+) -> List[Dict[str, Any]]:
     """
     Suggest module and task examples by calling Spotter's AI component.
 
     :param api_endpoint: Steampunk Spotter API endpoint
     :param storage_path: Path to storage
     :param api_token: Steampunk Spotter API token
     :param username: Steampunk Spotter username
```

## Comparing `spotter/parsing/noqa_comments.py` & `spotter/library/parsing/noqa_comments.py`

 * *Files 4% similar despite different names*

```diff
@@ -50,25 +50,27 @@
         Parse noqa comment and construct SpotterNoqa objects.
 
         :param comment: Comment
         :param use_noqa_regex: Match against noqa regex
         :return: List of SpotterNoqa objects
         """
         noqa_regex = r"#\s*noqa:(.*)"
-        comment_parts_regex = r"\s*([A-Z]\d+)\s*(\[[^\]]+\])?"
+        comment_parts_regex = r"\s*([EWH]\d+)\s*(\[[^]]+])?"
 
         matches = [comment]
         if use_noqa_regex:
             matches = re.findall(noqa_regex, comment)
             if not matches:
                 return []
 
         comment_parts = re.findall(comment_parts_regex, matches[0])
-        return [cls(event=part[0], subevent_code=cls._parse_subevent_code(part[1]), fqcn=cls._parse_fqcn(part[1])) for
-                part in comment_parts]
+        return [
+            cls(event=part[0], subevent_code=cls._parse_subevent_code(part[1]), fqcn=cls._parse_fqcn(part[1]))
+            for part in comment_parts
+        ]
 
 
 def _construct_noqa_from_comment_list(comment_list: List[Any]) -> List[SpotterNoqa]:
     """
     Construct SpotterNoqa objects from list of comments.
 
     :param comment_list: List of YAML comments
@@ -85,15 +87,15 @@
                 if isinstance(comment_token_item, yaml.CommentToken):
                     noqas += SpotterNoqa.parse_noqa_comment(comment_token_item.value)
 
     return noqas
 
 
 def _construct_noqa_from_commented_item(
-        commented_item: Union[yaml.CommentedSeq, yaml.CommentedMap]
+    commented_item: Union[yaml.CommentedSeq, yaml.CommentedMap]
 ) -> List[SpotterNoqa]:
     """
     Construct SpotterNoqa objects from commented sequence or map.
 
     :param commented_item: CommentedSeq or CommentedMap YAML object
     :return: List of SpotterNoqa objects
     """
@@ -140,8 +142,8 @@
 
     :param commented_map: CommentedMap YAML object
     """
     try:
         noqas = _construct_noqa_from_commented_item(commented_map) + _construct_noqa_from_dict_recursive(commented_map)
         commented_map["__noqa__"] = noqas
     except Exception as e:  # pylint: disable=broad-except
-        print(f"Error while mapping YAML comments to tasks: {e}", file=sys.stderr)
+        print(f"Error: mapping YAML comments to tasks failed: {e}", file=sys.stderr)
```

## Comparing `spotter/parsing/parsing.py` & `spotter/library/parsing/parsing.py`

 * *Files 7% similar despite different names*

```diff
@@ -5,17 +5,17 @@
 from pathlib import Path
 from typing import Dict, List, Tuple, Any, cast, Optional, Union, Callable
 
 import pydantic.dataclasses
 import ruamel.yaml as yaml
 from detect_secrets.core.secrets_collection import SecretsCollection
 from detect_secrets.settings import default_settings
-from pydantic.json import pydantic_encoder
 
-from spotter.parsing.noqa_comments import match_comments_with_task
+from spotter.library.compat.pydantic import compat_to_jsonable_python
+from spotter.library.parsing.noqa_comments import match_comments_with_task
 
 
 @pydantic.dataclasses.dataclass
 class SpotterObfuscated:
     """Class where we save metadata about which fields were obfuscated."""
 
     type: str
@@ -48,46 +48,42 @@
     def tasks_without_metadata(self) -> List[Dict[str, Any]]:
         """
         Remove sensitive data from input tasks.
 
         :return: Cleaned list of input tasks
         """
         return [
-            {
-                "task_id": t["task_id"],
-                "task_args": t["task_args"],
-                "spotter_noqa": t["spotter_noqa"]
-            } for t in self.tasks
+            {"task_id": t["task_id"], "task_args": t["task_args"], "spotter_noqa": t["spotter_noqa"]}
+            for t in self.tasks
         ]
 
     def playbooks_without_metadata(self) -> List[Dict[str, Union[str, List[Dict[str, Any]]]]]:
         """
         Remove sensitive data from input playbooks.
 
         :return: Cleaned list of input playbooks
         """
         return [
             {
                 "playbook_id": t["playbook_id"],
-                "plays": [
-                    {
-                        "play_id": x.get("play_id", None),
-                        "play_args": x["play_args"]
-                    }
-                    for x in t["plays"]
-                ]
-            } for t in self.playbooks
+                "plays": [{"play_id": x.get("play_id", None), "play_args": x["play_args"]} for x in t["plays"]],
+            }
+            for t in self.playbooks
         ]
 
 
 class SafeLineLoader(yaml.RoundTripLoader):  # type: ignore
     """YAML loader that adds line numbers."""
 
-    def __init__(self, stream: yaml.StreamTextType, version: Optional[yaml.VersionType] = None,
-                 preserve_quotes: Optional[bool] = None) -> None:
+    def __init__(
+        self,
+        stream: yaml.StreamTextType,
+        version: Optional[yaml.VersionType] = None,
+        preserve_quotes: Optional[bool] = None,
+    ) -> None:
         """
         Initialize the YAML loader.
 
         :param stream: YAML stream
         """
         super().__init__(stream, version, preserve_quotes)
         # add constructors for !vault and !unsafe tags, throw away their values because they are sensitive
@@ -130,39 +126,160 @@
     """
     Enum that stores significant keywords for playbooks that help us automatically discover Ansible file types.
 
     Keywords were gathered from: https://docs.ansible.com/ansible/latest/reference_appendices/playbooks_keywords.html.
     """
 
     PLAY = {
-        "any_errors_fatal", "become", "become_exe", "become_flags", "become_method", "become_user", "check_mode",
-        "collections", "connection", "debugger", "diff", "environment", "fact_path", "force_handlers", "gather_facts",
-        "gather_subset", "gather_timeout", "handlers", "hosts", "ignore_errors", "ignore_unreachable",
-        "max_fail_percentage", "module_defaults", "name", "no_log", "order", "port", "post_tasks", "pre_tasks",
-        "remote_user", "roles", "run_once", "serial", "strategy", "tags", "tasks", "throttle", "timeout", "vars",
-        "vars_files", "vars_prompt"
+        "any_errors_fatal",
+        "become",
+        "become_exe",
+        "become_flags",
+        "become_method",
+        "become_user",
+        "check_mode",
+        "collections",
+        "connection",
+        "debugger",
+        "diff",
+        "environment",
+        "fact_path",
+        "force_handlers",
+        "gather_facts",
+        "gather_subset",
+        "gather_timeout",
+        "handlers",
+        "hosts",
+        "ignore_errors",
+        "ignore_unreachable",
+        "max_fail_percentage",
+        "module_defaults",
+        "name",
+        "no_log",
+        "order",
+        "port",
+        "post_tasks",
+        "pre_tasks",
+        "remote_user",
+        "roles",
+        "run_once",
+        "serial",
+        "strategy",
+        "tags",
+        "tasks",
+        "throttle",
+        "timeout",
+        "vars",
+        "vars_files",
+        "vars_prompt",
     }
     ROLE = {
-        "any_errors_fatal", "become", "become_exe", "become_flags", "become_method", "become_user", "check_mode",
-        "collections", "connection", "debugger", "delegate_facts", "delegate_to", "diff", "environment",
-        "ignore_errors", "ignore_unreachable", "module_defaults", "name", "no_log", "port", "remote_user", "run_once",
-        "tags", "throttle", "timeout", "vars", "when"
+        "any_errors_fatal",
+        "become",
+        "become_exe",
+        "become_flags",
+        "become_method",
+        "become_user",
+        "check_mode",
+        "collections",
+        "connection",
+        "debugger",
+        "delegate_facts",
+        "delegate_to",
+        "diff",
+        "environment",
+        "ignore_errors",
+        "ignore_unreachable",
+        "module_defaults",
+        "name",
+        "no_log",
+        "port",
+        "remote_user",
+        "run_once",
+        "tags",
+        "throttle",
+        "timeout",
+        "vars",
+        "when",
     }
     BLOCK = {
-        "always", "any_errors_fatal", "become", "become_exe", "become_flags", "become_method", "become_user", "block",
-        "check_mode", "collections", "connection", "debugger", "delegate_facts", "delegate_to", "diff", "environment",
-        "ignore_errors", "ignore_unreachable", "module_defaults", "name", "no_log", "notify", "port", "remote_user",
-        "rescue", "run_once", "tags", "throttle", "timeout", "vars", "when"
+        "always",
+        "any_errors_fatal",
+        "become",
+        "become_exe",
+        "become_flags",
+        "become_method",
+        "become_user",
+        "block",
+        "check_mode",
+        "collections",
+        "connection",
+        "debugger",
+        "delegate_facts",
+        "delegate_to",
+        "diff",
+        "environment",
+        "ignore_errors",
+        "ignore_unreachable",
+        "module_defaults",
+        "name",
+        "no_log",
+        "notify",
+        "port",
+        "remote_user",
+        "rescue",
+        "run_once",
+        "tags",
+        "throttle",
+        "timeout",
+        "vars",
+        "when",
     }
     TASK = {
-        "action", "any_errors_fatal", "args", "async", "become", "become_exe", "become_flags", "become_method",
-        "become_user", "changed_when", "check_mode", "collections", "connection", "debugger", "delay", "delegate_facts",
-        "delegate_to", "diff", "environment", "failed_when", "ignore_errors", "ignore_unreachable", "local_action",
-        "loop", "loop_control", "module_defaults", "name", "no_log", "notify", "poll", "port", "register",
-        "remote_user", "retries", "run_once", "tags", "throttle", "timeout", "until", "vars", "when"
+        "action",
+        "any_errors_fatal",
+        "args",
+        "async",
+        "become",
+        "become_exe",
+        "become_flags",
+        "become_method",
+        "become_user",
+        "changed_when",
+        "check_mode",
+        "collections",
+        "connection",
+        "debugger",
+        "delay",
+        "delegate_facts",
+        "delegate_to",
+        "diff",
+        "environment",
+        "failed_when",
+        "ignore_errors",
+        "ignore_unreachable",
+        "local_action",
+        "loop",
+        "loop_control",
+        "module_defaults",
+        "name",
+        "no_log",
+        "notify",
+        "poll",
+        "port",
+        "register",
+        "remote_user",
+        "retries",
+        "run_once",
+        "tags",
+        "throttle",
+        "timeout",
+        "until",
+        "vars",
+        "when",
     }
 
 
 def _load_yaml_file(path: Path) -> Any:
     """
     Load YAML file and return corresponding Python object if parsing has been successful.
 
@@ -185,15 +302,16 @@
     Check if file is a playbook = a YAML file containing one or more plays in a list.
 
     :param loaded_yaml: Parsed YAML file as a Python object
     :return: True or False
     """
     # use only keywords that are unique for play and do not intersect with other keywords
     playbook_keywords = _PlaybookKeywords.PLAY.difference(
-        _PlaybookKeywords.TASK.union(_PlaybookKeywords.BLOCK).union(_PlaybookKeywords.ROLE))
+        _PlaybookKeywords.TASK.union(_PlaybookKeywords.BLOCK).union(_PlaybookKeywords.ROLE)
+    )
 
     if isinstance(loaded_yaml, list):
         if any(len(playbook_keywords.intersection(e.keys())) > 0 for e in loaded_yaml if isinstance(e, dict)):
             return True
 
     return False
 
@@ -367,16 +485,14 @@
     :return: List of parsed Ansible tasks
     """
     try:
         parsed_tasks = []
         secrets = detect_secrets_in_file(file_name)
 
         for task in [t for t in tasks if t is not None]:
-            # _clean_action_and_local_action(task, parse_values)
-
             contains_block_section = False
             for block_section in ("block", "rescue", "always"):
                 if block_section in task:
                     contains_block_section = True
                     if isinstance(task[block_section], list):
                         parsed_tasks += _parse_tasks(task[block_section], file_name, parse_values)
             if contains_block_section:
@@ -398,29 +514,29 @@
                     obfuscated.extend(item.to_parent(task_key) for item in hidden)
 
             meta = {
                 "file": file_name,
                 "line": task_meta["__line__"],
                 "column": task_meta["__column__"],
                 "start_mark_index": task_meta["__start_mark_index__"],
-                "end_mark_index": task_meta["__end_mark_index__"]
+                "end_mark_index": task_meta["__end_mark_index__"],
             }
 
             task_dict = {
                 "task_id": str(uuid.uuid4()),
                 "task_args": _remove_deep_metadata(task_copy),
                 "spotter_metadata": meta,
-                "spotter_obfuscated": [pydantic_encoder(x) for x in obfuscated],
-                "spotter_noqa": [pydantic_encoder(x) for x in task_noqa]
+                "spotter_obfuscated": [compat_to_jsonable_python(x) for x in obfuscated],
+                "spotter_noqa": [compat_to_jsonable_python(x) for x in task_noqa],
             }
             parsed_tasks.append(task_dict)
 
         return parsed_tasks
     except Exception as e:  # pylint: disable=broad-except
-        print(f"Error while parsing tasks from {file_name}: {e}", file=sys.stderr)
+        print(f"Error: parsing tasks from {file_name} failed: {e}", file=sys.stderr)
         return []
 
 
 def _parse_play(play: Dict[str, Any], file_name: str, parse_values: bool = False) -> Dict[str, Any]:
     """
     Parse Ansible play and prepare it for scanning.
 
@@ -442,32 +558,33 @@
                 obfuscated.extend(item.to_parent(play_key) for item in hidden)
 
         meta = {
             "file": file_name,
             "line": play_meta["__line__"],
             "column": play_meta["__column__"],
             "start_mark_index": play_meta["__start_mark_index__"],
-            "end_mark_index": play_meta["__end_mark_index__"]
+            "end_mark_index": play_meta["__end_mark_index__"],
         }
 
         play_dict = {
             "play_id": str(uuid.uuid4()),
             "play_args": _remove_deep_metadata(play),
             "spotter_metadata": meta,
-            "spotter_obfuscated": [pydantic_encoder(x) for x in obfuscated]
+            "spotter_obfuscated": [compat_to_jsonable_python(x) for x in obfuscated],
         }
 
         return play_dict
     except Exception as e:  # pylint: disable=broad-except
-        print(f"Error while parsing play from {file_name}: {e}", file=sys.stderr)
+        print(f"Error: parsing play from {file_name} failed: {e}", file=sys.stderr)
         return {}
 
 
-def _parse_playbook(playbook: List[Dict[str, Any]], file_name: str, parse_values: bool = False) -> \
-        Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
+def _parse_playbook(
+    playbook: List[Dict[str, Any]], file_name: str, parse_values: bool = False
+) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
     """
     Parse Ansible playbook and prepare it for scanning.
 
     :param playbook: Ansible playbook as dict
     :param file_name: Name of the original file with playbook
     :param parse_values: True if also read values (apart from parameter names) from task parameters, False if not
     :return: Tuple containing list of parsed Ansible tasks and parsed playbook as dict
@@ -502,15 +619,15 @@
     Parse Ansible role.
 
     :param directory: Role directory
     :param parse_values: True if also read values (apart from parameter names) from task parameters, False if not
     :return: Tuple containing list of parsed Ansible tasks and parsed playbook as dict
     """
     parsed_role_tasks = []
-    for task_file in (list((directory / "tasks").rglob("*")) + list((directory / "handlers").rglob("*"))):
+    for task_file in sorted(list((directory / "tasks").rglob("*")) + list((directory / "handlers").rglob("*"))):
         if task_file.is_file() and task_file.suffix in YAML_SUFFIXES:
             loaded_yaml = _load_yaml_file(task_file)
             if isinstance(loaded_yaml, list):
                 parsed_role_tasks += _parse_tasks(loaded_yaml, str(task_file), parse_values)
     return parsed_role_tasks, []
 
 
@@ -520,43 +637,44 @@
 
     :param directory: Collection directory
     :param parse_values: True if also read values (apart from parameter names) from task parameters, False if not
     :return: Tuple containing list of parsed Ansible tasks and parsed playbook as dict
     """
     parsed_collection_tasks = []
     parsed_collection_playbooks = []
-    for role in (list((directory / "roles").rglob("*"))):
+    for role in sorted(list((directory / "roles").rglob("*"))):
         if role.is_dir():
             parsed_tasks, _ = _parse_role(role, parse_values)
             parsed_collection_tasks += parsed_tasks
-    for playbook in (list((directory / "playbooks").rglob("*"))):
+    for playbook in sorted(list((directory / "playbooks").rglob("*"))):
         if playbook.is_file() and playbook.suffix in YAML_SUFFIXES:
             loaded_yaml = _load_yaml_file(playbook)
             if _is_playbook(loaded_yaml):
                 parsed_tasks, parsed_playbooks = _parse_playbook(loaded_yaml, str(playbook), parse_values)
                 parsed_collection_tasks += parsed_tasks
                 parsed_collection_playbooks += parsed_playbooks
-    for role in (list((directory / "tests" / "integration" / "targets").glob("*"))):
+    for role in sorted(list((directory / "tests" / "integration" / "targets").glob("*"))):
         parsed_tasks, parsed_playbooks = _parse_role(role, parse_values)
         parsed_collection_tasks += parsed_tasks
         parsed_collection_playbooks += parsed_playbooks
-    for path in (list(directory.glob("*.yml")) + list(directory.glob("*.yaml"))):
+    for path in sorted(list(directory.glob("*.yml")) + list(directory.glob("*.yaml"))):
         if path.is_file() and path.suffix in YAML_SUFFIXES:
             loaded_yaml = _load_yaml_file(path)
             if _is_playbook(loaded_yaml):
                 parsed_tasks, parsed_playbooks = _parse_playbook(loaded_yaml, str(path), parse_values)
                 parsed_collection_tasks += parsed_tasks
                 parsed_collection_playbooks += parsed_playbooks
             elif isinstance(loaded_yaml, list):
                 parsed_collection_tasks += _parse_tasks(loaded_yaml, str(path), parse_values)
     return parsed_collection_tasks, parsed_collection_playbooks
 
 
-def parse_unknown_ansible_artifact(path: Path, parse_values: bool = False) -> \
-        Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
+def parse_unknown_ansible_artifact(
+    path: Path, parse_values: bool = False
+) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
     """
     Parse Ansible artifact (unknown by type) by applying automatic Ansible file type detection.
 
     We are able to can discover task files, playbooks, roles and collections at any level recursively.
 
     :param path: Path to file or directory
     :param parse_values: True if also read values (apart from parameter names) from task parameters, False if not
@@ -579,15 +697,15 @@
             parsed_ansible_artifacts_tasks += parsed_tasks
             parsed_ansible_artifacts_playbooks += parsed_playbooks
         elif _is_role(path):
             parsed_tasks, parsed_playbooks = _parse_role(path, parse_values)
             parsed_ansible_artifacts_tasks += parsed_tasks
             parsed_ansible_artifacts_playbooks += parsed_playbooks
         else:
-            for sub_path in path.iterdir():
+            for sub_path in sorted(path.iterdir()):
                 parsed_tasks, parsed_playbooks = parse_unknown_ansible_artifact(sub_path, parse_values)
                 parsed_ansible_artifacts_tasks += parsed_tasks
                 parsed_ansible_artifacts_playbooks += parsed_playbooks
 
     return parsed_ansible_artifacts_tasks, parsed_ansible_artifacts_playbooks
```

## Comparing `spotter/reporting/report.py` & `spotter/library/reporting/report.py`

 * *Files 26% similar despite different names*

```diff
@@ -4,79 +4,106 @@
 import sys
 import xml.etree.ElementTree as ET
 from abc import ABC, abstractmethod
 from typing import List, TYPE_CHECKING
 
 if TYPE_CHECKING:
     # cylic import
-    from spotter.commands.scan import CheckResult
+    from spotter.library.scanning.check_result import CheckResult
 
 
 class ReportingInterface(ABC):
     """Interface for generating modular reports in various formats."""
 
     @abstractmethod
     def render(self, check_results: List["CheckResult"], disable_docs_url: bool) -> str:
-        """Render the report based on the provided args as a string in the appropriate format."""
+        """
+        Render the report based on the provided args as a string in the appropriate format.
+
+        :param check_results: Root node
+        :param disable_docs_url: Disable outputting URL to documentation
+        :return: Report
+        """
 
 
 class JUnitXml(ReportingInterface):
     """Generate JUnit XML file."""
 
     def add_root_node(self) -> ET.Element:
         """Add root node to the XML report."""
         root = ET.Element("testsuites")
         return root
 
     def add_test_suite(self, root_node: ET.Element, name: str) -> ET.Element:
-        """Add test suite to the XML report."""
+        """
+        Add test suite to the XML report.
+
+        :param root_node: Root node
+        :param name: Test suite name
+        :return: ET.Element object
+        """
         test_suite = ET.SubElement(root_node, "testsuite", name=name)
         return test_suite
 
     def add_test_case(self, test_suite: ET.Element, name: str, classname: str) -> ET.Element:
-        """Add test case to the XML report."""
+        """
+        Add test case to the XML report.
+
+        :param test_suite: Test suite
+        :param name: Test case name
+        :param classname: Test case classname
+        :return: ET.Element object
+        """
         test_case = ET.SubElement(test_suite, "testcase", name=name, classname=classname)
         return test_case
 
     def add_failure_info(self, test_case: ET.Element, message: str, typ: str) -> ET.Element:
-        """Add failure info to the XML report."""
+        """
+        Add failure info to the XML report.
+
+        :param test_case: Test case
+        :param message: Error message
+        :param typ: Error type
+        :return: ET.Element object
+        """
         error_case = ET.SubElement(test_case, "error", message=message, type=typ)
         return error_case
 
     def add_attribute(self, element: ET.Element, key: str, value: str) -> None:
-        """Add attribute to the XML report."""
+        """
+        Add attribute to the XML report.
+
+        :param element: ET.Element object
+        :param key: Attribute key
+        :param value: Attribute value
+        """
         element.set(key, value)
 
-    def render(self, check_results: List["CheckResult"], disable_docs_url: bool) -> str:
-        """Render the report."""
+    def render(self, check_results: List["CheckResult"], disable_docs_url: bool = False) -> str:
+        """
+        Render the report.
+
+        :param check_results: List of check results
+        :param disable_docs_url: Disable outputting URL to documentation
+        :return: Junit XML report
+        """
         root_node = self.add_root_node()
         get_check_class = lambda res: res.catalog_info.check_class  # pylint: disable=unnecessary-lambda-assignment
-        for c_class, c_results in itertools.groupby(
-                sorted(check_results, key=get_check_class),
-                get_check_class):
-
+        for c_class, c_results in itertools.groupby(sorted(check_results, key=get_check_class), get_check_class):
             test_suite = self.add_test_suite(root_node, c_class)
             check_count = 0
 
             for result in c_results:
                 test_case = self.add_test_case(
                     test_suite,
                     f"{result.catalog_info.event_code}-{result.catalog_info.event_value}[{check_count}]",
-                    c_class
-                )
-                self.add_attribute(
-                    test_case,
-                    "id",
-                    str(result.catalog_info.event_code)
-                )
-                self.add_attribute(
-                    test_case,
-                    "file",
-                    str(result.metadata.file_name if result.metadata else "")
+                    c_class,
                 )
+                self.add_attribute(test_case, "id", str(result.catalog_info.event_code))
+                self.add_attribute(test_case, "file", str(result.metadata.file_name if result.metadata else ""))
                 self.add_failure_info(
                     test_case,
                     result.construct_output(True, disable_docs_url),
                     result.level.name.upper(),
                 )
 
                 check_count += 1
```

## Comparing `spotter/rewriting/models.py` & `spotter/library/rewriting/models.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,20 +1,23 @@
 """Base class for whole inline rewriting."""
 
 from abc import abstractmethod, ABC
 from enum import Enum
-
+import itertools
 from pathlib import Path
 from re import Match
-from typing import Tuple, Optional, Dict, Any
+from typing import Iterator, List, Tuple, Optional, Dict, Any
 
-from colorama import Fore, Back, Style
+import os
 import pydantic.dataclasses
+from colorama import Fore, Back, Style
 from pydantic import BaseModel
 
+from spotter.library.scanning.display_level import DisplayLevel
+
 
 class RewriteResult(BaseModel):
     """Rewrite Result."""
 
     content: str
     diff_size: int
 
@@ -37,40 +40,47 @@
         return str(self.name.lower())
 
     @classmethod
     def from_string(cls, check_type: str) -> "CheckType":
         """
         Convert string level to CheckType object.
 
-        :param level: Check result level
+        :param check_type: Check type
         :return: CheckType object
         """
         try:
             return cls[check_type.upper()]
         except KeyError:
-            print(f"Warning: nonexistent check result type: {check_type}, "
-                  f"valid values are: {list(str(e) for e in CheckType)}.")
+            print(
+                f"Warning: nonexistent check result type: {check_type}, "
+                f"valid values are: {list(str(e) for e in CheckType)}."
+            )
             return CheckType.OTHER
 
 
 @pydantic.dataclasses.dataclass
 class RewriteSuggestion:
     """Suggestion for rewriting Ansible task or play."""
 
     check_type: CheckType
     item_args: Dict[str, Any]
     file: Path
     file_parent: Path
     start_mark: int
     end_mark: int
     suggestion_spec: Dict[str, Any]
+    display_level: DisplayLevel
 
     @classmethod
     def from_item(
-        cls, check_type: CheckType, item: Dict[str, Any], suggestion_spec: Optional[Dict[str, Any]]
+        cls,
+        check_type: CheckType,
+        item: Dict[str, Any],
+        suggestion_spec: Optional[Dict[str, Any]],
+        display_level: DisplayLevel,
     ) -> Optional["RewriteSuggestion"]:
         """Create Suggestion object for rewriting Ansible task or play."""
         if not suggestion_spec:
             return None
 
         if check_type == CheckType.TASK:
             item_args = item["task_args"]
@@ -84,33 +94,39 @@
         return cls(
             check_type=check_type,
             item_args=item_args,
             file=file_path,
             file_parent=file_path.parent,
             start_mark=item["spotter_metadata"]["start_mark_index"],
             end_mark=item["spotter_metadata"]["end_mark_index"],
-            suggestion_spec=suggestion_spec
+            suggestion_spec=suggestion_spec,
+            display_level=display_level,
         )
 
+    @property
+    def is_fix_requirements(self) -> bool:
+        """Is suggestion of type FIX_REQUIREMENTS."""
+        return self.suggestion_spec.get("action") == "FIX_REQUIREMENTS"
+
 
 class Replacement:
     """
     Replacement object that holds the entire context of replacement.
 
     Implemented as a separate object because, after matching, we still want to have multiple
     options of what to do with the match.
     One scenario is to show the diff first and only apply changes after user conformation.
     """
 
     def __init__(
-            self,
-            content: str,
-            suggestion: RewriteSuggestion,
-            match: Match,  # type: ignore[type-arg]  # type not generic in Python <=3.8
-            replacement: str
+        self,
+        content: str,
+        suggestion: RewriteSuggestion,
+        match: Match,  # type: ignore[type-arg]  # type not generic in Python <=3.8
+        replacement: str,
     ) -> None:
         """
         Construct Replacement object.
 
         :param content: Text to which we will apply rewritng
         :param suggestion: Suggestion object from which we calculated match
         :param match: Regex match, that was found inside content.
@@ -128,30 +144,30 @@
 
         :return: Rewrite result.
         """
         content = self.content
         suggestion = self.suggestion
 
         content_before = content[: suggestion.start_mark + self.s_index]
-        content_after = content[suggestion.start_mark + self.e_index:]
+        content_after = content[suggestion.start_mark + self.e_index :]
         end_content = content_before + self.after + content_after
 
         len_before = self.e_index - self.s_index
         return RewriteResult(content=end_content, diff_size=len(self.after) - len_before)
 
     def get_diff(self) -> Tuple[str, str]:
         """
         Calculate a string diff that may be shown to the user.
 
         :return: Tuple with content before and after.
         """
-        moved = self.content[self.suggestion.start_mark:]
-        bounding_before = moved[self.s_bounding_index:self.e_bounding_index]
+        moved = self.content[self.suggestion.start_mark :]
+        bounding_before = moved[self.s_bounding_index : self.e_bounding_index]
         bounding_after = (
-            moved[self.s_bounding_index:self.s_index] + self.after + moved[self.e_index:self.e_bounding_index]
+            moved[self.s_bounding_index : self.s_index] + self.after + moved[self.e_index : self.e_bounding_index]
         )
         return bounding_before, bounding_after
 
 
 class RewriteBase(ABC):
     """Base class with all common logic for inplace rewriting."""
 
@@ -159,32 +175,32 @@
         """
         Get a block of content that has all context that needs to be rewriten, usually a complete task.
 
         :param content: Old task content
         :param suggestion: Suggestion object for a specific task
         :return: Block of text that is relevant.
         """
-        part = content[suggestion.start_mark:suggestion.end_mark]
+        part = content[suggestion.start_mark : suggestion.end_mark]
         return part
 
     def get_indent_index(self, content: str, start_mark: int) -> int:
         """
         Get index of first character.
 
         :param content: content block (usually a whole task).
         :param start_mark: starting mark index of task in content
         """
         l_content = content[:start_mark]
         index = l_content.rfind("\n") + 1
         return start_mark - index
 
     def _color_print(self, content: str, suggestion: RewriteSuggestion) -> None:
-        before = content[:suggestion.start_mark]
-        item = content[suggestion.start_mark:suggestion.end_mark]
-        after = content[suggestion.end_mark:]
+        before = content[: suggestion.start_mark]
+        item = content[suggestion.start_mark : suggestion.end_mark]
+        after = content[suggestion.end_mark :]
         print(f"{before}{Fore.RED}{Back.GREEN}{item}{Style.RESET_ALL}{after}")
 
     def shorten_match(self, content: str, suggestion: RewriteSuggestion) -> Tuple[RewriteSuggestion, str]:
         """
         Shorted a match for all whitespaces.
 
         Missing part is to also skip all comments.
@@ -218,7 +234,45 @@
         The first match group will be used as a block of text with context, which will be
         shown to the user to inspect what will change.
         The second match group is the text that we will actually replace.
 
         :param text_before: Exact text that will be replaced with new value
         :return: A regex string that can be compiled into a regex.
         """
+
+
+class INodeSuggestion:
+    """
+    Helper class for removing duplicate suggestions.
+
+    In case of soft and hard links, we can get multiple results for one error. This class
+    helps to remove duplicates. It does it by mathing and grouping files by theirs inode values.
+    """
+
+    def __init__(self, file: Path, suggestions: Iterator[RewriteSuggestion]) -> None:
+        """
+        Construct INodeSuggestion object.
+
+        :param file: Path of file, that is listed inside suggestions
+        :param suggestions: Suggestions that should be applied
+        """
+        self.file = file
+        self.suggestions = list(suggestions)
+
+    @staticmethod
+    def _get_file(rewrite_suggestion: RewriteSuggestion) -> Path:
+        return rewrite_suggestion.file
+
+    @staticmethod
+    def _get_inode(suggestion: "INodeSuggestion") -> int:
+        return os.stat(suggestion.file).st_ino
+
+    @classmethod
+    def from_suggestions(cls, suggestions: List[RewriteSuggestion]) -> List["INodeSuggestion"]:
+        """
+        Construct list of INodeSuggestion objects.
+
+        :param suggestions: Suggestions that should be used
+        """
+        files = [INodeSuggestion(file, suggests) for file, suggests in itertools.groupby(suggestions, cls._get_file)]
+        inodes = [next(group) for _, group in itertools.groupby(sorted(files, key=cls._get_inode), cls._get_inode)]
+        return inodes
```

## Comparing `spotter/rewriting/processor.py` & `spotter/library/rewriting/processor.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,24 +1,21 @@
 """Entry point of rewriting functionality."""
-
-import itertools
-import os
 from typing import Optional, List, cast
 
-import ruamel.yaml as yaml
-
-from spotter.rewriting.models import Replacement, RewriteResult
-from spotter.rewriting.models import RewriteSuggestion
-from spotter.rewriting.rewrite_action_inline import RewriteActionInline
-from spotter.rewriting.rewrite_action_object import RewriteActionObject
-from spotter.rewriting.rewrite_always_run import RewriteAlwaysRun
-from spotter.rewriting.rewrite_fqcn import RewriteFqcn
-from spotter.rewriting.rewrite_inline import RewriteInline
-from spotter.rewriting.rewrite_local_action_inline import RewriteLocalActionInline
-from spotter.rewriting.rewrite_local_object import RewriteLocalActionObject
+from spotter.library.rewriting.models import INodeSuggestion, Replacement, RewriteResult
+from spotter.library.rewriting.models import RewriteSuggestion
+from spotter.library.rewriting.rewrite_action_inline import RewriteActionInline
+from spotter.library.rewriting.rewrite_action_object import RewriteActionObject
+from spotter.library.rewriting.rewrite_always_run import RewriteAlwaysRun
+from spotter.library.rewriting.rewrite_fqcn import RewriteFqcn
+from spotter.library.rewriting.rewrite_inline import RewriteInline
+from spotter.library.rewriting.rewrite_local_action_inline import RewriteLocalActionInline
+from spotter.library.rewriting.rewrite_local_object import RewriteLocalActionObject
+from spotter.library.rewriting.rewrite_requirements import update_requirements
+from spotter.library.scanning.display_level import DisplayLevel
 
 
 class RewriteProcessor:
     """Factory that will use correct implementation depending on 'action' inside 'suggestion'."""
 
     rewriter_mapping = {
         "FIX_FQCN": RewriteFqcn,
@@ -43,39 +40,46 @@
         replacement = cls.get_replacement(content, suggestion)
         if not replacement:
             return RewriteResult(content=content, diff_size=0)
 
         return replacement.apply()
 
     @classmethod
-    def multi_execute(cls, content: str, suggestions: List[RewriteSuggestion]) -> RewriteResult:
+    def multi_execute(
+        cls, content: str, suggestions: List[RewriteSuggestion], display_level: DisplayLevel
+    ) -> RewriteResult:
         """
         Update task content with multiple suggestions.
 
         :param content: Old task content
         :param suggestions: List of suggestions of specific tasks
         :return: List of tuples with updated content and content length difference, or none if matching failed
         """
         suggestion_start_position = -1
         previous_suggestion = None
+        cut_all = False
         length_diff = 0
         for suggestion in suggestions:
             len_before = len(content)
             if suggestion_start_position == suggestion.start_mark and previous_suggestion:
                 suggestion.end_mark = previous_suggestion.end_mark
+                if suggestion.display_level.value < display_level.value or cut_all:
+                    cut_all = True
+                    continue
+            cut_all = False
             suggestion_start_position = suggestion.start_mark
+            previous_suggestion = suggestion
 
             replacement = cls.get_replacement(content, suggestion)
             if replacement is None:
                 raise TypeError()
             rewrite_result = replacement.apply()
             new_content, _ = rewrite_result.content, rewrite_result.diff_size
             length_diff = len(new_content) - len_before
             suggestion.end_mark = suggestion.end_mark + length_diff
-            previous_suggestion = suggestion
             content = new_content
 
         return RewriteResult(content=content, diff_size=length_diff)
 
     @classmethod
     def get_replacement(cls, content: str, suggestion: RewriteSuggestion) -> Optional[Replacement]:
         """
@@ -93,81 +97,37 @@
             return None
 
         rewriter = rewriter_class()  # type: ignore[abstract]  # we assume the mapping only contains implementations
         replacement = rewriter.get_replacement(content, suggestion)
         return replacement
 
 
-def update_files(suggestions: List[RewriteSuggestion]) -> None:  # pylint: disable=too-many-locals
+# pylint: disable=too-many-locals
+def update_files(suggestions: List[RewriteSuggestion], display_level: DisplayLevel) -> None:
     """
     Update files by following suggestions.
 
     :param suggestions: List of suggestions as Suggestion objects
     """
-    get_file_func = lambda x: x.file  # pylint: disable=unnecessary-lambda-assignment
-    files = [(file, list(suggests)) for file, suggests in itertools.groupby(suggestions, get_file_func)]
-
-    get_inode_func = lambda x: os.stat(x[0]).st_ino  # pylint: disable=unnecessary-lambda-assignment
-    inodes = [next(group) for _, group in itertools.groupby(sorted(files, key=get_inode_func), get_inode_func)]
+    inodes = INodeSuggestion.from_suggestions(suggestions)
+    update_requirements(inodes, display_level)
 
-    requirements_update_suggestions = set()
-    for file, suggests in inodes:
+    for inode in inodes:
         # python sort is stable, so items with same start mark, should stay in same order
-        suggestions_reversed = sorted(suggests, key=lambda x: -x.start_mark)
-        suggestions_requirements = \
-            [x for x in suggestions_reversed if x.suggestion_spec.get("action") == "FIX_REQUIREMENTS"]
-        suggestions_items = \
-            [x for x in suggestions_reversed if x.suggestion_spec.get("action") != "FIX_REQUIREMENTS"]
+        suggestions_reversed = sorted(inode.suggestions, key=lambda x: -x.start_mark)
+        suggestions_items = [x for x in suggestions_reversed if not x.is_fix_requirements]
 
-        with file.open("r", encoding="utf-8") as f:
+        with inode.file.open("r", encoding="utf-8") as f:
             content = f.read()
 
         end_content = content
         try:
-            #  Requirements
-            for suggestion in suggestions_requirements:
-                suggestion_dict = suggestion.suggestion_spec
-                if suggestion_dict.get("action") == "FIX_REQUIREMENTS":
-                    collection_name = suggestion_dict["data"]["collection_name"]
-                    collection_version = suggestion_dict["data"]["version"]
-                    # TODO: Update path when we are able to get it from scan input or scan result
-                    requirements_yml_path = suggestion.file_parent / "requirements.yml"
-                    requirements_update_suggestions.add((requirements_yml_path, collection_name, collection_version))
-                    continue
-
-            # other
-            rewrite_result = RewriteProcessor.multi_execute(end_content, suggestions_items)
+            rewrite_result = RewriteProcessor.multi_execute(end_content, suggestions_items, display_level)
             if rewrite_result is None:
                 continue
             end_content = rewrite_result.content
         except Exception as e:  # pylint: disable=broad-except
-            print(f"Error when rewriting {file}: {e}")
+            print(f"Error: rewriting {inode.file} failed: {e}")
 
         if end_content != content:
-            with file.open("w", encoding="utf-8") as f:
+            with inode.file.open("w", encoding="utf-8") as f:
                 f.write(end_content)
-
-    # TODO: Consider updating this when we will be updating detection and rewriting of collection requirements
-    for (
-            requirements_yml_path,
-            collection_name,
-            collection_version,
-    ) in requirements_update_suggestions:
-        with requirements_yml_path.open("a+", encoding="utf-8") as requirements_file:
-            requirements_file.seek(0)
-            try:
-                data = yaml.safe_load(requirements_file)
-            except yaml.YAMLError:
-                # overwrite erroneous requirement file
-                data = None
-            if not data:
-                data = {}
-            if not isinstance(data, dict):
-                # should we overwrite in this case as well?
-                continue
-            if "collections" not in data or ("collections" in data and data["collections"] is None):
-                data["collections"] = []
-
-            data["collections"].append({"name": collection_name, "version": collection_version})
-            requirements_file.seek(0)
-            requirements_file.truncate()
-            requirements_file.write(yaml.round_trip_dump(data, default_flow_style=False))
```

## Comparing `spotter/rewriting/rewrite_action_inline.py` & `spotter/library/rewriting/rewrite_action_inline.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 """RewriteActionInline implementation."""
 
 import re
 from typing import Optional, Tuple
 
-from spotter.rewriting.rewrite_module_inline import RewriteModuleInline
-from spotter.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
+from spotter.library.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
+from spotter.library.rewriting.rewrite_module_inline import RewriteModuleInline
 
 
 class RewriteActionInline(RewriteBase):
     """RewriteActionInline implementation."""
 
     def get_regex(self, text_before: str) -> str:  # noqa: D102
         return rf"^(\s*({text_before}\s*):)"
@@ -19,15 +19,15 @@
 
         :param content: Content that we want to rewrite
         :param suggestion: Suggestion object
         """
         module_replacement = RewriteModuleInline().get_replacement(content, suggestion)
         if module_replacement is None:
             module_name = suggestion.suggestion_spec["data"]["module_name"]
-            print(f"Applying suggestion failed: could not find \"{module_name}\" to replace.")
+            print(f'Applying suggestion failed: could not find "{module_name}" to replace.')
             raise TypeError()
         rewrite_result = module_replacement.apply()
         suggestion.end_mark += rewrite_result.diff_size
         return rewrite_result.content, suggestion
 
     def get_replacement(self, content: str, suggestion: RewriteSuggestion) -> Optional[Replacement]:  # noqa: D102
         suggestion_data = suggestion.suggestion_spec["data"]
```

## Comparing `spotter/rewriting/rewrite_action_object.py` & `spotter/library/rewriting/rewrite_action_object.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 """RewriteLocalActionInline implementation."""
 
 import re
 from typing import Optional, Tuple
 
-import yaml
+import ruamel.yaml as yaml
 
-from spotter.rewriting.rewrite_module_object import RewriteModuleObject
-from spotter.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
+from spotter.library.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
+from spotter.library.rewriting.rewrite_module_object import RewriteModuleObject
 
 
 class RewriteActionObject(RewriteBase):
     """RewriteActionObject implementation."""
 
     def get_regex(self, text_before: str) -> str:  # noqa: D102
         return rf"^(\s*({text_before}\s*):)"
@@ -21,31 +21,31 @@
 
         :param content: Content that we want to rewrite
         :param suggestion: Suggestion object
         """
         module_replacement = RewriteModuleObject().get_replacement(content, suggestion)
         if module_replacement is None:
             module_name = suggestion.suggestion_spec["data"]["module_name"]
-            print(f"Applying suggestion failed: could not find \"{module_name}\" to replace.")
+            print(f'Applying suggestion failed: could not find "{module_name}" to replace.')
             raise TypeError()
         rewrite_result = module_replacement.apply()
         suggestion.end_mark += rewrite_result.diff_size
         return rewrite_result.content, suggestion
 
     def get_replacement(self, content: str, suggestion: RewriteSuggestion) -> Optional[Replacement]:  # noqa: D102
         # 1. Remove line "module: ..." from task arguments
         suggestion_data = suggestion.suggestion_spec["data"]
         content, suggestion = self.remove_module_row(content, suggestion)
         part = self.get_context(content, suggestion)
 
         # 2. Add "delegate_to": localhost
         if suggestion_data["additional"]:
             index = self.get_indent_index(content, suggestion.start_mark)
-            additional = " " * index + yaml.dump(suggestion_data["additional"][0])
-            new_content = content[:suggestion.end_mark] + additional + content[suggestion.end_mark]
+            additional = " " * index + yaml.round_trip_dump(suggestion_data["additional"][0])
+            new_content = content[: suggestion.end_mark] + additional + content[suggestion.end_mark]
         else:
             new_content = content
 
         # 3. Replace "action:" with "<module_name>:"
         before = suggestion_data["original_module_name"]
         after = suggestion_data["module_name"]
         regex = self.get_regex(before)
```

## Comparing `spotter/rewriting/rewrite_always_run.py` & `spotter/library/rewriting/rewrite_always_run.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """RewriteInline implementation."""
 
 import re
 from typing import Optional
 
-from spotter.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
+from spotter.library.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
 
 
 class RewriteAlwaysRun(RewriteBase):
     """RewriteAlwaysRun implementation."""
 
     def get_regex(self, text_before: str) -> str:  # noqa: D102
         return rf"^(\s*({text_before}\s*:.*))"
```

## Comparing `spotter/rewriting/rewrite_fqcn.py` & `spotter/library/rewriting/rewrite_fqcn.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 """RewriteFqcn implementation."""
 
-from typing import Optional
-
 import re
+from typing import Optional
 
-from spotter.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
+from spotter.library.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
 
 
 class RewriteFqcn(RewriteBase):
     """RewriteFqcn implementation."""
 
     def get_regex(self, text_before: str) -> str:  # noqa: D102
         return rf"^(\s*({text_before}\s*):)"
```

## Comparing `spotter/rewriting/rewrite_inline.py` & `spotter/library/rewriting/rewrite_inline.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """RewriteInline implementation."""
 
 import re
 from typing import Optional
 
 import ruamel.yaml as yaml
 
-from spotter.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
+from spotter.library.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
 
 
 class RewriteInline(RewriteBase):
     """RewriteInline implementation."""
 
     def get_regex(self, text_before: str) -> str:  # noqa: D102
         return rf"^(\s*{text_before}\s*:(.*))"
```

## Comparing `spotter/rewriting/rewrite_local_action_inline.py` & `spotter/library/rewriting/rewrite_local_object.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,52 +1,52 @@
-"""RewriteActionInline implementation."""
+"""RewriteLocalActionInline implementation."""
 
 import re
 from typing import Optional, Tuple
 
 import ruamel.yaml as yaml
 
-from spotter.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
-from spotter.rewriting.rewrite_module_inline import RewriteModuleInline
+from spotter.library.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
+from spotter.library.rewriting.rewrite_module_object import RewriteModuleObject
 
 
-class RewriteLocalActionInline(RewriteBase):
-    """RewriteActionInline implementation."""
+class RewriteLocalActionObject(RewriteBase):
+    """RewriteLocalActionInline implementation."""
 
     def get_regex(self, text_before: str) -> str:  # noqa: D102
         return rf"^(\s*({text_before}\s*):)"
 
-    def remove_module_name(self, content: str, suggestion: RewriteSuggestion) -> Tuple[str, RewriteSuggestion]:
+    def remove_module_row(self, content: str, suggestion: RewriteSuggestion) -> Tuple[str, RewriteSuggestion]:
         """
-        Remove module name from content.
+        Remove module line from content.
 
         :param content: Content that we want to rewrite
         :param suggestion: Suggestion object
         """
-        module_replacement = RewriteModuleInline().get_replacement(content, suggestion)
+        module_replacement = RewriteModuleObject().get_replacement(content, suggestion)
         if module_replacement is None:
             module_name = suggestion.suggestion_spec["data"]["module_name"]
-            print(f"Applying suggestion failed: could not find \"{module_name}\" to replace.")
+            print(f'Applying suggestion failed: could not find "{module_name}" to replace.')
             raise TypeError()
         rewrite_result = module_replacement.apply()
         suggestion.end_mark += rewrite_result.diff_size
         return rewrite_result.content, suggestion
 
     def get_replacement(self, content: str, suggestion: RewriteSuggestion) -> Optional[Replacement]:  # noqa: D102
         # 0. clean spaces in suggestion
         suggestion, start_char = self.shorten_match(content, suggestion)
 
         # 1. remove module name from arguments
-        content, suggestion = self.remove_module_name(content, suggestion)
+        content, suggestion = self.remove_module_row(content, suggestion)
 
         # 2. add delegate_to: localhost
         suggestion_data = suggestion.suggestion_spec["data"]
         index = self.get_indent_index(content, suggestion.start_mark)
         additional = start_char + " " * index + yaml.round_trip_dump(suggestion_data["additional"][0])
-        new_content = content[:suggestion.end_mark] + additional + content[suggestion.end_mark:]
+        new_content = content[: suggestion.end_mark] + additional + content[suggestion.end_mark :]
         suggestion.end_mark += len(additional)
 
         # 3. replace local_action keyword with module name
         part = self.get_context(content, suggestion)
         before = suggestion_data["original_module_name"]
         after = suggestion_data["module_name"]
```

## Comparing `spotter/rewriting/rewrite_local_object.py` & `spotter/library/rewriting/rewrite_local_action_inline.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,52 +1,52 @@
-"""RewriteLocalActionInline implementation."""
+"""RewriteActionInline implementation."""
 
 import re
 from typing import Optional, Tuple
 
 import ruamel.yaml as yaml
 
-from spotter.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
-from spotter.rewriting.rewrite_module_object import RewriteModuleObject
+from spotter.library.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
+from spotter.library.rewriting.rewrite_module_inline import RewriteModuleInline
 
 
-class RewriteLocalActionObject(RewriteBase):
-    """RewriteLocalActionInline implementation."""
+class RewriteLocalActionInline(RewriteBase):
+    """RewriteActionInline implementation."""
 
     def get_regex(self, text_before: str) -> str:  # noqa: D102
         return rf"^(\s*({text_before}\s*):)"
 
-    def remove_module_row(self, content: str, suggestion: RewriteSuggestion) -> Tuple[str, RewriteSuggestion]:
+    def remove_module_name(self, content: str, suggestion: RewriteSuggestion) -> Tuple[str, RewriteSuggestion]:
         """
-        Remove module line from content.
+        Remove module name from content.
 
         :param content: Content that we want to rewrite
         :param suggestion: Suggestion object
         """
-        module_replacement = RewriteModuleObject().get_replacement(content, suggestion)
+        module_replacement = RewriteModuleInline().get_replacement(content, suggestion)
         if module_replacement is None:
             module_name = suggestion.suggestion_spec["data"]["module_name"]
             print(f'Applying suggestion failed: could not find "{module_name}" to replace.')
             raise TypeError()
         rewrite_result = module_replacement.apply()
         suggestion.end_mark += rewrite_result.diff_size
         return rewrite_result.content, suggestion
 
     def get_replacement(self, content: str, suggestion: RewriteSuggestion) -> Optional[Replacement]:  # noqa: D102
         # 0. clean spaces in suggestion
         suggestion, start_char = self.shorten_match(content, suggestion)
 
         # 1. remove module name from arguments
-        content, suggestion = self.remove_module_row(content, suggestion)
+        content, suggestion = self.remove_module_name(content, suggestion)
 
         # 2. add delegate_to: localhost
         suggestion_data = suggestion.suggestion_spec["data"]
         index = self.get_indent_index(content, suggestion.start_mark)
         additional = start_char + " " * index + yaml.round_trip_dump(suggestion_data["additional"][0])
-        new_content = content[:suggestion.end_mark] + additional + content[suggestion.end_mark:]
+        new_content = content[: suggestion.end_mark] + additional + content[suggestion.end_mark :]
         suggestion.end_mark += len(additional)
 
         # 3. replace local_action keyword with module name
         part = self.get_context(content, suggestion)
         before = suggestion_data["original_module_name"]
         after = suggestion_data["module_name"]
```

## Comparing `spotter/rewriting/rewrite_module_inline.py` & `spotter/library/rewriting/rewrite_module_inline.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """RewriteModuleInline implementation."""
 
 import re
 from typing import Optional
 
-from spotter.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
+from spotter.library.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
 
 
 class RewriteModuleInline(RewriteBase):
     """RewriteModuleInline implementation."""
 
     def get_regex(self, text_before: str) -> str:  # noqa: D102
         return rf"((\s{text_before}|{text_before}\s))"
```

## Comparing `spotter/rewriting/rewrite_module_object.py` & `spotter/library/rewriting/rewrite_module_object.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """RewriteModuleObject implementation."""
 
 import re
 from typing import Optional
 
-from spotter.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
+from spotter.library.rewriting.models import Replacement, RewriteBase, RewriteSuggestion
 
 
 class RewriteModuleObject(RewriteBase):
     """RewriteModuleObject implementation."""
 
     def get_regex(self, text_before: str) -> str:  # noqa: D102
         return rf"((\n\s+{text_before}:\s[^\n]+))"
```

## Comparing `steampunk_spotter-2.1.0rc2.dist-info/LICENSE` & `steampunk_spotter-2.1.1a1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `steampunk_spotter-2.1.0rc2.dist-info/METADATA` & `steampunk_spotter-2.1.1a1.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 Metadata-Version: 2.1
 Name: steampunk-spotter
-Version: 2.1.0rc2
+Version: 2.1.1a1
 Summary: Ansible Playbook Scanning Tool
 Home-page: https://spotter.steampunk.si/
 Author: XLAB d.o.o.
-Author-email: pypi@xlab.si
+Author-email: "XLAB d.o.o." <pypi@xlab.si>
+Project-URL: Homepage, https://spotter.steampunk.si/
 Project-URL: Source Code, https://gitlab.com/xlab-steampunk/steampunk-spotter-client/spotter-cli
 Project-URL: Bug Tracker, https://gitlab.com/xlab-steampunk/steampunk-spotter-client/spotter-cli/-/issues
 Project-URL: Documentation, https://gitlab.com/xlab-steampunk/steampunk-spotter-client/spotter-cli/-/blob/main/DOCUMENTATION.md
 Project-URL: Changelog, https://gitlab.com/xlab-steampunk/steampunk-spotter-client/spotter-cli/-/blob/main/CHANGELOG.txt
 Keywords: ansible,automation,spotter,scanner,quality,playbook,steampunk
 Classifier: Development Status :: 4 - Beta
 Classifier: Environment :: Console
@@ -30,18 +31,18 @@
 Classifier: Topic :: Software Development :: Quality Assurance
 Classifier: Topic :: Software Development :: Testing
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Classifier: Framework :: Ansible
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: ruamel.yaml (<0.18,>=0.17.22)
+Requires-Dist: ruamel.yaml <0.18,>=0.17.22
 Requires-Dist: requests
 Requires-Dist: colorama
-Requires-Dist: pydantic (>=1.10.0)
+Requires-Dist: pydantic >=1.10.0
 Requires-Dist: detect-secrets
 
 # Steampunk Spotter Command-Line Interface (CLI)
 
 [![PyPI](https://img.shields.io/pypi/v/steampunk-spotter)](https://pypi.org/project/steampunk-spotter/)
 
 [Steampunk Spotter] provides an Ansible Playbook Scanning Tool that analyzes
```

