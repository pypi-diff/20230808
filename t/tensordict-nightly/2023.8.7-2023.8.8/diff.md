# Comparing `tmp/tensordict_nightly-2023.8.7-cp39-cp39-win_amd64.whl.zip` & `tmp/tensordict_nightly-2023.8.8-cp39-cp39-manylinux1_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,33 +1,33 @@
-Zip file size: 216239 bytes, number of entries: 31
--rw-rw-rw-  2.0 fat     1326 b- defN 23-Aug-07 13:49 tensordict/__init__.py
--rw-rw-rw-  2.0 fat     6156 b- defN 23-Aug-07 13:49 tensordict/_contextlib.py
--rw-rw-rw-  2.0 fat     2946 b- defN 23-Aug-07 13:49 tensordict/_pytree.py
--rw-rw-rw-  2.0 fat   113152 b- defN 23-Aug-07 13:51 tensordict/_tensordict.pyd
--rw-rw-rw-  2.0 fat    31534 b- defN 23-Aug-07 13:49 tensordict/memmap.py
--rw-rw-rw-  2.0 fat    35426 b- defN 23-Aug-07 13:49 tensordict/persistent.py
--rw-rw-rw-  2.0 fat    34209 b- defN 23-Aug-07 13:49 tensordict/tensorclass.py
--rw-rw-rw-  2.0 fat   342447 b- defN 23-Aug-07 13:49 tensordict/tensordict.py
--rw-rw-rw-  2.0 fat    43446 b- defN 23-Aug-07 13:49 tensordict/utils.py
--rw-rw-rw-  2.0 fat       86 b- defN 23-Aug-07 13:50 tensordict/version.py
--rw-rw-rw-  2.0 fat     1493 b- defN 23-Aug-07 13:49 tensordict/nn/__init__.py
--rw-rw-rw-  2.0 fat    53776 b- defN 23-Aug-07 13:49 tensordict/nn/common.py
--rw-rw-rw-  2.0 fat     5924 b- defN 23-Aug-07 13:49 tensordict/nn/ensemble.py
--rw-rw-rw-  2.0 fat    25861 b- defN 23-Aug-07 13:49 tensordict/nn/functional_modules.py
--rw-rw-rw-  2.0 fat    25709 b- defN 23-Aug-07 13:49 tensordict/nn/params.py
--rw-rw-rw-  2.0 fat    23469 b- defN 23-Aug-07 13:49 tensordict/nn/probabilistic.py
--rw-rw-rw-  2.0 fat    19940 b- defN 23-Aug-07 13:49 tensordict/nn/sequence.py
--rw-rw-rw-  2.0 fat    13006 b- defN 23-Aug-07 13:49 tensordict/nn/utils.py
--rw-rw-rw-  2.0 fat      512 b- defN 23-Aug-07 13:49 tensordict/nn/distributions/__init__.py
--rw-rw-rw-  2.0 fat     7272 b- defN 23-Aug-07 13:49 tensordict/nn/distributions/continuous.py
--rw-rw-rw-  2.0 fat     2667 b- defN 23-Aug-07 13:49 tensordict/nn/distributions/discrete.py
--rw-rw-rw-  2.0 fat     6694 b- defN 23-Aug-07 13:49 tensordict/nn/distributions/truncated_normal.py
--rw-rw-rw-  2.0 fat     1266 b- defN 23-Aug-07 13:49 tensordict/nn/distributions/utils.py
--rw-rw-rw-  2.0 fat      393 b- defN 23-Aug-07 13:49 tensordict/prototype/__init__.py
--rw-rw-rw-  2.0 fat     7875 b- defN 23-Aug-07 13:49 tensordict/prototype/fx.py
--rw-rw-rw-  2.0 fat      764 b- defN 23-Aug-07 13:49 tensordict/prototype/tensorclass.py
--rw-rw-rw-  2.0 fat     1119 b- defN 23-Aug-07 13:51 tensordict_nightly-2023.8.7.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    15786 b- defN 23-Aug-07 13:51 tensordict_nightly-2023.8.7.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 23-Aug-07 13:51 tensordict_nightly-2023.8.7.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       11 b- defN 23-Aug-07 13:51 tensordict_nightly-2023.8.7.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2679 b- defN 23-Aug-07 13:51 tensordict_nightly-2023.8.7.dist-info/RECORD
-31 files, 827044 bytes uncompressed, 211943 bytes compressed:  74.4%
+Zip file size: 976348 bytes, number of entries: 31
+-rw-r--r--  2.0 unx     1273 b- defN 23-Aug-08 13:53 tensordict/__init__.py
+-rw-r--r--  2.0 unx     6000 b- defN 23-Aug-08 13:53 tensordict/_contextlib.py
+-rw-r--r--  2.0 unx     2849 b- defN 23-Aug-08 13:53 tensordict/_pytree.py
+-rwxr-xr-x  2.0 unx  3205448 b- defN 23-Aug-08 13:54 tensordict/_tensordict.so
+-rw-r--r--  2.0 unx    30677 b- defN 23-Aug-08 13:53 tensordict/memmap.py
+-rw-r--r--  2.0 unx    34628 b- defN 23-Aug-08 13:53 tensordict/persistent.py
+-rw-r--r--  2.0 unx    33241 b- defN 23-Aug-08 13:53 tensordict/tensorclass.py
+-rw-r--r--  2.0 unx   337221 b- defN 23-Aug-08 13:53 tensordict/tensordict.py
+-rw-r--r--  2.0 unx    42201 b- defN 23-Aug-08 13:53 tensordict/utils.py
+-rw-r--r--  2.0 unx       51 b- defN 23-Aug-08 13:53 tensordict/version.py
+-rw-r--r--  2.0 unx     1438 b- defN 23-Aug-08 13:53 tensordict/nn/__init__.py
+-rw-r--r--  2.0 unx    52499 b- defN 23-Aug-08 13:53 tensordict/nn/common.py
+-rw-r--r--  2.0 unx     5793 b- defN 23-Aug-08 13:53 tensordict/nn/ensemble.py
+-rw-r--r--  2.0 unx    25204 b- defN 23-Aug-08 13:53 tensordict/nn/functional_modules.py
+-rw-r--r--  2.0 unx    24905 b- defN 23-Aug-08 13:53 tensordict/nn/params.py
+-rw-r--r--  2.0 unx    22945 b- defN 23-Aug-08 13:53 tensordict/nn/probabilistic.py
+-rw-r--r--  2.0 unx    19491 b- defN 23-Aug-08 13:53 tensordict/nn/sequence.py
+-rw-r--r--  2.0 unx    12675 b- defN 23-Aug-08 13:53 tensordict/nn/utils.py
+-rw-r--r--  2.0 unx      499 b- defN 23-Aug-08 13:53 tensordict/nn/distributions/__init__.py
+-rw-r--r--  2.0 unx     7073 b- defN 23-Aug-08 13:53 tensordict/nn/distributions/continuous.py
+-rw-r--r--  2.0 unx     2580 b- defN 23-Aug-08 13:53 tensordict/nn/distributions/discrete.py
+-rw-r--r--  2.0 unx     6504 b- defN 23-Aug-08 13:53 tensordict/nn/distributions/truncated_normal.py
+-rw-r--r--  2.0 unx     1226 b- defN 23-Aug-08 13:53 tensordict/nn/distributions/utils.py
+-rw-r--r--  2.0 unx      381 b- defN 23-Aug-08 13:53 tensordict/prototype/__init__.py
+-rw-r--r--  2.0 unx     7677 b- defN 23-Aug-08 13:53 tensordict/prototype/fx.py
+-rw-r--r--  2.0 unx      739 b- defN 23-Aug-08 13:53 tensordict/prototype/tensorclass.py
+-rw-r--r--  2.0 unx     1098 b- defN 23-Aug-08 13:54 tensordict_nightly-2023.8.8.dist-info/LICENSE
+-rw-r--r--  2.0 unx    15381 b- defN 23-Aug-08 13:54 tensordict_nightly-2023.8.8.dist-info/METADATA
+-rw-r--r--  2.0 unx      103 b- defN 23-Aug-08 13:54 tensordict_nightly-2023.8.8.dist-info/WHEEL
+-rw-r--r--  2.0 unx       11 b- defN 23-Aug-08 13:54 tensordict_nightly-2023.8.8.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2679 b- defN 23-Aug-08 13:54 tensordict_nightly-2023.8.8.dist-info/RECORD
+31 files, 3904490 bytes uncompressed, 972054 bytes compressed:  75.1%
```

## zipnote {}

```diff
@@ -3,15 +3,15 @@
 
 Filename: tensordict/_contextlib.py
 Comment: 
 
 Filename: tensordict/_pytree.py
 Comment: 
 
-Filename: tensordict/_tensordict.pyd
+Filename: tensordict/_tensordict.so
 Comment: 
 
 Filename: tensordict/memmap.py
 Comment: 
 
 Filename: tensordict/persistent.py
 Comment: 
@@ -72,23 +72,23 @@
 
 Filename: tensordict/prototype/fx.py
 Comment: 
 
 Filename: tensordict/prototype/tensorclass.py
 Comment: 
 
-Filename: tensordict_nightly-2023.8.7.dist-info/LICENSE
+Filename: tensordict_nightly-2023.8.8.dist-info/LICENSE
 Comment: 
 
-Filename: tensordict_nightly-2023.8.7.dist-info/METADATA
+Filename: tensordict_nightly-2023.8.8.dist-info/METADATA
 Comment: 
 
-Filename: tensordict_nightly-2023.8.7.dist-info/WHEEL
+Filename: tensordict_nightly-2023.8.8.dist-info/WHEEL
 Comment: 
 
-Filename: tensordict_nightly-2023.8.7.dist-info/top_level.txt
+Filename: tensordict_nightly-2023.8.8.dist-info/top_level.txt
 Comment: 
 
-Filename: tensordict_nightly-2023.8.7.dist-info/RECORD
+Filename: tensordict_nightly-2023.8.8.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tensordict/__init__.py

 * *Ordering differences only*

```diff
@@ -1,53 +1,53 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from tensordict.memmap import MemmapTensor, set_transfer_ownership
-from tensordict.persistent import PersistentTensorDict
-from tensordict.tensorclass import tensorclass
-from tensordict.tensordict import (
-    dense_stack_tds,
-    is_batchedtensor,
-    is_memmap,
-    is_tensor_collection,
-    LazyStackedTensorDict,
-    make_tensordict,
-    merge_tensordicts,
-    pad,
-    pad_sequence,
-    SubTensorDict,
-    TensorDict,
-    TensorDictBase,
-)
-from tensordict.utils import is_tensorclass
-
-try:
-    from tensordict.version import __version__
-except ImportError:
-    __version__ = None
-
-from tensordict._pytree import *
-
-from tensordict._tensordict import unravel_key, unravel_key_list
-
-__all__ = [
-    "LazyStackedTensorDict",
-    "MemmapTensor",
-    "SubTensorDict",
-    "TensorDict",
-    "TensorDictBase",
-    "merge_tensordicts",
-    "set_transfer_ownership",
-    "pad_sequence",
-    "make_tensordict",
-    "is_memmap",
-    "is_batchedtensor",
-    "is_tensor_collection",
-    "pad",
-    "PersistentTensorDict",
-    "tensorclass",
-    "dense_stack_tds",
-]
-
-# from tensordict._pytree import *
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from tensordict.memmap import MemmapTensor, set_transfer_ownership
+from tensordict.persistent import PersistentTensorDict
+from tensordict.tensorclass import tensorclass
+from tensordict.tensordict import (
+    dense_stack_tds,
+    is_batchedtensor,
+    is_memmap,
+    is_tensor_collection,
+    LazyStackedTensorDict,
+    make_tensordict,
+    merge_tensordicts,
+    pad,
+    pad_sequence,
+    SubTensorDict,
+    TensorDict,
+    TensorDictBase,
+)
+from tensordict.utils import is_tensorclass
+
+try:
+    from tensordict.version import __version__
+except ImportError:
+    __version__ = None
+
+from tensordict._pytree import *
+
+from tensordict._tensordict import unravel_key, unravel_key_list
+
+__all__ = [
+    "LazyStackedTensorDict",
+    "MemmapTensor",
+    "SubTensorDict",
+    "TensorDict",
+    "TensorDictBase",
+    "merge_tensordicts",
+    "set_transfer_ownership",
+    "pad_sequence",
+    "make_tensordict",
+    "is_memmap",
+    "is_batchedtensor",
+    "is_tensor_collection",
+    "pad",
+    "PersistentTensorDict",
+    "tensorclass",
+    "dense_stack_tds",
+]
+
+# from tensordict._pytree import *
```

## tensordict/_contextlib.py

 * *Ordering differences only*

```diff
@@ -1,156 +1,156 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-# This is a copy from https://github.com/pytorch/pytorch/blob/main/torch/utils/_contextlib.py#L120
-# We use it for compatibility with torch >= 1.10 where the implementation fails
-# for some tests in torchrl.
-
-# Extra utilities for working with context managers that should have been
-# in the standard library but are not
-
-import functools
-import inspect
-import sys
-import warnings
-from typing import Any, Callable, cast, TypeVar
-
-# Used for annotating the decorator usage of _DecoratorContextManager (e.g.,
-# 'no_grad' and 'enable_grad').
-# See https://mypy.readthedocs.io/en/latest/generics.html#declaring-decorators
-FuncType = Callable[..., Any]
-F = TypeVar("F", bound=FuncType)
-
-
-def _wrap_generator(ctx_factory, func):
-    """Wrap each generator invocation with the context manager factory.
-
-    The input should be a function that returns a context manager,
-    not a context manager itself, to handle one-shot context managers.
-    """
-
-    @functools.wraps(func)
-    def generator_context(*args, **kwargs):
-        gen = func(*args, **kwargs)
-
-        # Generators are suspended and unsuspended at `yield`, hence we
-        # make sure the grad mode is properly set every time the execution
-        # flow returns into the wrapped generator and restored when it
-        # returns through our `yield` to our caller (see PR #49017).
-        try:
-            # Issuing `None` to a generator fires it up
-            with ctx_factory():
-                response = gen.send(None)
-
-            while True:
-                try:
-                    # Forward the response to our caller and get its next request
-                    request = yield response
-
-                except GeneratorExit:
-                    # Inform the still active generator about its imminent closure
-                    with ctx_factory():
-                        gen.close()
-                    raise
-
-                except BaseException:
-                    # Propagate the exception thrown at us by the caller
-                    with ctx_factory():
-                        response = gen.throw(*sys.exc_info())
-
-                else:
-                    # Pass the last request to the generator and get its response
-                    with ctx_factory():
-                        response = gen.send(request)
-
-        # We let the exceptions raised above by the generator's `.throw` or
-        # `.send` methods bubble up to our caller, except for StopIteration
-        except StopIteration as e:
-            # The generator informed us that it is done: take whatever its
-            # returned value (if any) was and indicate that we're done too
-            # by returning it (see docs for python's return-statement).
-            return e.value
-
-    return generator_context
-
-
-def context_decorator(ctx, func):
-    """Like contextlib.ContextDecorator.
-
-    Except:
-
-    1. Is done by wrapping, rather than inheritance, so it works with context
-       managers that are implemented from C and thus cannot easily inherit from
-       Python classes
-    2. Wraps generators in the intuitive way (c.f. https://bugs.python.org/issue37743)
-    3. Errors out if you try to wrap a class, because it is ambiguous whether
-       or not you intended to wrap only the constructor
-
-    The input argument can either be a context manager (in which case it must
-    be a multi-shot context manager that can be directly invoked multiple times)
-    or a callable that produces a context manager.
-    """
-    assert not (callable(ctx) and hasattr(ctx, "__enter__")), (
-        f"Passed in {ctx} is both callable and also a valid context manager "
-        "(has __enter__), making it ambiguous which interface to use.  If you "
-        "intended to pass a context manager factory, rewrite your call as "
-        "context_decorator(lambda: ctx()); if you intended to pass a context "
-        "manager directly, rewrite your call as context_decorator(lambda: ctx)"
-    )
-
-    if not callable(ctx):
-
-        def ctx_factory():
-            return ctx
-
-    else:
-        ctx_factory = ctx
-
-    if inspect.isclass(func):
-        raise RuntimeError(
-            "Cannot decorate classes; it is ambiguous whether or not only the "
-            "constructor or all methods should have the context manager applied; "
-            "additionally, decorating a class at definition-site will prevent "
-            "use of the identifier as a conventional type.  "
-            "To specify which methods to decorate, decorate each of them "
-            "individually."
-        )
-
-    if inspect.isgeneratorfunction(func):
-        return _wrap_generator(ctx_factory, func)
-
-    @functools.wraps(func)
-    def decorate_context(*args, **kwargs):
-        with ctx_factory():
-            return func(*args, **kwargs)
-
-    return decorate_context
-
-
-class _DecoratorContextManager:
-    """Allows a context manager to be used as a decorator."""
-
-    def __call__(self, orig_func: F) -> F:
-        if inspect.isclass(orig_func):
-            warnings.warn(
-                "Decorating classes is deprecated and will be disabled in "
-                "future versions. You should only decorate functions or methods. "
-                "To preserve the current behavior of class decoration, you can "
-                "directly decorate the `__init__` method and nothing else."
-            )
-            func = cast(F, lambda *args, **kwargs: orig_func(*args, **kwargs))
-        else:
-            func = orig_func
-
-        return cast(F, context_decorator(self.clone, func))
-
-    def __enter__(self) -> None:
-        raise NotImplementedError
-
-    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
-        raise NotImplementedError
-
-    def clone(self):
-        # override this method if your children class takes __init__ parameters
-        return self.__class__()
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+# This is a copy from https://github.com/pytorch/pytorch/blob/main/torch/utils/_contextlib.py#L120
+# We use it for compatibility with torch >= 1.10 where the implementation fails
+# for some tests in torchrl.
+
+# Extra utilities for working with context managers that should have been
+# in the standard library but are not
+
+import functools
+import inspect
+import sys
+import warnings
+from typing import Any, Callable, cast, TypeVar
+
+# Used for annotating the decorator usage of _DecoratorContextManager (e.g.,
+# 'no_grad' and 'enable_grad').
+# See https://mypy.readthedocs.io/en/latest/generics.html#declaring-decorators
+FuncType = Callable[..., Any]
+F = TypeVar("F", bound=FuncType)
+
+
+def _wrap_generator(ctx_factory, func):
+    """Wrap each generator invocation with the context manager factory.
+
+    The input should be a function that returns a context manager,
+    not a context manager itself, to handle one-shot context managers.
+    """
+
+    @functools.wraps(func)
+    def generator_context(*args, **kwargs):
+        gen = func(*args, **kwargs)
+
+        # Generators are suspended and unsuspended at `yield`, hence we
+        # make sure the grad mode is properly set every time the execution
+        # flow returns into the wrapped generator and restored when it
+        # returns through our `yield` to our caller (see PR #49017).
+        try:
+            # Issuing `None` to a generator fires it up
+            with ctx_factory():
+                response = gen.send(None)
+
+            while True:
+                try:
+                    # Forward the response to our caller and get its next request
+                    request = yield response
+
+                except GeneratorExit:
+                    # Inform the still active generator about its imminent closure
+                    with ctx_factory():
+                        gen.close()
+                    raise
+
+                except BaseException:
+                    # Propagate the exception thrown at us by the caller
+                    with ctx_factory():
+                        response = gen.throw(*sys.exc_info())
+
+                else:
+                    # Pass the last request to the generator and get its response
+                    with ctx_factory():
+                        response = gen.send(request)
+
+        # We let the exceptions raised above by the generator's `.throw` or
+        # `.send` methods bubble up to our caller, except for StopIteration
+        except StopIteration as e:
+            # The generator informed us that it is done: take whatever its
+            # returned value (if any) was and indicate that we're done too
+            # by returning it (see docs for python's return-statement).
+            return e.value
+
+    return generator_context
+
+
+def context_decorator(ctx, func):
+    """Like contextlib.ContextDecorator.
+
+    Except:
+
+    1. Is done by wrapping, rather than inheritance, so it works with context
+       managers that are implemented from C and thus cannot easily inherit from
+       Python classes
+    2. Wraps generators in the intuitive way (c.f. https://bugs.python.org/issue37743)
+    3. Errors out if you try to wrap a class, because it is ambiguous whether
+       or not you intended to wrap only the constructor
+
+    The input argument can either be a context manager (in which case it must
+    be a multi-shot context manager that can be directly invoked multiple times)
+    or a callable that produces a context manager.
+    """
+    assert not (callable(ctx) and hasattr(ctx, "__enter__")), (
+        f"Passed in {ctx} is both callable and also a valid context manager "
+        "(has __enter__), making it ambiguous which interface to use.  If you "
+        "intended to pass a context manager factory, rewrite your call as "
+        "context_decorator(lambda: ctx()); if you intended to pass a context "
+        "manager directly, rewrite your call as context_decorator(lambda: ctx)"
+    )
+
+    if not callable(ctx):
+
+        def ctx_factory():
+            return ctx
+
+    else:
+        ctx_factory = ctx
+
+    if inspect.isclass(func):
+        raise RuntimeError(
+            "Cannot decorate classes; it is ambiguous whether or not only the "
+            "constructor or all methods should have the context manager applied; "
+            "additionally, decorating a class at definition-site will prevent "
+            "use of the identifier as a conventional type.  "
+            "To specify which methods to decorate, decorate each of them "
+            "individually."
+        )
+
+    if inspect.isgeneratorfunction(func):
+        return _wrap_generator(ctx_factory, func)
+
+    @functools.wraps(func)
+    def decorate_context(*args, **kwargs):
+        with ctx_factory():
+            return func(*args, **kwargs)
+
+    return decorate_context
+
+
+class _DecoratorContextManager:
+    """Allows a context manager to be used as a decorator."""
+
+    def __call__(self, orig_func: F) -> F:
+        if inspect.isclass(orig_func):
+            warnings.warn(
+                "Decorating classes is deprecated and will be disabled in "
+                "future versions. You should only decorate functions or methods. "
+                "To preserve the current behavior of class decoration, you can "
+                "directly decorate the `__init__` method and nothing else."
+            )
+            func = cast(F, lambda *args, **kwargs: orig_func(*args, **kwargs))
+        else:
+            func = orig_func
+
+        return cast(F, context_decorator(self.clone, func))
+
+    def __enter__(self) -> None:
+        raise NotImplementedError
+
+    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
+        raise NotImplementedError
+
+    def clone(self):
+        # override this method if your children class takes __init__ parameters
+        return self.__class__()
```

## tensordict/_pytree.py

 * *Ordering differences only*

```diff
@@ -1,97 +1,97 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-from typing import Any, Dict, List, Tuple
-
-from tensordict import (
-    LazyStackedTensorDict,
-    PersistentTensorDict,
-    SubTensorDict,
-    TensorDict,
-)
-
-from torch.utils._pytree import _register_pytree_node, Context
-
-PYTREE_REGISTERED_TDS = (
-    LazyStackedTensorDict,
-    SubTensorDict,
-    TensorDict,
-    PersistentTensorDict,
-)
-
-
-def _str_to_dict(str_spec: str) -> Tuple[List[str], str]:
-    assert str_spec[1] == "("
-    assert str_spec[-1] == ")"
-    context_and_child_strings = str_spec[2:-1]
-
-    child_strings = []
-    context_strings = []
-    nested_parentheses = 0
-    start_index = 0
-    for i, char in enumerate(context_and_child_strings):
-        if char == ":":
-            if nested_parentheses == 0:
-                context_strings.append(context_and_child_strings[start_index:i])
-                start_index = i + 1
-        elif char == "(":
-            nested_parentheses += 1
-        elif char == ")":
-            nested_parentheses -= 1
-
-        if nested_parentheses == 0 and char == ",":
-            child_strings.append(context_and_child_strings[start_index:i])
-            start_index = i + 1
-
-    child_strings.append(context_and_child_strings[start_index:])
-    return context_strings, ",".join(child_strings)
-
-
-def _str_to_tensordictdict(str_spec: str) -> Tuple[List[str], str]:
-    context_and_child_strings = str_spec[2:-1]
-
-    child_strings = []
-    context_strings = []
-    nested_parentheses = 0
-    start_index = 0
-    for i, char in enumerate(context_and_child_strings):
-        if char == ":":
-            if nested_parentheses == 0:
-                context_strings.append(context_and_child_strings[start_index:i])
-                start_index = i + 1
-        elif char == "(":
-            nested_parentheses += 1
-        elif char == ")":
-            nested_parentheses -= 1
-
-        if nested_parentheses == 0 and char == ",":
-            child_strings.append(context_and_child_strings[start_index:i])
-            start_index = i + 1
-
-    child_strings.append(context_and_child_strings[start_index:])
-    return context_strings, ",".join(child_strings)
-
-
-def _tensordict_flatten(d: TensorDict) -> Tuple[List[Any], Context]:
-    return list(d.values()), {
-        "keys": list(d.keys()),
-        "batch_size": d.batch_size,
-        "names": d.names,
-    }
-
-
-def _tensordictdict_unflatten(values: List[Any], context: Context) -> Dict[Any, Any]:
-    return TensorDict(
-        dict(zip(context["keys"], values)),
-        context["batch_size"],
-        names=context["names"],
-    )
-
-
-for cls in PYTREE_REGISTERED_TDS:
-    _register_pytree_node(
-        cls,
-        _tensordict_flatten,
-        _tensordictdict_unflatten,
-    )
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+from typing import Any, Dict, List, Tuple
+
+from tensordict import (
+    LazyStackedTensorDict,
+    PersistentTensorDict,
+    SubTensorDict,
+    TensorDict,
+)
+
+from torch.utils._pytree import _register_pytree_node, Context
+
+PYTREE_REGISTERED_TDS = (
+    LazyStackedTensorDict,
+    SubTensorDict,
+    TensorDict,
+    PersistentTensorDict,
+)
+
+
+def _str_to_dict(str_spec: str) -> Tuple[List[str], str]:
+    assert str_spec[1] == "("
+    assert str_spec[-1] == ")"
+    context_and_child_strings = str_spec[2:-1]
+
+    child_strings = []
+    context_strings = []
+    nested_parentheses = 0
+    start_index = 0
+    for i, char in enumerate(context_and_child_strings):
+        if char == ":":
+            if nested_parentheses == 0:
+                context_strings.append(context_and_child_strings[start_index:i])
+                start_index = i + 1
+        elif char == "(":
+            nested_parentheses += 1
+        elif char == ")":
+            nested_parentheses -= 1
+
+        if nested_parentheses == 0 and char == ",":
+            child_strings.append(context_and_child_strings[start_index:i])
+            start_index = i + 1
+
+    child_strings.append(context_and_child_strings[start_index:])
+    return context_strings, ",".join(child_strings)
+
+
+def _str_to_tensordictdict(str_spec: str) -> Tuple[List[str], str]:
+    context_and_child_strings = str_spec[2:-1]
+
+    child_strings = []
+    context_strings = []
+    nested_parentheses = 0
+    start_index = 0
+    for i, char in enumerate(context_and_child_strings):
+        if char == ":":
+            if nested_parentheses == 0:
+                context_strings.append(context_and_child_strings[start_index:i])
+                start_index = i + 1
+        elif char == "(":
+            nested_parentheses += 1
+        elif char == ")":
+            nested_parentheses -= 1
+
+        if nested_parentheses == 0 and char == ",":
+            child_strings.append(context_and_child_strings[start_index:i])
+            start_index = i + 1
+
+    child_strings.append(context_and_child_strings[start_index:])
+    return context_strings, ",".join(child_strings)
+
+
+def _tensordict_flatten(d: TensorDict) -> Tuple[List[Any], Context]:
+    return list(d.values()), {
+        "keys": list(d.keys()),
+        "batch_size": d.batch_size,
+        "names": d.names,
+    }
+
+
+def _tensordictdict_unflatten(values: List[Any], context: Context) -> Dict[Any, Any]:
+    return TensorDict(
+        dict(zip(context["keys"], values)),
+        context["batch_size"],
+        names=context["names"],
+    )
+
+
+for cls in PYTREE_REGISTERED_TDS:
+    _register_pytree_node(
+        cls,
+        _tensordict_flatten,
+        _tensordictdict_unflatten,
+    )
```

## tensordict/memmap.py

 * *Ordering differences only*

```diff
@@ -1,857 +1,857 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import functools
-import os
-import tempfile
-import warnings
-from copy import copy, deepcopy
-from pathlib import Path
-from sys import getrefcount
-from tempfile import _TemporaryFileWrapper
-from typing import Any, Callable, Sequence
-
-import numpy as np
-import torch
-
-from tensordict.utils import (
-    _getitem_batch_size,
-    convert_ellipsis_to_idx,
-    DeviceType,
-    IndexType,
-    NUMPY_TO_TORCH_DTYPE_DICT,
-    prod,
-    TORCH_TO_NUMPY_DTYPE_DICT,
-)
-
-__all__ = ["MemmapTensor", "set_transfer_ownership"]
-
-
-NoneType = type(None)
-EllipsisType = type(Ellipsis)
-
-
-MEMMAP_HANDLED_FN: dict[Callable, Callable] = {}
-HAS_OWNERSHIP = {}
-# HAD_OWNERSHIP = {}
-TRANSFER_OWNERSHIP = {}
-
-
-def implements_for_memmap(torch_function: Callable) -> Callable:
-    """Register a torch function override for ScalarTensor."""
-
-    @functools.wraps(torch_function)
-    def decorator(func: Callable) -> Callable:
-        MEMMAP_HANDLED_FN[torch_function] = func
-        return func
-
-    return decorator
-
-
-def to_numpy(tensor: torch.Tensor | np.ndarray) -> np.ndarray:
-    if isinstance(tensor, torch.Tensor):
-        return tensor.detach().cpu().numpy()
-    else:
-        return tensor
-
-
-class MemmapTensor:
-    """A torch.tensor interface with a np.memmap array.
-
-    A temporary file is created and cleared once the object is out-of-scope.
-    This class is aimed at being used for data transfer in between processes
-    and remote workers that have access to
-    a common storage, and as such it supports serialization and
-    deserialization. It is possible to choose if the ownership is
-    transferred upon serialization / deserialization: If ownership is not
-    transferred (transfer_ownership=False, default), then the process where
-    the MemmapTensor was created will be responsible of clearing it once it
-    gets out of scope (in that process). Otherwise, the process that
-    deserialize the MemmapTensor will be responsible of clearing the files
-    once the object is out of scope.
-
-    Supports (almost) all tensor operations.
-
-    Args:
-        *tensor_or_size (torch.Tensor, MemmapTensor, torch.Size or sequence of integers):
-            If a size is provided (with a sequence of integers, a torch.Size object
-            or a list/tuple of integers) it indicates the size of the MemmapTensor created.
-            If a te is provided, its content will be stored on physical storage.
-            If MemmapTensor, a new MemmapTensor is created and the same data is stored in it.
-        device (torch.device or equivalent, optional): device where the loaded
-            tensor will be sent. This should not be used with MemmapTensors
-            created from torch.Tensor objects. Default is "cpu".
-        dtype (torch.dtype, optional): dtype of the loaded tensor.
-            This should not be used with MemmapTensors created from torch.Tensor
-            objects. Default is :obj:`torch.get_default_dtype()`.
-        transfer_ownership (bool, optional): affects the ownership after serialization:
-            if True, the current process looses ownership immediately after
-            serialization. If False, the current process keeps the ownership
-            of the temporary file.
-            Default: False.
-        prefix (str or path, optional): *Deprecated* prefix of the file location. Should
-            not be specified together with prefix.
-        filename (str or path, optional): location of the underlying memory-map. Should
-            not be specified together with prefix.
-
-    Examples:
-        >>> x = torch.ones(3,4)
-        >>> x_memmap = MemmapTensor.from_tensor(x)
-        >>> # indexing
-        >>> x0 = x_memmap[0]
-        >>> x0[:] = 2
-        >>> assert (x_memmap[0]==2).all()
-        >>>
-        >>> # device
-        >>> x = x.to('cuda:0')
-        >>> x_memmap = MemmapTensor.from_tensor(x)
-        >>> assert (x_memmap.clone()).device == torch.device('cuda:0')
-        >>>
-        >>> # operations
-        >>> assert (x_memmap + 1 == x+1).all()
-        >>> assert (x_memmap / 2 == x/2).all()
-        >>> assert (x_memmap * 2 == x*2).all()
-        >>>
-        >>> # temp file clearance
-        >>> filename = x_memmap.filename
-        >>> assert os.path.isfile(filename)
-        >>> del x_memmap
-        >>> assert not os.path.isfile(filename)
-
-    """
-
-    requires_grad: bool = False
-
-    def __init__(
-        self,
-        *size: int,
-        device: DeviceType | None = None,
-        dtype: torch.dtype | None = None,
-        transfer_ownership: bool = False,
-        prefix: str | None = None,
-        filename: str | None = None,
-        mode: str = "r+",
-    ) -> None:
-        self.idx = None
-        self._memmap_array = None
-        self.prefix = prefix
-        self.is_meta = False
-
-        if mode in ("r+", "w+", "c", "copyonwrite", "readwrite", "write"):
-            self.mode = mode
-        else:
-            raise ValueError(
-                'Accepted values for mode are "r+", "readwrite", "w+", "write", "c" or '
-                '"copyonwrite". PyTorch does not support tensors backed by read-only '
-                'NumPy arrays, so "r" and "readonly" are not supported.'
-            )
-
-        if prefix is not None:
-            warnings.warn(
-                "prefix has been deprecated. If you want to control the location of "
-                "the MemmapTensor on disk, consider using filename instead.",
-                stacklevel=2,
-            )
-            if filename is not None:
-                raise ValueError("filename and prefix should not both be specified")
-
-        # open the files in r+ mode so as to not overwrite any data that might exist
-        # there. the actual memmap will be instantiated with user-supplied mode
-        if filename is None:
-            self.file = tempfile.NamedTemporaryFile(
-                prefix=prefix, delete=False, mode="r+"
-            )
-        else:
-            # if filename doesn't exist we must create it
-            Path(filename).touch(exist_ok=True)
-            self.file = open(filename, mode="r+")
-
-        self.filename = copy(self.file.name)
-        self.file.close()  # we close the file for now, but don't delete it
-
-        if isinstance(size[0], (torch.Tensor, MemmapTensor, np.ndarray)):
-            raise NotImplementedError(
-                "Creating a Memmap array from a tensor is not permitted anymore. "
-                "Call MemmapTensor.from_tensor(tensor) instead."
-            )
-        else:
-            try:
-                shape = (
-                    torch.Size(list(size[0]))
-                    if len(size) == 1 and not isinstance(size[0], int)
-                    else torch.Size(list(size))
-                )
-            except TypeError:
-                raise TypeError(
-                    f"The *size must be either a single list or tuple of ints, or a sequence of ints. Got {size} instead."
-                )
-            device = device if device is not None else torch.device("cpu")
-            dtype = dtype if dtype is not None else torch.get_default_dtype()
-            self._init_shape(
-                shape=shape,
-                device=device,
-                dtype=dtype,
-                transfer_ownership=transfer_ownership,
-            )
-        if not hasattr(self, "_index"):
-            self._index = None
-
-    @classmethod
-    def from_tensor(
-        cls,
-        tensor: torch.Tensor | MemmapTensor | np.ndarray,
-        transfer_ownership: bool = False,
-        prefix: str | None = None,
-        filename: str | None = None,
-        mode: str = "r+",
-    ) -> MemmapTensor:
-        if isinstance(tensor, MemmapTensor):
-            if transfer_ownership:
-                raise RuntimeError(
-                    "from_tensor(memmap_tensor, transfer_ownership=True) is not permitted, as this method will "
-                    "simply return the original MemmapTensor instance."
-                )
-            elif prefix is None and (
-                filename is None
-                or Path(filename).absolute() == Path(tensor.filename).absolute()
-            ):
-                # either location was not specified, or memmap is already in the
-                # correct location, so just return the MemmapTensor unmodified
-                return tensor
-        elif isinstance(tensor, np.ndarray):
-            raise TypeError(
-                "Convert input to torch.Tensor before calling MemmapTensor."
-            )
-        if tensor.requires_grad:
-            raise RuntimeError(
-                "MemmapTensor is incompatible with tensor.requires_grad."
-            )
-        device = tensor.device if hasattr(tensor, "device") else torch.device("cpu")
-        dtype = (
-            tensor.dtype
-            if isinstance(tensor, (torch.Tensor, MemmapTensor))
-            else NUMPY_TO_TORCH_DTYPE_DICT[np.dtype(tensor.dtype.name)]
-        )
-        shape = tensor.shape
-        out = cls(
-            shape,
-            device=device,
-            dtype=dtype,
-            prefix=prefix,
-            transfer_ownership=transfer_ownership,
-            filename=filename,
-            mode=mode,
-        )
-        out.copy_(tensor)
-        return out
-
-    @classmethod
-    def empty_like(
-        cls,
-        tensor: torch.Tensor | MemmapTensor,
-        transfer_ownership: bool = False,
-        prefix: str | None = None,
-        filename: str | None = None,
-        mode: str = "r+",
-    ) -> MemmapTensor:
-        if isinstance(tensor, np.ndarray):
-            raise TypeError(
-                "Convert input to torch.Tensor before calling MemmapTensor."
-            )
-        device = tensor.device
-        dtype = tensor.dtype
-        shape = tensor.shape
-        out = cls(
-            shape,
-            device=device,
-            dtype=dtype,
-            prefix=prefix,
-            transfer_ownership=transfer_ownership,
-            filename=filename,
-            mode=mode,
-        )
-        return out
-
-    @staticmethod
-    def _create_memmap_with_index(memmap_tensor, index):
-        memmap_copy = copy(memmap_tensor)
-        if memmap_copy._index is None:
-            memmap_copy._index = []
-        else:
-            # avoid extending someone else's index
-            memmap_copy._index = deepcopy(memmap_copy._index)
-        memmap_copy._index.append(convert_ellipsis_to_idx(index, memmap_tensor.shape))
-        memmap_copy._shape_indexed = None
-        memmap_copy.file = memmap_tensor.file
-        memmap_copy._memmap_array = memmap_tensor._memmap_array
-
-        return memmap_copy
-
-    def __iter__(self):
-        for i in range(self.shape[0]):
-            yield self[i]
-
-    def _init_shape(
-        self,
-        shape: torch.Size,
-        device: DeviceType,
-        dtype: torch.dtype,
-        transfer_ownership: bool,
-    ):
-        self._device = device
-        self._shape = shape
-        self._shape_indexed = None
-        self.np_shape = tuple(self._shape)
-        self._dtype = dtype
-        self._ndim = len(shape)
-        self._numel = prod(shape)
-
-        TRANSFER_OWNERSHIP[self.filename] = transfer_ownership
-        HAS_OWNERSHIP[self.filename] = True
-        # HAD_OWNERSHIP[self.filename] = True
-
-        self._tensor_dir = torch.zeros(0, device=device, dtype=dtype).__dir__()
-        self._save_item(shape)
-
-    def _get_memmap_array(self) -> np.memmap:
-        if self._memmap_array is None:
-            self._memmap_array = np.memmap(
-                self.filename,
-                dtype=TORCH_TO_NUMPY_DTYPE_DICT[self.dtype],
-                mode=self.mode,
-                shape=self.np_shape,
-            )
-        return self._memmap_array
-
-    def _set_memmap_array(self, value: np.memmap) -> None:
-        self._memmap_array = value
-
-    memmap_array = property(_get_memmap_array, _set_memmap_array)
-
-    def _save_item(
-        self,
-        value: torch.Tensor | torch.Size | MemmapTensor | np.ndarray,
-        idx: int | None = None,
-    ):
-        if isinstance(value, MemmapTensor):
-            np_array = value.memmap_array
-        elif isinstance(value, (torch.Tensor,)):
-            np_array = value.cpu().numpy()
-        elif isinstance(value, torch.Size):
-            # create the memmap array on disk
-            _ = self.memmap_array
-            return
-        else:
-            np_array = value
-        memmap_array = self.memmap_array
-        if idx is None:
-            memmap_array[:] = np_array
-        else:
-            memmap_array[idx] = np_array
-
-    def _copy_item(self, filename: bytes | str) -> None:
-        self.memmap_array[:] = np.memmap(
-            filename,
-            dtype=TORCH_TO_NUMPY_DTYPE_DICT[self.dtype],
-            mode="r",
-            shape=self.np_shape,
-        )
-
-    def _get_item(self, idx: IndexType, memmap_array: np.ndarray) -> np.ndarray:
-        if isinstance(idx, torch.Tensor):
-            # indexing a numpy.memmap with a torch.Tensor doesn't behave as expected, we
-            # convert to numpy.ndarray for behaviour that is consistent with indexing
-            # a torch.Tensor with a torch.Tensor
-            idx = idx.cpu().numpy()
-        elif isinstance(idx, tuple) and any(
-            isinstance(sub_index, torch.Tensor) for sub_index in idx
-        ):
-            idx = tuple(
-                # see above comment about indexing numpy.memmap with torch.Tensor
-                sub_index.cpu().numpy()
-                if isinstance(sub_index, torch.Tensor)
-                else sub_index
-                for sub_index in idx
-            )
-        elif isinstance(idx, list):
-            # wrapping list index in tuple to avoid following warning when indexing
-            # FutureWarning: Using a non-tuple sequence for multidimensional indexing
-            # is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future
-            # this will be interpreted as an array index, `arr[np.array(seq)]`, which
-            # will result either in an error or a different result.
-            idx = (idx,)
-        memmap_array = memmap_array[idx]
-        return memmap_array
-
-    def _load_item(
-        self,
-        idx: int | tuple | list | None = None,
-        memmap_array: np.ndarray | None = None,
-        from_numpy: bool = False,
-    ) -> torch.Tensor:
-        if memmap_array is None:
-            memmap_array = self.memmap_array
-        if idx is not None:
-            if not isinstance(idx, list):
-                idx = [idx]
-            for _idx in idx:
-                memmap_array = self._get_item(_idx, memmap_array)
-        out = self._np_to_tensor(memmap_array, from_numpy=from_numpy)
-        if (
-            idx is not None
-            and not isinstance(idx, (int, np.integer, slice))
-            and len(idx) == 1
-            and not (isinstance(idx, torch.Tensor) and idx.dtype is torch.bool)
-        ):  # and isinstance(idx, torch.Tensor) and len(idx) == 1:
-            size = self.shape
-            out = out.view(size)
-        return out
-
-    def _np_to_tensor(self, memmap_array: np.ndarray, from_numpy: bool) -> torch.Tensor:
-        if from_numpy:
-            return torch.from_numpy(memmap_array)
-        return torch.as_tensor(memmap_array, device=self.device)
-
-    @classmethod
-    def __torch_function__(
-        cls,
-        func: Callable,
-        types: tuple[type, ...],
-        args: tuple[Any, ...] = (),
-        kwargs: dict[str, Any] | None = None,
-    ):
-        if kwargs is None:
-            kwargs = {}
-        if func not in MEMMAP_HANDLED_FN:
-            args = tuple(a._tensor if hasattr(a, "_tensor") else a for a in args)
-            ret = func(*args, **kwargs)
-            return ret
-
-        return MEMMAP_HANDLED_FN[func](*args, **kwargs)
-
-    @property
-    def _tensor(self) -> torch.Tensor:
-        if not os.path.isfile(self.filename):
-            # close ref to file if it has been deleted -- ensures all processes
-            # loose access to a file once it's deleted
-            # see https://stackoverflow.com/questions/44691030/numpy-memmap-with-file-deletion
-            self._memmap_array = None
-        return self._load_item(self._index)
-
-    @property
-    def _tensor_from_numpy(self) -> torch.Tensor:
-        # a tensor created with `from_numpy` to make sure that changes are done in-place
-        return self._load_item(from_numpy=True)
-
-    def ndimension(self) -> int:
-        return self._ndim
-
-    def numel(self) -> int:
-        return self._numel
-
-    def clone(self) -> MemmapTensor:
-        """Clones the MemmapTensor onto another tensor.
-
-        Returns:
-            a new torch.Tensor with the same data but a new storage.
-
-        """
-        return self._tensor.clone()
-
-    def contiguous(self) -> torch.Tensor:
-        """Copies the MemmapTensor onto a torch.Tensor object.
-
-        Returns:
-            a torch.Tensor instance with the data of the MemmapTensor
-        stored on the desired device.
-
-        """
-        return self._tensor
-
-    @property
-    def device(self) -> torch.device:
-        return self._device
-
-    @device.setter
-    def device(self, device: DeviceType) -> None:
-        self._device = torch.device(device)
-
-    @property
-    def dtype(self) -> torch.dtype:
-        return self._dtype
-
-    @property
-    def shape(self) -> torch.Size:
-        if self._shape_indexed is None:
-            size = self._shape
-            idx = self._index if self._index is not None else []
-            for _idx in idx:
-                size = _getitem_batch_size(size, _idx)
-            self._shape_indexed = size
-        return self._shape_indexed
-
-    def cpu(self) -> torch.Tensor:
-        """Defines the device of the MemmapTensor as "cpu".
-
-        Returns: a MemmapTensor where device has been modified in-place
-
-        """
-        self.device = torch.device("cpu")
-        return self
-
-    def cuda(self) -> torch.Tensor:
-        """Defines the device of the MemmapTensor as "cuda".
-
-        Returns: a MemmapTensor where device has been modified in-place
-
-        """
-        self.device = torch.device("cuda")
-        return self
-
-    def numpy(self) -> np.ndarray:
-        return self._tensor.numpy()
-
-    def copy_(self, other: torch.Tensor | MemmapTensor) -> MemmapTensor:
-        if isinstance(other, MemmapTensor) and other.filename == self.filename:
-            if not self.shape == other.shape:
-                raise ValueError(
-                    f"""Cannot copy a MemmapTensor of shape {other.shape} on a
-MemmapTensor of shape {self.shape}."""
-                )
-            self._index = other._index
-            return self
-        self._save_item(other)
-        return self
-
-    def set_transfer_ownership(self, value: bool = True) -> MemmapTensor:
-        """Controls whether the ownership will be transferred to another process upon serialization/deserialization.
-
-        Args:
-            value (bool): if True, the ownership will be transferred.
-                Otherwise the process will keep ownership of the
-                MemmapTensor temp file.
-                Default = True
-
-        Returns:
-            the MemmapTensor
-
-        """
-        if not isinstance(value, bool):
-            raise TypeError(
-                f"value provided to set_transfer_ownership should be a "
-                f"boolean, got {type(value)}"
-            )
-        TRANSFER_OWNERSHIP[self.filename] = value
-        return self
-
-    def __deepcopy__(self, memo: dict[int, Any] | None = None) -> MemmapTensor:
-        warnings.warn(
-            "calling deepcopy on a memmap tensor involves loading it in memory "
-            "and recreating a memmap tensor from scratch (as no file destination "
-            "can be passed to deepcopy(...).",
-            stacklevel=2,
-        )
-        return MemmapTensor.from_tensor(self.clone())
-
-    def __del__(self) -> None:
-        if not hasattr(self, "file"):
-            return
-        # for some reason Memmap keeps 2 refs to the file
-        if (
-            HAS_OWNERSHIP
-            and HAS_OWNERSHIP.get(self.filename, False)
-            and getrefcount(self.file) <= 2
-        ):
-            if isinstance(self.file, tempfile._TemporaryFileWrapper) and os.path.isfile(
-                self.filename
-            ):
-                # only delete file if we created a temporary file. Otherwise file should
-                # persist on disk
-                os.unlink(self.filename)
-            del self.file
-
-    def __eq__(self, other: Any) -> torch.Tensor:
-        return self._tensor == other
-
-    def __or__(self, other: Any) -> torch.Tensor:
-        return self._tensor | other
-
-    def __ne__(self, other: Any) -> torch.Tensor:
-        return self._tensor != other
-
-    def __invert__(self) -> torch.Tensor:
-        return ~self._tensor
-
-    def __getattr__(self, attr: str) -> Any:
-        if attr in self.__dir__():
-            return self.__getattribute__(
-                attr
-            )  # make sure that appropriate exceptions are raised
-
-        if ("_tensor_dir" not in self.__dir__()) or (
-            attr not in self.__getattribute__("_tensor_dir")
-        ):
-            raise AttributeError(f"{attr} not found")
-        _tensor = self.__getattribute__("_tensor")
-        return getattr(_tensor, attr)
-
-    def masked_fill_(self, mask: torch.Tensor, value: float) -> MemmapTensor:
-        self.memmap_array[mask.cpu().numpy()] = value
-        return self
-
-    def __len__(self) -> int:
-        return self.shape[0] if len(self.shape) else 0
-
-    def is_shared(self) -> bool:
-        return False
-
-    def __add__(self, other: float | MemmapTensor | torch.Tensor) -> torch.Tensor:
-        return torch.add(self, other)
-
-    def __truediv__(self, other: float | MemmapTensor | torch.Tensor) -> torch.Tensor:
-        return torch.div(self, other)
-
-    def __neg__(self: float | MemmapTensor | torch.Tensor) -> torch.Tensor:
-        return torch.neg(self)
-
-    def __sub__(self, other: float | MemmapTensor | torch.Tensor) -> torch.Tensor:
-        return torch.sub(self, other)
-
-    def __matmul__(self, other: float | MemmapTensor | torch.Tensor) -> torch.Tensor:
-        return torch.matmul(self, other)
-
-    def __mul__(self, other: float | MemmapTensor | torch.Tensor) -> torch.Tensor:
-        return torch.mul(self, other)
-
-    def __pow__(self, other: float | MemmapTensor | torch.Tensor) -> torch.Tensor:
-        return torch.pow(self, other)
-
-    def __repr__(self) -> str:
-        return f"MemmapTensor(shape={self.shape}, device={self.device}, dtype={self.dtype})"
-
-    def __getitem__(self, item: IndexType) -> torch.Tensor:
-        # return self._load_item(memmap_array=self.memmap_array[item])#[item]
-        # return self._load_item()[item]
-        if isinstance(item, (NoneType, EllipsisType, int, np.integer, slice)):
-            item = (item,)
-        return MemmapTensor._create_memmap_with_index(self, item)
-
-    def __setitem__(self, idx: IndexType, value: torch.Tensor) -> None:
-        if self.device == torch.device("cpu"):
-            self._load_item()[idx] = value
-        else:
-            if isinstance(idx, torch.Tensor):
-                idx = idx.cpu()
-            elif isinstance(idx, tuple) and any(
-                isinstance(_idx, torch.Tensor) for _idx in idx
-            ):
-                idx = tuple(
-                    _idx.cpu() if isinstance(_idx, torch.Tensor) else _idx
-                    for _idx in idx
-                )
-            self.memmap_array[idx] = to_numpy(value)
-
-    def __setstate__(self, state: dict[str, Any]) -> None:
-        filename = state["filename"]
-        if state["file"] is None:
-            # state["_had_ownership"] = state["_had_ownership"]
-            # state["_has_ownership"] = delete
-            # tmpfile = tempfile.NamedTemporaryFile(delete=False)
-            # tmpfile.close()
-            tmpfile = _TemporaryFileWrapper(None, filename, delete=True)
-            tmpfile.name = filename
-            tmpfile._closer.name = filename
-            state["file"] = tmpfile
-
-        # We only set the ownership if it's not set
-        if state["transfer_ownership"]:
-            TRANSFER_OWNERSHIP[filename] = True
-        else:
-            TRANSFER_OWNERSHIP.setdefault(filename, state["transfer_ownership"])
-        if state["_has_ownership"]:
-            HAS_OWNERSHIP[filename] = True
-        else:
-            HAS_OWNERSHIP.setdefault(filename, state["_has_ownership"])
-        self.__dict__.update(state)
-
-    def __getstate__(self) -> dict[str, Any]:
-        state = self.__dict__.copy()
-        id_file = state["filename"]
-        state["file"] = None
-        state["_memmap_array"] = None
-        state["_fake"] = None
-
-        # we are abandoning ownership if we need to transfer it and if we have it
-        if HAS_OWNERSHIP[id_file] and TRANSFER_OWNERSHIP[id_file]:
-            state["_has_ownership"] = True
-            state["transfer_ownership"] = True
-            HAS_OWNERSHIP[id_file] = False
-            TRANSFER_OWNERSHIP[id_file] = False
-        else:
-            state["_has_ownership"] = False
-            state["transfer_ownership"] = False
-
-        # self._had_ownership = self._has_ownership = state["_had_ownership"]
-        return state
-
-    def to(
-        self,
-        dest: DeviceType | torch.dtype,
-        non_blocking: bool = False,
-    ) -> torch.Tensor | MemmapTensor:
-        """Maps a MemmapTensor to a given dtype or device.
-
-        Args:
-            dest (device indicator or torch.dtype): where to cast the
-                MemmapTensor. For devices, this is a lazy operation
-                (as the data is stored on physical memory). For dtypes, the
-                tensor will be retrieved, mapped to the
-                desired dtype and cast to a new MemmapTensor.
-            non_blocking (bool, optional): no-op for MemmapTensors. Default: False.
-
-        Returns: the same memmap-tensor with the changed device.
-
-        """
-        if isinstance(dest, (int, str, torch.device)):
-            dest = torch.device(dest)
-            self.device = dest
-            return self
-        elif isinstance(dest, torch.dtype):
-            return MemmapTensor.from_tensor(self._tensor.to(dest))
-        else:
-            raise NotImplementedError(
-                f"argument dest={dest} to MemmapTensor.to(dest) is not "
-                f"handled. "
-                f"Please provide a dtype or a device."
-            )
-
-    def unbind(self, dim: int) -> tuple[torch.Tensor, ...]:
-        """Unbinds a MemmapTensor along the desired dimension.
-
-        Args:
-            dim (int): dimension along which the MemmapTensor will be split.
-
-        Returns:
-            A tuple of indexed MemmapTensors that share the same storage.
-
-        """
-        idx = [(*(slice(None) for _ in range(dim)), i) for i in range(self.shape[dim])]
-        return tuple(self[_idx] for _idx in idx)
-
-    def as_tensor(self) -> torch.Tensor:
-        """Represents a MemmapTensor as a tensor, with the same storage (ie without any copy)."""
-        if not self.device.type == "cpu":
-            raise RuntimeError(
-                f"memmap.as_tensor() can only be called with MemmapTensors stored on CPU. Got device={self.device}."
-            )
-        # TorchSnapshot doesn't know how to stream MemmapTensor, so we view MemmapTensor
-        # as a Tensor for saving and loading purposes. This doesn't incur any copy.
-        if self._index:
-            indexed_memmap = self._get_item(self._index[0], self.memmap_array)
-            for _idx in self._index[1:]:
-                indexed_memmap = self._get_item(_idx, indexed_memmap)
-            return tensor_from_memoryview(
-                dtype=self.dtype,
-                shape=list(self.shape),
-                mv=memoryview(indexed_memmap),
-            )
-        return tensor_from_memoryview(
-            dtype=self.dtype,
-            shape=list(self.shape),
-            mv=memoryview(self.memmap_array),
-        )
-
-    # backward compatibility
-    @property
-    def _has_ownership(self):
-        return HAS_OWNERSHIP[self.filename]
-
-    # @property
-    # def _had_ownership(self):
-    #     return HAD_OWNERSHIP[self.filename]
-
-    @property
-    def transfer_ownership(self):
-        return TRANSFER_OWNERSHIP[self.filename]
-
-
-def tensor_from_memoryview(
-    mv: memoryview, dtype: torch.dtype, shape: Sequence[int]
-) -> torch.Tensor:
-    # From torchsnapshot
-    # PyTorch issues a warning if the given memoryview is non-writable. This is
-    # not a concern for torchsnapshot, as tensors created from non-writable
-    # buffers are all read-only, intermediate tensors.
-    with warnings.catch_warnings():
-        warnings.simplefilter("ignore")
-        return torch.reshape(torch.frombuffer(mv, dtype=dtype), shape)
-
-
-def _stack(
-    sequence_of_memmap: Sequence[MemmapTensor],
-    dim: int,
-    out: torch.Tensor | MemmapTensor | None = None,
-) -> torch.Tensor:
-    list_of_tensors = [
-        a._tensor if isinstance(a, MemmapTensor) else a for a in sequence_of_memmap
-    ]
-    if isinstance(out, MemmapTensor):
-        list_of_tensors = [tensor.cpu() for tensor in list_of_tensors]
-        return torch.stack(list_of_tensors, dim, out=out._tensor_from_numpy)
-    else:
-        return torch.stack(list_of_tensors, dim, out=out)
-
-
-implements_for_memmap(torch.stack)(_stack)
-
-
-def _unbind(memmap: MemmapTensor, dim: int) -> tuple[torch.Tensor, ...]:
-    return memmap.unbind(dim)
-
-
-implements_for_memmap(torch.unbind)(_unbind)
-
-
-def _tensor(memmap: MemmapTensor) -> torch.Tensor:
-    return memmap._tensor
-
-
-implements_for_memmap(torch.tensor)(_tensor)
-
-
-def _cat(
-    sequence_of_memmap: Sequence[MemmapTensor],
-    dim: int,
-    out: torch.Tensor | MemmapTensor | None = None,
-) -> torch.Tensor:
-    list_of_tensors = [
-        a._tensor if isinstance(a, MemmapTensor) else a for a in sequence_of_memmap
-    ]
-    return torch.cat(list_of_tensors, dim, out=out)
-
-
-implements_for_memmap(torch.cat)(_cat)
-
-
-def set_transfer_ownership(memmap: MemmapTensor, value: bool = True) -> None:
-    """Changes the transfer_ownership attribute of a MemmapTensor."""
-    if isinstance(memmap, MemmapTensor):
-        memmap.set_transfer_ownership(value)
-
-
-def memmap_tensor_as_tensor(
-    mem_map_tensor: torch.Tensor | MemmapTensor,
-) -> torch.Tensor:
-    if not isinstance(mem_map_tensor, MemmapTensor):
-        return mem_map_tensor
-    # TorchSnapshot doesn't know how to stream MemmapTensor, so we view MemmapTensor
-    # as a Tensor for saving and loading purposes. This doesn't incur any copy.
-    return tensor_from_memoryview(
-        dtype=mem_map_tensor.dtype,
-        shape=list(mem_map_tensor.shape),
-        mv=memoryview(mem_map_tensor._memmap_array),
-    )
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import functools
+import os
+import tempfile
+import warnings
+from copy import copy, deepcopy
+from pathlib import Path
+from sys import getrefcount
+from tempfile import _TemporaryFileWrapper
+from typing import Any, Callable, Sequence
+
+import numpy as np
+import torch
+
+from tensordict.utils import (
+    _getitem_batch_size,
+    convert_ellipsis_to_idx,
+    DeviceType,
+    IndexType,
+    NUMPY_TO_TORCH_DTYPE_DICT,
+    prod,
+    TORCH_TO_NUMPY_DTYPE_DICT,
+)
+
+__all__ = ["MemmapTensor", "set_transfer_ownership"]
+
+
+NoneType = type(None)
+EllipsisType = type(Ellipsis)
+
+
+MEMMAP_HANDLED_FN: dict[Callable, Callable] = {}
+HAS_OWNERSHIP = {}
+# HAD_OWNERSHIP = {}
+TRANSFER_OWNERSHIP = {}
+
+
+def implements_for_memmap(torch_function: Callable) -> Callable:
+    """Register a torch function override for ScalarTensor."""
+
+    @functools.wraps(torch_function)
+    def decorator(func: Callable) -> Callable:
+        MEMMAP_HANDLED_FN[torch_function] = func
+        return func
+
+    return decorator
+
+
+def to_numpy(tensor: torch.Tensor | np.ndarray) -> np.ndarray:
+    if isinstance(tensor, torch.Tensor):
+        return tensor.detach().cpu().numpy()
+    else:
+        return tensor
+
+
+class MemmapTensor:
+    """A torch.tensor interface with a np.memmap array.
+
+    A temporary file is created and cleared once the object is out-of-scope.
+    This class is aimed at being used for data transfer in between processes
+    and remote workers that have access to
+    a common storage, and as such it supports serialization and
+    deserialization. It is possible to choose if the ownership is
+    transferred upon serialization / deserialization: If ownership is not
+    transferred (transfer_ownership=False, default), then the process where
+    the MemmapTensor was created will be responsible of clearing it once it
+    gets out of scope (in that process). Otherwise, the process that
+    deserialize the MemmapTensor will be responsible of clearing the files
+    once the object is out of scope.
+
+    Supports (almost) all tensor operations.
+
+    Args:
+        *tensor_or_size (torch.Tensor, MemmapTensor, torch.Size or sequence of integers):
+            If a size is provided (with a sequence of integers, a torch.Size object
+            or a list/tuple of integers) it indicates the size of the MemmapTensor created.
+            If a te is provided, its content will be stored on physical storage.
+            If MemmapTensor, a new MemmapTensor is created and the same data is stored in it.
+        device (torch.device or equivalent, optional): device where the loaded
+            tensor will be sent. This should not be used with MemmapTensors
+            created from torch.Tensor objects. Default is "cpu".
+        dtype (torch.dtype, optional): dtype of the loaded tensor.
+            This should not be used with MemmapTensors created from torch.Tensor
+            objects. Default is :obj:`torch.get_default_dtype()`.
+        transfer_ownership (bool, optional): affects the ownership after serialization:
+            if True, the current process looses ownership immediately after
+            serialization. If False, the current process keeps the ownership
+            of the temporary file.
+            Default: False.
+        prefix (str or path, optional): *Deprecated* prefix of the file location. Should
+            not be specified together with prefix.
+        filename (str or path, optional): location of the underlying memory-map. Should
+            not be specified together with prefix.
+
+    Examples:
+        >>> x = torch.ones(3,4)
+        >>> x_memmap = MemmapTensor.from_tensor(x)
+        >>> # indexing
+        >>> x0 = x_memmap[0]
+        >>> x0[:] = 2
+        >>> assert (x_memmap[0]==2).all()
+        >>>
+        >>> # device
+        >>> x = x.to('cuda:0')
+        >>> x_memmap = MemmapTensor.from_tensor(x)
+        >>> assert (x_memmap.clone()).device == torch.device('cuda:0')
+        >>>
+        >>> # operations
+        >>> assert (x_memmap + 1 == x+1).all()
+        >>> assert (x_memmap / 2 == x/2).all()
+        >>> assert (x_memmap * 2 == x*2).all()
+        >>>
+        >>> # temp file clearance
+        >>> filename = x_memmap.filename
+        >>> assert os.path.isfile(filename)
+        >>> del x_memmap
+        >>> assert not os.path.isfile(filename)
+
+    """
+
+    requires_grad: bool = False
+
+    def __init__(
+        self,
+        *size: int,
+        device: DeviceType | None = None,
+        dtype: torch.dtype | None = None,
+        transfer_ownership: bool = False,
+        prefix: str | None = None,
+        filename: str | None = None,
+        mode: str = "r+",
+    ) -> None:
+        self.idx = None
+        self._memmap_array = None
+        self.prefix = prefix
+        self.is_meta = False
+
+        if mode in ("r+", "w+", "c", "copyonwrite", "readwrite", "write"):
+            self.mode = mode
+        else:
+            raise ValueError(
+                'Accepted values for mode are "r+", "readwrite", "w+", "write", "c" or '
+                '"copyonwrite". PyTorch does not support tensors backed by read-only '
+                'NumPy arrays, so "r" and "readonly" are not supported.'
+            )
+
+        if prefix is not None:
+            warnings.warn(
+                "prefix has been deprecated. If you want to control the location of "
+                "the MemmapTensor on disk, consider using filename instead.",
+                stacklevel=2,
+            )
+            if filename is not None:
+                raise ValueError("filename and prefix should not both be specified")
+
+        # open the files in r+ mode so as to not overwrite any data that might exist
+        # there. the actual memmap will be instantiated with user-supplied mode
+        if filename is None:
+            self.file = tempfile.NamedTemporaryFile(
+                prefix=prefix, delete=False, mode="r+"
+            )
+        else:
+            # if filename doesn't exist we must create it
+            Path(filename).touch(exist_ok=True)
+            self.file = open(filename, mode="r+")
+
+        self.filename = copy(self.file.name)
+        self.file.close()  # we close the file for now, but don't delete it
+
+        if isinstance(size[0], (torch.Tensor, MemmapTensor, np.ndarray)):
+            raise NotImplementedError(
+                "Creating a Memmap array from a tensor is not permitted anymore. "
+                "Call MemmapTensor.from_tensor(tensor) instead."
+            )
+        else:
+            try:
+                shape = (
+                    torch.Size(list(size[0]))
+                    if len(size) == 1 and not isinstance(size[0], int)
+                    else torch.Size(list(size))
+                )
+            except TypeError:
+                raise TypeError(
+                    f"The *size must be either a single list or tuple of ints, or a sequence of ints. Got {size} instead."
+                )
+            device = device if device is not None else torch.device("cpu")
+            dtype = dtype if dtype is not None else torch.get_default_dtype()
+            self._init_shape(
+                shape=shape,
+                device=device,
+                dtype=dtype,
+                transfer_ownership=transfer_ownership,
+            )
+        if not hasattr(self, "_index"):
+            self._index = None
+
+    @classmethod
+    def from_tensor(
+        cls,
+        tensor: torch.Tensor | MemmapTensor | np.ndarray,
+        transfer_ownership: bool = False,
+        prefix: str | None = None,
+        filename: str | None = None,
+        mode: str = "r+",
+    ) -> MemmapTensor:
+        if isinstance(tensor, MemmapTensor):
+            if transfer_ownership:
+                raise RuntimeError(
+                    "from_tensor(memmap_tensor, transfer_ownership=True) is not permitted, as this method will "
+                    "simply return the original MemmapTensor instance."
+                )
+            elif prefix is None and (
+                filename is None
+                or Path(filename).absolute() == Path(tensor.filename).absolute()
+            ):
+                # either location was not specified, or memmap is already in the
+                # correct location, so just return the MemmapTensor unmodified
+                return tensor
+        elif isinstance(tensor, np.ndarray):
+            raise TypeError(
+                "Convert input to torch.Tensor before calling MemmapTensor."
+            )
+        if tensor.requires_grad:
+            raise RuntimeError(
+                "MemmapTensor is incompatible with tensor.requires_grad."
+            )
+        device = tensor.device if hasattr(tensor, "device") else torch.device("cpu")
+        dtype = (
+            tensor.dtype
+            if isinstance(tensor, (torch.Tensor, MemmapTensor))
+            else NUMPY_TO_TORCH_DTYPE_DICT[np.dtype(tensor.dtype.name)]
+        )
+        shape = tensor.shape
+        out = cls(
+            shape,
+            device=device,
+            dtype=dtype,
+            prefix=prefix,
+            transfer_ownership=transfer_ownership,
+            filename=filename,
+            mode=mode,
+        )
+        out.copy_(tensor)
+        return out
+
+    @classmethod
+    def empty_like(
+        cls,
+        tensor: torch.Tensor | MemmapTensor,
+        transfer_ownership: bool = False,
+        prefix: str | None = None,
+        filename: str | None = None,
+        mode: str = "r+",
+    ) -> MemmapTensor:
+        if isinstance(tensor, np.ndarray):
+            raise TypeError(
+                "Convert input to torch.Tensor before calling MemmapTensor."
+            )
+        device = tensor.device
+        dtype = tensor.dtype
+        shape = tensor.shape
+        out = cls(
+            shape,
+            device=device,
+            dtype=dtype,
+            prefix=prefix,
+            transfer_ownership=transfer_ownership,
+            filename=filename,
+            mode=mode,
+        )
+        return out
+
+    @staticmethod
+    def _create_memmap_with_index(memmap_tensor, index):
+        memmap_copy = copy(memmap_tensor)
+        if memmap_copy._index is None:
+            memmap_copy._index = []
+        else:
+            # avoid extending someone else's index
+            memmap_copy._index = deepcopy(memmap_copy._index)
+        memmap_copy._index.append(convert_ellipsis_to_idx(index, memmap_tensor.shape))
+        memmap_copy._shape_indexed = None
+        memmap_copy.file = memmap_tensor.file
+        memmap_copy._memmap_array = memmap_tensor._memmap_array
+
+        return memmap_copy
+
+    def __iter__(self):
+        for i in range(self.shape[0]):
+            yield self[i]
+
+    def _init_shape(
+        self,
+        shape: torch.Size,
+        device: DeviceType,
+        dtype: torch.dtype,
+        transfer_ownership: bool,
+    ):
+        self._device = device
+        self._shape = shape
+        self._shape_indexed = None
+        self.np_shape = tuple(self._shape)
+        self._dtype = dtype
+        self._ndim = len(shape)
+        self._numel = prod(shape)
+
+        TRANSFER_OWNERSHIP[self.filename] = transfer_ownership
+        HAS_OWNERSHIP[self.filename] = True
+        # HAD_OWNERSHIP[self.filename] = True
+
+        self._tensor_dir = torch.zeros(0, device=device, dtype=dtype).__dir__()
+        self._save_item(shape)
+
+    def _get_memmap_array(self) -> np.memmap:
+        if self._memmap_array is None:
+            self._memmap_array = np.memmap(
+                self.filename,
+                dtype=TORCH_TO_NUMPY_DTYPE_DICT[self.dtype],
+                mode=self.mode,
+                shape=self.np_shape,
+            )
+        return self._memmap_array
+
+    def _set_memmap_array(self, value: np.memmap) -> None:
+        self._memmap_array = value
+
+    memmap_array = property(_get_memmap_array, _set_memmap_array)
+
+    def _save_item(
+        self,
+        value: torch.Tensor | torch.Size | MemmapTensor | np.ndarray,
+        idx: int | None = None,
+    ):
+        if isinstance(value, MemmapTensor):
+            np_array = value.memmap_array
+        elif isinstance(value, (torch.Tensor,)):
+            np_array = value.cpu().numpy()
+        elif isinstance(value, torch.Size):
+            # create the memmap array on disk
+            _ = self.memmap_array
+            return
+        else:
+            np_array = value
+        memmap_array = self.memmap_array
+        if idx is None:
+            memmap_array[:] = np_array
+        else:
+            memmap_array[idx] = np_array
+
+    def _copy_item(self, filename: bytes | str) -> None:
+        self.memmap_array[:] = np.memmap(
+            filename,
+            dtype=TORCH_TO_NUMPY_DTYPE_DICT[self.dtype],
+            mode="r",
+            shape=self.np_shape,
+        )
+
+    def _get_item(self, idx: IndexType, memmap_array: np.ndarray) -> np.ndarray:
+        if isinstance(idx, torch.Tensor):
+            # indexing a numpy.memmap with a torch.Tensor doesn't behave as expected, we
+            # convert to numpy.ndarray for behaviour that is consistent with indexing
+            # a torch.Tensor with a torch.Tensor
+            idx = idx.cpu().numpy()
+        elif isinstance(idx, tuple) and any(
+            isinstance(sub_index, torch.Tensor) for sub_index in idx
+        ):
+            idx = tuple(
+                # see above comment about indexing numpy.memmap with torch.Tensor
+                sub_index.cpu().numpy()
+                if isinstance(sub_index, torch.Tensor)
+                else sub_index
+                for sub_index in idx
+            )
+        elif isinstance(idx, list):
+            # wrapping list index in tuple to avoid following warning when indexing
+            # FutureWarning: Using a non-tuple sequence for multidimensional indexing
+            # is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future
+            # this will be interpreted as an array index, `arr[np.array(seq)]`, which
+            # will result either in an error or a different result.
+            idx = (idx,)
+        memmap_array = memmap_array[idx]
+        return memmap_array
+
+    def _load_item(
+        self,
+        idx: int | tuple | list | None = None,
+        memmap_array: np.ndarray | None = None,
+        from_numpy: bool = False,
+    ) -> torch.Tensor:
+        if memmap_array is None:
+            memmap_array = self.memmap_array
+        if idx is not None:
+            if not isinstance(idx, list):
+                idx = [idx]
+            for _idx in idx:
+                memmap_array = self._get_item(_idx, memmap_array)
+        out = self._np_to_tensor(memmap_array, from_numpy=from_numpy)
+        if (
+            idx is not None
+            and not isinstance(idx, (int, np.integer, slice))
+            and len(idx) == 1
+            and not (isinstance(idx, torch.Tensor) and idx.dtype is torch.bool)
+        ):  # and isinstance(idx, torch.Tensor) and len(idx) == 1:
+            size = self.shape
+            out = out.view(size)
+        return out
+
+    def _np_to_tensor(self, memmap_array: np.ndarray, from_numpy: bool) -> torch.Tensor:
+        if from_numpy:
+            return torch.from_numpy(memmap_array)
+        return torch.as_tensor(memmap_array, device=self.device)
+
+    @classmethod
+    def __torch_function__(
+        cls,
+        func: Callable,
+        types: tuple[type, ...],
+        args: tuple[Any, ...] = (),
+        kwargs: dict[str, Any] | None = None,
+    ):
+        if kwargs is None:
+            kwargs = {}
+        if func not in MEMMAP_HANDLED_FN:
+            args = tuple(a._tensor if hasattr(a, "_tensor") else a for a in args)
+            ret = func(*args, **kwargs)
+            return ret
+
+        return MEMMAP_HANDLED_FN[func](*args, **kwargs)
+
+    @property
+    def _tensor(self) -> torch.Tensor:
+        if not os.path.isfile(self.filename):
+            # close ref to file if it has been deleted -- ensures all processes
+            # loose access to a file once it's deleted
+            # see https://stackoverflow.com/questions/44691030/numpy-memmap-with-file-deletion
+            self._memmap_array = None
+        return self._load_item(self._index)
+
+    @property
+    def _tensor_from_numpy(self) -> torch.Tensor:
+        # a tensor created with `from_numpy` to make sure that changes are done in-place
+        return self._load_item(from_numpy=True)
+
+    def ndimension(self) -> int:
+        return self._ndim
+
+    def numel(self) -> int:
+        return self._numel
+
+    def clone(self) -> MemmapTensor:
+        """Clones the MemmapTensor onto another tensor.
+
+        Returns:
+            a new torch.Tensor with the same data but a new storage.
+
+        """
+        return self._tensor.clone()
+
+    def contiguous(self) -> torch.Tensor:
+        """Copies the MemmapTensor onto a torch.Tensor object.
+
+        Returns:
+            a torch.Tensor instance with the data of the MemmapTensor
+        stored on the desired device.
+
+        """
+        return self._tensor
+
+    @property
+    def device(self) -> torch.device:
+        return self._device
+
+    @device.setter
+    def device(self, device: DeviceType) -> None:
+        self._device = torch.device(device)
+
+    @property
+    def dtype(self) -> torch.dtype:
+        return self._dtype
+
+    @property
+    def shape(self) -> torch.Size:
+        if self._shape_indexed is None:
+            size = self._shape
+            idx = self._index if self._index is not None else []
+            for _idx in idx:
+                size = _getitem_batch_size(size, _idx)
+            self._shape_indexed = size
+        return self._shape_indexed
+
+    def cpu(self) -> torch.Tensor:
+        """Defines the device of the MemmapTensor as "cpu".
+
+        Returns: a MemmapTensor where device has been modified in-place
+
+        """
+        self.device = torch.device("cpu")
+        return self
+
+    def cuda(self) -> torch.Tensor:
+        """Defines the device of the MemmapTensor as "cuda".
+
+        Returns: a MemmapTensor where device has been modified in-place
+
+        """
+        self.device = torch.device("cuda")
+        return self
+
+    def numpy(self) -> np.ndarray:
+        return self._tensor.numpy()
+
+    def copy_(self, other: torch.Tensor | MemmapTensor) -> MemmapTensor:
+        if isinstance(other, MemmapTensor) and other.filename == self.filename:
+            if not self.shape == other.shape:
+                raise ValueError(
+                    f"""Cannot copy a MemmapTensor of shape {other.shape} on a
+MemmapTensor of shape {self.shape}."""
+                )
+            self._index = other._index
+            return self
+        self._save_item(other)
+        return self
+
+    def set_transfer_ownership(self, value: bool = True) -> MemmapTensor:
+        """Controls whether the ownership will be transferred to another process upon serialization/deserialization.
+
+        Args:
+            value (bool): if True, the ownership will be transferred.
+                Otherwise the process will keep ownership of the
+                MemmapTensor temp file.
+                Default = True
+
+        Returns:
+            the MemmapTensor
+
+        """
+        if not isinstance(value, bool):
+            raise TypeError(
+                f"value provided to set_transfer_ownership should be a "
+                f"boolean, got {type(value)}"
+            )
+        TRANSFER_OWNERSHIP[self.filename] = value
+        return self
+
+    def __deepcopy__(self, memo: dict[int, Any] | None = None) -> MemmapTensor:
+        warnings.warn(
+            "calling deepcopy on a memmap tensor involves loading it in memory "
+            "and recreating a memmap tensor from scratch (as no file destination "
+            "can be passed to deepcopy(...).",
+            stacklevel=2,
+        )
+        return MemmapTensor.from_tensor(self.clone())
+
+    def __del__(self) -> None:
+        if not hasattr(self, "file"):
+            return
+        # for some reason Memmap keeps 2 refs to the file
+        if (
+            HAS_OWNERSHIP
+            and HAS_OWNERSHIP.get(self.filename, False)
+            and getrefcount(self.file) <= 2
+        ):
+            if isinstance(self.file, tempfile._TemporaryFileWrapper) and os.path.isfile(
+                self.filename
+            ):
+                # only delete file if we created a temporary file. Otherwise file should
+                # persist on disk
+                os.unlink(self.filename)
+            del self.file
+
+    def __eq__(self, other: Any) -> torch.Tensor:
+        return self._tensor == other
+
+    def __or__(self, other: Any) -> torch.Tensor:
+        return self._tensor | other
+
+    def __ne__(self, other: Any) -> torch.Tensor:
+        return self._tensor != other
+
+    def __invert__(self) -> torch.Tensor:
+        return ~self._tensor
+
+    def __getattr__(self, attr: str) -> Any:
+        if attr in self.__dir__():
+            return self.__getattribute__(
+                attr
+            )  # make sure that appropriate exceptions are raised
+
+        if ("_tensor_dir" not in self.__dir__()) or (
+            attr not in self.__getattribute__("_tensor_dir")
+        ):
+            raise AttributeError(f"{attr} not found")
+        _tensor = self.__getattribute__("_tensor")
+        return getattr(_tensor, attr)
+
+    def masked_fill_(self, mask: torch.Tensor, value: float) -> MemmapTensor:
+        self.memmap_array[mask.cpu().numpy()] = value
+        return self
+
+    def __len__(self) -> int:
+        return self.shape[0] if len(self.shape) else 0
+
+    def is_shared(self) -> bool:
+        return False
+
+    def __add__(self, other: float | MemmapTensor | torch.Tensor) -> torch.Tensor:
+        return torch.add(self, other)
+
+    def __truediv__(self, other: float | MemmapTensor | torch.Tensor) -> torch.Tensor:
+        return torch.div(self, other)
+
+    def __neg__(self: float | MemmapTensor | torch.Tensor) -> torch.Tensor:
+        return torch.neg(self)
+
+    def __sub__(self, other: float | MemmapTensor | torch.Tensor) -> torch.Tensor:
+        return torch.sub(self, other)
+
+    def __matmul__(self, other: float | MemmapTensor | torch.Tensor) -> torch.Tensor:
+        return torch.matmul(self, other)
+
+    def __mul__(self, other: float | MemmapTensor | torch.Tensor) -> torch.Tensor:
+        return torch.mul(self, other)
+
+    def __pow__(self, other: float | MemmapTensor | torch.Tensor) -> torch.Tensor:
+        return torch.pow(self, other)
+
+    def __repr__(self) -> str:
+        return f"MemmapTensor(shape={self.shape}, device={self.device}, dtype={self.dtype})"
+
+    def __getitem__(self, item: IndexType) -> torch.Tensor:
+        # return self._load_item(memmap_array=self.memmap_array[item])#[item]
+        # return self._load_item()[item]
+        if isinstance(item, (NoneType, EllipsisType, int, np.integer, slice)):
+            item = (item,)
+        return MemmapTensor._create_memmap_with_index(self, item)
+
+    def __setitem__(self, idx: IndexType, value: torch.Tensor) -> None:
+        if self.device == torch.device("cpu"):
+            self._load_item()[idx] = value
+        else:
+            if isinstance(idx, torch.Tensor):
+                idx = idx.cpu()
+            elif isinstance(idx, tuple) and any(
+                isinstance(_idx, torch.Tensor) for _idx in idx
+            ):
+                idx = tuple(
+                    _idx.cpu() if isinstance(_idx, torch.Tensor) else _idx
+                    for _idx in idx
+                )
+            self.memmap_array[idx] = to_numpy(value)
+
+    def __setstate__(self, state: dict[str, Any]) -> None:
+        filename = state["filename"]
+        if state["file"] is None:
+            # state["_had_ownership"] = state["_had_ownership"]
+            # state["_has_ownership"] = delete
+            # tmpfile = tempfile.NamedTemporaryFile(delete=False)
+            # tmpfile.close()
+            tmpfile = _TemporaryFileWrapper(None, filename, delete=True)
+            tmpfile.name = filename
+            tmpfile._closer.name = filename
+            state["file"] = tmpfile
+
+        # We only set the ownership if it's not set
+        if state["transfer_ownership"]:
+            TRANSFER_OWNERSHIP[filename] = True
+        else:
+            TRANSFER_OWNERSHIP.setdefault(filename, state["transfer_ownership"])
+        if state["_has_ownership"]:
+            HAS_OWNERSHIP[filename] = True
+        else:
+            HAS_OWNERSHIP.setdefault(filename, state["_has_ownership"])
+        self.__dict__.update(state)
+
+    def __getstate__(self) -> dict[str, Any]:
+        state = self.__dict__.copy()
+        id_file = state["filename"]
+        state["file"] = None
+        state["_memmap_array"] = None
+        state["_fake"] = None
+
+        # we are abandoning ownership if we need to transfer it and if we have it
+        if HAS_OWNERSHIP[id_file] and TRANSFER_OWNERSHIP[id_file]:
+            state["_has_ownership"] = True
+            state["transfer_ownership"] = True
+            HAS_OWNERSHIP[id_file] = False
+            TRANSFER_OWNERSHIP[id_file] = False
+        else:
+            state["_has_ownership"] = False
+            state["transfer_ownership"] = False
+
+        # self._had_ownership = self._has_ownership = state["_had_ownership"]
+        return state
+
+    def to(
+        self,
+        dest: DeviceType | torch.dtype,
+        non_blocking: bool = False,
+    ) -> torch.Tensor | MemmapTensor:
+        """Maps a MemmapTensor to a given dtype or device.
+
+        Args:
+            dest (device indicator or torch.dtype): where to cast the
+                MemmapTensor. For devices, this is a lazy operation
+                (as the data is stored on physical memory). For dtypes, the
+                tensor will be retrieved, mapped to the
+                desired dtype and cast to a new MemmapTensor.
+            non_blocking (bool, optional): no-op for MemmapTensors. Default: False.
+
+        Returns: the same memmap-tensor with the changed device.
+
+        """
+        if isinstance(dest, (int, str, torch.device)):
+            dest = torch.device(dest)
+            self.device = dest
+            return self
+        elif isinstance(dest, torch.dtype):
+            return MemmapTensor.from_tensor(self._tensor.to(dest))
+        else:
+            raise NotImplementedError(
+                f"argument dest={dest} to MemmapTensor.to(dest) is not "
+                f"handled. "
+                f"Please provide a dtype or a device."
+            )
+
+    def unbind(self, dim: int) -> tuple[torch.Tensor, ...]:
+        """Unbinds a MemmapTensor along the desired dimension.
+
+        Args:
+            dim (int): dimension along which the MemmapTensor will be split.
+
+        Returns:
+            A tuple of indexed MemmapTensors that share the same storage.
+
+        """
+        idx = [(*(slice(None) for _ in range(dim)), i) for i in range(self.shape[dim])]
+        return tuple(self[_idx] for _idx in idx)
+
+    def as_tensor(self) -> torch.Tensor:
+        """Represents a MemmapTensor as a tensor, with the same storage (ie without any copy)."""
+        if not self.device.type == "cpu":
+            raise RuntimeError(
+                f"memmap.as_tensor() can only be called with MemmapTensors stored on CPU. Got device={self.device}."
+            )
+        # TorchSnapshot doesn't know how to stream MemmapTensor, so we view MemmapTensor
+        # as a Tensor for saving and loading purposes. This doesn't incur any copy.
+        if self._index:
+            indexed_memmap = self._get_item(self._index[0], self.memmap_array)
+            for _idx in self._index[1:]:
+                indexed_memmap = self._get_item(_idx, indexed_memmap)
+            return tensor_from_memoryview(
+                dtype=self.dtype,
+                shape=list(self.shape),
+                mv=memoryview(indexed_memmap),
+            )
+        return tensor_from_memoryview(
+            dtype=self.dtype,
+            shape=list(self.shape),
+            mv=memoryview(self.memmap_array),
+        )
+
+    # backward compatibility
+    @property
+    def _has_ownership(self):
+        return HAS_OWNERSHIP[self.filename]
+
+    # @property
+    # def _had_ownership(self):
+    #     return HAD_OWNERSHIP[self.filename]
+
+    @property
+    def transfer_ownership(self):
+        return TRANSFER_OWNERSHIP[self.filename]
+
+
+def tensor_from_memoryview(
+    mv: memoryview, dtype: torch.dtype, shape: Sequence[int]
+) -> torch.Tensor:
+    # From torchsnapshot
+    # PyTorch issues a warning if the given memoryview is non-writable. This is
+    # not a concern for torchsnapshot, as tensors created from non-writable
+    # buffers are all read-only, intermediate tensors.
+    with warnings.catch_warnings():
+        warnings.simplefilter("ignore")
+        return torch.reshape(torch.frombuffer(mv, dtype=dtype), shape)
+
+
+def _stack(
+    sequence_of_memmap: Sequence[MemmapTensor],
+    dim: int,
+    out: torch.Tensor | MemmapTensor | None = None,
+) -> torch.Tensor:
+    list_of_tensors = [
+        a._tensor if isinstance(a, MemmapTensor) else a for a in sequence_of_memmap
+    ]
+    if isinstance(out, MemmapTensor):
+        list_of_tensors = [tensor.cpu() for tensor in list_of_tensors]
+        return torch.stack(list_of_tensors, dim, out=out._tensor_from_numpy)
+    else:
+        return torch.stack(list_of_tensors, dim, out=out)
+
+
+implements_for_memmap(torch.stack)(_stack)
+
+
+def _unbind(memmap: MemmapTensor, dim: int) -> tuple[torch.Tensor, ...]:
+    return memmap.unbind(dim)
+
+
+implements_for_memmap(torch.unbind)(_unbind)
+
+
+def _tensor(memmap: MemmapTensor) -> torch.Tensor:
+    return memmap._tensor
+
+
+implements_for_memmap(torch.tensor)(_tensor)
+
+
+def _cat(
+    sequence_of_memmap: Sequence[MemmapTensor],
+    dim: int,
+    out: torch.Tensor | MemmapTensor | None = None,
+) -> torch.Tensor:
+    list_of_tensors = [
+        a._tensor if isinstance(a, MemmapTensor) else a for a in sequence_of_memmap
+    ]
+    return torch.cat(list_of_tensors, dim, out=out)
+
+
+implements_for_memmap(torch.cat)(_cat)
+
+
+def set_transfer_ownership(memmap: MemmapTensor, value: bool = True) -> None:
+    """Changes the transfer_ownership attribute of a MemmapTensor."""
+    if isinstance(memmap, MemmapTensor):
+        memmap.set_transfer_ownership(value)
+
+
+def memmap_tensor_as_tensor(
+    mem_map_tensor: torch.Tensor | MemmapTensor,
+) -> torch.Tensor:
+    if not isinstance(mem_map_tensor, MemmapTensor):
+        return mem_map_tensor
+    # TorchSnapshot doesn't know how to stream MemmapTensor, so we view MemmapTensor
+    # as a Tensor for saving and loading purposes. This doesn't incur any copy.
+    return tensor_from_memoryview(
+        dtype=mem_map_tensor.dtype,
+        shape=list(mem_map_tensor.shape),
+        mv=memoryview(mem_map_tensor._memmap_array),
+    )
```

## tensordict/persistent.py

```diff
@@ -1,936 +1,939 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-"""Persistent tensordicts (H5 and others)."""
-from __future__ import annotations
-
-import tempfile
-import warnings
-from pathlib import Path
-from typing import Any
-
-from tensordict._tensordict import _unravel_key_to_tuple
-
-H5_ERR = None
-try:
-    import h5py
-
-    _has_h5 = True
-except ModuleNotFoundError as err:
-    H5_ERR = err
-    _has_h5 = False
-
-import numpy as np
-import torch
-
-from tensordict import MemmapTensor
-from tensordict.tensordict import (
-    _TensorDictKeysView,
-    CompatibleType,
-    is_tensor_collection,
-    NO_DEFAULT,
-    TensorDict,
-    TensorDictBase,
-)
-from tensordict.utils import (
-    cache,
-    DeviceType,
-    expand_right,
-    IndexType,
-    lock_blocked,
-    NestedKey,
-    NUMPY_TO_TORCH_DTYPE_DICT,
-)
-
-
-class _Visitor:
-    def __init__(self, fun=None):
-        self.elts = []
-        self.fun = fun
-
-    def __call__(self, name):
-        self.elts.append(name)
-
-    def __iter__(self):
-        if self.fun is None:
-            yield from self.elts
-        else:
-            for elt in self.elts:
-                yield self.fun(elt)
-
-
-class _PersistentTDKeysView(_TensorDictKeysView):
-    def __iter__(self):
-        if self.include_nested:
-            visitor = _Visitor(lambda key: tuple(key.split("/")))
-            self.tensordict.file.visit(visitor)
-            if self.leaves_only:
-                for key in visitor:
-                    if self.tensordict._get_metadata(key).get("array", None):
-                        yield key
-            else:
-                yield from visitor
-        else:
-            yield from self.tensordict._valid_keys()
-
-    def __contains__(self, key):
-        if isinstance(key, tuple) and len(key) == 1:
-            key = key[0]
-        for a_key in self:
-            if isinstance(a_key, tuple) and len(a_key) == 1:
-                a_key = a_key[0]
-            if key == a_key:
-                return True
-        else:
-            return False
-
-
-class PersistentTensorDict(TensorDictBase):
-    """Persistent TensorDict implementation.
-
-    :class:`PersistentTensorDict` instances provide an interface with data stored
-    on disk such that access to this data is made easy while still taking advantage
-    from the fast access provided by the backend.
-
-    Like other :class:`TensorDictBase` subclasses, :class:`PersistentTensorDict`
-    has a ``device`` attribute. This does *not* mean that the data is being stored
-    on that device, but rather that when loaded, the data will be cast onto
-    the desired device.
-
-    Args:
-        batch_size (torch.Size or compatible): the tensordict batch size.
-        filename (str, optional): the path to the h5 file. Exclusive with ``group``.
-        group (h5py.Group, optional): a file or a group that contains data. Exclusive with ``filename``.
-        mode (str, optional): Reading mode. Defaults to ``"r"``.
-        backend (str, optional): storage backend. Currently only ``"h5"`` is supported.
-        device (torch.device or compatible, optional): device of the tensordict.
-            Defaults to ``None`` (ie. default PyTorch device).
-        **kwargs: kwargs to be passed to :meth:`h5py.File.create_dataset`.
-
-    .. note::
-      Currently, PersistentTensorDict instances are not closed when getting out-of-scope.
-      This means that it is the responsibility of the user to close them if necessary.
-
-    Examples:
-        >>> import tempfile
-        >>> with tempfile.NamedTemporaryFile() as f:
-        ...     data = PersistentTensorDict(file=f, batch_size=[3], mode="w")
-        ...     data["a", "b"] = torch.randn(3, 4)
-        ...     print(data)
-
-    """
-
-    def __new__(cls, *args, **kwargs):
-        cls._td_dim_names = None
-        return super().__new__(cls, *args, **kwargs)
-
-    def __init__(
-        self,
-        *,
-        batch_size,
-        filename=None,
-        group=None,
-        mode="r",
-        backend="h5",
-        device=None,
-        **kwargs,
-    ):
-        self._locked_tensordicts = []
-        self._lock_id = set()
-        if not _has_h5:
-            raise ModuleNotFoundError("Could not load h5py.") from H5_ERR
-        super().__init__()
-        self.filename = filename
-        self.mode = mode
-        if backend != "h5":
-            raise NotImplementedError
-        if filename is not None and group is None:
-            self.file = h5py.File(filename, mode)
-        elif group is not None:
-            self.file = group
-        else:
-            raise RuntimeError(
-                f"Either group or filename must be provided, and not both. Got group={group} and filename={filename}."
-            )
-        self._batch_size = torch.Size(batch_size)
-        self._device = device
-        self._is_shared = False
-        self._is_memmap = False
-        self.kwargs = kwargs
-
-        # we use this to allow nested tensordicts to have a different batch-size
-        self._nested_tensordicts = {}
-        self._pin_mem = False
-
-        # this must be kept last
-        self._check_batch_size(self._batch_size)
-
-    @classmethod
-    def from_h5(cls, filename, mode="r"):
-        """Creates a PersistentTensorDict from a h5 file.
-
-        This function will automatically determine the batch-size for each nested
-        tensordict.
-
-        Args:
-            filename (str): the path to the h5 file.
-            mode (str, optional): reading mode. Defaults to ``"r"``.
-        """
-        out = cls(filename=filename, mode=mode, batch_size=[])
-        # determine batch size
-        _set_max_batch_size(out)
-        return out
-
-    @classmethod
-    def from_dict(cls, input_dict, filename, batch_size=None, device=None, **kwargs):
-        """Converts a dictionary or a TensorDict to a h5 file.
-
-        Args:
-            input_dict (dict, TensorDict or compatible): data to be stored as h5.
-            filename (str or path): path to the h5 file.
-            batch_size (tensordict batch-size, optional): if provided, batch size
-                of the tensordict. If not, the batch size will be gathered from the
-                input structure (if present) or determined automatically.
-            device (torch.device or compatible, optional): the device where to
-                expect the tensor once they are returned. Defaults to ``None``
-                (on cpu by default).
-            **kwargs: kwargs to be passed to :meth:`h5py.File.create_dataset`.
-
-        Returns:
-            A :class:`PersitentTensorDict` instance linked to the newly created file.
-
-        """
-        file = h5py.File(filename, "w")
-        _has_batch_size = True
-        if batch_size is None:
-            if is_tensor_collection(input_dict):
-                batch_size = input_dict.batch_size
-            else:
-                _has_batch_size = False
-                batch_size = torch.Size([])
-
-        # let's make a tensordict first
-        out = cls(group=file, batch_size=batch_size, device=device, **kwargs)
-        if is_tensor_collection(input_dict):
-            out.update(input_dict)
-        else:
-            out.update(TensorDict(input_dict, batch_size=batch_size))
-        if not _has_batch_size:
-            _set_max_batch_size(out)
-        return out
-
-    def close(self):
-        """Closes the persistent tensordict."""
-        self.file.close()
-
-    def _process_key(self, key):
-        key = _unravel_key_to_tuple(key)
-        return "/".join(key)
-
-    def _check_batch_size(self, batch_size) -> None:
-        for key in self.keys(include_nested=True, leaves_only=True):
-            key = self._process_key(key)
-            size = self.file[key].shape
-            if torch.Size(size[: len(batch_size)]) != batch_size:
-                raise ValueError(
-                    f"batch size and array size mismatch: array.shape={size}, batch_size={batch_size}."
-                )
-
-    def _get_array(self, key, default=NO_DEFAULT):
-        try:
-            key = self._process_key(key)
-            array = self.file[key]
-            return array
-        except KeyError:
-            if default is not NO_DEFAULT:
-                return default
-            raise KeyError(f"key {key} not found in PersistentTensorDict {self}")
-
-    def _process_array(self, key, array):
-        if isinstance(array, (h5py.Dataset,)):
-            if self.device is not None:
-                device = self.device
-            else:
-                device = torch.device("cpu")
-            # we convert to an array first to avoid "Creating a tensor from a list of numpy.ndarrays is extremely slow."
-            array = array[()]
-            out = torch.as_tensor(array, device=device)
-            if self._pin_mem:
-                return out.pin_memory()
-            return out
-        else:
-            out = self._nested_tensordicts.get(key, None)
-            if out is None:
-                out = self._nested_tensordicts[key] = PersistentTensorDict(
-                    group=array,
-                    batch_size=self.batch_size,
-                    device=self.device,
-                )
-            return out
-
-    @cache  # noqa: B019
-    def get(self, key, default=NO_DEFAULT):
-        array = self._get_array(key, default)
-        if array is default:
-            return array
-        return self._process_array(key, array)
-
-    _get_str = get
-    _get_tuple = get
-
-    def get_at(
-        self, key: str, idx: IndexType, default: CompatibleType = NO_DEFAULT
-    ) -> CompatibleType:
-        array = self._get_array(key, default)
-        if isinstance(array, (h5py.Dataset,)):
-            if self.device is not None:
-                device = self.device
-            else:
-                device = torch.device("cpu")
-            # indexing must be done before converting to tensor.
-            idx = self._process_index(idx, array)
-            # `get_at` is there to save us.
-            try:
-                out = torch.as_tensor(array[idx], device=device)
-            except TypeError as err:
-                if "Boolean indexing array has incompatible shape" in str(err):
-                    # Known bug in h5py: cannot broadcast boolean mask on the right as
-                    # done in np and torch. Therefore we put a performance warning
-                    # and convert to torch tensor first.
-                    warnings.warn(
-                        "Indexing an h5py.Dataset object with a boolean mask "
-                        "that needs broadcasting does not work directly. "
-                        "tensordict will cast the entire array in memory and index it using the mask. "
-                        "This is suboptimal and may lead to performance issue."
-                    )
-                    out = torch.as_tensor(np.asarray(array), device=device)[idx]
-                else:
-                    raise err
-            if self._pin_mem:
-                return out.pin_memory()
-            return out
-        elif array is not default:
-            out = self._nested_tensordicts.get(key, None)
-            if out is None:
-                out = self._nested_tensordicts[key] = PersistentTensorDict(
-                    group=array,
-                    batch_size=self.batch_size,
-                    device=self.device,
-                )
-            return out.get_sub_tensordict(idx)
-        else:
-            return default
-
-    def _get_metadata(self, key):
-        """Gets the metadata for an entry.
-
-        This method avoids creating a tensor from scratch, and just reads the metadata of the array.
-        """
-        array = self._get_array(key)
-        if (
-            isinstance(array, (h5py.Dataset,))
-            and array.dtype in NUMPY_TO_TORCH_DTYPE_DICT
-        ):
-            shape = torch.Size(array.shape)
-            return {
-                "dtype": NUMPY_TO_TORCH_DTYPE_DICT[array.dtype],
-                "shape": shape,
-                "dim": len(shape),
-                "array": True,
-            }
-        elif (
-            isinstance(array, (h5py.Dataset,))
-            and array.dtype not in NUMPY_TO_TORCH_DTYPE_DICT
-        ):
-            return {}
-        else:
-            shape = self.get(key).shape
-            return {
-                "dtype": None,
-                "shape": shape,
-                "dim": len(shape),
-                "array": False,
-            }
-
-    @classmethod
-    def _process_index(cls, idx, array=None):
-        if isinstance(idx, tuple):
-            return tuple(cls._process_index(_idx, array) for _idx in idx)
-        if isinstance(idx, torch.Tensor):
-            return idx.cpu().detach().numpy()
-        if isinstance(idx, (range, list)):
-            return np.asarray(idx)
-        return idx
-
-    def __getitem__(self, item):
-        if isinstance(item, str) or (
-            isinstance(item, tuple) and all(isinstance(val, str) for val in item)
-        ):
-            return self.get(item)
-        if isinstance(item, list):
-            # convert to tensor
-            item = torch.tensor(item)
-        return self.get_sub_tensordict(item)
-
-    __getitems__ = __getitem__
-
-    def __setitem__(self, index, value):
-        index_unravel = _unravel_key_to_tuple(index)
-        if index_unravel:
-            return self.set(index_unravel, value, inplace=True)
-
-        if isinstance(index, list):
-            # convert to tensor
-            index = torch.tensor(index)
-        sub_td = self.get_sub_tensordict(index)
-        err_set_batch_size = None
-        if not isinstance(value, TensorDictBase):
-            value = TensorDict.from_dict(value, batch_size=[])
-            # try to assign the current shape. If that does not work, we can
-            # try to expand
-            try:
-                value.batch_size = sub_td.batch_size
-            except RuntimeError as err0:
-                err_set_batch_size = err0
-        if value.shape != sub_td.shape:
-            try:
-                value = value.expand(sub_td.shape)
-            except RuntimeError as err:
-                if err_set_batch_size is not None:
-                    raise err from err_set_batch_size
-                raise RuntimeError(
-                    f"Cannot broadcast the tensordict {value} to the shape of the indexed persistent tensordict {self}[{index}]."
-                ) from err
-        sub_td.update(value, inplace=True)
-
-    @cache  # noqa: B019
-    def _valid_keys(self):
-        keys = []
-        for key in self.file.keys():
-            if self._get_metadata(key):
-                keys.append(key)
-        return keys
-
-    # @cache  # noqa: B019
-    def keys(
-        self, include_nested: bool = False, leaves_only: bool = False
-    ) -> _PersistentTDKeysView:
-        return _PersistentTDKeysView(
-            tensordict=self,
-            include_nested=include_nested,
-            leaves_only=leaves_only,
-        )
-
-    def _items_metadata(self, include_nested=False, leaves_only=False):
-        """Iterates over the metadata of the PersistentTensorDict."""
-        for key in self.keys(include_nested, leaves_only):
-            yield (key, self._get_metadata(key))
-
-    def _values_metadata(self, include_nested=False, leaves_only=False):
-        """Iterates over the metadata of the PersistentTensorDict."""
-        for key in self.keys(include_nested, leaves_only):
-            yield self._get_metadata(key)
-
-    def _change_batch_size(self, value):
-        raise NotImplementedError
-
-    def _stack_onto_(
-        self, list_item: list[CompatibleType], dim: int
-    ) -> PersistentTensorDict:
-        for key in self.keys():
-            vals = [td._get_str(key, None) for td in list_item]
-            if all(v is None for v in vals):
-                continue
-            stacked = torch.stack(vals, dim=dim)
-            self.set_(key, stacked)
-        return self
-
-    @property
-    def batch_size(self):
-        return self._batch_size
-
-    @batch_size.setter
-    def batch_size(self, value):
-        _batch_size = self._batch_size
-        try:
-            self._batch_size = torch.Size(value)
-            self._check_batch_size(self._batch_size)
-        except ValueError:
-            self._batch_size = _batch_size
-
-    _erase_names = TensorDict._erase_names
-    names = TensorDict.names
-    _has_names = TensorDict._has_names
-
-    def _rename_subtds(self, names):
-        if names is None:
-            names = [None] * self.ndim
-        for item in self._nested_tensordicts.values():
-            if is_tensor_collection(item):
-                td_names = list(names) + [None] * (item.ndim - self.ndim)
-                item.rename_(*td_names)
-
-    def contiguous(self):
-        """Materializes a PersistentTensorDict on a regular TensorDict."""
-        return self.to_tensordict()
-
-    @lock_blocked
-    def del_(self, key):
-        key = self._process_key(key)
-        del self.file[key]
-        return self
-
-    def detach_(self):
-        # PersistentTensorDict do not carry gradients. This is a no-op
-        return self
-
-    @property
-    def device(self):
-        return self._device
-
-    def entry_class(self, key: NestedKey) -> type:
-        entry_class = self._get_metadata(key)
-        is_array = entry_class.get("array", None)
-        if is_array:
-            return torch.Tensor
-        elif is_array is False:
-            return PersistentTensorDict
-        else:
-            raise RuntimeError(f"Encountered a non-numeric data {key}.")
-
-    def is_contiguous(self):
-        return False
-
-    def masked_fill(self, mask, value):
-        return self.to_tensordict().masked_fill(mask, value)
-
-    def masked_fill_(self, mask, value):
-        for key in self.keys(include_nested=True, leaves_only=True):
-            array = self._get_array(key)
-            array[expand_right(mask, array.shape).cpu().numpy()] = value
-        return self
-
-    def memmap_(
-        self, prefix: str | None = None, copy_existing: bool = False
-    ) -> PersistentTensorDict:
-        raise RuntimeError(
-            "Cannot build a memmap TensorDict in-place from a PersistentTensorDict. Use `td.memmap()` instead."
-        )
-
-    def memmap(
-        self,
-        prefix: str | None = None,
-    ) -> TensorDict:
-        """Converts the PersistentTensorDict to a memmap equivalent."""
-        mm_like = self.memmap_like(prefix)
-        for key in self.keys(include_nested=True, leaves_only=True):
-            mm_val = mm_like[key]
-            mm_val._memmap_array[:] = self._get_array(key)
-        return mm_like
-
-    def memmap_like(self, prefix: str | None = None) -> TensorDictBase:
-        # re-implements this to make it faster using the meta-data
-        if prefix is not None:
-            prefix = Path(prefix)
-            if not prefix.exists():
-                prefix.mkdir(exist_ok=True)
-            torch.save(
-                {"batch_size": self.batch_size, "device": self.device},
-                prefix / "meta.pt",
-            )
-        if not self.keys():
-            raise Exception(
-                "memmap_like() must be called when the TensorDict is (partially) "
-                "populated. Set a tensor first."
-            )
-        tensordict = TensorDict({}, self.batch_size, device=self.device)
-        for key, value in self._items_metadata():
-            if not value["array"]:
-                value = self.get(key)
-                if prefix is not None:
-                    # ensure subdirectory exists
-                    (prefix / key).mkdir(exist_ok=True)
-                    tensordict[key] = value.memmap_like(
-                        prefix=prefix / key,
-                    )
-                    torch.save(
-                        {"batch_size": value.batch_size, "device": value.device},
-                        prefix / key / "meta.pt",
-                    )
-                else:
-                    tensordict[key] = value.memmap_like()
-                continue
-            else:
-                tensordict[key] = MemmapTensor(
-                    value["shape"],
-                    device="cpu",
-                    dtype=value["dtype"],
-                    filename=str(prefix / f"{key}.memmap")
-                    if prefix is not None
-                    else None,
-                )
-            if prefix is not None:
-                torch.save(
-                    {
-                        "shape": value["shape"],
-                        "device": torch.device("cpu"),
-                        "dtype": value["dtype"],
-                    },
-                    prefix / f"{key}.meta.pt",
-                )
-        tensordict._is_memmap = True
-        tensordict.lock_()
-        return tensordict
-
-    def pin_memory(self):
-        """Returns a new PersistentTensorDict where any given Tensor key returns a tensor with pin_memory=True.
-
-        This will fail with PersistentTensorDict with a ``cuda`` device attribute.
-
-        """
-        if self.device.type == "cuda":
-            raise RuntimeError("cannot pin memory on a tensordict stored on cuda.")
-        out = self.clone(False)
-        out._pin_mem = True
-        out._nested_tensordicts = {
-            key: val.pin_memory() for key, val in out._nested_tensordicts.items()
-        }
-        return out
-
-    def rename_key_(
-        self, old_key: str, new_key: str, safe: bool = False
-    ) -> PersistentTensorDict:
-        old_key = self._process_key(old_key)
-        new_key = self._process_key(new_key)
-        try:
-            self.file.move(old_key, new_key)
-        except ValueError as err:
-            raise KeyError(f"key {new_key} already present in TensorDict.") from err
-        return self
-
-    def fill_(self, key: str, value: float | bool) -> TensorDictBase:
-        """Fills a tensor pointed by the key with the a given value.
-
-        Args:
-            key (str): key to be remaned
-            value (Number, bool): value to use for the filling
-
-        Returns:
-            self
-
-        """
-        md = self._get_metadata(key)
-        if md.get("array", None):
-            array = self._get_array(key)
-            array[:] = value
-        else:
-            nested = self.get(key)
-            for subkey in nested.keys():
-                nested.fill_(subkey, value)
-        return self
-
-    def _create_nested_str(self, key):
-        self.file.create_group(key)
-        target_td = self._get_str(key)
-        return target_td
-
-    def select(
-        self, *keys: str, inplace: bool = False, strict: bool = True
-    ) -> PersistentTensorDict:
-        raise NotImplementedError(
-            "Cannot call select on a PersistentTensorDict. "
-            "Create a regular tensordict first using the `to_tensordict` method."
-        )
-
-    def exclude(self, *keys: str, inplace: bool = False) -> PersistentTensorDict:
-        raise NotImplementedError(
-            "Cannot call exclude on a PersistentTensorDict. "
-            "Create a regular tensordict first using the `to_tensordict` method."
-        )
-
-    def share_memory_(self):
-        raise NotImplementedError(
-            "Cannot call share_memory_ on a PersistentTensorDict. "
-            "Create a regular tensordict first using the `to_tensordict` method."
-        )
-
-    def to(
-        self, dest: DeviceType | torch.Size | type, **kwargs: Any
-    ) -> PersistentTensorDict:
-        if isinstance(dest, type) and issubclass(dest, TensorDictBase):
-            if isinstance(self, dest):
-                return self
-            td = dest(source=self, **kwargs)
-            return td
-        elif isinstance(dest, (torch.device, str, int)):
-            # must be device
-            dest = torch.device(dest)
-            if self.device is not None and dest == self.device:
-                return self
-            out = self.clone(False)
-            out._device = dest
-            for key, nested in list(out._nested_tensordicts.items()):
-                out._nested_tensordicts[key] = nested.to(dest)
-            return out
-        elif isinstance(dest, torch.Size):
-            self.batch_size = dest
-            return self
-        else:
-            raise NotImplementedError(
-                f"dest must be a string, torch.device or a TensorDict "
-                f"instance, {dest} not allowed"
-            )
-
-    def _to_numpy(self, value):
-        if hasattr(value, "requires_grad") and value.requires_grad:
-            raise RuntimeError("Cannot set a tensor that has requires_grad=True.")
-        if isinstance(value, torch.Tensor):
-            out = value.cpu().detach().numpy()
-        elif isinstance(value, MemmapTensor):
-            out = value._memmap_array
-        elif isinstance(value, dict):
-            out = TensorDict(value, [])
-        elif is_tensor_collection(value):
-            out = value
-        elif isinstance(value, (np.ndarray,)):
-            out = value
-        else:
-            raise NotImplementedError(
-                f"Cannot set values of type {value} in a PersistentTensorDict."
-            )
-        return out
-
-    def _set(
-        self,
-        key: str,
-        value,
-        inplace: bool = False,
-        idx=None,
-        validated=False,
-    ) -> PersistentTensorDict:
-        if not validated:
-            value = self._validate_value(value, check_shape=idx is None)
-        value = self._to_numpy(value)
-        if not inplace:
-            if idx is not None:
-                raise RuntimeError("Cannot pass an index to _set when inplace=False.")
-            elif self.is_locked:
-                raise RuntimeError(self.LOCK_ERROR)
-        # shortcut set if we're placing a tensordict
-        if is_tensor_collection(value):
-            if isinstance(key, tuple):
-                key, subkey = key[0], key[1:]
-            else:
-                key, subkey = key, []
-            target_td = self._get_str(key, default=None)
-            if target_td is None:
-                self.file.create_group(key)
-                target_td = self._get_str(key)
-                target_td.batch_size = value.batch_size
-            elif not is_tensor_collection(target_td):
-                raise RuntimeError(
-                    f"cannot set a tensor collection in place of a non-tensor collection in {self.__class__.__name__}. "
-                    f"Got self.get({key})={target_td} and value={value}."
-                )
-            if idx is None:
-                if len(subkey):
-                    target_td.set(subkey, value, inplace=inplace)
-                else:
-                    target_td.update(value, inplace=inplace)
-            else:
-                if len(subkey):
-                    target_td.set_at_(subkey, value, idx=idx)
-                else:
-                    target_td.update_at_(value, idx=idx)
-
-            return self
-
-        if inplace:
-            # could be called before but will go under further refactoring of set
-            key = self._process_key(key)
-            array = self.file[key]
-            if idx is None:
-                idx = ()
-            else:
-                idx = self._process_index(idx, array)
-            try:
-                array[idx] = value
-            except TypeError as err:
-                if "Boolean indexing array has incompatible shape" in str(err):
-                    # Known bug in h5py: cannot broadcast boolean mask on the right as
-                    # done in np and torch. Therefore we put a performance warning
-                    # and convert to torch tensor first.
-                    warnings.warn(
-                        "Indexing an h5py.Dataset object with a boolean mask "
-                        "that needs broadcasting does not work directly. "
-                        "tensordict will cast the entire array in memory and index it using the mask. "
-                        "This is suboptimal and may lead to performance issue."
-                    )
-                    idx = tuple(
-                        expand_right(torch.as_tensor(_idx), array.shape).numpy()
-                        if _idx.dtype == np.dtype("bool")
-                        else _idx
-                        for _idx in idx
-                    )
-                    array[idx] = torch.as_tensor(value)
-                else:
-                    raise err
-
-        else:
-            key = self._process_key(key)
-            try:
-                self.file.create_dataset(key, data=value, **self.kwargs)
-            except (ValueError, OSError) as err:
-                if "name already exists" in str(err):
-                    warnings.warn(
-                        "Replacing an array with another one is inefficient. "
-                        "Consider using different names or populating in-place using `inplace=True`."
-                    )
-                    del self.file[key]
-                    self.file.create_dataset(key, data=value, **self.kwargs)
-        return self
-
-    def _convert_inplace(self, inplace, key):
-        key = self._process_key(key)
-        if inplace is not False:
-            has_key = key in self.file
-            if inplace is True and not has_key:  # inplace could be None
-                raise KeyError(
-                    TensorDictBase.KEY_ERROR.format(
-                        key, self.__class__.__name__, sorted(self.keys())
-                    )
-                )
-            inplace = has_key
-        return inplace
-
-    def _set_str(self, key, value, *, inplace, validated):
-        inplace = self._convert_inplace(inplace, key)
-        return self._set(key, value, inplace=inplace, validated=validated)
-
-    def _set_tuple(self, key, value, *, inplace, validated):
-        if len(key) == 1:
-            return self._set_str(key[0], value, inplace=inplace, validated=validated)
-        elif key[0] in self.keys():
-            return self._get_str(key[0])._set_tuple(
-                key[1:], value, inplace=inplace, validated=validated
-            )
-        inplace = self._convert_inplace(inplace, key)
-        return self._set(key, value, inplace=inplace, validated=validated)
-
-    def _set_at_str(self, key, value, idx, *, validated):
-        return self._set(key, value, inplace=True, idx=idx, validated=validated)
-
-    def _set_at_tuple(self, key, value, idx, *, validated):
-        return self._set(key, value, inplace=True, idx=idx, validated=validated)
-
-    def _set_metadata(self, orig_metadata_container: PersistentTensorDict):
-        for key, td in orig_metadata_container._nested_tensordicts.items():
-            array = self._get_array(key)
-            self._nested_tensordicts[key] = PersistentTensorDict(
-                group=array,
-                batch_size=td.batch_size,
-                device=td.device,
-            )
-            self._nested_tensordicts[key].names = td._td_dim_names
-            self._nested_tensordicts[key]._set_metadata(td)
-
-    def clone(self, recurse: bool = True, newfile=None) -> PersistentTensorDict:
-        if recurse:
-            # this should clone the h5 to a new location indicated by newfile
-            if newfile is None:
-                warnings.warn(
-                    "A destination should be provided when cloning a "
-                    "PersistentTensorDict. A temporary file will be used "
-                    "instead. Use `recurse=False` to keep track of the original data "
-                    "with a new PersistentTensorDict instance."
-                )
-                tmpfile = tempfile.NamedTemporaryFile()
-                newfile = tmpfile.name
-            f_dest = h5py.File(newfile, "w")
-            f_src = self.file
-            for key in self.keys(include_nested=True, leaves_only=True):
-                key = self._process_key(key)
-                f_dest.create_dataset(key, data=f_src[key], **self.kwargs)
-                # f_src.copy(f_src[key],  f_dest[key], "DataSet")
-            # create a non-recursive copy and update the file
-            # this way, we can keep the batch-size of every nested tensordict
-            clone = self.clone(False)
-            clone.file = f_dest
-            clone.filename = newfile
-            clone._pin_mem = False
-            clone.names = self._td_dim_names
-            clone._nested_tensordicts = {}
-            clone._set_metadata(self)
-            return clone
-        else:
-            # we need to keep the batch-size of nested tds, which we do manually
-            nested_tds = {
-                key: td.clone(False) for key, td in self._nested_tensordicts.items()
-            }
-            filename = self.filename
-            file = self.file if filename is None else None
-            clone = PersistentTensorDict(
-                filename=filename,
-                group=file,
-                mode=self.mode,
-                backend="h5",
-                device=self.device,
-                batch_size=self.batch_size,
-            )
-            clone._nested_tensordicts = nested_tds
-            clone._pin_mem = False
-            clone.names = self._td_dim_names
-            return clone
-
-    def __getstate__(self):
-        state = self.__dict__.copy()
-        filename = state["file"].file.filename
-        group_name = state["file"].name
-        state["file"] = None
-        state["filename"] = filename
-        state["group_name"] = group_name
-        return state
-
-    def __setstate__(self, state):
-        state["file"] = h5py.File(state["filename"], mode=state["mode"])
-        if state["group_name"] != "/":
-            state["file"] = state["file"][state["group_name"]]
-        del state["group_name"]
-        self.__dict__.update(state)
-
-    def _add_batch_dim(self, *, in_dim, vmap_level):
-        raise RuntimeError("Persistent tensordicts cannot be used with vmap.")
-
-    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
-        # not accessible
-        ...
-
-
-def _set_max_batch_size(source: PersistentTensorDict):
-    """Updates a tensordict with its maximium batch size."""
-    tensor_data = list(source._items_metadata())
-    for key, val in tensor_data:
-        if not val["array"]:
-            _set_max_batch_size(source.get(key))
-
-    batch_size = []
-    if not tensor_data:  # when source is empty
-        source.batch_size = batch_size
-        return
-
-    curr_dim = 0
-    tensor_data = list(source._values_metadata())
-    while True:
-        if tensor_data[0]["dim"] > curr_dim:
-            curr_dim_size = tensor_data[0]["shape"][curr_dim]
-        else:
-            source.batch_size = batch_size
-            return
-        for tensor in tensor_data[1:]:
-            if tensor["dim"] <= curr_dim or tensor["shape"][curr_dim] != curr_dim_size:
-                source.batch_size = batch_size
-                return
-        batch_size.append(curr_dim_size)
-        curr_dim += 1
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+"""Persistent tensordicts (H5 and others)."""
+from __future__ import annotations
+
+import tempfile
+import warnings
+from pathlib import Path
+from typing import Any
+
+from tensordict._tensordict import _unravel_key_to_tuple
+
+H5_ERR = None
+try:
+    import h5py
+
+    _has_h5 = True
+except ModuleNotFoundError as err:
+    H5_ERR = err
+    _has_h5 = False
+
+import numpy as np
+import torch
+
+from tensordict import MemmapTensor
+from tensordict.tensordict import (
+    _TensorDictKeysView,
+    CompatibleType,
+    is_tensor_collection,
+    NO_DEFAULT,
+    TensorDict,
+    TensorDictBase,
+)
+from tensordict.utils import (
+    cache,
+    DeviceType,
+    expand_right,
+    IndexType,
+    lock_blocked,
+    NestedKey,
+    NUMPY_TO_TORCH_DTYPE_DICT,
+)
+
+
+class _Visitor:
+    def __init__(self, fun=None):
+        self.elts = []
+        self.fun = fun
+
+    def __call__(self, name):
+        self.elts.append(name)
+
+    def __iter__(self):
+        if self.fun is None:
+            yield from self.elts
+        else:
+            for elt in self.elts:
+                yield self.fun(elt)
+
+
+class _PersistentTDKeysView(_TensorDictKeysView):
+    def __iter__(self):
+        if self.include_nested:
+            visitor = _Visitor(lambda key: tuple(key.split("/")))
+            self.tensordict.file.visit(visitor)
+            if self.leaves_only:
+                for key in visitor:
+                    if self.tensordict._get_metadata(key).get("array", None):
+                        yield key
+            else:
+                yield from visitor
+        else:
+            yield from self.tensordict._valid_keys()
+
+    def __contains__(self, key):
+        if isinstance(key, tuple) and len(key) == 1:
+            key = key[0]
+        for a_key in self:
+            if isinstance(a_key, tuple) and len(a_key) == 1:
+                a_key = a_key[0]
+            if key == a_key:
+                return True
+        else:
+            return False
+
+
+class PersistentTensorDict(TensorDictBase):
+    """Persistent TensorDict implementation.
+
+    :class:`PersistentTensorDict` instances provide an interface with data stored
+    on disk such that access to this data is made easy while still taking advantage
+    from the fast access provided by the backend.
+
+    Like other :class:`TensorDictBase` subclasses, :class:`PersistentTensorDict`
+    has a ``device`` attribute. This does *not* mean that the data is being stored
+    on that device, but rather that when loaded, the data will be cast onto
+    the desired device.
+
+    Args:
+        batch_size (torch.Size or compatible): the tensordict batch size.
+        filename (str, optional): the path to the h5 file. Exclusive with ``group``.
+        group (h5py.Group, optional): a file or a group that contains data. Exclusive with ``filename``.
+        mode (str, optional): Reading mode. Defaults to ``"r"``.
+        backend (str, optional): storage backend. Currently only ``"h5"`` is supported.
+        device (torch.device or compatible, optional): device of the tensordict.
+            Defaults to ``None`` (ie. default PyTorch device).
+        **kwargs: kwargs to be passed to :meth:`h5py.File.create_dataset`.
+
+    .. note::
+      Currently, PersistentTensorDict instances are not closed when getting out-of-scope.
+      This means that it is the responsibility of the user to close them if necessary.
+
+    Examples:
+        >>> import tempfile
+        >>> with tempfile.NamedTemporaryFile() as f:
+        ...     data = PersistentTensorDict(file=f, batch_size=[3], mode="w")
+        ...     data["a", "b"] = torch.randn(3, 4)
+        ...     print(data)
+
+    """
+
+    def __new__(cls, *args, **kwargs):
+        cls._td_dim_names = None
+        return super().__new__(cls, *args, **kwargs)
+
+    def __init__(
+        self,
+        *,
+        batch_size,
+        filename=None,
+        group=None,
+        mode="r",
+        backend="h5",
+        device=None,
+        **kwargs,
+    ):
+        self._locked_tensordicts = []
+        self._lock_id = set()
+        if not _has_h5:
+            raise ModuleNotFoundError("Could not load h5py.") from H5_ERR
+        super().__init__()
+        self.filename = filename
+        self.mode = mode
+        if backend != "h5":
+            raise NotImplementedError
+        if filename is not None and group is None:
+            self.file = h5py.File(filename, mode)
+        elif group is not None:
+            self.file = group
+        else:
+            raise RuntimeError(
+                f"Either group or filename must be provided, and not both. Got group={group} and filename={filename}."
+            )
+        self._batch_size = torch.Size(batch_size)
+        self._device = device
+        self._is_shared = False
+        self._is_memmap = False
+        self.kwargs = kwargs
+
+        # we use this to allow nested tensordicts to have a different batch-size
+        self._nested_tensordicts = {}
+        self._pin_mem = False
+
+        # this must be kept last
+        self._check_batch_size(self._batch_size)
+
+    @classmethod
+    def from_h5(cls, filename, mode="r"):
+        """Creates a PersistentTensorDict from a h5 file.
+
+        This function will automatically determine the batch-size for each nested
+        tensordict.
+
+        Args:
+            filename (str): the path to the h5 file.
+            mode (str, optional): reading mode. Defaults to ``"r"``.
+        """
+        out = cls(filename=filename, mode=mode, batch_size=[])
+        # determine batch size
+        _set_max_batch_size(out)
+        return out
+
+    @classmethod
+    def from_dict(cls, input_dict, filename, batch_size=None, device=None, **kwargs):
+        """Converts a dictionary or a TensorDict to a h5 file.
+
+        Args:
+            input_dict (dict, TensorDict or compatible): data to be stored as h5.
+            filename (str or path): path to the h5 file.
+            batch_size (tensordict batch-size, optional): if provided, batch size
+                of the tensordict. If not, the batch size will be gathered from the
+                input structure (if present) or determined automatically.
+            device (torch.device or compatible, optional): the device where to
+                expect the tensor once they are returned. Defaults to ``None``
+                (on cpu by default).
+            **kwargs: kwargs to be passed to :meth:`h5py.File.create_dataset`.
+
+        Returns:
+            A :class:`PersitentTensorDict` instance linked to the newly created file.
+
+        """
+        file = h5py.File(filename, "w")
+        _has_batch_size = True
+        if batch_size is None:
+            if is_tensor_collection(input_dict):
+                batch_size = input_dict.batch_size
+            else:
+                _has_batch_size = False
+                batch_size = torch.Size([])
+
+        # let's make a tensordict first
+        out = cls(group=file, batch_size=batch_size, device=device, **kwargs)
+        if is_tensor_collection(input_dict):
+            out.update(input_dict)
+        else:
+            out.update(TensorDict(input_dict, batch_size=batch_size))
+        if not _has_batch_size:
+            _set_max_batch_size(out)
+        return out
+
+    def close(self):
+        """Closes the persistent tensordict."""
+        self.file.close()
+
+    def _process_key(self, key):
+        key = _unravel_key_to_tuple(key)
+        return "/".join(key)
+
+    def _check_batch_size(self, batch_size) -> None:
+        for key in self.keys(include_nested=True, leaves_only=True):
+            key = self._process_key(key)
+            size = self.file[key].shape
+            if torch.Size(size[: len(batch_size)]) != batch_size:
+                raise ValueError(
+                    f"batch size and array size mismatch: array.shape={size}, batch_size={batch_size}."
+                )
+
+    def _get_array(self, key, default=NO_DEFAULT):
+        try:
+            key = self._process_key(key)
+            array = self.file[key]
+            return array
+        except KeyError:
+            if default is not NO_DEFAULT:
+                return default
+            raise KeyError(f"key {key} not found in PersistentTensorDict {self}")
+
+    def _process_array(self, key, array):
+        if isinstance(array, (h5py.Dataset,)):
+            if self.device is not None:
+                device = self.device
+            else:
+                device = torch.device("cpu")
+            # we convert to an array first to avoid "Creating a tensor from a list of numpy.ndarrays is extremely slow."
+            array = array[()]
+            out = torch.as_tensor(array, device=device)
+            if self._pin_mem:
+                return out.pin_memory()
+            return out
+        else:
+            out = self._nested_tensordicts.get(key, None)
+            if out is None:
+                out = self._nested_tensordicts[key] = PersistentTensorDict(
+                    group=array,
+                    batch_size=self.batch_size,
+                    device=self.device,
+                )
+            return out
+
+    @cache  # noqa: B019
+    def get(self, key, default=NO_DEFAULT):
+        array = self._get_array(key, default)
+        if array is default:
+            return array
+        return self._process_array(key, array)
+
+    _get_str = get
+    _get_tuple = get
+
+    def get_at(
+        self, key: str, idx: IndexType, default: CompatibleType = NO_DEFAULT
+    ) -> CompatibleType:
+        array = self._get_array(key, default)
+        if isinstance(array, (h5py.Dataset,)):
+            if self.device is not None:
+                device = self.device
+            else:
+                device = torch.device("cpu")
+            # indexing must be done before converting to tensor.
+            idx = self._process_index(idx, array)
+            # `get_at` is there to save us.
+            try:
+                out = torch.as_tensor(array[idx], device=device)
+            except TypeError as err:
+                if "Boolean indexing array has incompatible shape" in str(err):
+                    # Known bug in h5py: cannot broadcast boolean mask on the right as
+                    # done in np and torch. Therefore we put a performance warning
+                    # and convert to torch tensor first.
+                    warnings.warn(
+                        "Indexing an h5py.Dataset object with a boolean mask "
+                        "that needs broadcasting does not work directly. "
+                        "tensordict will cast the entire array in memory and index it using the mask. "
+                        "This is suboptimal and may lead to performance issue."
+                    )
+                    out = torch.as_tensor(np.asarray(array), device=device)[idx]
+                else:
+                    raise err
+            if self._pin_mem:
+                return out.pin_memory()
+            return out
+        elif array is not default:
+            out = self._nested_tensordicts.get(key, None)
+            if out is None:
+                out = self._nested_tensordicts[key] = PersistentTensorDict(
+                    group=array,
+                    batch_size=self.batch_size,
+                    device=self.device,
+                )
+            return out.get_sub_tensordict(idx)
+        else:
+            return default
+
+    def _get_metadata(self, key):
+        """Gets the metadata for an entry.
+
+        This method avoids creating a tensor from scratch, and just reads the metadata of the array.
+        """
+        array = self._get_array(key)
+        if (
+            isinstance(array, (h5py.Dataset,))
+            and array.dtype in NUMPY_TO_TORCH_DTYPE_DICT
+        ):
+            shape = torch.Size(array.shape)
+            return {
+                "dtype": NUMPY_TO_TORCH_DTYPE_DICT[array.dtype],
+                "shape": shape,
+                "dim": len(shape),
+                "array": True,
+            }
+        elif (
+            isinstance(array, (h5py.Dataset,))
+            and array.dtype not in NUMPY_TO_TORCH_DTYPE_DICT
+        ):
+            return {}
+        else:
+            shape = self.get(key).shape
+            return {
+                "dtype": None,
+                "shape": shape,
+                "dim": len(shape),
+                "array": False,
+            }
+
+    @classmethod
+    def _process_index(cls, idx, array=None):
+        if isinstance(idx, tuple):
+            return tuple(cls._process_index(_idx, array) for _idx in idx)
+        if isinstance(idx, torch.Tensor):
+            return idx.cpu().detach().numpy()
+        if isinstance(idx, (range, list)):
+            return np.asarray(idx)
+        return idx
+
+    def __getitem__(self, item):
+        if isinstance(item, str) or (
+            isinstance(item, tuple) and all(isinstance(val, str) for val in item)
+        ):
+            return self.get(item)
+        if isinstance(item, list):
+            # convert to tensor
+            item = torch.tensor(item)
+        return self.get_sub_tensordict(item)
+
+    __getitems__ = __getitem__
+
+    def __setitem__(self, index, value):
+        index_unravel = _unravel_key_to_tuple(index)
+        if index_unravel:
+            return self.set(index_unravel, value, inplace=True)
+
+        if isinstance(index, list):
+            # convert to tensor
+            index = torch.tensor(index)
+        sub_td = self.get_sub_tensordict(index)
+        err_set_batch_size = None
+        if not isinstance(value, TensorDictBase):
+            value = TensorDict.from_dict(value, batch_size=[])
+            # try to assign the current shape. If that does not work, we can
+            # try to expand
+            try:
+                value.batch_size = sub_td.batch_size
+            except RuntimeError as err0:
+                err_set_batch_size = err0
+        if value.shape != sub_td.shape:
+            try:
+                value = value.expand(sub_td.shape)
+            except RuntimeError as err:
+                if err_set_batch_size is not None:
+                    raise err from err_set_batch_size
+                raise RuntimeError(
+                    f"Cannot broadcast the tensordict {value} to the shape of the indexed persistent tensordict {self}[{index}]."
+                ) from err
+        sub_td.update(value, inplace=True)
+
+    @cache  # noqa: B019
+    def _valid_keys(self):
+        keys = []
+        for key in self.file.keys():
+            if self._get_metadata(key):
+                keys.append(key)
+        return keys
+
+    # @cache  # noqa: B019
+    def keys(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> _PersistentTDKeysView:
+        return _PersistentTDKeysView(
+            tensordict=self,
+            include_nested=include_nested,
+            leaves_only=leaves_only,
+        )
+
+    def _items_metadata(self, include_nested=False, leaves_only=False):
+        """Iterates over the metadata of the PersistentTensorDict."""
+        for key in self.keys(include_nested, leaves_only):
+            yield (key, self._get_metadata(key))
+
+    def _values_metadata(self, include_nested=False, leaves_only=False):
+        """Iterates over the metadata of the PersistentTensorDict."""
+        for key in self.keys(include_nested, leaves_only):
+            yield self._get_metadata(key)
+
+    def _change_batch_size(self, value):
+        raise NotImplementedError
+
+    def _stack_onto_(
+        self, list_item: list[CompatibleType], dim: int
+    ) -> PersistentTensorDict:
+        for key in self.keys():
+            vals = [td._get_str(key, None) for td in list_item]
+            if all(v is None for v in vals):
+                continue
+            stacked = torch.stack(vals, dim=dim)
+            self.set_(key, stacked)
+        return self
+
+    @property
+    def batch_size(self):
+        return self._batch_size
+
+    @batch_size.setter
+    def batch_size(self, value):
+        _batch_size = self._batch_size
+        try:
+            self._batch_size = torch.Size(value)
+            self._check_batch_size(self._batch_size)
+        except ValueError:
+            self._batch_size = _batch_size
+
+    _erase_names = TensorDict._erase_names
+    names = TensorDict.names
+    _has_names = TensorDict._has_names
+
+    def _rename_subtds(self, names):
+        if names is None:
+            names = [None] * self.ndim
+        for item in self._nested_tensordicts.values():
+            if is_tensor_collection(item):
+                td_names = list(names) + [None] * (item.ndim - self.ndim)
+                item.rename_(*td_names)
+
+    def contiguous(self):
+        """Materializes a PersistentTensorDict on a regular TensorDict."""
+        return self.to_tensordict()
+
+    @lock_blocked
+    def del_(self, key):
+        key = self._process_key(key)
+        del self.file[key]
+        return self
+
+    def detach_(self):
+        # PersistentTensorDict do not carry gradients. This is a no-op
+        return self
+
+    @property
+    def device(self):
+        return self._device
+
+    def entry_class(self, key: NestedKey) -> type:
+        entry_class = self._get_metadata(key)
+        is_array = entry_class.get("array", None)
+        if is_array:
+            return torch.Tensor
+        elif is_array is False:
+            return PersistentTensorDict
+        else:
+            raise RuntimeError(f"Encountered a non-numeric data {key}.")
+
+    def is_contiguous(self):
+        return False
+
+    def masked_fill(self, mask, value):
+        return self.to_tensordict().masked_fill(mask, value)
+
+    def where(self, condition, other, *, out=None):
+        return self.to_tensordict().where(condition=condition, other=other, out=out)
+
+    def masked_fill_(self, mask, value):
+        for key in self.keys(include_nested=True, leaves_only=True):
+            array = self._get_array(key)
+            array[expand_right(mask, array.shape).cpu().numpy()] = value
+        return self
+
+    def memmap_(
+        self, prefix: str | None = None, copy_existing: bool = False
+    ) -> PersistentTensorDict:
+        raise RuntimeError(
+            "Cannot build a memmap TensorDict in-place from a PersistentTensorDict. Use `td.memmap()` instead."
+        )
+
+    def memmap(
+        self,
+        prefix: str | None = None,
+    ) -> TensorDict:
+        """Converts the PersistentTensorDict to a memmap equivalent."""
+        mm_like = self.memmap_like(prefix)
+        for key in self.keys(include_nested=True, leaves_only=True):
+            mm_val = mm_like[key]
+            mm_val._memmap_array[:] = self._get_array(key)
+        return mm_like
+
+    def memmap_like(self, prefix: str | None = None) -> TensorDictBase:
+        # re-implements this to make it faster using the meta-data
+        if prefix is not None:
+            prefix = Path(prefix)
+            if not prefix.exists():
+                prefix.mkdir(exist_ok=True)
+            torch.save(
+                {"batch_size": self.batch_size, "device": self.device},
+                prefix / "meta.pt",
+            )
+        if not self.keys():
+            raise Exception(
+                "memmap_like() must be called when the TensorDict is (partially) "
+                "populated. Set a tensor first."
+            )
+        tensordict = TensorDict({}, self.batch_size, device=self.device)
+        for key, value in self._items_metadata():
+            if not value["array"]:
+                value = self.get(key)
+                if prefix is not None:
+                    # ensure subdirectory exists
+                    (prefix / key).mkdir(exist_ok=True)
+                    tensordict[key] = value.memmap_like(
+                        prefix=prefix / key,
+                    )
+                    torch.save(
+                        {"batch_size": value.batch_size, "device": value.device},
+                        prefix / key / "meta.pt",
+                    )
+                else:
+                    tensordict[key] = value.memmap_like()
+                continue
+            else:
+                tensordict[key] = MemmapTensor(
+                    value["shape"],
+                    device="cpu",
+                    dtype=value["dtype"],
+                    filename=str(prefix / f"{key}.memmap")
+                    if prefix is not None
+                    else None,
+                )
+            if prefix is not None:
+                torch.save(
+                    {
+                        "shape": value["shape"],
+                        "device": torch.device("cpu"),
+                        "dtype": value["dtype"],
+                    },
+                    prefix / f"{key}.meta.pt",
+                )
+        tensordict._is_memmap = True
+        tensordict.lock_()
+        return tensordict
+
+    def pin_memory(self):
+        """Returns a new PersistentTensorDict where any given Tensor key returns a tensor with pin_memory=True.
+
+        This will fail with PersistentTensorDict with a ``cuda`` device attribute.
+
+        """
+        if self.device.type == "cuda":
+            raise RuntimeError("cannot pin memory on a tensordict stored on cuda.")
+        out = self.clone(False)
+        out._pin_mem = True
+        out._nested_tensordicts = {
+            key: val.pin_memory() for key, val in out._nested_tensordicts.items()
+        }
+        return out
+
+    def rename_key_(
+        self, old_key: str, new_key: str, safe: bool = False
+    ) -> PersistentTensorDict:
+        old_key = self._process_key(old_key)
+        new_key = self._process_key(new_key)
+        try:
+            self.file.move(old_key, new_key)
+        except ValueError as err:
+            raise KeyError(f"key {new_key} already present in TensorDict.") from err
+        return self
+
+    def fill_(self, key: str, value: float | bool) -> TensorDictBase:
+        """Fills a tensor pointed by the key with the a given value.
+
+        Args:
+            key (str): key to be remaned
+            value (Number, bool): value to use for the filling
+
+        Returns:
+            self
+
+        """
+        md = self._get_metadata(key)
+        if md.get("array", None):
+            array = self._get_array(key)
+            array[:] = value
+        else:
+            nested = self.get(key)
+            for subkey in nested.keys():
+                nested.fill_(subkey, value)
+        return self
+
+    def _create_nested_str(self, key):
+        self.file.create_group(key)
+        target_td = self._get_str(key)
+        return target_td
+
+    def select(
+        self, *keys: str, inplace: bool = False, strict: bool = True
+    ) -> PersistentTensorDict:
+        raise NotImplementedError(
+            "Cannot call select on a PersistentTensorDict. "
+            "Create a regular tensordict first using the `to_tensordict` method."
+        )
+
+    def exclude(self, *keys: str, inplace: bool = False) -> PersistentTensorDict:
+        raise NotImplementedError(
+            "Cannot call exclude on a PersistentTensorDict. "
+            "Create a regular tensordict first using the `to_tensordict` method."
+        )
+
+    def share_memory_(self):
+        raise NotImplementedError(
+            "Cannot call share_memory_ on a PersistentTensorDict. "
+            "Create a regular tensordict first using the `to_tensordict` method."
+        )
+
+    def to(
+        self, dest: DeviceType | torch.Size | type, **kwargs: Any
+    ) -> PersistentTensorDict:
+        if isinstance(dest, type) and issubclass(dest, TensorDictBase):
+            if isinstance(self, dest):
+                return self
+            td = dest(source=self, **kwargs)
+            return td
+        elif isinstance(dest, (torch.device, str, int)):
+            # must be device
+            dest = torch.device(dest)
+            if self.device is not None and dest == self.device:
+                return self
+            out = self.clone(False)
+            out._device = dest
+            for key, nested in list(out._nested_tensordicts.items()):
+                out._nested_tensordicts[key] = nested.to(dest)
+            return out
+        elif isinstance(dest, torch.Size):
+            self.batch_size = dest
+            return self
+        else:
+            raise NotImplementedError(
+                f"dest must be a string, torch.device or a TensorDict "
+                f"instance, {dest} not allowed"
+            )
+
+    def _to_numpy(self, value):
+        if hasattr(value, "requires_grad") and value.requires_grad:
+            raise RuntimeError("Cannot set a tensor that has requires_grad=True.")
+        if isinstance(value, torch.Tensor):
+            out = value.cpu().detach().numpy()
+        elif isinstance(value, MemmapTensor):
+            out = value._memmap_array
+        elif isinstance(value, dict):
+            out = TensorDict(value, [])
+        elif is_tensor_collection(value):
+            out = value
+        elif isinstance(value, (np.ndarray,)):
+            out = value
+        else:
+            raise NotImplementedError(
+                f"Cannot set values of type {value} in a PersistentTensorDict."
+            )
+        return out
+
+    def _set(
+        self,
+        key: str,
+        value,
+        inplace: bool = False,
+        idx=None,
+        validated=False,
+    ) -> PersistentTensorDict:
+        if not validated:
+            value = self._validate_value(value, check_shape=idx is None)
+        value = self._to_numpy(value)
+        if not inplace:
+            if idx is not None:
+                raise RuntimeError("Cannot pass an index to _set when inplace=False.")
+            elif self.is_locked:
+                raise RuntimeError(self.LOCK_ERROR)
+        # shortcut set if we're placing a tensordict
+        if is_tensor_collection(value):
+            if isinstance(key, tuple):
+                key, subkey = key[0], key[1:]
+            else:
+                key, subkey = key, []
+            target_td = self._get_str(key, default=None)
+            if target_td is None:
+                self.file.create_group(key)
+                target_td = self._get_str(key)
+                target_td.batch_size = value.batch_size
+            elif not is_tensor_collection(target_td):
+                raise RuntimeError(
+                    f"cannot set a tensor collection in place of a non-tensor collection in {self.__class__.__name__}. "
+                    f"Got self.get({key})={target_td} and value={value}."
+                )
+            if idx is None:
+                if len(subkey):
+                    target_td.set(subkey, value, inplace=inplace)
+                else:
+                    target_td.update(value, inplace=inplace)
+            else:
+                if len(subkey):
+                    target_td.set_at_(subkey, value, idx=idx)
+                else:
+                    target_td.update_at_(value, idx=idx)
+
+            return self
+
+        if inplace:
+            # could be called before but will go under further refactoring of set
+            key = self._process_key(key)
+            array = self.file[key]
+            if idx is None:
+                idx = ()
+            else:
+                idx = self._process_index(idx, array)
+            try:
+                array[idx] = value
+            except TypeError as err:
+                if "Boolean indexing array has incompatible shape" in str(err):
+                    # Known bug in h5py: cannot broadcast boolean mask on the right as
+                    # done in np and torch. Therefore we put a performance warning
+                    # and convert to torch tensor first.
+                    warnings.warn(
+                        "Indexing an h5py.Dataset object with a boolean mask "
+                        "that needs broadcasting does not work directly. "
+                        "tensordict will cast the entire array in memory and index it using the mask. "
+                        "This is suboptimal and may lead to performance issue."
+                    )
+                    idx = tuple(
+                        expand_right(torch.as_tensor(_idx), array.shape).numpy()
+                        if _idx.dtype == np.dtype("bool")
+                        else _idx
+                        for _idx in idx
+                    )
+                    array[idx] = torch.as_tensor(value)
+                else:
+                    raise err
+
+        else:
+            key = self._process_key(key)
+            try:
+                self.file.create_dataset(key, data=value, **self.kwargs)
+            except (ValueError, OSError) as err:
+                if "name already exists" in str(err):
+                    warnings.warn(
+                        "Replacing an array with another one is inefficient. "
+                        "Consider using different names or populating in-place using `inplace=True`."
+                    )
+                    del self.file[key]
+                    self.file.create_dataset(key, data=value, **self.kwargs)
+        return self
+
+    def _convert_inplace(self, inplace, key):
+        key = self._process_key(key)
+        if inplace is not False:
+            has_key = key in self.file
+            if inplace is True and not has_key:  # inplace could be None
+                raise KeyError(
+                    TensorDictBase.KEY_ERROR.format(
+                        key, self.__class__.__name__, sorted(self.keys())
+                    )
+                )
+            inplace = has_key
+        return inplace
+
+    def _set_str(self, key, value, *, inplace, validated):
+        inplace = self._convert_inplace(inplace, key)
+        return self._set(key, value, inplace=inplace, validated=validated)
+
+    def _set_tuple(self, key, value, *, inplace, validated):
+        if len(key) == 1:
+            return self._set_str(key[0], value, inplace=inplace, validated=validated)
+        elif key[0] in self.keys():
+            return self._get_str(key[0])._set_tuple(
+                key[1:], value, inplace=inplace, validated=validated
+            )
+        inplace = self._convert_inplace(inplace, key)
+        return self._set(key, value, inplace=inplace, validated=validated)
+
+    def _set_at_str(self, key, value, idx, *, validated):
+        return self._set(key, value, inplace=True, idx=idx, validated=validated)
+
+    def _set_at_tuple(self, key, value, idx, *, validated):
+        return self._set(key, value, inplace=True, idx=idx, validated=validated)
+
+    def _set_metadata(self, orig_metadata_container: PersistentTensorDict):
+        for key, td in orig_metadata_container._nested_tensordicts.items():
+            array = self._get_array(key)
+            self._nested_tensordicts[key] = PersistentTensorDict(
+                group=array,
+                batch_size=td.batch_size,
+                device=td.device,
+            )
+            self._nested_tensordicts[key].names = td._td_dim_names
+            self._nested_tensordicts[key]._set_metadata(td)
+
+    def clone(self, recurse: bool = True, newfile=None) -> PersistentTensorDict:
+        if recurse:
+            # this should clone the h5 to a new location indicated by newfile
+            if newfile is None:
+                warnings.warn(
+                    "A destination should be provided when cloning a "
+                    "PersistentTensorDict. A temporary file will be used "
+                    "instead. Use `recurse=False` to keep track of the original data "
+                    "with a new PersistentTensorDict instance."
+                )
+                tmpfile = tempfile.NamedTemporaryFile()
+                newfile = tmpfile.name
+            f_dest = h5py.File(newfile, "w")
+            f_src = self.file
+            for key in self.keys(include_nested=True, leaves_only=True):
+                key = self._process_key(key)
+                f_dest.create_dataset(key, data=f_src[key], **self.kwargs)
+                # f_src.copy(f_src[key],  f_dest[key], "DataSet")
+            # create a non-recursive copy and update the file
+            # this way, we can keep the batch-size of every nested tensordict
+            clone = self.clone(False)
+            clone.file = f_dest
+            clone.filename = newfile
+            clone._pin_mem = False
+            clone.names = self._td_dim_names
+            clone._nested_tensordicts = {}
+            clone._set_metadata(self)
+            return clone
+        else:
+            # we need to keep the batch-size of nested tds, which we do manually
+            nested_tds = {
+                key: td.clone(False) for key, td in self._nested_tensordicts.items()
+            }
+            filename = self.filename
+            file = self.file if filename is None else None
+            clone = PersistentTensorDict(
+                filename=filename,
+                group=file,
+                mode=self.mode,
+                backend="h5",
+                device=self.device,
+                batch_size=self.batch_size,
+            )
+            clone._nested_tensordicts = nested_tds
+            clone._pin_mem = False
+            clone.names = self._td_dim_names
+            return clone
+
+    def __getstate__(self):
+        state = self.__dict__.copy()
+        filename = state["file"].file.filename
+        group_name = state["file"].name
+        state["file"] = None
+        state["filename"] = filename
+        state["group_name"] = group_name
+        return state
+
+    def __setstate__(self, state):
+        state["file"] = h5py.File(state["filename"], mode=state["mode"])
+        if state["group_name"] != "/":
+            state["file"] = state["file"][state["group_name"]]
+        del state["group_name"]
+        self.__dict__.update(state)
+
+    def _add_batch_dim(self, *, in_dim, vmap_level):
+        raise RuntimeError("Persistent tensordicts cannot be used with vmap.")
+
+    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
+        # not accessible
+        ...
+
+
+def _set_max_batch_size(source: PersistentTensorDict):
+    """Updates a tensordict with its maximium batch size."""
+    tensor_data = list(source._items_metadata())
+    for key, val in tensor_data:
+        if not val["array"]:
+            _set_max_batch_size(source.get(key))
+
+    batch_size = []
+    if not tensor_data:  # when source is empty
+        source.batch_size = batch_size
+        return
+
+    curr_dim = 0
+    tensor_data = list(source._values_metadata())
+    while True:
+        if tensor_data[0]["dim"] > curr_dim:
+            curr_dim_size = tensor_data[0]["shape"][curr_dim]
+        else:
+            source.batch_size = batch_size
+            return
+        for tensor in tensor_data[1:]:
+            if tensor["dim"] <= curr_dim or tensor["shape"][curr_dim] != curr_dim_size:
+                source.batch_size = batch_size
+                return
+        batch_size.append(curr_dim_size)
+        curr_dim += 1
```

## tensordict/tensorclass.py

 * *Ordering differences only*

```diff
@@ -1,968 +1,968 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import dataclasses
-import functools
-import inspect
-import numbers
-import re
-import sys
-import warnings
-from copy import copy
-from dataclasses import dataclass
-from textwrap import indent
-from typing import Any, Callable, Sequence, TypeVar
-
-import tensordict as tensordict_lib
-
-import torch
-from tensordict._tensordict import _unravel_key_to_tuple
-from tensordict.memmap import MemmapTensor
-from tensordict.tensordict import (
-    _get_repr,
-    is_tensor_collection,
-    NO_DEFAULT,
-    TD_HANDLED_FUNCTIONS,
-    TensorDict,
-    TensorDictBase,
-)
-
-from tensordict.utils import DeviceType, IndexType, is_tensorclass, NestedKey
-from torch import Tensor
-
-T = TypeVar("T", bound=TensorDictBase)
-PY37 = sys.version_info < (3, 8)
-
-# Regex precompiled patterns
-OPTIONAL_PATTERN = re.compile(r"Optional\[(.*?)\]")
-UNION_PATTERN = re.compile(r"Union\[(.*?)\]")
-
-# methods where non_tensordict data should be cleared in the return value
-_CLEAR_METADATA = {"all", "any"}
-# torch functions where we can wrap the corresponding TensorDict version
-_TD_PASS_THROUGH = {
-    torch.unbind,
-    torch.full_like,
-    torch.zeros_like,
-    torch.ones_like,
-    torch.clone,
-    torch.squeeze,
-    torch.unsqueeze,
-    torch.split,
-    torch.permute,
-    torch.split,
-    torch.stack,
-    torch.cat,
-    torch.gather,
-}
-
-
-def tensorclass(cls: T) -> T:
-    """A decorator to create :obj:`tensorclass` classes.
-
-    :obj:`tensorclass` classes are specialized :obj:`dataclass` instances that
-    can execute some pre-defined tensor operations out of the box, such as
-    indexing, item assignment, reshaping, casting to device or storage and many
-    others.
-
-    Examples:
-        >>> from tensordict import tensorclass
-        >>> import torch
-        >>> from typing import Optional
-        >>>
-        >>> @tensorclass
-        ... class MyData:
-        ...     X: torch.Tensor
-        ...     y: torch.Tensor
-        ...     z: str
-        ...     def expand_and_mask(self):
-        ...         X = self.X.unsqueeze(-1).expand_as(self.y)
-        ...         X = X[self.y]
-        ...         return X
-        ...
-        >>> data = MyData(
-        ...     X=torch.ones(3, 4, 1),
-        ...     y=torch.zeros(3, 4, 2, 2, dtype=torch.bool),
-        ...     z="test"
-        ...     batch_size=[3, 4])
-        >>> print(data)
-        MyData(
-            X=Tensor(torch.Size([3, 4, 1]), dtype=torch.float32),
-            y=Tensor(torch.Size([3, 4, 2, 2]), dtype=torch.bool),
-            z="test"
-            batch_size=[3, 4],
-            device=None,
-            is_shared=False)
-        >>> print(data.expand_and_mask())
-        tensor([])
-
-    It is also possible to nest tensorclasses instances within each other:
-        Examples:
-        >>> from tensordict import tensorclass
-        >>> import torch
-        >>> from typing import Optional
-        >>>
-        >>> @tensorclass
-        ... class NestingMyData:
-        ...     nested: MyData
-        ...
-        >>> nesting_data = NestingMyData(nested=data, batch_size=[3, 4])
-        >>> # although the data is stored as a TensorDict, the type hint helps us
-        >>> # to appropriately cast the data to the right type
-        >>> assert isinstance(nesting_data.nested, type(data))
-
-
-    """
-
-    def __torch_function__(
-        cls,
-        func: Callable,
-        types: tuple[type, ...],
-        args: tuple[Any, ...] = (),
-        kwargs: dict[str, Any] | None = None,
-    ) -> Callable:
-        if func not in _TD_PASS_THROUGH or not all(
-            issubclass(t, (Tensor, cls)) for t in types
-        ):
-            return NotImplemented
-
-        if kwargs is None:
-            kwargs = {}
-
-        # get the output type from the arguments / keyword arguments
-        if len(args) > 0:
-            tc = args[0]
-        else:
-            tc = kwargs.get("input", kwargs["tensors"])
-        if isinstance(tc, (tuple, list)):
-            tc = tc[0]
-
-        args = tuple(_arg_to_tensordict(arg) for arg in args)
-        kwargs = {key: _arg_to_tensordict(value) for key, value in kwargs.items()}
-
-        res = TD_HANDLED_FUNCTIONS[func](*args, **kwargs)
-        if isinstance(res, (list, tuple)):
-            return res.__class__(_from_tensordict_with_copy(tc, td) for td in res)
-        return _from_tensordict_with_copy(tc, res)
-
-    cls = dataclass(cls)
-    expected_keys = set(cls.__dataclass_fields__)
-
-    for attr in cls.__dataclass_fields__:
-        if attr in dir(TensorDict):
-            raise AttributeError(
-                f"Attribute name {attr} can't be used with @tensorclass"
-            )
-
-    cls.__init__ = _init_wrapper(cls.__init__)
-    cls._from_tensordict = classmethod(_from_tensordict_wrapper(expected_keys))
-    cls.from_tensordict = cls._from_tensordict
-    cls.__torch_function__ = classmethod(__torch_function__)
-    cls.__getstate__ = _getstate
-    cls.__setstate__ = _setstate
-    cls.__getattribute__ = _getattribute_wrapper(cls.__getattribute__)
-    cls.__setattr__ = _setattr_wrapper(cls.__setattr__, expected_keys)
-    cls.__getattr__ = _getattr
-    cls.__getitem__ = _getitem
-    cls.__getitems__ = _getitem
-    cls.__setitem__ = _setitem
-    cls.__repr__ = _repr
-    cls.__len__ = _len
-    cls.__eq__ = __eq__
-    cls.__ne__ = __ne__
-    cls.set = _set
-    cls.set_at_ = _set_at_
-    cls.del_ = _del_
-    cls.get = _get
-    cls.get_at = _get_at
-    cls.unbind = _unbind
-    cls.state_dict = _state_dict
-    cls.load_state_dict = _load_state_dict
-
-    for attr in TensorDict.__dict__.keys():
-        func = getattr(TensorDict, attr)
-        if (
-            inspect.ismethod(func) and func.__self__ is TensorDict
-        ):  # detects classmethods
-            setattr(cls, attr, _wrap_classmethod(cls, func))
-
-    cls.to_tensordict = _to_tensordict
-    cls.device = property(_device, _device_setter)
-    cls.batch_size = property(_batch_size, _batch_size_setter)
-
-    cls.__doc__ = f"{cls.__name__}{inspect.signature(cls)}"
-
-    tensordict_lib.tensordict._ACCEPTED_CLASSES = (
-        *tensordict_lib.tensordict._ACCEPTED_CLASSES,
-        cls,
-    )
-    return cls
-
-
-def _arg_to_tensordict(arg):
-    # if arg is a tensorclass or sequence of tensorclasses, extract the underlying
-    # tensordicts and return those instead
-    if is_tensorclass(arg):
-        return arg._tensordict
-    elif isinstance(arg, (tuple, list)) and all(is_tensorclass(item) for item in arg):
-        return arg.__class__(item._tensordict for item in arg)
-    return arg
-
-
-def _from_tensordict_with_copy(tc, tensordict):
-    # creates a new tensorclass with the same type as tc, and a copy of the
-    # non_tensordict data
-    return tc._from_tensordict(
-        tensordict=tensordict, non_tensordict=copy(tc._non_tensordict)
-    )
-
-
-def _from_tensordict_with_none(tc, tensordict):
-    # creates a new tensorclass with the same type as tc, and all non_tensordict entries
-    # set to None
-    return tc._from_tensordict(
-        tensordict=tensordict,
-        non_tensordict={key: None for key in tc._non_tensordict},
-    )
-
-
-def _init_wrapper(init: Callable) -> Callable:
-    init_sig = inspect.signature(init)
-    params = list(init_sig.parameters.values())
-    # drop first entry of params which corresponds to self and isn't passed by the user
-    required_params = [p.name for p in params[1:] if p.default is inspect._empty]
-
-    @functools.wraps(init)
-    def wrapper(
-        self,
-        *args: Any,
-        batch_size: Sequence[int] | torch.Size | int,
-        device: DeviceType | None = None,
-        **kwargs,
-    ):
-        for value, key in zip(args, self.__dataclass_fields__):
-            if key in kwargs:
-                raise ValueError(f"The key {key} is already set in kwargs")
-            kwargs[key] = value
-
-        for key, field in self.__dataclass_fields__.items():
-            if field.default_factory is not dataclasses.MISSING:
-                default = field.default_factory()
-            else:
-                default = field.default
-            if default not in (None, dataclasses.MISSING):
-                kwargs.setdefault(key, default)
-
-        missing_params = [p for p in required_params if p not in kwargs]
-        if missing_params:
-            n_missing = len(missing_params)
-            raise TypeError(
-                f"{self.__class__.__name__}.__init__() missing {n_missing} "
-                f"required positional argument{'' if n_missing == 1 else 's'}: "
-                f"""{", ".join(f"'{name}'" for name in missing_params)}"""
-            )
-
-        self._tensordict = TensorDict(
-            {}, batch_size=torch.Size(batch_size), device=device, _run_checks=False
-        )
-        # To save non tensor data (Nested tensor classes also go here)
-        self._non_tensordict = {}
-        init(self, **kwargs)
-
-    new_params = [
-        inspect.Parameter("batch_size", inspect.Parameter.KEYWORD_ONLY),
-        inspect.Parameter("device", inspect.Parameter.KEYWORD_ONLY, default=None),
-    ]
-    wrapper.__signature__ = init_sig.replace(parameters=params + new_params)
-
-    return wrapper
-
-
-def _from_tensordict_wrapper(expected_keys):
-    def wrapper(cls, tensordict, non_tensordict=None):  # noqa: D417
-        """Tensor class wrapper to instantiate a new tensor class object.
-
-        Args:
-            tensordict (TensorDict): Dictionary of tensor types
-            non_tensordict (dict): Dictionary with non-tensor and nested tensor class objects
-
-        """
-        if not isinstance(tensordict, TensorDictBase):
-            raise RuntimeError(
-                f"Expected a TensorDictBase instance but got {type(tensordict)}"
-            )
-        # Validating keys of tensordict
-        for key in tensordict.keys():
-            if key not in expected_keys:
-                raise ValueError(
-                    f"Keys from the tensordict ({set(tensordict.keys())}) must "
-                    f"correspond to the class attributes ({expected_keys})."
-                )
-
-        # Validating non-tensor keys and for key clash
-        tensor_keys = set(tensordict.keys())
-        if non_tensordict is not None:
-            for key in non_tensordict.keys():
-                if key not in expected_keys:
-                    raise ValueError(
-                        f"Keys from the non-tensor data ({set(non_tensordict.keys())}) must "
-                        f"correspond to the class attributes ({expected_keys})."
-                    )
-                if key in tensor_keys:
-                    raise KeyError(
-                        f"{key} is present in both tensor and non-tensor dicts"
-                    )
-        # bypass initialisation. this means we don't incur any overhead creating an
-        # empty tensordict and writing values to it. we can skip this because we already
-        # have a tensordict to use as the underlying tensordict
-        tc = cls.__new__(cls)
-        tc.__dict__["_tensordict"] = tensordict
-
-        tc.__dict__["_non_tensordict"] = (
-            non_tensordict if non_tensordict is not None else {}
-        )
-        # since we aren't calling the dataclass init method, we need to manually check
-        # whether a __post_init__ method has been defined and invoke it if so
-        if hasattr(tc, "__post_init__"):
-            tc.__post_init__()
-        return tc
-
-    return wrapper
-
-
-def _getstate(self) -> dict[str, Any]:
-    """Returns a state dict which consists of tensor and non_tensor dicts for serialization.
-
-    Returns:
-        dictionary of state of tensor class
-
-    """
-    return {"tensordict": self._tensordict, "non_tensordict": self._non_tensordict}
-
-
-def _setstate(self, state: dict[str, Any]) -> None:  # noqa: D417
-    """Used to set the state of an object using state parameter.
-
-    Args:
-        state (dict): State parameter to set the object
-    """
-    self._tensordict = state.get("tensordict", None)
-    self._non_tensordict = state.get("non_tensordict", None)
-
-
-def _getattribute_wrapper(getattribute: Callable) -> Callable:
-    """Retrieve the value of an object's attribute or raise AttributeError.
-
-    Args:
-        item (str) : name of the attribute to retrieve
-
-    Returns:
-        value of the attribute
-
-    """
-
-    @functools.wraps(getattribute)
-    def wrapper(self, item: str) -> Any:
-        if not item.startswith("__"):
-            if (
-                "_tensordict" in self.__dict__
-                and item in self.__dict__["_tensordict"].keys()
-            ):
-                out = self._tensordict.get(item)
-                return out
-            elif (
-                "_non_tensordict" in self.__dict__
-                and item in self.__dict__["_non_tensordict"]
-            ):
-                out = self._non_tensordict[item]
-                return out
-        return getattribute(self, item)
-
-    return wrapper
-
-
-SET_ATTRIBUTES = ("batch_size", "device", "_locked_tensordicts")
-
-
-def _setattr_wrapper(setattr_: Callable, expected_keys: set[str]) -> Callable:
-    @functools.wraps(setattr_)
-    def wrapper(self, key: str, value: Any) -> None:  # noqa: D417
-        """Set the value of an attribute for the tensor class object.
-
-        Args:
-            key (str): the name of the attribute to set
-            value (any): the value to set for the attribute
-
-        """
-        __dict__ = self.__dict__
-        if (
-            "_tensordict" not in __dict__
-            or "_non_tensordict" not in __dict__
-            or key in SET_ATTRIBUTES
-        ):
-            return setattr_(self, key, value)
-
-        out = self.set(key, value)
-        if out is not self:
-            raise RuntimeError(
-                "Cannot set attribute on a locked tensorclass, even if "
-                "clone_on_set is set to True. Use my_obj.set(...) instead."
-            )
-
-    return wrapper
-
-
-def _wrap_method(self, attr, func):
-    @functools.wraps(func)
-    def wrapped_func(*args, **kwargs):
-        args = tuple(_arg_to_tensordict(arg) for arg in args)
-        kwargs = {key: _arg_to_tensordict(value) for key, value in kwargs.items()}
-        res = func(*args, **kwargs)
-        if isinstance(res, TensorDictBase):
-            if attr.endswith("_"):
-                # in-place operation, return the current object
-                return self
-            elif attr in _CLEAR_METADATA:
-                # this is an attribute where copying the metadata makes no sense, e.g.
-                # .all or .any, so we replace all values with None
-                return self._from_tensordict(
-                    res, {k: None for k in self._non_tensordict}
-                )
-            # create a new tensorclass from res and copy the metadata from self
-            return self._from_tensordict(res, copy(self._non_tensordict))
-        return res
-
-    return wrapped_func
-
-
-def _wrap_classmethod(cls, func):
-    @functools.wraps(func)
-    def wrapped_func(*args, **kwargs):
-        res = func.__get__(cls)(*args, **kwargs)
-        # res = func(*args, **kwargs)
-        if isinstance(res, TensorDictBase):
-            # create a new tensorclass from res and copy the metadata from self
-            return cls._from_tensordict(res)
-        return res
-
-    return wrapped_func
-
-
-def _getattr(self, attr: str) -> Any:
-    """Retrieve the value of an object's attribute, or a method output if attr is callable.
-
-    Args:
-        attr: name of the attribute to retrieve or function to compute
-
-    Returns:
-        value of the attribute, or a method output applied on the instance
-
-    """
-    res = getattr(self._tensordict, attr)
-    if not callable(res):
-        return res
-    func = res
-    return _wrap_method(self, attr, func)
-
-
-def _getitem(self, item: NestedKey) -> Any:
-    """Retrieve the class object at the given index. Indexing will happen for nested tensors as well.
-
-    Args:
-       item (int or any other valid index type): index of the object to retrieve
-
-    Returns:
-        Tensor class object at the given index
-
-    """
-    if isinstance(item, str) or (
-        isinstance(item, tuple) and all(isinstance(_item, str) for _item in item)
-    ):
-        raise ValueError(f"Invalid indexing arguments: {item}.")
-    tensor_res = self._tensordict[item]
-    return _from_tensordict_with_copy(self, tensor_res)  # device=res.device)
-
-
-def _setitem(self, item: NestedKey, value: Any) -> None:  # noqa: D417
-    """Set the value of the Tensor class object at the given index. Note that there is no strict validation on non-tensor values.
-
-    Args:
-        item (int or any other valid index type): index of the object to set
-        value (any): value to set for the item
-
-    """
-    if isinstance(item, str) or (
-        isinstance(item, tuple) and all(isinstance(_item, str) for _item in item)
-    ):
-        raise ValueError("Invalid indexing arguments.")
-
-    if not is_tensorclass(value) and not isinstance(
-        value, (TensorDictBase, numbers.Number, Tensor, MemmapTensor)
-    ):
-        raise ValueError(
-            f"__setitem__ only supports tensorclasses, tensordicts,"
-            f" numeric scalars and tensors. Got {type(value)}"
-        )
-
-    if is_tensorclass(value):
-        if not isinstance(value, self.__class__):
-            self_keys = set().union(self._non_tensordict, self._tensordict.keys())
-            value_keys = set().union(value._non_tensordict, value._tensordict.keys())
-            if self_keys != value_keys:
-                # if tensorclass but different class ensure that all keys are equal
-                raise ValueError(
-                    "__setitem__ is only allowed for same-class or "
-                    "compatible class (i.e. same members) assignment"
-                )
-
-        # Validating the non-tensor data before setting the item
-        for key, val in value._non_tensordict.items():
-            # Raise a warning if non_tensor data doesn't match
-            if (
-                key in self._non_tensordict.keys()
-                and val is not self._non_tensordict[key]
-            ):
-                warnings.warn(
-                    f"Meta data at {repr(key)} may or may not be equal, "
-                    f"this may result in undefined behaviours",
-                    category=UserWarning,
-                    stacklevel=2,
-                )
-
-        for key in value._tensordict.keys():
-            # Making sure that the key-clashes won't happen, if the key is present
-            # in tensor data in value we will honor that and remove the key-value
-            # pair from non-tensor data
-            if key in self._non_tensordict.keys():
-                del self._non_tensordict[key]
-
-        self._tensordict[item] = value._tensordict
-    else:  # it is one of accepted "broadcast" types
-        # attempt broadcast on all tensordata and nested tensorclasses
-        self._tensordict[item] = value
-        for key, val in self._non_tensordict.items():
-            if is_tensorclass(val):
-                _setitem(self._non_tensordict[key], item, value)
-
-
-def _repr(self) -> str:
-    """Return a string representation of Tensor class object."""
-    fields = _all_td_fields_as_str(self._tensordict)
-    field_str = [fields] if fields else []
-    non_tensor_fields = _all_non_td_fields_as_str(self._non_tensordict)
-    batch_size_str = indent(f"batch_size={self.batch_size}", 4 * " ")
-    device_str = indent(f"device={self.device}", 4 * " ")
-    is_shared_str = indent(f"is_shared={self.is_shared()}", 4 * " ")
-    if len(non_tensor_fields) > 0:
-        non_tensor_field_str = indent(
-            ",\n".join(non_tensor_fields),
-            4 * " ",
-        )
-        string = ",\n".join(
-            field_str
-            + [non_tensor_field_str, batch_size_str, device_str, is_shared_str]
-        )
-    else:
-        string = ",\n".join(field_str + [batch_size_str, device_str, is_shared_str])
-    return f"{self.__class__.__name__}(\n{string})"
-
-
-def _len(self) -> int:
-    """Returns the length of first dimension, if there is, otherwise 0."""
-    return len(self._tensordict)
-
-
-def _to_tensordict(self) -> TensorDict:
-    """Convert the tensorclass into a regular TensorDict.
-
-    Makes a copy of all entries. Memmap and shared memory tensors are converted to
-    regular tensors.
-
-    Returns:
-        A new TensorDict object containing the same values as the tensorclass.
-
-    """
-    td = self._tensordict.to_tensordict()
-    return td
-
-
-def _device(self) -> torch.device:
-    """Retrieves the device type of tensor class."""
-    return self._tensordict.device
-
-
-def _device_setter(self, value: DeviceType) -> None:
-    raise RuntimeError(
-        "device cannot be set using tensorclass.device = device, "
-        "because device cannot be updated in-place. To update device, use "
-        "tensorclass.to(new_device), which will return a new tensorclass "
-        "on the new device."
-    )
-
-
-def _set(self, key: NestedKey, value: Any, inplace: bool = False):
-    """Sets a new key-value pair.
-
-    Args:
-        key (str, tuple of str): name of the key to be set.
-           If tuple of str it is equivalent to chained calls of getattr
-           followed by a final setattr.
-        value (Any): value to be stored in the tensorclass
-        inplace (bool, optional): if ``True``, set will tentatively try to
-            update the value in-place. If ``False`` or if the key isn't present,
-            the value will be simply written at its destination.
-
-    Returns:
-        self
-
-    """
-    if isinstance(key, str):
-        __dict__ = self.__dict__
-        if __dict__["_tensordict"].is_locked:
-            raise RuntimeError(TensorDictBase.LOCK_ERROR)
-        expected_keys = self.__dataclass_fields__
-        if key not in expected_keys:
-            raise AttributeError(
-                f"Cannot set the attribute '{key}', expected attributes are {expected_keys}."
-            )
-
-        if isinstance(value, tuple(tensordict_lib.tensordict._ACCEPTED_CLASSES)):
-            # Avoiding key clash, honoring the user input to assign tensor type data to the key
-            if key in self._non_tensordict.keys():
-                if inplace:
-                    raise RuntimeError(
-                        f"Cannot update an existing entry of type {type(self._non_tensordict.get(key))} with a value of type {type(value)}."
-                    )
-                del self._non_tensordict[key]
-            self._tensordict.set(key, value, inplace=inplace)
-        else:
-            # Avoiding key clash, honoring the user input to assign non-tensor data to the key
-            if key in self._tensordict.keys():
-                if inplace:
-                    raise RuntimeError(
-                        f"Cannot update an existing entry of type {type(self._tensordict.get(key))} with a value of type {type(value)}."
-                    )
-                self._tensordict.del_(key)
-            # Saving all non-tensor attributes
-            self._non_tensordict[key] = value
-        return self
-
-    if isinstance(key, tuple) and len(key):
-        key = _unravel_key_to_tuple(key)
-        if len(key) > 1:
-            return self.set(key[0], getattr(self, key[0]).set(key[1:], value))
-        out = self.set(key[0], value)
-        return out
-    raise ValueError(
-        f"Supported type for key are str and tuple, got {key} of type {type(key)}"
-    )
-
-
-def _del_(self, key):
-    key = _unravel_key_to_tuple(key)
-    if len(key) > 1:
-        td = self.get(key[0])
-        td.del_(key[1:])
-        return
-    if key[0] in self._tensordict.keys():
-        self._tensordict.del_(key[0])
-        # self.set(key[0], None)
-    elif key[0] in self._non_tensordict.keys():
-        self._non_tensordict[key[0]] = None
-    else:
-        raise KeyError(f"Key {key} could not be found in tensorclass {self}.")
-    return
-
-
-def _set_at_(self, key: NestedKey, value: Any, idx: IndexType):
-    if key in self._non_tensordict:
-        del self._non_tensordict[key]
-    return self._tensordict.set_at_(key, value, idx)
-
-
-def _get(self, key: NestedKey, default: Any = NO_DEFAULT):
-    """Gets the value stored with the input key.
-
-    Args:
-        key (str, tuple of str): key to be queried. If tuple of str it is
-            equivalent to chained calls of getattr.
-        default: default value if the key is not found in the tensorclass.
-
-    Returns:
-        value stored with the input key
-
-    """
-    if isinstance(key, str):
-        key = (key,)
-
-    if isinstance(key, tuple):
-        try:
-            if len(key) > 1:
-                return getattr(self, key[0]).get(key[1:])
-            return getattr(self, key[0])
-        except AttributeError:
-            if default is NO_DEFAULT:
-                raise
-            return default
-    raise ValueError(f"Supported type for key are str and tuple, got {type(key)}")
-
-
-def _get_at(self, key: NestedKey, idx, default: Any = NO_DEFAULT):
-    try:
-        return self.get(key, NO_DEFAULT)[idx]
-    except AttributeError:
-        if default is NO_DEFAULT:
-            raise
-        return default
-
-
-def _batch_size(self) -> torch.Size:
-    """Retrieves the batch size for the tensor class.
-
-    Returns:
-        batch size (torch.Size)
-
-    """
-    return self._tensordict.batch_size
-
-
-def _batch_size_setter(self, new_size: torch.Size) -> None:  # noqa: D417
-    """Set the value of batch_size.
-
-    Args:
-        new_size (torch.Size): new_batch size to be set
-
-    """
-    self._tensordict._batch_size_setter(new_size)
-
-
-def _state_dict(self) -> dict[str, Any]:
-    """Returns a state_dict dictionary that can be used to save and load data from a tensorclass."""
-    state_dict = {"_tensordict": self._tensordict.state_dict()}
-    state_dict["_non_tensordict"] = copy(self._non_tensordict)
-    return state_dict
-
-
-def _load_state_dict(self, state_dict: dict[str, Any]):
-    """Loads a state_dict attemptedly in-place on the destination tensorclass."""
-    for key, item in state_dict.items():
-        # keys will never be nested which facilitates everything, but let's
-        # double check in case someone does something nasty
-        if not isinstance(key, str):
-            raise TypeError("Only str keys are allowed when calling load_state_dict.")
-        if key == "_non_tensordict":
-            for sub_key, sub_item in item.items():
-                # sub_item is the state dict of a tensorclass
-                if isinstance(sub_item, dict) and "_non_tensordict" in sub_item:
-                    raise RuntimeError(
-                        "Loading a saved tensorclass on a uninitialized tensorclass is not allowed"
-                    )
-                else:
-                    # check that sub_key is part of the tensorclass
-                    if sub_key not in self.__class__.__dataclass_fields__:
-                        raise KeyError(
-                            f"Key '{sub_key}' wasn't expected in the state-dict."
-                        )
-                    self._non_tensordict[sub_key] = sub_item
-        elif key == "_tensordict":
-            for sub_key in item.keys():
-                if (
-                    sub_key not in self.__class__.__dataclass_fields__
-                    and sub_key not in ("__batch_size", "__device")
-                ):
-                    raise KeyError(
-                        f"Key '{sub_key}' wasn't expected in the state-dict."
-                    )
-
-            self._tensordict.load_state_dict(item)
-        else:
-            raise KeyError(f"Key '{key}' wasn't expected in the state-dict.")
-
-    return self
-
-
-def __eq__(self, other: object) -> bool:
-    """Compares the Tensor class object to another object for equality. However, the equality check for non-tensor data is not performed.
-
-    Args:
-        other: object to compare to this object. Can be a tensorclass, a
-            tensordict or any compatible type (int, float or tensor), in
-            which case the equality check will be propagated to the leaves.
-
-    Returns:
-        False if the objects are of different class types, Tensorclass of boolean
-        values for tensor attributes and None for non-tensor attributes
-
-    Examples:
-        >>> @tensorclass
-        ... class MyClass:
-        ...     x: Tensor
-        ...     y: "MyClass"
-        ...     z: str
-        ...
-        >>> c1 = MyClass(
-        ...     x=torch.randn(3, 4),
-        ...     y=MyClass(
-        ...         x=torch.randn(3, 4, 1),
-        ...         y=None,
-        ...         z="bar",
-        ...         batch_size=[3, 4, 1],
-        ...     ),
-        ...     z="foo",
-        ...     batch_size=[3, 4],
-        ... )
-        >>> c2 = c1.clone()
-        >>> print(c1 == c2)
-        MyClass(
-            x=Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.bool, is_shared=False),
-            y=MyClass(
-                x=Tensor(shape=torch.Size([3, 4, 1]), device=cpu, dtype=torch.bool, is_shared=False),
-                y=None,
-                z=None,
-                batch_size=torch.Size([3, 4, 1]),
-                device=None,
-                is_shared=False),
-            z=None,
-            batch_size=torch.Size([3, 4]),
-            device=None,
-            is_shared=False)
-        >>> assert (c1 == c2).all()
-        >>> assert (c1[:2] == c2[:2]).all()
-        >>> assert not (c1 == c2.apply(lambda x: x+1)).all()
-
-    """
-    if not is_tensor_collection(other) and not isinstance(
-        other, (dict, numbers.Number, Tensor, MemmapTensor)
-    ):
-        return False
-    if is_tensorclass(other):
-        tensor = self._tensordict == other._tensordict
-    else:
-        tensor = self._tensordict == other
-    return _from_tensordict_with_none(self, tensor)
-
-
-def __ne__(self, other: object) -> bool:
-    """Compare the Tensor class object to another object for inequality. However, the equality check for non-tensor data is not performed.
-
-    Args:
-        other: object to compare to this object
-
-    Returns:
-        False if the objects are of different class types, Tensorclass of boolean values for tensor attributes and None for non-tensor attributes
-
-    Examples:
-        >>> @tensorclass
-        ... class MyClass:
-        ...     x: Tensor
-        ...     y: "MyClass"
-        ...     z: str
-        ...
-        >>> c1 = MyClass(
-        ...     x=torch.randn(3, 4),
-        ...     y=MyClass(
-        ...         x=torch.randn(3, 4, 1),
-        ...         y=None,
-        ...         z="bar",
-        ...         batch_size=[3, 4, 1],
-        ...     ),
-        ...     z="foo",
-        ...     batch_size=[3, 4],
-        ... )
-        >>> c2 = c1.clone()
-        >>> print(c1 != c2)
-        MyClass(
-            x=Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.bool, is_shared=False),
-            y=MyClass(
-                x=Tensor(shape=torch.Size([3, 4, 1]), device=cpu, dtype=torch.bool, is_shared=False),
-                y=None,
-                z=None,
-                batch_size=torch.Size([3, 4, 1]),
-                device=None,
-                is_shared=False),
-            z=None,
-            batch_size=torch.Size([3, 4]),
-            device=None,
-            is_shared=False)
-        >>> c2 = c2.apply(lambda x: x+1)
-        >>> assert (c1 != c2).all()
-
-    """
-    if not is_tensor_collection(other) and not isinstance(
-        other, (dict, numbers.Number, Tensor, MemmapTensor)
-    ):
-        return True
-    if is_tensorclass(other):
-        tensor = self._tensordict != other._tensordict
-    else:
-        tensor = self._tensordict != other
-    return _from_tensordict_with_none(self, tensor)
-
-
-def _single_td_field_as_str(key, item, tensordict):
-    """Returns a string as a  key-value pair of tensordict.
-
-    Args:
-        key (str): key of tensor dict item
-        item (tensor type): value to be returned for key
-        tensordict (Tensordict): Tensordict object
-
-    Returns:
-        String representation of a key-value pair
-
-    """
-    if is_tensor_collection(type(item)):
-        return f"{key}={repr(tensordict[key])}"
-    return f"{key}={_get_repr(item)}"
-
-
-def _all_td_fields_as_str(td: TensorDictBase) -> str:
-    """Returns indented representation of tensor dict values as a key-value pairs.
-
-    Args:
-        td (TensorDict) : Tensordict object
-
-    Returns:
-        String representation of all tensor data
-
-    """
-    return indent(
-        ",\n".join(
-            sorted([_single_td_field_as_str(key, item, td) for key, item in td.items()])
-        ),
-        4 * " ",
-    )
-
-
-def _all_non_td_fields_as_str(src_dict) -> list:
-    """Returns a list of string representation of non-tensor key-value pairs.
-
-    Args:
-        src_dict (dict): non_tensor_dict
-
-    Returns:
-        result (list): list of strings with key-value representation
-
-    """
-    result = []
-    for key, val in src_dict.items():
-        if not is_tensor_collection(val):
-            result.append(f"{key}={repr(val)}")
-
-    return result
-
-
-def _unbind(self, dim: int):
-    """Returns a tuple of indexed tensorclass instances unbound along the indicated dimension.
-
-    Resulting tensorclass instances will share the storage of the initial tensorclass instance.
-
-    """
-    return tuple(
-        self._from_tensordict(td, non_tensordict=copy(self._non_tensordict))
-        for td in self._tensordict.unbind(dim)
-    )
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import dataclasses
+import functools
+import inspect
+import numbers
+import re
+import sys
+import warnings
+from copy import copy
+from dataclasses import dataclass
+from textwrap import indent
+from typing import Any, Callable, Sequence, TypeVar
+
+import tensordict as tensordict_lib
+
+import torch
+from tensordict._tensordict import _unravel_key_to_tuple
+from tensordict.memmap import MemmapTensor
+from tensordict.tensordict import (
+    _get_repr,
+    is_tensor_collection,
+    NO_DEFAULT,
+    TD_HANDLED_FUNCTIONS,
+    TensorDict,
+    TensorDictBase,
+)
+
+from tensordict.utils import DeviceType, IndexType, is_tensorclass, NestedKey
+from torch import Tensor
+
+T = TypeVar("T", bound=TensorDictBase)
+PY37 = sys.version_info < (3, 8)
+
+# Regex precompiled patterns
+OPTIONAL_PATTERN = re.compile(r"Optional\[(.*?)\]")
+UNION_PATTERN = re.compile(r"Union\[(.*?)\]")
+
+# methods where non_tensordict data should be cleared in the return value
+_CLEAR_METADATA = {"all", "any"}
+# torch functions where we can wrap the corresponding TensorDict version
+_TD_PASS_THROUGH = {
+    torch.unbind,
+    torch.full_like,
+    torch.zeros_like,
+    torch.ones_like,
+    torch.clone,
+    torch.squeeze,
+    torch.unsqueeze,
+    torch.split,
+    torch.permute,
+    torch.split,
+    torch.stack,
+    torch.cat,
+    torch.gather,
+}
+
+
+def tensorclass(cls: T) -> T:
+    """A decorator to create :obj:`tensorclass` classes.
+
+    :obj:`tensorclass` classes are specialized :obj:`dataclass` instances that
+    can execute some pre-defined tensor operations out of the box, such as
+    indexing, item assignment, reshaping, casting to device or storage and many
+    others.
+
+    Examples:
+        >>> from tensordict import tensorclass
+        >>> import torch
+        >>> from typing import Optional
+        >>>
+        >>> @tensorclass
+        ... class MyData:
+        ...     X: torch.Tensor
+        ...     y: torch.Tensor
+        ...     z: str
+        ...     def expand_and_mask(self):
+        ...         X = self.X.unsqueeze(-1).expand_as(self.y)
+        ...         X = X[self.y]
+        ...         return X
+        ...
+        >>> data = MyData(
+        ...     X=torch.ones(3, 4, 1),
+        ...     y=torch.zeros(3, 4, 2, 2, dtype=torch.bool),
+        ...     z="test"
+        ...     batch_size=[3, 4])
+        >>> print(data)
+        MyData(
+            X=Tensor(torch.Size([3, 4, 1]), dtype=torch.float32),
+            y=Tensor(torch.Size([3, 4, 2, 2]), dtype=torch.bool),
+            z="test"
+            batch_size=[3, 4],
+            device=None,
+            is_shared=False)
+        >>> print(data.expand_and_mask())
+        tensor([])
+
+    It is also possible to nest tensorclasses instances within each other:
+        Examples:
+        >>> from tensordict import tensorclass
+        >>> import torch
+        >>> from typing import Optional
+        >>>
+        >>> @tensorclass
+        ... class NestingMyData:
+        ...     nested: MyData
+        ...
+        >>> nesting_data = NestingMyData(nested=data, batch_size=[3, 4])
+        >>> # although the data is stored as a TensorDict, the type hint helps us
+        >>> # to appropriately cast the data to the right type
+        >>> assert isinstance(nesting_data.nested, type(data))
+
+
+    """
+
+    def __torch_function__(
+        cls,
+        func: Callable,
+        types: tuple[type, ...],
+        args: tuple[Any, ...] = (),
+        kwargs: dict[str, Any] | None = None,
+    ) -> Callable:
+        if func not in _TD_PASS_THROUGH or not all(
+            issubclass(t, (Tensor, cls)) for t in types
+        ):
+            return NotImplemented
+
+        if kwargs is None:
+            kwargs = {}
+
+        # get the output type from the arguments / keyword arguments
+        if len(args) > 0:
+            tc = args[0]
+        else:
+            tc = kwargs.get("input", kwargs["tensors"])
+        if isinstance(tc, (tuple, list)):
+            tc = tc[0]
+
+        args = tuple(_arg_to_tensordict(arg) for arg in args)
+        kwargs = {key: _arg_to_tensordict(value) for key, value in kwargs.items()}
+
+        res = TD_HANDLED_FUNCTIONS[func](*args, **kwargs)
+        if isinstance(res, (list, tuple)):
+            return res.__class__(_from_tensordict_with_copy(tc, td) for td in res)
+        return _from_tensordict_with_copy(tc, res)
+
+    cls = dataclass(cls)
+    expected_keys = set(cls.__dataclass_fields__)
+
+    for attr in cls.__dataclass_fields__:
+        if attr in dir(TensorDict):
+            raise AttributeError(
+                f"Attribute name {attr} can't be used with @tensorclass"
+            )
+
+    cls.__init__ = _init_wrapper(cls.__init__)
+    cls._from_tensordict = classmethod(_from_tensordict_wrapper(expected_keys))
+    cls.from_tensordict = cls._from_tensordict
+    cls.__torch_function__ = classmethod(__torch_function__)
+    cls.__getstate__ = _getstate
+    cls.__setstate__ = _setstate
+    cls.__getattribute__ = _getattribute_wrapper(cls.__getattribute__)
+    cls.__setattr__ = _setattr_wrapper(cls.__setattr__, expected_keys)
+    cls.__getattr__ = _getattr
+    cls.__getitem__ = _getitem
+    cls.__getitems__ = _getitem
+    cls.__setitem__ = _setitem
+    cls.__repr__ = _repr
+    cls.__len__ = _len
+    cls.__eq__ = __eq__
+    cls.__ne__ = __ne__
+    cls.set = _set
+    cls.set_at_ = _set_at_
+    cls.del_ = _del_
+    cls.get = _get
+    cls.get_at = _get_at
+    cls.unbind = _unbind
+    cls.state_dict = _state_dict
+    cls.load_state_dict = _load_state_dict
+
+    for attr in TensorDict.__dict__.keys():
+        func = getattr(TensorDict, attr)
+        if (
+            inspect.ismethod(func) and func.__self__ is TensorDict
+        ):  # detects classmethods
+            setattr(cls, attr, _wrap_classmethod(cls, func))
+
+    cls.to_tensordict = _to_tensordict
+    cls.device = property(_device, _device_setter)
+    cls.batch_size = property(_batch_size, _batch_size_setter)
+
+    cls.__doc__ = f"{cls.__name__}{inspect.signature(cls)}"
+
+    tensordict_lib.tensordict._ACCEPTED_CLASSES = (
+        *tensordict_lib.tensordict._ACCEPTED_CLASSES,
+        cls,
+    )
+    return cls
+
+
+def _arg_to_tensordict(arg):
+    # if arg is a tensorclass or sequence of tensorclasses, extract the underlying
+    # tensordicts and return those instead
+    if is_tensorclass(arg):
+        return arg._tensordict
+    elif isinstance(arg, (tuple, list)) and all(is_tensorclass(item) for item in arg):
+        return arg.__class__(item._tensordict for item in arg)
+    return arg
+
+
+def _from_tensordict_with_copy(tc, tensordict):
+    # creates a new tensorclass with the same type as tc, and a copy of the
+    # non_tensordict data
+    return tc._from_tensordict(
+        tensordict=tensordict, non_tensordict=copy(tc._non_tensordict)
+    )
+
+
+def _from_tensordict_with_none(tc, tensordict):
+    # creates a new tensorclass with the same type as tc, and all non_tensordict entries
+    # set to None
+    return tc._from_tensordict(
+        tensordict=tensordict,
+        non_tensordict={key: None for key in tc._non_tensordict},
+    )
+
+
+def _init_wrapper(init: Callable) -> Callable:
+    init_sig = inspect.signature(init)
+    params = list(init_sig.parameters.values())
+    # drop first entry of params which corresponds to self and isn't passed by the user
+    required_params = [p.name for p in params[1:] if p.default is inspect._empty]
+
+    @functools.wraps(init)
+    def wrapper(
+        self,
+        *args: Any,
+        batch_size: Sequence[int] | torch.Size | int,
+        device: DeviceType | None = None,
+        **kwargs,
+    ):
+        for value, key in zip(args, self.__dataclass_fields__):
+            if key in kwargs:
+                raise ValueError(f"The key {key} is already set in kwargs")
+            kwargs[key] = value
+
+        for key, field in self.__dataclass_fields__.items():
+            if field.default_factory is not dataclasses.MISSING:
+                default = field.default_factory()
+            else:
+                default = field.default
+            if default not in (None, dataclasses.MISSING):
+                kwargs.setdefault(key, default)
+
+        missing_params = [p for p in required_params if p not in kwargs]
+        if missing_params:
+            n_missing = len(missing_params)
+            raise TypeError(
+                f"{self.__class__.__name__}.__init__() missing {n_missing} "
+                f"required positional argument{'' if n_missing == 1 else 's'}: "
+                f"""{", ".join(f"'{name}'" for name in missing_params)}"""
+            )
+
+        self._tensordict = TensorDict(
+            {}, batch_size=torch.Size(batch_size), device=device, _run_checks=False
+        )
+        # To save non tensor data (Nested tensor classes also go here)
+        self._non_tensordict = {}
+        init(self, **kwargs)
+
+    new_params = [
+        inspect.Parameter("batch_size", inspect.Parameter.KEYWORD_ONLY),
+        inspect.Parameter("device", inspect.Parameter.KEYWORD_ONLY, default=None),
+    ]
+    wrapper.__signature__ = init_sig.replace(parameters=params + new_params)
+
+    return wrapper
+
+
+def _from_tensordict_wrapper(expected_keys):
+    def wrapper(cls, tensordict, non_tensordict=None):  # noqa: D417
+        """Tensor class wrapper to instantiate a new tensor class object.
+
+        Args:
+            tensordict (TensorDict): Dictionary of tensor types
+            non_tensordict (dict): Dictionary with non-tensor and nested tensor class objects
+
+        """
+        if not isinstance(tensordict, TensorDictBase):
+            raise RuntimeError(
+                f"Expected a TensorDictBase instance but got {type(tensordict)}"
+            )
+        # Validating keys of tensordict
+        for key in tensordict.keys():
+            if key not in expected_keys:
+                raise ValueError(
+                    f"Keys from the tensordict ({set(tensordict.keys())}) must "
+                    f"correspond to the class attributes ({expected_keys})."
+                )
+
+        # Validating non-tensor keys and for key clash
+        tensor_keys = set(tensordict.keys())
+        if non_tensordict is not None:
+            for key in non_tensordict.keys():
+                if key not in expected_keys:
+                    raise ValueError(
+                        f"Keys from the non-tensor data ({set(non_tensordict.keys())}) must "
+                        f"correspond to the class attributes ({expected_keys})."
+                    )
+                if key in tensor_keys:
+                    raise KeyError(
+                        f"{key} is present in both tensor and non-tensor dicts"
+                    )
+        # bypass initialisation. this means we don't incur any overhead creating an
+        # empty tensordict and writing values to it. we can skip this because we already
+        # have a tensordict to use as the underlying tensordict
+        tc = cls.__new__(cls)
+        tc.__dict__["_tensordict"] = tensordict
+
+        tc.__dict__["_non_tensordict"] = (
+            non_tensordict if non_tensordict is not None else {}
+        )
+        # since we aren't calling the dataclass init method, we need to manually check
+        # whether a __post_init__ method has been defined and invoke it if so
+        if hasattr(tc, "__post_init__"):
+            tc.__post_init__()
+        return tc
+
+    return wrapper
+
+
+def _getstate(self) -> dict[str, Any]:
+    """Returns a state dict which consists of tensor and non_tensor dicts for serialization.
+
+    Returns:
+        dictionary of state of tensor class
+
+    """
+    return {"tensordict": self._tensordict, "non_tensordict": self._non_tensordict}
+
+
+def _setstate(self, state: dict[str, Any]) -> None:  # noqa: D417
+    """Used to set the state of an object using state parameter.
+
+    Args:
+        state (dict): State parameter to set the object
+    """
+    self._tensordict = state.get("tensordict", None)
+    self._non_tensordict = state.get("non_tensordict", None)
+
+
+def _getattribute_wrapper(getattribute: Callable) -> Callable:
+    """Retrieve the value of an object's attribute or raise AttributeError.
+
+    Args:
+        item (str) : name of the attribute to retrieve
+
+    Returns:
+        value of the attribute
+
+    """
+
+    @functools.wraps(getattribute)
+    def wrapper(self, item: str) -> Any:
+        if not item.startswith("__"):
+            if (
+                "_tensordict" in self.__dict__
+                and item in self.__dict__["_tensordict"].keys()
+            ):
+                out = self._tensordict.get(item)
+                return out
+            elif (
+                "_non_tensordict" in self.__dict__
+                and item in self.__dict__["_non_tensordict"]
+            ):
+                out = self._non_tensordict[item]
+                return out
+        return getattribute(self, item)
+
+    return wrapper
+
+
+SET_ATTRIBUTES = ("batch_size", "device", "_locked_tensordicts")
+
+
+def _setattr_wrapper(setattr_: Callable, expected_keys: set[str]) -> Callable:
+    @functools.wraps(setattr_)
+    def wrapper(self, key: str, value: Any) -> None:  # noqa: D417
+        """Set the value of an attribute for the tensor class object.
+
+        Args:
+            key (str): the name of the attribute to set
+            value (any): the value to set for the attribute
+
+        """
+        __dict__ = self.__dict__
+        if (
+            "_tensordict" not in __dict__
+            or "_non_tensordict" not in __dict__
+            or key in SET_ATTRIBUTES
+        ):
+            return setattr_(self, key, value)
+
+        out = self.set(key, value)
+        if out is not self:
+            raise RuntimeError(
+                "Cannot set attribute on a locked tensorclass, even if "
+                "clone_on_set is set to True. Use my_obj.set(...) instead."
+            )
+
+    return wrapper
+
+
+def _wrap_method(self, attr, func):
+    @functools.wraps(func)
+    def wrapped_func(*args, **kwargs):
+        args = tuple(_arg_to_tensordict(arg) for arg in args)
+        kwargs = {key: _arg_to_tensordict(value) for key, value in kwargs.items()}
+        res = func(*args, **kwargs)
+        if isinstance(res, TensorDictBase):
+            if attr.endswith("_"):
+                # in-place operation, return the current object
+                return self
+            elif attr in _CLEAR_METADATA:
+                # this is an attribute where copying the metadata makes no sense, e.g.
+                # .all or .any, so we replace all values with None
+                return self._from_tensordict(
+                    res, {k: None for k in self._non_tensordict}
+                )
+            # create a new tensorclass from res and copy the metadata from self
+            return self._from_tensordict(res, copy(self._non_tensordict))
+        return res
+
+    return wrapped_func
+
+
+def _wrap_classmethod(cls, func):
+    @functools.wraps(func)
+    def wrapped_func(*args, **kwargs):
+        res = func.__get__(cls)(*args, **kwargs)
+        # res = func(*args, **kwargs)
+        if isinstance(res, TensorDictBase):
+            # create a new tensorclass from res and copy the metadata from self
+            return cls._from_tensordict(res)
+        return res
+
+    return wrapped_func
+
+
+def _getattr(self, attr: str) -> Any:
+    """Retrieve the value of an object's attribute, or a method output if attr is callable.
+
+    Args:
+        attr: name of the attribute to retrieve or function to compute
+
+    Returns:
+        value of the attribute, or a method output applied on the instance
+
+    """
+    res = getattr(self._tensordict, attr)
+    if not callable(res):
+        return res
+    func = res
+    return _wrap_method(self, attr, func)
+
+
+def _getitem(self, item: NestedKey) -> Any:
+    """Retrieve the class object at the given index. Indexing will happen for nested tensors as well.
+
+    Args:
+       item (int or any other valid index type): index of the object to retrieve
+
+    Returns:
+        Tensor class object at the given index
+
+    """
+    if isinstance(item, str) or (
+        isinstance(item, tuple) and all(isinstance(_item, str) for _item in item)
+    ):
+        raise ValueError(f"Invalid indexing arguments: {item}.")
+    tensor_res = self._tensordict[item]
+    return _from_tensordict_with_copy(self, tensor_res)  # device=res.device)
+
+
+def _setitem(self, item: NestedKey, value: Any) -> None:  # noqa: D417
+    """Set the value of the Tensor class object at the given index. Note that there is no strict validation on non-tensor values.
+
+    Args:
+        item (int or any other valid index type): index of the object to set
+        value (any): value to set for the item
+
+    """
+    if isinstance(item, str) or (
+        isinstance(item, tuple) and all(isinstance(_item, str) for _item in item)
+    ):
+        raise ValueError("Invalid indexing arguments.")
+
+    if not is_tensorclass(value) and not isinstance(
+        value, (TensorDictBase, numbers.Number, Tensor, MemmapTensor)
+    ):
+        raise ValueError(
+            f"__setitem__ only supports tensorclasses, tensordicts,"
+            f" numeric scalars and tensors. Got {type(value)}"
+        )
+
+    if is_tensorclass(value):
+        if not isinstance(value, self.__class__):
+            self_keys = set().union(self._non_tensordict, self._tensordict.keys())
+            value_keys = set().union(value._non_tensordict, value._tensordict.keys())
+            if self_keys != value_keys:
+                # if tensorclass but different class ensure that all keys are equal
+                raise ValueError(
+                    "__setitem__ is only allowed for same-class or "
+                    "compatible class (i.e. same members) assignment"
+                )
+
+        # Validating the non-tensor data before setting the item
+        for key, val in value._non_tensordict.items():
+            # Raise a warning if non_tensor data doesn't match
+            if (
+                key in self._non_tensordict.keys()
+                and val is not self._non_tensordict[key]
+            ):
+                warnings.warn(
+                    f"Meta data at {repr(key)} may or may not be equal, "
+                    f"this may result in undefined behaviours",
+                    category=UserWarning,
+                    stacklevel=2,
+                )
+
+        for key in value._tensordict.keys():
+            # Making sure that the key-clashes won't happen, if the key is present
+            # in tensor data in value we will honor that and remove the key-value
+            # pair from non-tensor data
+            if key in self._non_tensordict.keys():
+                del self._non_tensordict[key]
+
+        self._tensordict[item] = value._tensordict
+    else:  # it is one of accepted "broadcast" types
+        # attempt broadcast on all tensordata and nested tensorclasses
+        self._tensordict[item] = value
+        for key, val in self._non_tensordict.items():
+            if is_tensorclass(val):
+                _setitem(self._non_tensordict[key], item, value)
+
+
+def _repr(self) -> str:
+    """Return a string representation of Tensor class object."""
+    fields = _all_td_fields_as_str(self._tensordict)
+    field_str = [fields] if fields else []
+    non_tensor_fields = _all_non_td_fields_as_str(self._non_tensordict)
+    batch_size_str = indent(f"batch_size={self.batch_size}", 4 * " ")
+    device_str = indent(f"device={self.device}", 4 * " ")
+    is_shared_str = indent(f"is_shared={self.is_shared()}", 4 * " ")
+    if len(non_tensor_fields) > 0:
+        non_tensor_field_str = indent(
+            ",\n".join(non_tensor_fields),
+            4 * " ",
+        )
+        string = ",\n".join(
+            field_str
+            + [non_tensor_field_str, batch_size_str, device_str, is_shared_str]
+        )
+    else:
+        string = ",\n".join(field_str + [batch_size_str, device_str, is_shared_str])
+    return f"{self.__class__.__name__}(\n{string})"
+
+
+def _len(self) -> int:
+    """Returns the length of first dimension, if there is, otherwise 0."""
+    return len(self._tensordict)
+
+
+def _to_tensordict(self) -> TensorDict:
+    """Convert the tensorclass into a regular TensorDict.
+
+    Makes a copy of all entries. Memmap and shared memory tensors are converted to
+    regular tensors.
+
+    Returns:
+        A new TensorDict object containing the same values as the tensorclass.
+
+    """
+    td = self._tensordict.to_tensordict()
+    return td
+
+
+def _device(self) -> torch.device:
+    """Retrieves the device type of tensor class."""
+    return self._tensordict.device
+
+
+def _device_setter(self, value: DeviceType) -> None:
+    raise RuntimeError(
+        "device cannot be set using tensorclass.device = device, "
+        "because device cannot be updated in-place. To update device, use "
+        "tensorclass.to(new_device), which will return a new tensorclass "
+        "on the new device."
+    )
+
+
+def _set(self, key: NestedKey, value: Any, inplace: bool = False):
+    """Sets a new key-value pair.
+
+    Args:
+        key (str, tuple of str): name of the key to be set.
+           If tuple of str it is equivalent to chained calls of getattr
+           followed by a final setattr.
+        value (Any): value to be stored in the tensorclass
+        inplace (bool, optional): if ``True``, set will tentatively try to
+            update the value in-place. If ``False`` or if the key isn't present,
+            the value will be simply written at its destination.
+
+    Returns:
+        self
+
+    """
+    if isinstance(key, str):
+        __dict__ = self.__dict__
+        if __dict__["_tensordict"].is_locked:
+            raise RuntimeError(TensorDictBase.LOCK_ERROR)
+        expected_keys = self.__dataclass_fields__
+        if key not in expected_keys:
+            raise AttributeError(
+                f"Cannot set the attribute '{key}', expected attributes are {expected_keys}."
+            )
+
+        if isinstance(value, tuple(tensordict_lib.tensordict._ACCEPTED_CLASSES)):
+            # Avoiding key clash, honoring the user input to assign tensor type data to the key
+            if key in self._non_tensordict.keys():
+                if inplace:
+                    raise RuntimeError(
+                        f"Cannot update an existing entry of type {type(self._non_tensordict.get(key))} with a value of type {type(value)}."
+                    )
+                del self._non_tensordict[key]
+            self._tensordict.set(key, value, inplace=inplace)
+        else:
+            # Avoiding key clash, honoring the user input to assign non-tensor data to the key
+            if key in self._tensordict.keys():
+                if inplace:
+                    raise RuntimeError(
+                        f"Cannot update an existing entry of type {type(self._tensordict.get(key))} with a value of type {type(value)}."
+                    )
+                self._tensordict.del_(key)
+            # Saving all non-tensor attributes
+            self._non_tensordict[key] = value
+        return self
+
+    if isinstance(key, tuple) and len(key):
+        key = _unravel_key_to_tuple(key)
+        if len(key) > 1:
+            return self.set(key[0], getattr(self, key[0]).set(key[1:], value))
+        out = self.set(key[0], value)
+        return out
+    raise ValueError(
+        f"Supported type for key are str and tuple, got {key} of type {type(key)}"
+    )
+
+
+def _del_(self, key):
+    key = _unravel_key_to_tuple(key)
+    if len(key) > 1:
+        td = self.get(key[0])
+        td.del_(key[1:])
+        return
+    if key[0] in self._tensordict.keys():
+        self._tensordict.del_(key[0])
+        # self.set(key[0], None)
+    elif key[0] in self._non_tensordict.keys():
+        self._non_tensordict[key[0]] = None
+    else:
+        raise KeyError(f"Key {key} could not be found in tensorclass {self}.")
+    return
+
+
+def _set_at_(self, key: NestedKey, value: Any, idx: IndexType):
+    if key in self._non_tensordict:
+        del self._non_tensordict[key]
+    return self._tensordict.set_at_(key, value, idx)
+
+
+def _get(self, key: NestedKey, default: Any = NO_DEFAULT):
+    """Gets the value stored with the input key.
+
+    Args:
+        key (str, tuple of str): key to be queried. If tuple of str it is
+            equivalent to chained calls of getattr.
+        default: default value if the key is not found in the tensorclass.
+
+    Returns:
+        value stored with the input key
+
+    """
+    if isinstance(key, str):
+        key = (key,)
+
+    if isinstance(key, tuple):
+        try:
+            if len(key) > 1:
+                return getattr(self, key[0]).get(key[1:])
+            return getattr(self, key[0])
+        except AttributeError:
+            if default is NO_DEFAULT:
+                raise
+            return default
+    raise ValueError(f"Supported type for key are str and tuple, got {type(key)}")
+
+
+def _get_at(self, key: NestedKey, idx, default: Any = NO_DEFAULT):
+    try:
+        return self.get(key, NO_DEFAULT)[idx]
+    except AttributeError:
+        if default is NO_DEFAULT:
+            raise
+        return default
+
+
+def _batch_size(self) -> torch.Size:
+    """Retrieves the batch size for the tensor class.
+
+    Returns:
+        batch size (torch.Size)
+
+    """
+    return self._tensordict.batch_size
+
+
+def _batch_size_setter(self, new_size: torch.Size) -> None:  # noqa: D417
+    """Set the value of batch_size.
+
+    Args:
+        new_size (torch.Size): new_batch size to be set
+
+    """
+    self._tensordict._batch_size_setter(new_size)
+
+
+def _state_dict(self) -> dict[str, Any]:
+    """Returns a state_dict dictionary that can be used to save and load data from a tensorclass."""
+    state_dict = {"_tensordict": self._tensordict.state_dict()}
+    state_dict["_non_tensordict"] = copy(self._non_tensordict)
+    return state_dict
+
+
+def _load_state_dict(self, state_dict: dict[str, Any]):
+    """Loads a state_dict attemptedly in-place on the destination tensorclass."""
+    for key, item in state_dict.items():
+        # keys will never be nested which facilitates everything, but let's
+        # double check in case someone does something nasty
+        if not isinstance(key, str):
+            raise TypeError("Only str keys are allowed when calling load_state_dict.")
+        if key == "_non_tensordict":
+            for sub_key, sub_item in item.items():
+                # sub_item is the state dict of a tensorclass
+                if isinstance(sub_item, dict) and "_non_tensordict" in sub_item:
+                    raise RuntimeError(
+                        "Loading a saved tensorclass on a uninitialized tensorclass is not allowed"
+                    )
+                else:
+                    # check that sub_key is part of the tensorclass
+                    if sub_key not in self.__class__.__dataclass_fields__:
+                        raise KeyError(
+                            f"Key '{sub_key}' wasn't expected in the state-dict."
+                        )
+                    self._non_tensordict[sub_key] = sub_item
+        elif key == "_tensordict":
+            for sub_key in item.keys():
+                if (
+                    sub_key not in self.__class__.__dataclass_fields__
+                    and sub_key not in ("__batch_size", "__device")
+                ):
+                    raise KeyError(
+                        f"Key '{sub_key}' wasn't expected in the state-dict."
+                    )
+
+            self._tensordict.load_state_dict(item)
+        else:
+            raise KeyError(f"Key '{key}' wasn't expected in the state-dict.")
+
+    return self
+
+
+def __eq__(self, other: object) -> bool:
+    """Compares the Tensor class object to another object for equality. However, the equality check for non-tensor data is not performed.
+
+    Args:
+        other: object to compare to this object. Can be a tensorclass, a
+            tensordict or any compatible type (int, float or tensor), in
+            which case the equality check will be propagated to the leaves.
+
+    Returns:
+        False if the objects are of different class types, Tensorclass of boolean
+        values for tensor attributes and None for non-tensor attributes
+
+    Examples:
+        >>> @tensorclass
+        ... class MyClass:
+        ...     x: Tensor
+        ...     y: "MyClass"
+        ...     z: str
+        ...
+        >>> c1 = MyClass(
+        ...     x=torch.randn(3, 4),
+        ...     y=MyClass(
+        ...         x=torch.randn(3, 4, 1),
+        ...         y=None,
+        ...         z="bar",
+        ...         batch_size=[3, 4, 1],
+        ...     ),
+        ...     z="foo",
+        ...     batch_size=[3, 4],
+        ... )
+        >>> c2 = c1.clone()
+        >>> print(c1 == c2)
+        MyClass(
+            x=Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.bool, is_shared=False),
+            y=MyClass(
+                x=Tensor(shape=torch.Size([3, 4, 1]), device=cpu, dtype=torch.bool, is_shared=False),
+                y=None,
+                z=None,
+                batch_size=torch.Size([3, 4, 1]),
+                device=None,
+                is_shared=False),
+            z=None,
+            batch_size=torch.Size([3, 4]),
+            device=None,
+            is_shared=False)
+        >>> assert (c1 == c2).all()
+        >>> assert (c1[:2] == c2[:2]).all()
+        >>> assert not (c1 == c2.apply(lambda x: x+1)).all()
+
+    """
+    if not is_tensor_collection(other) and not isinstance(
+        other, (dict, numbers.Number, Tensor, MemmapTensor)
+    ):
+        return False
+    if is_tensorclass(other):
+        tensor = self._tensordict == other._tensordict
+    else:
+        tensor = self._tensordict == other
+    return _from_tensordict_with_none(self, tensor)
+
+
+def __ne__(self, other: object) -> bool:
+    """Compare the Tensor class object to another object for inequality. However, the equality check for non-tensor data is not performed.
+
+    Args:
+        other: object to compare to this object
+
+    Returns:
+        False if the objects are of different class types, Tensorclass of boolean values for tensor attributes and None for non-tensor attributes
+
+    Examples:
+        >>> @tensorclass
+        ... class MyClass:
+        ...     x: Tensor
+        ...     y: "MyClass"
+        ...     z: str
+        ...
+        >>> c1 = MyClass(
+        ...     x=torch.randn(3, 4),
+        ...     y=MyClass(
+        ...         x=torch.randn(3, 4, 1),
+        ...         y=None,
+        ...         z="bar",
+        ...         batch_size=[3, 4, 1],
+        ...     ),
+        ...     z="foo",
+        ...     batch_size=[3, 4],
+        ... )
+        >>> c2 = c1.clone()
+        >>> print(c1 != c2)
+        MyClass(
+            x=Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.bool, is_shared=False),
+            y=MyClass(
+                x=Tensor(shape=torch.Size([3, 4, 1]), device=cpu, dtype=torch.bool, is_shared=False),
+                y=None,
+                z=None,
+                batch_size=torch.Size([3, 4, 1]),
+                device=None,
+                is_shared=False),
+            z=None,
+            batch_size=torch.Size([3, 4]),
+            device=None,
+            is_shared=False)
+        >>> c2 = c2.apply(lambda x: x+1)
+        >>> assert (c1 != c2).all()
+
+    """
+    if not is_tensor_collection(other) and not isinstance(
+        other, (dict, numbers.Number, Tensor, MemmapTensor)
+    ):
+        return True
+    if is_tensorclass(other):
+        tensor = self._tensordict != other._tensordict
+    else:
+        tensor = self._tensordict != other
+    return _from_tensordict_with_none(self, tensor)
+
+
+def _single_td_field_as_str(key, item, tensordict):
+    """Returns a string as a  key-value pair of tensordict.
+
+    Args:
+        key (str): key of tensor dict item
+        item (tensor type): value to be returned for key
+        tensordict (Tensordict): Tensordict object
+
+    Returns:
+        String representation of a key-value pair
+
+    """
+    if is_tensor_collection(type(item)):
+        return f"{key}={repr(tensordict[key])}"
+    return f"{key}={_get_repr(item)}"
+
+
+def _all_td_fields_as_str(td: TensorDictBase) -> str:
+    """Returns indented representation of tensor dict values as a key-value pairs.
+
+    Args:
+        td (TensorDict) : Tensordict object
+
+    Returns:
+        String representation of all tensor data
+
+    """
+    return indent(
+        ",\n".join(
+            sorted([_single_td_field_as_str(key, item, td) for key, item in td.items()])
+        ),
+        4 * " ",
+    )
+
+
+def _all_non_td_fields_as_str(src_dict) -> list:
+    """Returns a list of string representation of non-tensor key-value pairs.
+
+    Args:
+        src_dict (dict): non_tensor_dict
+
+    Returns:
+        result (list): list of strings with key-value representation
+
+    """
+    result = []
+    for key, val in src_dict.items():
+        if not is_tensor_collection(val):
+            result.append(f"{key}={repr(val)}")
+
+    return result
+
+
+def _unbind(self, dim: int):
+    """Returns a tuple of indexed tensorclass instances unbound along the indicated dimension.
+
+    Resulting tensorclass instances will share the storage of the initial tensorclass instance.
+
+    """
+    return tuple(
+        self._from_tensordict(td, non_tensordict=copy(self._non_tensordict))
+        for td in self._tensordict.unbind(dim)
+    )
```

## tensordict/tensordict.py

```diff
@@ -1,8894 +1,8988 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import abc
-import collections
-import functools
-import numbers
-import os
-import re
-import textwrap
-import warnings
-from collections import defaultdict
-from collections.abc import MutableMapping
-from copy import copy, deepcopy
-from numbers import Number
-from pathlib import Path
-from textwrap import indent
-from typing import (
-    Any,
-    Callable,
-    Generator,
-    Iterable,
-    Iterator,
-    OrderedDict,
-    Sequence,
-    Union,
-)
-from warnings import warn
-
-import numpy as np
-
-import torch
-from tensordict._tensordict import _unravel_key_to_tuple
-from tensordict.memmap import memmap_tensor_as_tensor, MemmapTensor
-from tensordict.utils import (
-    _device,
-    _dtype,
-    _GENERIC_NESTED_ERR,
-    _get_item,
-    _getitem_batch_size,
-    _is_shared,
-    _is_tensorclass,
-    _NON_STR_KEY_ERR,
-    _NON_STR_KEY_TUPLE_ERR,
-    _set_item,
-    _shape,
-    _StringOnlyDict,
-    _sub_index,
-    as_decorator,
-    cache,
-    convert_ellipsis_to_idx,
-    DeviceType,
-    erase_cache,
-    expand_as_right,
-    expand_right,
-    IndexType,
-    int_generator,
-    is_tensorclass,
-    lock_blocked,
-    NestedKey,
-    prod,
-)
-from torch import distributed as dist, Tensor
-from torch.utils._pytree import tree_map
-
-try:
-    from torch.jit._shape_functions import infer_size_impl
-except ImportError:
-    from tensordict.utils import infer_size_impl
-
-
-_has_functorch = False
-try:
-    try:
-        from functorch._C import is_batchedtensor
-    except ImportError:
-        from torch._C._functorch import (
-            _add_batch_dim,
-            _remove_batch_dim,
-            is_batchedtensor,
-        )
-
-    _has_functorch = True
-except ImportError:
-    _has_functorch = False
-
-    def is_batchedtensor(tensor: Tensor) -> bool:
-        """Placeholder for the functorch function."""
-        return False
-
-
-try:
-    from torchrec import KeyedJaggedTensor
-
-    _has_torchrec = True
-except ImportError as err:
-    _has_torchrec = False
-
-    class KeyedJaggedTensor:  # noqa: D103, D101
-        pass
-
-    TORCHREC_ERR = str(err)
-
-NO_DEFAULT = "_no_default_"
-
-
-class _BEST_ATTEMPT_INPLACE:
-    def __bool__(self):
-        raise NotImplementedError
-
-
-BEST_ATTEMPT_INPLACE = _BEST_ATTEMPT_INPLACE()
-
-# some complex string used as separator to concatenate and split keys in
-# distributed frameworks
-DIST_SEPARATOR = ".-|-."
-TD_HANDLED_FUNCTIONS: dict[Callable, Callable] = {}
-LAZY_TD_HANDLED_FUNCTIONS: dict[Callable, Callable] = {}
-CompatibleType = Union[
-    Tensor,
-    MemmapTensor,
-]  # None? # leaves space for TensorDictBase
-
-if _has_torchrec:
-    CompatibleType = Union[
-        Tensor,
-        MemmapTensor,
-        KeyedJaggedTensor,
-    ]
-_STR_MIXED_INDEX_ERROR = "Received a mixed string-non string index. Only string-only or string-free indices are supported."
-
-_HEURISTIC_EXCLUDED = (Tensor, tuple, list, set, dict, np.ndarray)
-
-_TENSOR_COLLECTION_MEMO = {}
-
-
-def _is_tensor_collection(datatype):
-    out = _TENSOR_COLLECTION_MEMO.get(datatype, None)
-    if out is None:
-        if issubclass(datatype, TensorDictBase):
-            out = True
-        elif _is_tensorclass(datatype):
-            out = True
-        else:
-            out = False
-        _TENSOR_COLLECTION_MEMO[datatype] = out
-    return out
-
-
-def is_tensor_collection(datatype: type | Any) -> bool:
-    """Checks if a data object or a type is a tensor container from the tensordict lib.
-
-    Returns:
-        ``True`` if the input is a TensorDictBase subclass, a tensorclass or an istance of these.
-        ``False`` otherwise.
-
-    Examples:
-        >>> is_tensor_collection(TensorDictBase)  # True
-        >>> is_tensor_collection(TensorDict({}, []))  # True
-        >>> @tensorclass
-        ... class MyClass:
-        ...     pass
-        ...
-        >>> is_tensor_collection(MyClass)  # True
-        >>> is_tensor_collection(MyClass(batch_size=[]))  # True
-
-    """
-    # memoizing is 2x faster
-    if not isinstance(datatype, type):
-        datatype = type(datatype)
-    return _is_tensor_collection(datatype)
-
-
-def is_memmap(datatype: type | Any) -> bool:
-    """Returns ``True`` if the class is a subclass of :class:`~.MemmapTensor` or the object an instance of it."""
-    return (
-        issubclass(datatype, MemmapTensor)
-        if isinstance(datatype, type)
-        else isinstance(datatype, MemmapTensor)
-    )
-
-
-class _TensorDictKeysView:
-    """A Key view for TensorDictBase instance.
-
-    _TensorDictKeysView is returned when accessing tensordict.keys() and holds a
-    reference to the original TensorDict. This class enables us to support nested keys
-    when performing membership checks and when iterating over keys.
-
-    Examples:
-        >>> import torch
-        >>> from tensordict import TensorDict
-
-        >>> td = TensorDict(
-        >>>     {"a": TensorDict({"b": torch.rand(1, 2)}, [1, 2]), "c": torch.rand(1)},
-        >>>     [1],
-        >>> )
-
-        >>> assert "a" in td.keys()
-        >>> assert ("a",) in td.keys()
-        >>> assert ("a", "b") in td.keys()
-        >>> assert ("a", "c") not in td.keys()
-
-        >>> assert set(td.keys()) == {("a", "b"), "c"}
-    """
-
-    def __init__(
-        self,
-        tensordict: TensorDictBase,
-        include_nested: bool,
-        leaves_only: bool,
-    ) -> None:
-        self.tensordict = tensordict
-        self.include_nested = include_nested
-        self.leaves_only = leaves_only
-
-    def __iter__(self) -> Iterable[str] | Iterable[tuple[str, ...]]:
-        if not self.include_nested:
-            if self.leaves_only:
-                for key in self._keys():
-                    target_class = self.tensordict.entry_class(key)
-                    if _is_tensor_collection(target_class):
-                        continue
-                    yield key
-            else:
-                yield from self._keys()
-        else:
-            yield from (
-                key if len(key) > 1 else key[0]
-                for key in self._iter_helper(self.tensordict)
-            )
-
-    def _iter_helper(
-        self, tensordict: TensorDictBase, prefix: str | None = None
-    ) -> Iterable[str] | Iterable[tuple[str, ...]]:
-        for key, value in self._items(tensordict):
-            full_key = self._combine_keys(prefix, key)
-            cls = value.__class__
-            if self.include_nested and (
-                _is_tensor_collection(cls) or issubclass(cls, KeyedJaggedTensor)
-            ):
-                subkeys = tuple(self._iter_helper(value, prefix=full_key))
-                yield from subkeys
-            if not self.leaves_only or not _is_tensor_collection(cls):
-                yield full_key
-
-    def _combine_keys(self, prefix: tuple | None, key: str) -> tuple:
-        if prefix is not None:
-            return prefix + (key,)
-        return (key,)
-
-    def __len__(self) -> int:
-        return sum(1 for _ in self)
-
-    def _items(
-        self, tensordict: TensorDict | None = None
-    ) -> Iterable[tuple[NestedKey, CompatibleType]]:
-        if tensordict is None:
-            tensordict = self.tensordict
-        if isinstance(tensordict, TensorDict) or is_tensorclass(tensordict):
-            return tensordict._tensordict.items()
-        elif isinstance(tensordict, LazyStackedTensorDict):
-            return _iter_items_lazystack(tensordict)
-        elif isinstance(tensordict, KeyedJaggedTensor):
-            return tuple((key, tensordict[key]) for key in tensordict.keys())
-        elif isinstance(tensordict, _CustomOpTensorDict):
-            # it's possible that a TensorDict contains a nested LazyStackedTensorDict,
-            # or _CustomOpTensorDict, so as we iterate through the contents we need to
-            # be careful to not rely on tensordict._tensordict existing.
-            return (
-                (key, tensordict._get_str(key, NO_DEFAULT))
-                for key in tensordict._source.keys()
-            )
-
-    def _keys(self) -> _TensorDictKeysView:
-        return self.tensordict._tensordict.keys()
-
-    def __contains__(self, key: NestedKey) -> bool:
-        key = _unravel_key_to_tuple(key)
-        if not key:
-            raise TypeError(_NON_STR_KEY_ERR)
-
-        if isinstance(key, str):
-            if key in self._keys():
-                if self.leaves_only:
-                    return not _is_tensor_collection(self.tensordict.entry_class(key))
-                return True
-            return False
-        else:
-            # thanks to _unravel_key_to_tuple we know the key is a tuple
-            if len(key) == 1:
-                return key[0] in self._keys()
-            elif self.include_nested:
-                if key[0] in self._keys():
-                    entry_type = self.tensordict.entry_class(key[0])
-                    if entry_type in (Tensor, MemmapTensor):
-                        return False
-                    if entry_type is KeyedJaggedTensor:
-                        if len(key) > 2:
-                            return False
-                        return key[1] in self.tensordict.get(key[0]).keys()
-                    _is_tensordict = _is_tensor_collection(entry_type)
-                    if _is_tensordict:
-                        # # this will call _unravel_key_to_tuple many times
-                        # return key[1:] in self.tensordict._get_str(key[0], NO_DEFAULT).keys(include_nested=self.include_nested)
-                        # this won't call _unravel_key_to_tuple but requires to get the default which can be suboptimal
-                        leaf_td = self.tensordict._get_tuple(key[:-1], None)
-                        if leaf_td is None or (
-                            not _is_tensor_collection(leaf_td.__class__)
-                            and not isinstance(leaf_td, KeyedJaggedTensor)
-                        ):
-                            return False
-                        return key[-1] in leaf_td.keys()
-                return False
-            # this is reached whenever there is more than one key but include_nested is False
-            if all(isinstance(subkey, str) for subkey in key):
-                raise TypeError(_NON_STR_KEY_TUPLE_ERR)
-
-    def __repr__(self):
-        include_nested = f"include_nested={self.include_nested}"
-        leaves_only = f"leaves_only={self.leaves_only}"
-        return f"{self.__class__.__name__}({list(self)},\n{indent(include_nested, 4*' ')},\n{indent(leaves_only, 4*' ')})"
-
-
-def _renamed_inplace_method(fn):
-    def wrapper(*args, **kwargs):
-        warn(
-            f"{fn.__name__.rstrip('_')} has been deprecated, use {fn.__name__} instead"
-        )
-        return fn(*args, **kwargs)
-
-    return wrapper
-
-
-class TensorDictBase(MutableMapping):
-    """TensorDictBase is an abstract parent class for TensorDicts, a torch.Tensor data container."""
-
-    LOCK_ERROR = (
-        "Cannot modify locked TensorDict. For in-place modification, consider "
-        "using the `set_()` method and make sure the key is present."
-    )
-    KEY_ERROR = 'key "{}" not found in {} with ' "keys {}"
-
-    def __new__(cls, *args: Any, **kwargs: Any) -> TensorDictBase:
-        self = super().__new__(cls)
-        self._safe = kwargs.get("_safe", False)
-        self._lazy = kwargs.get("_lazy", False)
-        self._inplace_set = kwargs.get("_inplace_set", False)
-        self.is_meta = kwargs.get("is_meta", False)
-        self._is_locked = kwargs.get("_is_locked", False)
-        self._cache = None
-        self._last_op = None
-        self.__last_op_queue = None
-        return self
-
-    def __getstate__(self) -> dict[str, Any]:
-        state = self.__dict__.copy()
-        return state
-
-    def __setstate__(self, state: dict[str, Any]) -> dict[str, Any]:
-        self.__dict__.update(state)
-
-    @staticmethod
-    def from_module(module):
-        """Copies the params and buffers of a module in a tensordict.
-
-        Examples:
-            >>> from torch import nn
-            >>> module = nn.TransformerDecoder(
-            ...     decoder_layer=nn.TransformerDecoderLayer(nhead=4, d_model=4),
-            ...     num_layers=1)
-            >>> params = TensorDict.from_module(module)
-            >>> print(params["layers", "0", "linear1"])
-            TensorDict(
-                fields={
-                    bias: Parameter(shape=torch.Size([2048]), device=cpu, dtype=torch.float32, is_shared=False),
-                    weight: Parameter(shape=torch.Size([2048, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-        """
-        td = TensorDict(dict(module.named_parameters()), [])
-        td.update(dict(module.named_buffers()))
-        td = td.unflatten_keys(".")
-        td.lock_()
-        return td
-
-    @property
-    def shape(self) -> torch.Size:
-        """See :obj:`TensorDictBase.batch_size`."""
-        return self.batch_size
-
-    @property
-    @abc.abstractmethod
-    def batch_size(self) -> torch.Size:
-        """Shape of (or batch_size) of a TensorDict.
-
-        The shape of a tensordict corresponds to the common N first
-        dimensions of the tensors it contains, where N is an arbitrary
-        number. The TensorDict shape is controlled by the user upon
-        initialization (i.e. it is not inferred from the tensor shapes) and
-        it should not be changed dynamically.
-
-        Returns:
-            a torch.Size object describing the TensorDict batch size.
-
-        """
-        raise NotImplementedError
-
-    def _erase_cache(self):
-        self._cache = None
-
-    @property
-    @abc.abstractmethod
-    def names(self):
-        raise NotImplementedError
-
-    @abc.abstractmethod
-    def _erase_names(self):
-        raise NotImplementedError
-
-    @abc.abstractmethod
-    def _rename_subtds(self, value):
-        # renames all the sub-tensordicts dimension according to value.
-        # If value has less dimensions than the TD, the rest is just assumed to be None
-        raise NotImplementedError
-
-    def _check_dim_name(self, name):
-        if name is None:
-            return False
-        if self._has_names() and name in self.names:
-            return True
-        for key in self.keys():
-            if _is_tensor_collection(self.entry_class(key)):
-                if self._get_str(key, NO_DEFAULT)._check_dim_name(name):
-                    return True
-        else:
-            return False
-
-    def refine_names(self, *names):
-        """Refines the dimension names of self according to names.
-
-        Refining is a special case of renaming that “lifts” unnamed dimensions.
-        A None dim can be refined to have any name; a named dim can only be
-        refined to have the same name.
-
-        Because named tensors can coexist with unnamed tensors, refining names
-        gives a nice way to write named-tensor-aware code that works with both
-        named and unnamed tensors.
-
-        names may contain up to one Ellipsis (...). The Ellipsis is expanded
-        greedily; it is expanded in-place to fill names to the same length as
-        self.dim() using names from the corresponding indices of self.names.
-
-        Returns: the tensordict with dimensions named accordingly.
-
-        """
-        # replace ellipsis if any
-        names_copy = copy(names)
-        if any(name is Ellipsis for name in names):
-            ellipsis_name = [NO_DEFAULT for _ in range(self.ndim - len(names) + 1)]
-            names = []
-            for name in names_copy:
-                if name is Ellipsis:
-                    names += ellipsis_name
-                else:
-                    names.append(name)
-        # check that the names that are set are either None or identical
-        curr_names = self.names
-        for i, name in enumerate(names):
-            if name is NO_DEFAULT:
-                # whatever value is ok
-                names[i] = curr_names[i]
-                continue
-            else:
-                if curr_names[i] is None:
-                    continue
-                if self.names[i] == name:
-                    continue
-                else:
-                    raise RuntimeError(
-                        f"refine_names: cannot coerce TensorDict names {self.names} with {names_copy}."
-                    )
-        self.names = names
-        # we also need to rename the sub-tensordicts
-        # self._rename_subtds(self.names)
-        return self
-
-    def rename(self, *names, **rename_map):
-        clone = self.clone(recurse=False)
-        if len(names) == 1 and names[0] is None:
-            clone.names = None
-        if rename_map and names:
-            raise ValueError(
-                "Passed both a name map and a name list. Only one is accepted."
-            )
-        elif not rename_map and not names:
-            raise ValueError(
-                "Neither a name map nor a name list was passed. "
-                "Only one is accepted."
-            )
-        elif rename_map:
-            cnames = list(clone.names)
-            for i, name in enumerate(cnames):
-                new_name = rename_map.pop(name, NO_DEFAULT)
-                if new_name is not NO_DEFAULT:
-                    cnames[i] = new_name
-            clone.names = cnames
-            if rename_map:
-                raise ValueError(
-                    f"Some names to be renamed were not part of the tensordict names: {rename_map.keys()} vs {self.names}."
-                )
-        else:
-            clone.names = names
-        return clone
-
-    def rename_(self, *names, **rename_map):
-        if len(names) == 1 and names[0] is None:
-            self.names = None
-        if rename_map and names:
-            raise ValueError(
-                "Passed both a name map and a name list. " "Only one is accepted."
-            )
-        elif not rename_map and not names and self.batch_dims:
-            raise ValueError(
-                "Neither a name map nor a name list was passed. "
-                "Only one is accepted."
-            )
-        elif rename_map:
-            cnames = list(self.names)
-            for i, name in enumerate(cnames):
-                new_name = rename_map.pop(name, NO_DEFAULT)
-                if new_name is not NO_DEFAULT:
-                    cnames[i] = new_name
-            if rename_map:
-                raise ValueError(
-                    f"Some names to be renamed were not part of the tensordict names: {rename_map.keys()} vs {self.names}."
-                )
-            self.names = cnames
-        else:
-            self.names = names
-        return self
-
-    @abc.abstractmethod
-    def _has_names(self):
-        raise NotImplementedError
-
-    @property
-    def _last_op_queue(self):
-        last_op_queue = self.__last_op_queue
-        if last_op_queue is None:
-            last_op_queue = self.__last_op_queue = collections.deque()
-        return last_op_queue
-
-    def size(self, dim: int | None = None) -> torch.Size | int:
-        """Returns the size of the dimension indicated by :obj:`dim`.
-
-        If dim is not specified, returns the batch_size (or shape) of the TensorDict.
-
-        """
-        if dim is None:
-            return self.batch_size
-        return self.batch_size[dim]
-
-    @property
-    def requires_grad(self) -> bool:
-        return any(v.requires_grad for v in self.values())
-
-    def _batch_size_setter(self, new_batch_size: torch.Size) -> None:
-        if new_batch_size == self.batch_size:
-            return
-        if self._lazy:
-            raise RuntimeError(
-                "modifying the batch size of a lazy repesentation of a "
-                "tensordict is not permitted. Consider instantiating the "
-                "tensordict first by calling `td = td.to_tensordict()` before "
-                "resetting the batch size."
-            )
-        if not isinstance(new_batch_size, torch.Size):
-            new_batch_size = torch.Size(new_batch_size)
-        for key in self.keys():
-            if _is_tensor_collection(self.entry_class(key)):
-                tensordict = self.get(key)
-                if len(tensordict.batch_size) < len(new_batch_size):
-                    # document as edge case
-                    tensordict.batch_size = new_batch_size
-                    self._set_str(key, tensordict, inplace=True, validated=True)
-        self._check_new_batch_size(new_batch_size)
-        self._change_batch_size(new_batch_size)
-        if self._has_names():
-            names = self.names
-            if len(names) < len(new_batch_size):
-                self.names = names + [None] * (len(new_batch_size) - len(names))
-            else:
-                self.names = names[: self.batch_dims]
-
-    @property
-    def batch_dims(self) -> int:
-        """Length of the tensordict batch size.
-
-        Returns:
-            int describing the number of dimensions of the tensordict.
-
-        """
-        return len(self.batch_size)
-
-    def ndimension(self) -> int:
-        return self.batch_dims
-
-    @property
-    def ndim(self) -> int:
-        return self.batch_dims
-
-    def dim(self) -> int:
-        return self.batch_dims
-
-    @property
-    @abc.abstractmethod
-    def device(self) -> torch.device | None:
-        """Device of a TensorDict.
-
-        If the TensorDict has a specified device, all
-        tensors of a tensordict must live on the same device. If the TensorDict device
-        is None, then different values can be located on different devices.
-
-        Returns:
-            torch.device object indicating the device where the tensors
-            are placed, or None if TensorDict does not have a device.
-
-        """
-        raise NotImplementedError
-
-    @device.setter
-    @abc.abstractmethod
-    def device(self, value: DeviceType) -> None:
-        raise NotImplementedError
-
-    def clear_device_(self) -> TensorDictBase:
-        """Clears the device of the tensordict.
-
-        Returns: self
-
-        """
-        self._device = None
-        for value in self.values():
-            if _is_tensor_collection(value.__class__):
-                value.clear_device_()
-        return self
-
-    clear_device = _renamed_inplace_method(clear_device_)
-
-    def is_shared(self) -> bool:
-        """Checks if tensordict is in shared memory.
-
-        If a TensorDict instance is in shared memory, any new tensor written
-        in it will be placed in shared memory. If a TensorDict is created with
-        tensors that are all in shared memory, this does not mean that it will be
-        in shared memory (as a new tensor may not be in shared memory).
-        Only if one calls `tensordict.share_memory_()` or places the tensordict
-        on a device where the content is shared will the tensordict be considered
-        in shared memory.
-
-        This is always True for CUDA tensordicts, except when stored as
-        MemmapTensors.
-
-        """
-        if self.device and not self._is_memmap:
-            return self.device.type == "cuda" or self._is_shared
-        return self._is_shared
-
-    def state_dict(self) -> OrderedDict[str, Any]:
-        out = collections.OrderedDict()
-        for key, item in self.apply(memmap_tensor_as_tensor).items():
-            out[key] = (
-                item if not _is_tensor_collection(item.__class__) else item.state_dict()
-            )
-        if "__batch_size" in out:
-            raise KeyError(
-                "Cannot retrieve the state_dict of a TensorDict with `'__batch_size'` key"
-            )
-        if "__device" in out:
-            raise KeyError(
-                "Cannot retrieve the state_dict of a TensorDict with `'__batch_size'` key"
-            )
-        out["__batch_size"] = self.batch_size
-        out["__device"] = self.device
-        return out
-
-    def load_state_dict(self, state_dict: OrderedDict[str, Any]) -> TensorDictBase:
-        # copy since we'll be using pop
-        state_dict = copy(state_dict)
-        self.batch_size = state_dict.pop("__batch_size")
-        device = state_dict.pop("__device")
-        if device is not None:
-            self.to(device)
-        for key, item in state_dict.items():
-            if isinstance(item, dict):
-                self.set(
-                    key,
-                    self.get(key, default=TensorDict({}, [])).load_state_dict(item),
-                    inplace=True,
-                )
-            else:
-                self.set(key, item, inplace=True)
-        return self
-
-    def is_memmap(self) -> bool:
-        """Checks if tensordict is stored with MemmapTensors."""
-        return self._is_memmap
-
-    def numel(self) -> int:
-        """Total number of elements in the batch."""
-        return max(1, prod(self.batch_size))
-
-    def _check_batch_size(self) -> None:
-        bs = [value.shape[: self.batch_dims] for value in self.values()] + [
-            self.batch_size
-        ]
-        if len(set(bs)) > 1:
-            raise RuntimeError(
-                f"batch_size are incongruent, got {list(set(bs))}, "
-                f"-- expected {self.batch_size}"
-            )
-
-    def _check_is_shared(self) -> bool:
-        raise NotImplementedError(f"{self.__class__.__name__}")
-
-    def _check_device(self) -> None:
-        raise NotImplementedError(f"{self.__class__.__name__}")
-
-    @abc.abstractmethod
-    def entry_class(self, key: NestedKey) -> type:
-        """Returns the class of an entry, avoiding a call to `isinstance(td.get(key), type)`."""
-        raise NotImplementedError(f"{self.__class__.__name__}")
-
-    def set(
-        self, key: NestedKey, item: CompatibleType, inplace: bool = False, **kwargs: Any
-    ) -> TensorDictBase:
-        """Sets a new key-value pair.
-
-        Args:
-            key (str, tuple of str): name of the key to be set.
-                If tuple of str it is equivalent to chained calls of getattr
-            item (torch.Tensor): value to be stored in the tensordict
-            inplace (bool, optional): if True and if a key matches an existing
-                key in the tensordict, then the update will occur in-place
-                for that key-value pair. Default is :obj:`False`.
-
-        Returns:
-            self
-
-        """
-        key = _unravel_key_to_tuple(key)
-        # inplace is loose here, but for set_ it is constraining. We translate it
-        # to None to tell _set_str and others to drop it if the key isn't found
-        inplace = BEST_ATTEMPT_INPLACE if inplace else False
-        return self._set_tuple(key, item, inplace=inplace, validated=False)
-
-    def _convert_inplace(self, inplace, key):
-        if inplace is not False:
-            has_key = key in self.keys()
-            if inplace is True and not has_key:  # inplace could be None
-                raise KeyError(
-                    TensorDictBase.KEY_ERROR.format(
-                        key, self.__class__.__name__, sorted(self.keys())
-                    )
-                )
-            inplace = has_key
-        return inplace
-
-    @abc.abstractmethod
-    def _set_str(self, key, value, *, inplace, validated):
-        ...
-
-    @abc.abstractmethod
-    def _set_tuple(self, key, value, *, inplace, validated):
-        ...
-
-    def set_at_(
-        self, key: NestedKey, value: CompatibleType, index: IndexType
-    ) -> TensorDictBase:
-        """Sets the values in-place at the index indicated by :obj:`idx`.
-
-        Args:
-            key (str, tuple of str): key to be modified.
-            value (torch.Tensor): value to be set at the index `idx`
-            index (int, tensor or tuple): index where to write the values.
-
-        Returns:
-            self
-
-        """
-        key = _unravel_key_to_tuple(key)
-        return self._set_at_tuple(key, value, index, validated=False)
-
-    @abc.abstractmethod
-    def _set_at_str(self, key, value, idx, *, validated):
-        ...
-
-    @abc.abstractmethod
-    def _set_at_tuple(self, key, value, idx, *, validated):
-        ...
-
-    def set_(
-        self,
-        key: NestedKey,
-        item: CompatibleType,
-    ) -> TensorDictBase:
-        """Sets a value to an existing key while keeping the original storage.
-
-        Args:
-            key (str): name of the value
-            item (torch.Tensor): value to be stored in the tensordict
-
-        Returns:
-            self
-
-        """
-        key = _unravel_key_to_tuple(key)
-        return self._set_tuple(key, item, inplace=True, validated=False)
-
-    @abc.abstractmethod
-    def _stack_onto_(
-        self,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> TensorDictBase:
-        """Stacks a list of values onto an existing key while keeping the original storage.
-
-        Args:
-            key (str): name of the value
-            list_item (list of torch.Tensor): value to be stacked and stored in the tensordict.
-            dim (int): dimension along which the tensors should be stacked.
-
-        Returns:
-            self
-
-        """
-        raise NotImplementedError(f"{self.__class__.__name__}")
-
-    def gather_and_stack(self, dst: int) -> TensorDictBase | None:
-        """Gathers tensordicts from various workers and stacks them onto self in the destination worker.
-
-        Args:
-            dst (int): the rank of the destination worker where :func:`gather_and_stack` will be called.
-
-        Example:
-            >>> from torch import multiprocessing as mp
-            >>> from tensordict import TensorDict
-            >>> import torch
-            >>>
-            >>> def client():
-            ...     torch.distributed.init_process_group(
-            ...         "gloo",
-            ...         rank=1,
-            ...         world_size=2,
-            ...         init_method=f"tcp://localhost:10003",
-            ...     )
-            ...     # Create a single tensordict to be sent to server
-            ...     td = TensorDict(
-            ...         {("a", "b"): torch.randn(2),
-            ...          "c": torch.randn(2)}, [2]
-            ...     )
-            ...     td.gather_and_stack(0)
-            ...
-            >>> def server():
-            ...     torch.distributed.init_process_group(
-            ...         "gloo",
-            ...         rank=0,
-            ...         world_size=2,
-            ...         init_method=f"tcp://localhost:10003",
-            ...     )
-            ...     # Creates the destination tensordict on server.
-            ...     # The first dim must be equal to world_size-1
-            ...     td = TensorDict(
-            ...         {("a", "b"): torch.zeros(2),
-            ...          "c": torch.zeros(2)}, [2]
-            ...     ).expand(1, 2).contiguous()
-            ...     td.gather_and_stack(0)
-            ...     assert td["a", "b"] != 0
-            ...     print("yuppie")
-            ...
-            >>> if __name__ == "__main__":
-            ...     mp.set_start_method("spawn")
-            ...
-            ...     main_worker = mp.Process(target=server)
-            ...     secondary_worker = mp.Process(target=client)
-            ...
-            ...     main_worker.start()
-            ...     secondary_worker.start()
-            ...
-            ...     main_worker.join()
-            ...     secondary_worker.join()
-        """
-        output = (
-            [None for _ in range(dist.get_world_size())]
-            if dst == dist.get_rank()
-            else None
-        )
-        dist.gather_object(self, output, dst=dst)
-        if dst == dist.get_rank():
-            # remove self from output
-            output = [item for i, item in enumerate(output) if i != dst]
-            self.update(torch.stack(output, 0), inplace=True)
-            return self
-        return None
-
-    def send(self, dst: int, init_tag: int = 0, pseudo_rand: bool = False) -> None:
-        """Sends the content of a tensordict to a distant worker.
-
-        Args:
-            dst (int): the rank of the destination worker where the content
-                should be sent.
-            init_tag (int): the initial tag to be used to mark the tensors.
-                Note that this will be incremented by as much as the number of
-                tensors contained in the TensorDict.
-            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
-                random, allowing to send multiple data from different nodes
-                without overlap. Notice that the generation of these pseudo-random
-                numbers is expensive (1e-5 sec/number), meaning that it could
-                slow down the runtime of your algorithm.
-                Defaults to ``False``.
-
-        Example:
-            >>> from torch import multiprocessing as mp
-            >>> from tensordict import TensorDict
-            >>> import torch
-            >>>
-            >>>
-            >>> def client():
-            ...     torch.distributed.init_process_group(
-            ...         "gloo",
-            ...         rank=1,
-            ...         world_size=2,
-            ...         init_method=f"tcp://localhost:10003",
-            ...     )
-            ...
-            ...     td = TensorDict(
-            ...         {
-            ...             ("a", "b"): torch.randn(2),
-            ...             "c": torch.randn(2, 3),
-            ...             "_": torch.ones(2, 1, 5),
-            ...         },
-            ...         [2],
-            ...     )
-            ...     td.send(0)
-            ...
-            >>>
-            >>> def server(queue):
-            ...     torch.distributed.init_process_group(
-            ...         "gloo",
-            ...         rank=0,
-            ...         world_size=2,
-            ...         init_method=f"tcp://localhost:10003",
-            ...     )
-            ...     td = TensorDict(
-            ...         {
-            ...             ("a", "b"): torch.zeros(2),
-            ...             "c": torch.zeros(2, 3),
-            ...             "_": torch.zeros(2, 1, 5),
-            ...         },
-            ...         [2],
-            ...     )
-            ...     td.recv(1)
-            ...     assert (td != 0).all()
-            ...     queue.put("yuppie")
-            ...
-            >>>
-            >>> if __name__=="__main__":
-            ...     queue = mp.Queue(1)
-            ...     main_worker = mp.Process(target=server, args=(queue,))
-            ...     secondary_worker = mp.Process(target=client)
-            ...
-            ...     main_worker.start()
-            ...     secondary_worker.start()
-            ...     out = queue.get(timeout=10)
-            ...     assert out == "yuppie"
-            ...     main_worker.join()
-            ...     secondary_worker.join()
-
-        """
-        self._send(dst, _tag=init_tag - 1, pseudo_rand=pseudo_rand)
-
-    def _send(self, dst: int, _tag: int = -1, pseudo_rand: bool = False) -> int:
-        for key in self.sorted_keys:
-            value = self._get_str(key, NO_DEFAULT)
-            if isinstance(value, Tensor):
-                pass
-            elif _is_tensor_collection(value.__class__):
-                _tag = value._send(dst, _tag=_tag, pseudo_rand=pseudo_rand)
-                continue
-            elif isinstance(value, MemmapTensor):
-                value = value.as_tensor()
-            else:
-                raise NotImplementedError(f"Type {type(value)} is not supported.")
-            if not pseudo_rand:
-                _tag += 1
-            else:
-                _tag = int_generator(_tag + 1)
-            dist.send(value, dst=dst, tag=_tag)
-
-        return _tag
-
-    def recv(self, src: int, init_tag: int = 0, pseudo_rand: bool = False) -> int:
-        """Receives the content of a tensordict and updates content with it.
-
-        Check the example in the `send` method for context.
-
-        Args:
-            src (int): the rank of the source worker.
-            init_tag (int): the ``init_tag`` used by the source worker.
-            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
-                random, allowing to send multiple data from different nodes
-                without overlap. Notice that the generation of these pseudo-random
-                numbers is expensive (1e-5 sec/number), meaning that it could
-                slow down the runtime of your algorithm.
-                This value must match the one passed to :func:`send`.
-                Defaults to ``False``.
-
-        """
-        return self._recv(src, _tag=init_tag - 1, pseudo_rand=pseudo_rand)
-
-    def _recv(self, src: int, _tag: int = -1, pseudo_rand: bool = False) -> int:
-        for key in self.sorted_keys:
-            value = self._get_str(key, NO_DEFAULT)
-            if isinstance(value, Tensor):
-                pass
-            elif _is_tensor_collection(value.__class__):
-                _tag = value._recv(src, _tag=_tag, pseudo_rand=pseudo_rand)
-                continue
-            elif isinstance(value, MemmapTensor):
-                value = value.as_tensor()
-            else:
-                raise NotImplementedError(f"Type {type(value)} is not supported.")
-            if not pseudo_rand:
-                _tag += 1
-            else:
-                _tag = int_generator(_tag + 1)
-            dist.recv(value, src=src, tag=_tag)
-            self._set_str(key, value, inplace=True, validated=True)
-
-        return _tag
-
-    def isend(self, dst: int, init_tag: int = 0, pseudo_rand: bool = False) -> int:
-        """Sends the content of the tensordict asynchronously.
-
-        Args:
-            dst (int): the rank of the destination worker where the content
-                should be sent.
-            init_tag (int): the initial tag to be used to mark the tensors.
-                Note that this will be incremented by as much as the number of
-                tensors contained in the TensorDict.
-            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
-                random, allowing to send multiple data from different nodes
-                without overlap. Notice that the generation of these pseudo-random
-                numbers is expensive (1e-5 sec/number), meaning that it could
-                slow down the runtime of your algorithm.
-                Defaults to ``False``.
-
-        Example:
-            >>> import torch
-            >>> from tensordict import TensorDict
-            >>> from torch import multiprocessing as mp
-            >>> def client():
-            ...     torch.distributed.init_process_group(
-            ...         "gloo",
-            ...         rank=1,
-            ...         world_size=2,
-            ...         init_method=f"tcp://localhost:10003",
-            ...     )
-            ...
-            ...     td = TensorDict(
-            ...         {
-            ...             ("a", "b"): torch.randn(2),
-            ...             "c": torch.randn(2, 3),
-            ...             "_": torch.ones(2, 1, 5),
-            ...         },
-            ...         [2],
-            ...     )
-            ...     td.isend(0)
-            ...
-            >>>
-            >>> def server(queue, return_premature=True):
-            ...     torch.distributed.init_process_group(
-            ...         "gloo",
-            ...         rank=0,
-            ...         world_size=2,
-            ...         init_method=f"tcp://localhost:10003",
-            ...     )
-            ...     td = TensorDict(
-            ...         {
-            ...             ("a", "b"): torch.zeros(2),
-            ...             "c": torch.zeros(2, 3),
-            ...             "_": torch.zeros(2, 1, 5),
-            ...         },
-            ...         [2],
-            ...     )
-            ...     out = td.irecv(1, return_premature=return_premature)
-            ...     if return_premature:
-            ...         for fut in out:
-            ...             fut.wait()
-            ...     assert (td != 0).all()
-            ...     queue.put("yuppie")
-            ...
-            >>>
-            >>> if __name__ == "__main__":
-            ...     queue = mp.Queue(1)
-            ...     main_worker = mp.Process(
-            ...         target=server,
-            ...         args=(queue, )
-            ...         )
-            ...     secondary_worker = mp.Process(target=client)
-            ...
-            ...     main_worker.start()
-            ...     secondary_worker.start()
-            ...     out = queue.get(timeout=10)
-            ...     assert out == "yuppie"
-            ...     main_worker.join()
-            ...     secondary_worker.join()
-
-        """
-        return self._isend(dst, init_tag - 1, pseudo_rand=pseudo_rand)
-
-    def _isend(
-        self,
-        dst: int,
-        _tag: int = -1,
-        _futures: list[torch.Future] | None = None,
-        pseudo_rand: bool = False,
-    ) -> int:
-        root = False
-        if _futures is None:
-            root = True
-            _futures = []
-        for key in self.sorted_keys:
-            value = self._get_str(key, NO_DEFAULT)
-            if _is_tensor_collection(value.__class__):
-                _tag = value._isend(
-                    dst, _tag=_tag, pseudo_rand=pseudo_rand, _futures=_futures
-                )
-                continue
-            elif isinstance(value, Tensor):
-                pass
-            elif isinstance(value, MemmapTensor):
-                value = value.as_tensor()
-            else:
-                raise NotImplementedError(f"Type {type(value)} is not supported.")
-            if not pseudo_rand:
-                _tag += 1
-            else:
-                _tag = int_generator(_tag + 1)
-            _future = dist.isend(value, dst=dst, tag=_tag)
-            _futures.append(_future)
-        if root:
-            for _future in _futures:
-                _future.wait()
-        return _tag
-
-    def irecv(
-        self,
-        src: int,
-        return_premature: bool = False,
-        init_tag: int = 0,
-        pseudo_rand: bool = False,
-    ) -> tuple[int, list[torch.Future]] | list[torch.Future] | None:
-        """Receives the content of a tensordict and updates content with it asynchronously.
-
-        Check the example in the `isend` method for context.
-
-        Args:
-            src (int): the rank of the source worker.
-            return_premature (bool): if ``True``, returns a list of futures to wait
-                upon until the tensordict is updated. Defaults to ``False``,
-                i.e. waits until update is completed withing the call.
-            init_tag (int): the ``init_tag`` used by the source worker.
-            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
-                random, allowing to send multiple data from different nodes
-                without overlap. Notice that the generation of these pseudo-random
-                numbers is expensive (1e-5 sec/number), meaning that it could
-                slow down the runtime of your algorithm.
-                This value must match the one passed to :func:`isend`.
-                Defaults to ``False``.
-
-        Returns:
-            if ``return_premature=True``, a list of futures to wait
-                upon until the tensordict is updated.
-        """
-        return self._irecv(
-            src, return_premature, _tag=init_tag - 1, pseudo_rand=pseudo_rand
-        )
-
-    def _irecv(
-        self,
-        src: int,
-        return_premature: bool = False,
-        _tag: int = -1,
-        _future_list: list[torch.Future] = None,
-        pseudo_rand: bool = False,
-    ) -> tuple[int, list[torch.Future]] | list[torch.Future] | None:
-        root = False
-        if _future_list is None:
-            _future_list = []
-            root = True
-
-        for key in self.sorted_keys:
-            value = self._get_str(key, NO_DEFAULT)
-            if _is_tensor_collection(value.__class__):
-                _tag, _future_list = value._irecv(
-                    src,
-                    _tag=_tag,
-                    _future_list=_future_list,
-                    pseudo_rand=pseudo_rand,
-                )
-                continue
-            elif isinstance(value, MemmapTensor):
-                value = value.as_tensor()
-            elif isinstance(value, Tensor):
-                pass
-            else:
-                raise NotImplementedError(f"Type {type(value)} is not supported.")
-            if not pseudo_rand:
-                _tag += 1
-            else:
-                _tag = int_generator(_tag + 1)
-            _future_list.append(dist.irecv(value, src=src, tag=_tag))
-        if not root:
-            return _tag, _future_list
-        elif return_premature:
-            return _future_list
-        else:
-            for future in _future_list:
-                future.wait()
-            return
-
-    def reduce(self, dst, op=dist.ReduceOp.SUM, async_op=False, return_premature=False):
-        """Reduces the tensordict across all machines.
-
-        Only the process with ``rank`` dst is going to receive the final result.
-
-        """
-        return self._reduce(dst, op, async_op, return_premature)
-
-    def _reduce(
-        self,
-        dst,
-        op=dist.ReduceOp.SUM,
-        async_op=False,
-        return_premature=False,
-        _future_list=None,
-    ):
-        root = False
-        if _future_list is None:
-            _future_list = []
-            root = True
-        for key in self.sorted_keys:
-            value = self._get_str(key, NO_DEFAULT)
-            if _is_tensor_collection(value.__class__):
-                _future_list = value._reduce(
-                    dst=dst,
-                    op=op,
-                    async_op=async_op,
-                    _future_list=_future_list,
-                )
-                continue
-            elif isinstance(value, MemmapTensor):
-                value = value.as_tensor()
-            elif isinstance(value, Tensor):
-                pass
-            else:
-                raise NotImplementedError(f"Type {type(value)} is not supported.")
-            _future_list.append(dist.reduce(value, dst=dst, op=op, async_op=async_op))
-        if not root:
-            return _future_list
-        elif async_op and return_premature:
-            return _future_list
-        elif async_op:
-            for future in _future_list:
-                future.wait()
-            return
-
-    def _stack_onto_at_(
-        self,
-        key: str,
-        list_item: list[CompatibleType],
-        dim: int,
-        idx: IndexType,
-    ) -> TensorDictBase:
-        """Similar to _stack_onto_ but on a specific index. Only works with regular TensorDicts."""
-        raise RuntimeError(
-            f"Cannot call _stack_onto_at_ with {self.__class__.__name__}. "
-            "This error is probably caused by a call to a lazy operation before stacking. "
-            "Make sure your sub-classed tensordicts are turned into regular tensordicts by calling to_tensordict() "
-            "before calling __getindex__ and stack."
-        )
-
-    def _default_get(
-        self, key: str, default: str | CompatibleType = NO_DEFAULT
-    ) -> CompatibleType:
-        if default is not NO_DEFAULT:
-            return default
-        else:
-            # raise KeyError
-            raise KeyError(
-                TensorDictBase.KEY_ERROR.format(
-                    key, self.__class__.__name__, sorted(self.keys())
-                )
-            )
-
-    def get(
-        self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
-    ) -> CompatibleType:
-        """Gets the value stored with the input key.
-
-        Args:
-            key (str, tuple of str): key to be queried. If tuple of str it is
-                equivalent to chained calls of getattr.
-            default: default value if the key is not found in the tensordict.
-
-        """
-        key = _unravel_key_to_tuple(key)
-        if not key:
-            raise KeyError(_GENERIC_NESTED_ERR)
-        return self._get_tuple(key, default=default)
-
-    @abc.abstractmethod
-    def _get_str(self, key, default):
-        ...
-
-    @abc.abstractmethod
-    def _get_tuple(self, key, default):
-        ...
-
-    def get_item_shape(self, key: NestedKey):
-        """Returns the shape of the entry."""
-        return self.get(key).shape
-
-    def pop(
-        self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
-    ) -> CompatibleType:
-        key = _unravel_key_to_tuple(key)
-        if not key:
-            raise KeyError(_GENERIC_NESTED_ERR)
-        try:
-            # using try/except for get/del is suboptimal, but
-            # this is faster that checkink if key in self keys
-            out = self.get(key, default)
-            self.del_(key)
-        except KeyError as err:
-            # if default provided, 'out' value will return, else raise error
-            if default == NO_DEFAULT:
-                raise KeyError(
-                    f"You are trying to pop key `{key}` which is not in dict "
-                    f"without providing default value."
-                ) from err
-        return out
-
-    def apply_(self, fn: Callable, *others) -> TensorDictBase:
-        """Applies a callable to all values stored in the tensordict and re-writes them in-place.
-
-        Args:
-            fn (Callable): function to be applied to the tensors in the
-                tensordict.
-            *others (sequence of TensorDictBase, optional): the other
-                tensordicts to be used.
-
-        Returns:
-            self or a copy of self with the function applied
-
-        """
-        return self.apply(fn, *others, inplace=True)
-
-    def apply(
-        self,
-        fn: Callable,
-        *others: TensorDictBase,
-        batch_size: Sequence[int] | None = None,
-        device: torch.device | None = None,
-        names: Sequence[str] | None = None,
-        inplace: bool = False,
-        **constructor_kwargs,
-    ) -> TensorDictBase:
-        """Applies a callable to all values stored in the tensordict and sets them in a new tensordict.
-
-        The apply method will return an TensorDict instance, regardless of the
-        input type. To keep the same type, one can execute
-
-          >>> out = td.clone(False).update(td.apply(...))
-
-        Args:
-            fn (Callable): function to be applied to the tensors in the
-                tensordict.
-            *others (TensorDictBase instances, optional): if provided, these
-                tensordicts should have a structure matching the one of the
-                current tensordict. The :obj:`fn` argument should receive as many
-                inputs as the number of tensordicts, including the one where apply is
-                being called.
-            batch_size (sequence of int, optional): if provided,
-                the resulting TensorDict will have the desired batch_size.
-                The :obj:`batch_size` argument should match the batch_size after
-                the transformation. This is a keyword only argument.
-            device (torch.device, optional): the resulting device, if any.
-            names (list of str, optional): the new dimension names, in case the
-                batch_size is modified.
-            inplace (bool, optional): if True, changes are made in-place.
-                Default is False. This is a keyword only argument.
-            **constructor_kwargs: additional keyword arguments to be passed to the
-                TensorDict constructor.
-
-        Returns:
-            a new tensordict with transformed_in tensors.
-
-        Example:
-            >>> td = TensorDict({"a": -torch.ones(3), "b": {"c": torch.ones(3)}}, batch_size=[3])
-            >>> td_1 = td.apply(lambda x: x+1)
-            >>> assert (td["a"] == 0).all()
-            >>> assert (td["b", "c"] == 2).all()
-            >>> td_2 = td.apply(lambda x, y: x+y, td)
-            >>> assert (td_2["a"] == -2).all()
-            >>> assert (td_2["b", "c"] == 2).all()
-        """
-        if inplace:
-            out = self
-        elif batch_size is not None:
-            out = TensorDict(
-                {},
-                batch_size=torch.Size(batch_size),
-                names=names,
-                device=self.device if not device else device,
-                _run_checks=False,
-                **constructor_kwargs,
-            )
-        else:
-            out = TensorDict(
-                {},
-                batch_size=self.batch_size,
-                device=self.device if not device else device,
-                names=self.names if self._has_names() else None,
-                _run_checks=False,
-                **constructor_kwargs,
-            )
-
-        is_locked = out.is_locked
-        if not inplace and is_locked:
-            out.unlock_()
-
-        for key, item in self.items():
-            _others = [_other.get(key) for _other in others]
-            if _is_tensor_collection(item.__class__):
-                item_trsf = item.apply(
-                    fn,
-                    *_others,
-                    inplace=inplace,
-                    batch_size=batch_size,
-                    device=device,
-                    **constructor_kwargs,
-                )
-            else:
-                item_trsf = fn(item, *_others)
-            if item_trsf is not None:
-                # if `self` is a `SubTensorDict` we want to process the input,
-                # hence we call `set` rather than `_set_str`.
-                if isinstance(self, SubTensorDict):
-                    out.set(key, item_trsf, inplace=inplace)
-                else:
-                    out._set_str(
-                        key,
-                        item_trsf,
-                        inplace=BEST_ATTEMPT_INPLACE if inplace else False,
-                        validated=False,
-                    )
-
-        if not inplace and is_locked:
-            out.lock_()
-        return out
-
-    @cache  # noqa: B019
-    def _add_batch_dim(self, *, in_dim, vmap_level):
-        if self.is_memmap():
-            td = self.cpu().as_tensor()
-        else:
-            td = self
-        out = TensorDict(
-            {
-                key: value._add_batch_dim(in_dim=in_dim, vmap_level=vmap_level)
-                if is_tensor_collection(value)
-                else _add_batch_dim(value, in_dim, vmap_level)
-                for key, value in td.items()
-            },
-            batch_size=[b for i, b in enumerate(td.batch_size) if i != in_dim],
-            names=[name for i, name in enumerate(td.names) if i != in_dim],
-        )
-        return out
-
-    @cache  # noqa: B019
-    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
-        new_batch_size = list(self.batch_size)
-        new_batch_size.insert(out_dim, batch_size)
-        new_names = list(self.names)
-        new_names.insert(out_dim, None)
-        out = TensorDict(
-            {
-                key: value._remove_batch_dim(
-                    vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
-                )
-                if is_tensor_collection(value)
-                else _remove_batch_dim(value, vmap_level, batch_size, out_dim)
-                for key, value in self.items()
-            },
-            batch_size=new_batch_size,
-            names=new_names,
-        )
-        return out
-
-    def as_tensor(self):
-        """Calls as_tensor on all the tensors contained in the object.
-
-        This is reserved to classes that contain exclusively MemmapTensors,
-        and will raise an exception in all other cases.
-
-        """
-        try:
-            return self.apply(lambda x: x.as_tensor())
-        except AttributeError as err:
-            raise AttributeError(
-                f"{self.__class__.__name__} does not have an 'as_tensor' method "
-                f"because at least one of its tensors does not support this method."
-            ) from err
-
-    def update(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
-        clone: bool = False,
-        inplace: bool = False,
-    ) -> TensorDictBase:
-        """Updates the TensorDict with values from either a dictionary or another TensorDict.
-
-        Args:
-            input_dict_or_td (TensorDictBase or dict): Does not keyword arguments
-                (unlike :obj:`dict.update()`).
-            clone (bool, optional): whether the tensors in the input (
-                tensor) dict should be cloned before being set. Default is
-                `False`.
-            inplace (bool, optional): if True and if a key matches an existing
-                key in the tensordict, then the update will occur in-place
-                for that key-value pair. Default is :obj:`False`.
-            **kwargs: keyword arguments for the :obj:`TensorDict.set` method
-
-        Returns:
-            self
-
-        """
-        if input_dict_or_td is self:
-            # no op
-            return self
-        keys = set(self.keys(False))
-        for key, value in list(input_dict_or_td.items()):
-            if clone and hasattr(value, "clone"):
-                value = value.clone()
-            if isinstance(key, tuple):
-                key, subkey = key[0], key[1:]
-            else:
-                subkey = []
-            # the key must be a string by now. Let's check if it is present
-            if key in keys:
-                target_type = self.entry_class(key)
-                if _is_tensor_collection(target_type):
-                    target = self.get(key)
-                    if len(subkey):
-                        target.update({subkey: value}, inplace=inplace, clone=clone)
-                        continue
-                    elif isinstance(value, (dict,)) or _is_tensor_collection(
-                        value.__class__
-                    ):
-                        if isinstance(value, LazyStackedTensorDict) and not isinstance(
-                            target, LazyStackedTensorDict
-                        ):
-                            self.set(
-                                key,
-                                LazyStackedTensorDict(
-                                    *target.unbind(value.stack_dim),
-                                    stack_dim=value.stack_dim,
-                                ).update(value, inplace=inplace, clone=clone),
-                            )
-                        else:
-                            target.update(value, inplace=inplace, clone=clone)
-                        continue
-            if len(subkey):
-                self.set((key, *subkey), value, inplace=inplace)
-            else:
-                self.set(key, value, inplace=inplace)
-        return self
-
-    def update_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
-        clone: bool = False,
-    ) -> TensorDictBase:
-        """Updates the TensorDict in-place with values from either a dictionary or another TensorDict.
-
-        Unlike TensorDict.update, this function will
-        throw an error if the key is unknown to the TensorDict
-
-        Args:
-            input_dict_or_td (TensorDictBase or dict): Does not keyword
-                arguments (unlike :obj:`dict.update()`).
-            clone (bool, optional): whether the tensors in the input (
-                tensor) dict should be cloned before being set. Default is
-                `False`.
-
-        Returns:
-            self
-
-        """
-        if input_dict_or_td is self:
-            # no op
-            return self
-        for key, value in input_dict_or_td.items():
-            # if not isinstance(value, _accepted_classes):
-            #     raise TypeError(
-            #         f"Expected value to be one of types {_accepted_classes} "
-            #         f"but got {type(value)}"
-            #     )
-            if clone:
-                value = value.clone()
-            self.set_(key, value)
-        return self
-
-    def update_at_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
-        idx: IndexType,
-        clone: bool = False,
-    ) -> TensorDictBase:
-        """Updates the TensorDict in-place at the specified index with values from either a dictionary or another TensorDict.
-
-        Unlike  TensorDict.update, this function will throw an error if the key is unknown to the TensorDict.
-
-        Args:
-            input_dict_or_td (TensorDictBase or dict): Does not keyword arguments
-                (unlike :obj:`dict.update()`).
-            idx (int, torch.Tensor, iterable, slice): index of the tensordict
-                where the update should occur.
-            clone (bool, optional): whether the tensors in the input (
-                tensor) dict should be cloned before being set. Default is
-                `False`.
-
-        Returns:
-            self
-
-        Examples:
-            >>> td = TensorDict(source={'a': torch.zeros(3, 4, 5),
-            ...    'b': torch.zeros(3, 4, 10)}, batch_size=[3, 4])
-            >>> td.update_at_(
-            ...    TensorDict(source={'a': torch.ones(1, 4, 5),
-            ...        'b': torch.ones(1, 4, 10)}, batch_size=[1, 4]),
-            ...    slice(1, 2))
-            TensorDict(
-                fields={
-                    a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32),
-                    b: Tensor(torch.Size([3, 4, 10]), dtype=torch.float32)},
-                batch_size=torch.Size([3, 4]),
-                device=None,
-                is_shared=False)
-
-        """
-        for key, value in input_dict_or_td.items():
-            if not isinstance(value, _ACCEPTED_CLASSES):
-                raise TypeError(
-                    f"Expected value to be one of types {_ACCEPTED_CLASSES} "
-                    f"but got {type(value)}"
-                )
-            if clone:
-                value = value.clone()
-            self.set_at_(key, value, idx)
-        return self
-
-    def _convert_to_tensor(self, array: np.ndarray) -> Tensor | MemmapTensor:
-        if isinstance(array, np.bool_):
-            array = array.item()
-        if isinstance(array, list):
-            array = np.asarray(array)
-        return torch.as_tensor(array, device=self.device)
-
-    def _convert_to_tensordict(self, dict_value: dict[str, Any]) -> TensorDictBase:
-        return TensorDict(
-            dict_value,
-            batch_size=self.batch_size,
-            device=self.device,
-            _is_shared=self._is_shared,
-            _is_memmap=self._is_memmap,
-        )
-
-    def _validate_key(self, key: NestedKey) -> NestedKey:
-        key = _unravel_key_to_tuple(key)
-        if not key:
-            raise KeyError(_GENERIC_NESTED_ERR)
-        return key
-
-    def _validate_value(
-        self,
-        value: CompatibleType | dict[str, CompatibleType],
-        *,
-        check_shape: bool = True,
-    ) -> CompatibleType | dict[str, CompatibleType]:
-        cls = value.__class__
-        is_tc = _is_tensor_collection(cls)
-        if is_tc or issubclass(cls, _ACCEPTED_CLASSES):
-            pass
-        elif issubclass(cls, dict):
-            value = self._convert_to_tensordict(value)
-            is_tc = True
-        else:
-            try:
-                value = self._convert_to_tensor(value)
-            except ValueError as err:
-                raise ValueError(
-                    f"TensorDict conversion only supports tensorclasses, tensordicts,"
-                    f" numeric scalars and tensors. Got {type(value)}"
-                ) from err
-        bs = self.batch_size
-        if check_shape and bs and _shape(value)[: len(bs)] != bs:
-            # if TensorDict, let's try to map it to the desired shape
-            if is_tc:
-                value = value.clone(recurse=False)
-                value.batch_size = self.batch_size
-            else:
-                raise RuntimeError(
-                    f"batch dimension mismatch, got self.batch_size"
-                    f"={self.batch_size} and value.shape[:self.batch_dims]"
-                    f"={_shape(value)[: self.batch_dims]} with value {value}"
-                )
-        device = self.device
-        if device is not None and value.device != device:
-            value = value.to(device, non_blocking=True)
-        if is_tc and check_shape:
-            has_names = self._has_names()
-            if has_names and value.names[: self.ndim] != self.names:
-                value = value.clone(False).refine_names(*self.names)
-            elif not has_names and value._has_names():
-                self.names = value.names[: self.batch_dims]
-
-        return value
-
-    @abc.abstractmethod
-    def pin_memory(self) -> TensorDictBase:
-        """Calls :obj:`pin_memory` on the stored tensors."""
-        raise NotImplementedError(f"{self.__class__.__name__}")
-
-    def items(
-        self, include_nested: bool = False, leaves_only: bool = False
-    ) -> Iterator[tuple[str, CompatibleType]]:
-        """Returns a generator of key-value pairs for the tensordict."""
-        # check the conditions once only
-        if include_nested and leaves_only:
-            for k in self.keys():
-                val = self._get_str(k, NO_DEFAULT)
-                if _is_tensor_collection(val.__class__):
-                    yield from (
-                        (_unravel_key_to_tuple((k, _key)), _val)
-                        for _key, _val in val.items(
-                            include_nested=include_nested, leaves_only=leaves_only
-                        )
-                    )
-                else:
-                    yield k, val
-        elif include_nested:
-            for k in self.keys():
-                val = self._get_str(k, NO_DEFAULT)
-                yield k, val
-                if _is_tensor_collection(val.__class__):
-                    yield from (
-                        (_unravel_key_to_tuple((k, _key)), _val)
-                        for _key, _val in val.items(
-                            include_nested=include_nested, leaves_only=leaves_only
-                        )
-                    )
-        elif leaves_only:
-            for k in self.keys():
-                val = self._get_str(k, NO_DEFAULT)
-                if not _is_tensor_collection(val.__class__):
-                    yield k, val
-        else:
-            for k in self.keys():
-                yield k, self._get_str(k, NO_DEFAULT)
-
-    def values(
-        self, include_nested: bool = False, leaves_only: bool = False
-    ) -> Iterator[CompatibleType]:
-        """Returns a generator representing the values for the tensordict."""
-        # check the conditions once only
-        if include_nested and leaves_only:
-            for k in self.keys():
-                val = self._get_str(k, NO_DEFAULT)
-                if _is_tensor_collection(val.__class__):
-                    yield from val.values(
-                        include_nested=include_nested, leaves_only=leaves_only
-                    )
-                else:
-                    yield val
-        elif include_nested:
-            for k in self.keys():
-                val = self._get_str(k, NO_DEFAULT)
-                yield val
-                if _is_tensor_collection(val.__class__):
-                    yield from val.values(
-                        include_nested=include_nested, leaves_only=leaves_only
-                    )
-        elif leaves_only:
-            for k in self.keys():
-                val = self._get_str(k, NO_DEFAULT)
-                if not _is_tensor_collection(val.__class__):
-                    yield val
-        else:
-            for k in self.keys():
-                yield self._get_str(k, NO_DEFAULT)
-
-    @abc.abstractmethod
-    def keys(
-        self, include_nested: bool = False, leaves_only: bool = False
-    ) -> _TensorDictKeysView:
-        """Returns a generator of tensordict keys."""
-        raise NotImplementedError(f"{self.__class__.__name__}")
-
-    @property
-    @cache  # noqa: B019
-    def sorted_keys(self) -> list[NestedKey]:
-        """Returns the keys sorted in alphabetical order.
-
-        Does not support extra argument.
-
-        If the TensorDict is locked, the keys are cached until the tensordict
-        is unlocked.
-
-        """
-        return sorted(self.keys())
-
-    def expand(self, *shape: int) -> TensorDictBase:
-        """Expands each tensors of the tensordict according to the torch.expand function.
-
-        In practice, this amends to: :obj:`tensor.expand(*shape, *tensor.shape)`.
-
-        Supports iterables to specify the shape
-
-        Examples:
-            >>> td = TensorDict(source={'a': torch.zeros(3, 4, 5),
-            ...     'b': torch.zeros(3, 4, 10)}, batch_size=[3, 4])
-            >>> td_expand = td.expand(10, 3, 4)
-            >>> assert td_expand.shape == torch.Size([10, 3, 4])
-            >>> assert td_expand.get("a").shape == torch.Size([10, 3, 4, 5])
-
-        """
-        d = {}
-        tensordict_dims = self.batch_dims
-
-        if len(shape) == 1 and isinstance(shape[0], Sequence):
-            shape = tuple(shape[0])
-
-        # new shape dim check
-        if len(shape) < len(self.shape):
-            raise RuntimeError(
-                "the number of sizes provided ({shape_dim}) must be greater or equal to the number of "
-                "dimensions in the TensorDict ({tensordict_dim})".format(
-                    shape_dim=len(shape), tensordict_dim=tensordict_dims
-                )
-            )
-
-        # new shape compatability check
-        for old_dim, new_dim in zip(self.batch_size, shape[-tensordict_dims:]):
-            if old_dim != 1 and new_dim != old_dim:
-                raise RuntimeError(
-                    "Incompatible expanded shape: The expanded shape length at non-singleton dimension should be same "
-                    "as the original length. target_shape = {new_shape}, existing_shape = {old_shape}".format(
-                        new_shape=shape, old_shape=self.batch_size
-                    )
-                )
-        for key, value in self.items():
-            tensor_dims = len(value.shape)
-            last_n_dims = tensor_dims - tensordict_dims
-            if last_n_dims > 0:
-                d[key] = value.expand((*shape, *value.shape[-last_n_dims:]))
-            else:
-                d[key] = value.expand(shape)
-        return TensorDict(
-            source=d,
-            batch_size=torch.Size(shape),
-            device=self.device,
-            _run_checks=False,
-        )
-
-    def flatten(self, start_dim=0, end_dim=-1):
-        """Flattens all the tensors of a tensordict.
-
-        Args:
-            start_dim (int) – the first dim to flatten
-            end_dim (int) – the last dim to flatten
-
-        Examples:
-            >>> td = TensorDict({"a": torch.arange(60).view(3, 4, 5), "b": torch.arange(12).view(3, 4)}, [3, 4])
-            >>> td_flat = td.flatten(0, 1)
-            >>> td_flat.batch_size
-            torch.Size([12])
-            >>> td_flat["a"]
-            tensor([[ 0,  1,  2,  3,  4],
-                    [ 5,  6,  7,  8,  9],
-                    [10, 11, 12, 13, 14],
-                    [15, 16, 17, 18, 19],
-                    [20, 21, 22, 23, 24],
-                    [25, 26, 27, 28, 29],
-                    [30, 31, 32, 33, 34],
-                    [35, 36, 37, 38, 39],
-                    [40, 41, 42, 43, 44],
-                    [45, 46, 47, 48, 49],
-                    [50, 51, 52, 53, 54],
-                    [55, 56, 57, 58, 59]])
-            >>> td_flat["b"]
-            tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
-
-        """
-        if end_dim < 0:
-            end_dim = self.ndim + end_dim
-            if end_dim < 0:
-                raise ValueError(
-                    f"Incompatible end_dim {end_dim} for tensordict with shape {self.shape}."
-                )
-        if end_dim <= start_dim:
-            raise ValueError(
-                "The end dimension must be strictly greater than the start dim."
-            )
-
-        def flatten(tensor):
-            return torch.flatten(tensor, start_dim, end_dim)
-
-        nelt = prod(self.batch_size[start_dim : end_dim + 1])
-        if start_dim > 0:
-            batch_size = (
-                list(self.batch_size)[:start_dim]
-                + [nelt]
-                + list(self.batch_size[end_dim + 1 :])
-            )
-        else:
-            batch_size = [nelt] + list(self.batch_size[end_dim + 1 :])
-        out = self.apply(flatten, batch_size=batch_size)
-        if self._has_names():
-            names = [
-                name
-                for i, name in enumerate(self.names)
-                if (i < start_dim or i > end_dim)
-            ]
-            names.insert(start_dim, None)
-            out.names = names
-        return out
-
-    def unflatten(self, dim, unflattened_size):
-        """Unflattens a tensordict dim expanding it to a desired shape.
-
-        Args:
-            dim (int): specifies the dimension of the input tensor to be unflattened.
-            unflattened_size (shape): is the new shape of the unflattened dimension of the tensordict.
-
-        Examples:
-            >>> td = TensorDict({"a": torch.arange(60).view(3, 4, 5), "b": torch.arange(12).view(3, 4)}, [3, 4])
-            >>> td_flat = td.flatten(0, 1)
-            >>> td_unflat = td_flat.unflatten(0, [3, 4])
-            >>> assert (td == td_unflat).all()
-        """
-        if dim < 0:
-            dim = self.ndim + dim
-            if dim < 0:
-                raise ValueError(
-                    f"Incompatible dim {dim} for tensordict with shape {self.shape}."
-                )
-
-        def unflatten(tensor):
-            return torch.unflatten(
-                tensor,
-                dim,
-                unflattened_size,
-            )
-
-        if dim > 0:
-            batch_size = (
-                list(self.batch_size)[:dim]
-                + list(unflattened_size)
-                + list(self.batch_size[dim + 1 :])
-            )
-        else:
-            batch_size = list(unflattened_size) + list(self.batch_size[1:])
-        out = self.apply(unflatten, batch_size=batch_size)
-        if self._has_names():
-            names = copy(self.names)
-            for _ in range(len(unflattened_size) - 1):
-                names.insert(dim, None)
-            out.names = names
-        return out
-
-    def __enter__(self):
-        self._last_op_queue.append(self._last_op)
-        return self
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        if exc_type is not None and issubclass(exc_type, Exception):
-            return False
-        _last_op = self._last_op_queue.pop()
-        if _last_op is not None:
-            last_op, (args, kwargs) = _last_op
-            if last_op is self.__class__.lock_.__name__:
-                return self.unlock_()
-            elif last_op is self.__class__.unlock_.__name__:
-                return self.lock_()
-            else:
-                raise NotImplementedError(f"Unrecognised function {last_op}.")
-        return self
-
-    def __bool__(self) -> bool:
-        raise ValueError("Converting a tensordict to boolean value is not permitted")
-
-    def __ne__(self, other: object) -> TensorDictBase:
-        """XOR operation over two tensordicts, for evey key.
-
-        The two tensordicts must have the same key set.
-
-        Args:
-            other (TensorDictBase, dict, or float): the value to compare against.
-
-        Returns:
-            a new TensorDict instance with all tensors are boolean
-            tensors of the same shape as the original tensors.
-
-        """
-        if _is_tensorclass(other.__class__):
-            return other != self
-        if isinstance(other, (dict,)) or _is_tensor_collection(other.__class__):
-            keys1 = set(self.keys())
-            keys2 = set(other.keys())
-            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
-                raise KeyError(
-                    f"keys in {self} and {other} mismatch, got {keys1} and {keys2}"
-                )
-            d = {}
-            for key, item1 in self.items():
-                d[key] = item1 != other.get(key)
-            return TensorDict(batch_size=self.batch_size, source=d, device=self.device)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return TensorDict(
-                {key: value != other for key, value in self.items()},
-                self.batch_size,
-                device=self.device,
-            )
-        return True
-
-    # @abc.abstractmethod
-    # def __hash__(self):
-    #     ...
-
-    def __eq__(self, other: object) -> TensorDictBase:
-        """Compares two tensordicts against each other, for every key. The two tensordicts must have the same key set.
-
-        Returns:
-            a new TensorDict instance with all tensors are boolean
-            tensors of the same shape as the original tensors.
-
-        """
-        if is_tensorclass(other):
-            return other == self
-        if isinstance(other, (dict,)) or _is_tensor_collection(other.__class__):
-            keys1 = set(self.keys())
-            keys2 = set(other.keys())
-            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
-                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
-            d = {}
-            for key, item1 in self.items():
-                d[key] = item1 == other.get(key)
-            return TensorDict(batch_size=self.batch_size, source=d, device=self.device)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return TensorDict(
-                {key: value == other for key, value in self.items()},
-                self.batch_size,
-                device=self.device,
-            )
-        return False
-
-    @abc.abstractmethod
-    def del_(self, key: NestedKey) -> TensorDictBase:
-        """Deletes a key of the tensordict.
-
-        Args:
-            key (NestedKey): key to be deleted
-
-        Returns:
-            self
-
-        """
-        raise NotImplementedError(f"{self.__class__.__name__}")
-
-    @abc.abstractmethod
-    def select(
-        self, *keys: str, inplace: bool = False, strict: bool = True
-    ) -> TensorDictBase:
-        """Selects the keys of the tensordict and returns an new tensordict with only the selected keys.
-
-        The values are not copied: in-place modifications a tensor of either
-        of the original or new tensordict will result in a change in both
-        tensordicts.
-
-        Args:
-            *keys (str): keys to select
-            inplace (bool): if True, the tensordict is pruned in place.
-                Default is :obj:`False`.
-            strict (bool, optional): whether selecting a key that is not present
-                will return an error or not. Default: :obj:`True`.
-
-        Returns:
-            A new tensordict with the selected keys only.
-
-        """
-        raise NotImplementedError(f"{self.__class__.__name__}")
-
-    def exclude(self, *keys: str, inplace: bool = False) -> TensorDictBase:
-        target = self if inplace else self.clone(recurse=False)
-        for key in keys:
-            if key in self.keys(True):
-                del target[key]
-        return target
-
-    def copy_(self, tensordict: TensorDictBase) -> TensorDictBase:
-        """See :obj:`TensorDictBase.update_`."""
-        return self.update_(tensordict)
-
-    def copy_at_(self, tensordict: TensorDictBase, idx: IndexType) -> TensorDictBase:
-        """See :obj:`TensorDictBase.update_at_`."""
-        return self.update_at_(tensordict, idx)
-
-    def get_at(
-        self, key: NestedKey, idx: IndexType, default: CompatibleType = NO_DEFAULT
-    ) -> CompatibleType:
-        """Get the value of a tensordict from the key `key` at the index `idx`.
-
-        Args:
-            key (str, tuple of str): key to be retrieved.
-            idx (int, slice, torch.Tensor, iterable): index of the tensor.
-            default (torch.Tensor): default value to return if the key is
-                not present in the tensordict.
-
-        Returns:
-            indexed tensor.
-
-        """
-        key = _unravel_key_to_tuple(key)
-        if not key:
-            raise KeyError(_GENERIC_NESTED_ERR)
-        # must be a tuple
-        return self._get_at_tuple(key, idx, default)
-
-    def _get_at_str(self, key, idx, default):
-        out = self._get_str(key, default)
-        if out is default:
-            return out
-        return out[idx]
-
-    def _get_at_tuple(self, key, idx, default):
-        out = self._get_tuple(key, default)
-        if out is default:
-            return out
-        return out[idx]
-
-    @abc.abstractmethod
-    def share_memory_(self) -> TensorDictBase:
-        """Places all the tensors in shared memory.
-
-        The TensorDict is then locked, meaning that the only writing operations that
-        can be executed must be done in-place.
-        Once the tensordict is unlocked, the share_memory attribute is turned to False,
-        because cross-process identity is not guaranteed anymore.
-
-        Returns:
-            self.
-
-        """
-        raise NotImplementedError(f"{self.__class__.__name__}")
-
-    @abc.abstractmethod
-    def memmap_(
-        self, prefix: str | None = None, copy_existing: bool = False
-    ) -> TensorDictBase:
-        """Writes all tensors onto a MemmapTensor.
-
-        Args:
-            prefix (str): directory prefix where the memmap tensors will have to
-                be stored.
-            copy_existing (bool): If False (default), an exception will be raised if an
-                entry in the tensordict is already a MemmapTensor but is not saved in
-                the correct location according to prefix. If True, any MemmapTensors
-                that are not in the correct location are copied to the new location.
-
-        The TensorDict is then locked, meaning that the only writing operations that
-        can be executed must be done in-place.
-        Once the tensordict is unlocked, the memmap attribute is turned to False,
-        because cross-process identity is not guaranteed anymore.
-
-        Returns:
-            self.
-
-        Note:
-            Serialising in this fashion might be slow with deeply nested tensordicts, so
-            we do not recommend calling this method inside a training loop.
-        """
-        raise NotImplementedError(f"{self.__class__.__name__}")
-
-    def memmap_like(self, prefix: str | None = None) -> TensorDictBase:
-        """Creates an empty Memory-mapped tensordict with the same content shape as the current one.
-
-        Args:
-            prefix (str): directory prefix where the memmap tensors will have to
-                be stored.
-
-        The resulting TensorDict will be locked and ``is_memmap() = True``,
-        meaning that the only writing operations that can be executed must be done in-place.
-        Once the tensordict is unlocked, the memmap attribute is turned to False,
-        because cross-process identity is not guaranteed anymore.
-
-        Returns:
-            a new ``TensorDict`` instance with data stored as memory-mapped tensors.
-
-        """
-        if prefix is not None:
-            prefix = Path(prefix)
-            if not prefix.exists():
-                os.makedirs(prefix, exist_ok=True)
-            torch.save(
-                {"batch_size": self.batch_size, "device": self.device},
-                prefix / "meta.pt",
-            )
-        if not self.keys():
-            raise Exception(
-                "memmap_like() must be called when the TensorDict is (partially) "
-                "populated. Set a tensor first."
-            )
-        tensordict = TensorDict(
-            {},
-            self.batch_size,
-            device=self.device,
-            names=self.names if self._has_names() else None,
-        )
-        for key, value in self.items():
-            if _is_tensor_collection(value.__class__):
-                if prefix is not None:
-                    # ensure subdirectory exists
-                    os.makedirs(prefix / key, exist_ok=True)
-                    tensordict[key] = value.memmap_like(
-                        prefix=prefix / key,
-                    )
-                    torch.save(
-                        {"batch_size": value.batch_size, "device": value.device},
-                        prefix / key / "meta.pt",
-                    )
-                else:
-                    tensordict[key] = value.memmap_like()
-                continue
-            else:
-                tensordict[key] = MemmapTensor.empty_like(
-                    value,
-                    filename=str(prefix / f"{key}.memmap")
-                    if prefix is not None
-                    else None,
-                )
-            if prefix is not None:
-                torch.save(
-                    {
-                        "shape": value.shape,
-                        "device": value.device,
-                        "dtype": value.dtype,
-                    },
-                    prefix / f"{key}.meta.pt",
-                )
-        tensordict._is_memmap = True
-        tensordict.lock_()
-        return tensordict
-
-    @abc.abstractmethod
-    def detach_(self) -> TensorDictBase:
-        """Detach the tensors in the tensordict in-place.
-
-        Returns:
-            self.
-
-        """
-        raise NotImplementedError(f"{self.__class__.__name__}")
-
-    def detach(self) -> TensorDictBase:
-        """Detach the tensors in the tensordict.
-
-        Returns:
-            a new tensordict with no tensor requiring gradient.
-
-        """
-        return self.apply(lambda x: x.detach())
-
-    def to_h5(
-        self,
-        filename,
-        **kwargs,
-    ):
-        """Converts a tensordict to a PersistentTensorDict with the h5 backend.
-
-        Args:
-            filename (str or path): path to the h5 file.
-            device (torch.device or compatible, optional): the device where to
-                expect the tensor once they are returned. Defaults to ``None``
-                (on cpu by default).
-            **kwargs: kwargs to be passed to :meth:`h5py.File.create_dataset`.
-
-        Returns:
-            A :class:`PersitentTensorDict` instance linked to the newly created file.
-
-        Examples:
-            >>> import tempfile
-            >>> import timeit
-            >>>
-            >>> from tensordict import TensorDict, MemmapTensor
-            >>> td = TensorDict({
-            ...     "a": MemmapTensor(1_000_000),
-            ...     "b": {"c": MemmapTensor(1_000_000, 3)},
-            ... }, [1_000_000])
-            >>>
-            >>> file = tempfile.NamedTemporaryFile()
-            >>> td_h5 = td.to_h5(file.name, compression="gzip", compression_opts=9)
-            >>> print(td_h5)
-            PersistentTensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([1000000]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: PersistentTensorDict(
-                        fields={
-                            c: Tensor(shape=torch.Size([1000000, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([1000000]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([1000000]),
-                device=None,
-                is_shared=False)
-
-
-        """
-        from .persistent import PersistentTensorDict
-
-        out = PersistentTensorDict.from_dict(
-            self,
-            filename=filename,
-            **kwargs,
-        )
-        if self._has_names():
-            out.names = self.names
-        return out
-
-    def to_tensordict(self):
-        """Returns a regular TensorDict instance from the TensorDictBase.
-
-        Returns:
-            a new TensorDict object containing the same values.
-
-        """
-        return TensorDict(
-            {
-                key: value.clone()
-                if not _is_tensor_collection(value.__class__)
-                else value.to_tensordict()
-                for key, value in self.items()
-            },
-            device=self.device,
-            batch_size=self.batch_size,
-            names=self.names if self._has_names() else None,
-        )
-
-    def zero_(self) -> TensorDictBase:
-        """Zeros all tensors in the tensordict in-place."""
-        for key in self.keys():
-            self.fill_(key, 0)
-        return self
-
-    def unbind(self, dim: int) -> tuple[TensorDictBase, ...]:
-        """Returns a tuple of indexed tensordicts unbound along the indicated dimension.
-
-        Resulting tensordicts will share the storage of the initial tensordict.
-
-        """
-        if dim < 0:
-            dim = self.batch_dims + dim
-        batch_size = torch.Size([s for i, s in enumerate(self.batch_size) if i != dim])
-        names = None
-        if self._has_names():
-            names = copy(self.names)
-            names = [name for i, name in enumerate(names) if i != dim]
-        out = []
-        unbind_self_dict = {key: tensor.unbind(dim) for key, tensor in self.items()}
-        for _idx in range(self.batch_size[dim]):
-            td = TensorDict(
-                {key: tensor[_idx] for key, tensor in unbind_self_dict.items()},
-                batch_size=batch_size,
-                _run_checks=False,
-                device=self.device,
-                _is_memmap=False,
-                _is_shared=False,
-                names=names,
-            )
-            out.append(td)
-            if self.is_shared():
-                out[-1].share_memory_()
-            elif self.is_memmap():
-                out[-1].memmap_()
-        return tuple(out)
-
-    def chunk(self, chunks: int, dim: int = 0) -> tuple[TensorDictBase, ...]:
-        """Splits a tendordict into the specified number of chunks, if possible.
-
-        Each chunk is a view of the input tensordict.
-
-        Args:
-            chunks (int): number of chunks to return
-            dim (int, optional): dimension along which to split the
-                tensordict. Default is 0.
-
-        """
-        if chunks < 1:
-            raise ValueError(
-                f"chunks must be a strictly positive integer, got {chunks}."
-            )
-        indices = []
-        _idx_start = 0
-        if chunks > 1:
-            interval = _idx_end = self.batch_size[dim] // chunks
-        else:
-            interval = _idx_end = self.batch_size[dim]
-        for c in range(chunks):
-            indices.append(slice(_idx_start, _idx_end))
-            _idx_start = _idx_end
-            _idx_end = _idx_end + interval if c < chunks - 2 else self.batch_size[dim]
-        if dim < 0:
-            dim = len(self.batch_size) + dim
-        return tuple(self[(*[slice(None) for _ in range(dim)], idx)] for idx in indices)
-
-    def clone(self, recurse: bool = True) -> TensorDictBase:
-        """Clones a TensorDictBase subclass instance onto a new TensorDictBase subclass of the same type.
-
-        To create a TensorDict instance from any other TensorDictBase subtype, call the :meth:`~.to_tensordict` method
-        instead.
-
-        Args:
-            recurse (bool, optional): if True, each tensor contained in the
-                TensorDict will be copied too. Default is `True`.
-
-        .. note::
-          For some TensorDictBase subtypes, such as :class:`SubTensorDict`, cloning
-          recursively makes little sense (in this specific case it would involve
-          copying the parent tensordict too). In those cases, :meth:`~.clone` will
-          fall back onto :meth:`~.to_tensordict`.
-
-        """
-        raise NotImplementedError
-
-    @classmethod
-    def __torch_function__(
-        cls,
-        func: Callable,
-        types: tuple[type, ...],
-        args: tuple[Any, ...] = (),
-        kwargs: dict[str, Any] | None = None,
-    ) -> Callable:
-        if kwargs is None:
-            kwargs = {}
-        if func not in TD_HANDLED_FUNCTIONS or not all(
-            issubclass(t, (Tensor, TensorDictBase)) for t in types
-        ):
-            return NotImplemented
-        return TD_HANDLED_FUNCTIONS[func](*args, **kwargs)
-
-    @abc.abstractmethod
-    def to(self, dest: DeviceType | type | torch.Size, **kwargs) -> TensorDictBase:
-        """Maps a TensorDictBase subclass either on a new device or to another TensorDictBase subclass (if permitted).
-
-        Casting tensors to a new dtype is not allowed, as tensordicts are not bound to contain a single
-        tensor dtype.
-
-        Args:
-            dest (device, size or TensorDictBase subclass): destination of the
-                tensordict. If it is a torch.Size object, the batch_size
-                will be updated provided that it is compatible with the
-                stored tensors.
-
-        Returns:
-            a new tensordict. If device indicated by dest differs from
-            the tensordict device, this is a no-op.
-
-        """
-        raise NotImplementedError
-
-    def _check_new_batch_size(self, new_size: torch.Size) -> None:
-        n = len(new_size)
-        for key, tensor in self.items():
-            if _shape(tensor)[:n] != new_size:
-                raise RuntimeError(
-                    f"the tensor {key} has shape {_shape(tensor)} which "
-                    f"is incompatible with the new shape {new_size}."
-                )
-
-    @abc.abstractmethod
-    def _change_batch_size(self, new_size: torch.Size) -> None:
-        raise NotImplementedError
-
-    def cpu(self) -> TensorDictBase:
-        """Casts a tensordict to CPU."""
-        return self.to("cpu")
-
-    def cuda(self, device: int = None) -> TensorDictBase:
-        """Casts a tensordict to a cuda device (if not already on it)."""
-        if device is None:
-            return self.to(torch.device("cuda"))
-        return self.to(f"cuda:{device}")
-
-    def _create_nested_str(self, key):
-        self._set_str(key, self.select(), inplace=False, validated=True)
-
-    def _create_nested_tuple(self, key):
-        self._create_nested_str(key[0])
-        if len(key) > 1:
-            td = self._get_str(key[0], NO_DEFAULT)
-            td._create_nested_tuple(key[1:])
-
-    @lock_blocked
-    def create_nested(self, key):
-        """Creates a nested tensordict of the same shape, device and dim names as the current tensordict.
-
-        If the value already exists, it will be overwritten by this operation.
-        This operation is blocked in locked tensordicts.
-
-        Examples:
-            >>> data = TensorDict({}, [3, 4, 5])
-            >>> data.create_nested("root")
-            >>> data.create_nested(("some", "nested", "value"))
-            >>> nested = data.get(("some", "nested", "value"))
-        """
-        key = _unravel_key_to_tuple(key)
-        self._create_nested_tuple(key)
-        return self
-
-    @abc.abstractmethod
-    def masked_fill_(self, mask: Tensor, value: float | bool) -> TensorDictBase:
-        """Fills the values corresponding to the mask with the desired value.
-
-        Args:
-            mask (boolean torch.Tensor): mask of values to be filled. Shape
-                must match tensordict shape.
-            value: value to used to fill the tensors.
-
-        Returns:
-            self
-
-        Examples:
-            >>> td = TensorDict(source={'a': torch.zeros(3, 4)},
-            ...     batch_size=[3])
-            >>> mask = torch.tensor([True, False, False])
-            >>> _ = td.masked_fill_(mask, 1.0)
-            >>> td.get("a")
-            tensor([[1., 1., 1., 1.],
-                    [0., 0., 0., 0.],
-                    [0., 0., 0., 0.]])
-        """
-        raise NotImplementedError
-
-    @abc.abstractmethod
-    def masked_fill(self, mask: Tensor, value: float | bool) -> TensorDictBase:
-        """Out-of-place version of masked_fill.
-
-        Args:
-            mask (boolean torch.Tensor): mask of values to be filled. Shape
-                must match tensordict shape.
-            value: value to used to fill the tensors.
-
-        Returns:
-            self
-
-        Examples:
-            >>> td = TensorDict(source={'a': torch.zeros(3, 4)},
-            ...     batch_size=[3])
-            >>> mask = torch.tensor([True, False, False])
-            >>> td1 = td.masked_fill(mask, 1.0)
-            >>> td1.get("a")
-            tensor([[1., 1., 1., 1.],
-                    [0., 0., 0., 0.],
-                    [0., 0., 0., 0.]])
-        """
-        raise NotImplementedError
-
-    def masked_select(self, mask: Tensor) -> TensorDictBase:
-        """Masks all tensors of the TensorDict and return a new TensorDict instance with similar keys pointing to masked values.
-
-        Args:
-            mask (torch.Tensor): boolean mask to be used for the tensors.
-                Shape must match the TensorDict batch_size.
-
-        Examples:
-            >>> td = TensorDict(source={'a': torch.zeros(3, 4)},
-            ...    batch_size=[3])
-            >>> mask = torch.tensor([True, False, False])
-            >>> td_mask = td.masked_select(mask)
-            >>> td_mask.get("a")
-            tensor([[0., 0., 0., 0.]])
-
-        """
-        d = {}
-        mask_expand = mask
-        while mask_expand.ndimension() > self.batch_dims:
-            mndim = mask_expand.ndimension()
-            mask_expand = mask_expand.squeeze(-1)
-            if mndim == mask_expand.ndimension():  # no more squeeze
-                break
-        for key, value in self.items():
-            d[key] = value[mask_expand]
-        dim = int(mask.sum().item())
-        other_dim = self.shape[mask.ndim :]
-        return TensorDict(
-            device=self.device, source=d, batch_size=torch.Size([dim, *other_dim])
-        )
-
-    @abc.abstractmethod
-    def is_contiguous(self) -> bool:
-        """Returns a boolean indicating if all the tensors are contiguous."""
-        raise NotImplementedError
-
-    @abc.abstractmethod
-    def contiguous(self) -> TensorDictBase:
-        """Returns a new tensordict of the same type with contiguous values (or self if values are already contiguous)."""
-        raise NotImplementedError
-
-    def to_dict(self) -> dict[str, Any]:
-        """Returns a dictionary with key-value pairs matching those of the tensordict."""
-        return {
-            key: value.to_dict() if _is_tensor_collection(value.__class__) else value
-            for key, value in self.items()
-        }
-
-    def unsqueeze(self, dim: int) -> TensorDictBase:
-        """Unsqueeze all tensors for a dimension comprised in between `-td.batch_dims` and `td.batch_dims` and returns them in a new tensordict.
-
-        Args:
-            dim (int): dimension along which to unsqueeze
-
-        """
-        if dim < 0:
-            dim = self.batch_dims + dim + 1
-
-        if (dim > self.batch_dims) or (dim < 0):
-            raise RuntimeError(
-                f"unsqueezing is allowed for dims comprised between "
-                f"`-td.batch_dims` and `td.batch_dims` only. Got "
-                f"dim={dim} with a batch size of {self.batch_size}."
-            )
-        return _UnsqueezedTensorDict(
-            source=self,
-            custom_op="unsqueeze",
-            inv_op="squeeze",
-            custom_op_kwargs={"dim": dim},
-            inv_op_kwargs={"dim": dim},
-        )
-
-    def squeeze(self, dim: int | None = None) -> TensorDictBase:
-        """Squeezes all tensors for a dimension comprised in between `-td.batch_dims+1` and `td.batch_dims-1` and returns them in a new tensordict.
-
-        Args:
-            dim (Optional[int]): dimension along which to squeeze. If dim is None, all singleton dimensions will be squeezed. dim is None by default.
-
-        """
-        if dim is None:
-            size = self.size()
-            if len(self.size()) == 1 or size.count(1) == 0:
-                return self
-            first_singleton_dim = size.index(1)
-
-            squeezed_dict = _SqueezedTensorDict(
-                source=self,
-                custom_op="squeeze",
-                inv_op="unsqueeze",
-                custom_op_kwargs={"dim": first_singleton_dim},
-                inv_op_kwargs={"dim": first_singleton_dim},
-            )
-            return squeezed_dict.squeeze(dim=None)
-
-        if dim < 0:
-            dim = self.batch_dims + dim
-
-        if self.batch_dims and (dim >= self.batch_dims or dim < 0):
-            raise RuntimeError(
-                f"squeezing is allowed for dims comprised between 0 and "
-                f"td.batch_dims only. Got dim={dim} and batch_size"
-                f"={self.batch_size}."
-            )
-
-        if dim >= self.batch_dims or self.batch_size[dim] != 1:
-            return self
-        return _SqueezedTensorDict(
-            source=self,
-            custom_op="squeeze",
-            inv_op="unsqueeze",
-            custom_op_kwargs={"dim": dim},
-            inv_op_kwargs={"dim": dim},
-        )
-
-    def reshape(
-        self,
-        *shape: int,
-        size: list | tuple | torch.Size | None = None,
-    ) -> TensorDictBase:
-        """Returns a contiguous, reshaped tensor of the desired shape.
-
-        Args:
-            *shape (int): new shape of the resulting tensordict.
-            size: iterable
-
-        Returns:
-            A TensorDict with reshaped keys
-
-        """
-        if len(shape) == 0 and size is not None:
-            return self.reshape(*size)
-        elif len(shape) == 1 and isinstance(shape[0], (list, tuple, torch.Size)):
-            return self.reshape(*shape[0])
-        elif not isinstance(shape, torch.Size):
-            shape = torch.Size(shape)
-
-        d = {}
-        for key, item in self.items():
-            d[key] = item.reshape((*shape, *item.shape[self.ndimension() :]))
-        if d:
-            batch_size = d[key].shape[: len(shape)]
-        else:
-            if any(not isinstance(i, int) or i < 0 for i in shape):
-                raise RuntimeError(
-                    "Implicit reshaping is not permitted with empty " "tensordicts"
-                )
-            batch_size = torch.Size(shape)
-        return TensorDict(d, batch_size, device=self.device, _run_checks=False)
-
-    def split(self, split_size: int | list[int], dim: int = 0) -> list[TensorDictBase]:
-        """Splits each tensor in the TensorDict with the specified size in the given dimension, like `torch.split`.
-
-        Returns a list of TensorDict with the view of split chunks of items. Nested TensorDicts will remain nested.
-
-        The list of TensorDict maintains the original order of the tensor chunks.
-
-        Args:
-            split_size (int or List(int)): size of a single chunk or list of sizes for each chunk
-            dim (int): dimension along which to split the tensor
-
-        Returns:
-            A list of TensorDict with specified size in given dimension.
-
-        """
-        batch_sizes = []
-        if self.batch_dims == 0:
-            raise RuntimeError("TensorDict with empty batch size is not splittable")
-        if not (-self.batch_dims <= dim < self.batch_dims):
-            raise IndexError(
-                f"Dimension out of range (expected to be in range of [-{self.batch_dims}, {self.batch_dims - 1}], but got {dim})"
-            )
-        if dim < 0:
-            dim += self.batch_dims
-        if isinstance(split_size, int):
-            rep, remainder = divmod(self.batch_size[dim], split_size)
-            rep_shape = torch.Size(
-                [
-                    split_size if idx == dim else size
-                    for (idx, size) in enumerate(self.batch_size)
-                ]
-            )
-            batch_sizes = [rep_shape for _ in range(rep)]
-            if remainder:
-                batch_sizes.append(
-                    torch.Size(
-                        [
-                            remainder if dim_idx == dim else dim_size
-                            for (dim_idx, dim_size) in enumerate(self.batch_size)
-                        ]
-                    )
-                )
-        elif isinstance(split_size, list) and all(
-            isinstance(element, int) for element in split_size
-        ):
-            if sum(split_size) != self.batch_size[dim]:
-                raise RuntimeError(
-                    f"Split method expects split_size to sum exactly to {self.batch_size[dim]} (tensor's size at dimension {dim}), but got split_size={split_size}"
-                )
-            for i in split_size:
-                batch_sizes.append(
-                    torch.Size(
-                        [
-                            i if dim_idx == dim else dim_size
-                            for (dim_idx, dim_size) in enumerate(self.batch_size)
-                        ]
-                    )
-                )
-        else:
-            raise TypeError(
-                "split(): argument 'split_size' must be int or list of ints"
-            )
-        dictionaries = [{} for _ in range(len(batch_sizes))]
-        for key, item in self.items():
-            split_tensors = torch.split(item, split_size, dim)
-            for idx, split_tensor in enumerate(split_tensors):
-                dictionaries[idx][key] = split_tensor
-        names = None
-        if self._has_names():
-            names = copy(self.names)
-        return [
-            TensorDict(
-                dictionaries[i],
-                batch_sizes[i],
-                device=self.device,
-                names=names,
-                _run_checks=False,
-                _is_shared=self.is_shared(),
-                _is_memmap=self.is_memmap(),
-            )
-            for i in range(len(dictionaries))
-        ]
-
-    def gather(
-        self, dim: int, index: Tensor, out: TensorDictBase | None = None
-    ) -> TensorDictBase:
-        """Gathers values along an axis specified by `dim`.
-
-        Args:
-            dim (int): the dimension along which collect the elements
-            index (torch.Tensor): a long tensor which number of dimension matches
-                the one of the tensordict with only one dimension differring between
-                the two (the gathering dimension). Its elements refer to the
-                index to be gathered along the required dimension.
-            out (TensorDictBase, optional): a destination tensordict. It must
-                have the same shape as the index.
-
-        Examples:
-            >>> td = TensorDict(
-            ...     {"a": torch.randn(3, 4, 5),
-            ...      "b": TensorDict({"c": torch.zeros(3, 4, 5)}, [3, 4, 5])},
-            ...     [3, 4])
-            >>> index = torch.randint(4, (3, 2))
-            >>> td_gather = td.gather(dim=1, index=index)
-            >>> print(td_gather)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([3, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            c: Tensor(shape=torch.Size([3, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([3, 2, 5]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([3, 2]),
-                device=None,
-                is_shared=False)
-
-        Gather keeps the dimension names.
-
-        Examples:
-            >>> td.names = ["a", "b"]
-            >>> td_gather = td.gather(dim=1, index=index)
-            >>> td_gather.names
-            ["a", "b"]
-        """
-        return torch.gather(self, dim, index, out=out)
-
-    def view(
-        self,
-        *shape: int,
-        size: list | tuple | torch.Size | None = None,
-    ) -> TensorDictBase:
-        """Returns a tensordict with views of the tensors according to a new shape, compatible with the tensordict batch_size.
-
-        Args:
-            *shape (int): new shape of the resulting tensordict.
-            size: iterable
-
-        Returns:
-            a new tensordict with the desired batch_size.
-
-        Examples:
-            >>> td = TensorDict(source={'a': torch.zeros(3,4,5),
-            ...    'b': torch.zeros(3,4,10,1)}, batch_size=torch.Size([3, 4]))
-            >>> td_view = td.view(12)
-            >>> print(td_view.get("a").shape)  # torch.Size([12, 5])
-            >>> print(td_view.get("b").shape)  # torch.Size([12, 10, 1])
-            >>> td_view = td.view(-1, 4, 3)
-            >>> print(td_view.get("a").shape)  # torch.Size([1, 4, 3, 5])
-            >>> print(td_view.get("b").shape)  # torch.Size([1, 4, 3, 10, 1])
-
-        """
-        if len(shape) == 0 and size is not None:
-            return self.view(*size)
-        elif len(shape) == 1 and isinstance(shape[0], (list, tuple, torch.Size)):
-            return self.view(*shape[0])
-        elif not isinstance(shape, torch.Size):
-            shape = infer_size_impl(shape, self.numel())
-            shape = torch.Size(shape)
-        if shape == self.shape:
-            return self
-        return _ViewedTensorDict(
-            source=self,
-            custom_op="view",
-            inv_op="view",
-            custom_op_kwargs={"size": shape},
-            inv_op_kwargs={"size": self.batch_size},
-        )
-
-    def transpose(self, dim0, dim1):
-        """Returns a tensordit that is a transposed version of input. The given dimensions ``dim0`` and ``dim1`` are swapped.
-
-        In-place or out-place modifications of the transposed tensordict will
-        impact the original tensordict too as the memory is shared and the operations
-        are mapped back on the original tensordict.
-
-        Examples:
-            >>> tensordict = TensorDict({"a": torch.randn(3, 4, 5)}, [3, 4])
-            >>> tensordict_transpose = tensordict.transpose(0, 1)
-            >>> print(tensordict_transpose.shape)
-            torch.Size([4, 3])
-            >>> tensordict_transpose.set("b",, torch.randn(4, 3))
-            >>> print(tensordict.get("b").shape)
-            torch.Size([3, 4])
-        """
-        if dim0 < 0:
-            dim0 = self.ndim + dim0
-        if dim1 < 0:
-            dim1 = self.ndim + dim1
-        if any((dim0 < 0, dim1 < 0)):
-            raise ValueError(
-                "The provided dimensions are incompatible with the tensordict batch-size."
-            )
-        if dim0 == dim1:
-            return self
-        return _TransposedTensorDict(
-            source=self,
-            custom_op="transpose",
-            inv_op="transpose",
-            custom_op_kwargs={"dim0": dim0, "dim1": dim1},
-            inv_op_kwargs={"dim0": dim0, "dim1": dim1},
-        )
-
-    def permute(
-        self,
-        *dims_list: int,
-        dims: list[int] | None = None,
-    ) -> TensorDictBase:
-        """Returns a view of a tensordict with the batch dimensions permuted according to dims.
-
-        Args:
-            *dims_list (int): the new ordering of the batch dims of the tensordict. Alternatively,
-                a single iterable of integers can be provided.
-            dims (list of int): alternative way of calling permute(...).
-
-        Returns:
-            a new tensordict with the batch dimensions in the desired order.
-
-        Examples:
-            >>> tensordict = TensorDict({"a": torch.randn(3, 4, 5)}, [3, 4])
-            >>> print(tensordict.permute([1, 0]))
-            PermutedTensorDict(
-                source=TensorDict(
-                    fields={
-                        a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32)},
-                    batch_size=torch.Size([3, 4]),
-                    device=cpu,
-                    is_shared=False),
-                op=permute(dims=[1, 0]))
-            >>> print(tensordict.permute(1, 0))
-            PermutedTensorDict(
-                source=TensorDict(
-                    fields={
-                        a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32)},
-                    batch_size=torch.Size([3, 4]),
-                    device=cpu,
-                    is_shared=False),
-                op=permute(dims=[1, 0]))
-            >>> print(tensordict.permute(dims=[1, 0]))
-            PermutedTensorDict(
-                source=TensorDict(
-                    fields={
-                        a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32)},
-                    batch_size=torch.Size([3, 4]),
-                    device=cpu,
-                    is_shared=False),
-                op=permute(dims=[1, 0]))
-        """
-        if len(dims_list) == 0:
-            dims_list = dims
-        elif len(dims_list) == 1 and not isinstance(dims_list[0], int):
-            dims_list = dims_list[0]
-        if len(dims_list) != len(self.shape):
-            raise RuntimeError(
-                f"number of dims don't match in permute (got {len(dims_list)}, expected {len(self.shape)}"
-            )
-
-        if not len(dims_list) and not self.batch_dims:
-            return self
-        if np.array_equal(dims_list, range(self.batch_dims)):
-            return self
-        min_dim, max_dim = -self.batch_dims, self.batch_dims - 1
-        seen = [False for dim in range(max_dim + 1)]
-        for idx in dims_list:
-            if idx < min_dim or idx > max_dim:
-                raise IndexError(
-                    f"dimension out of range (expected to be in range of [{min_dim}, {max_dim}], but got {idx})"
-                )
-            if seen[idx]:
-                raise RuntimeError("repeated dim in permute")
-            seen[idx] = True
-
-        return _PermutedTensorDict(
-            source=self,
-            custom_op="permute",
-            inv_op="permute",
-            custom_op_kwargs={"dims": dims_list},
-            inv_op_kwargs={"dims": dims_list},
-        )
-
-    def __repr__(self) -> str:
-        fields = _td_fields(self)
-        field_str = indent(f"fields={{{fields}}}", 4 * " ")
-        batch_size_str = indent(f"batch_size={self.batch_size}", 4 * " ")
-        device_str = indent(f"device={self.device}", 4 * " ")
-        is_shared_str = indent(f"is_shared={self.is_shared()}", 4 * " ")
-        string = ",\n".join([field_str, batch_size_str, device_str, is_shared_str])
-        return f"{type(self).__name__}(\n{string})"
-
-    def all(self, dim: int = None) -> bool | TensorDictBase:
-        """Checks if all values are True/non-null in the tensordict.
-
-        Args:
-            dim (int, optional): if None, returns a boolean indicating
-                whether all tensors return `tensor.all() == True`
-                If integer, all is called upon the dimension specified if
-                and only if this dimension is compatible with the tensordict
-                shape.
-
-        """
-        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
-            raise RuntimeError(
-                "dim must be greater than or equal to -tensordict.batch_dims and "
-                "smaller than tensordict.batch_dims"
-            )
-        if dim is not None:
-            if dim < 0:
-                dim = self.batch_dims + dim
-
-            names = None
-            if self._has_names():
-                names = copy(self.names)
-                names = [name for i, name in enumerate(names) if i != dim]
-
-            return TensorDict(
-                source={key: value.all(dim=dim) for key, value in self.items()},
-                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
-                device=self.device,
-                names=names,
-            )
-        return all(value.all() for value in self.values())
-
-    def any(self, dim: int = None) -> bool | TensorDictBase:
-        """Checks if any value is True/non-null in the tensordict.
-
-        Args:
-            dim (int, optional): if None, returns a boolean indicating
-                whether all tensors return `tensor.any() == True`.
-                If integer, all is called upon the dimension specified if
-                and only if this dimension is compatible with
-                the tensordict shape.
-
-        """
-        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
-            raise RuntimeError(
-                "dim must be greater than or equal to -tensordict.batch_dims and "
-                "smaller than tensordict.batch_dims"
-            )
-        if dim is not None:
-            if dim < 0:
-                dim = self.batch_dims + dim
-
-            names = None
-            if self._has_names():
-                names = copy(self.names)
-                names = [name for i, name in enumerate(names) if i != dim]
-
-            return TensorDict(
-                source={key: value.any(dim=dim) for key, value in self.items()},
-                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
-                device=self.device,
-                names=names,
-            )
-        return any([value.any() for value in self.values()])
-
-    def get_sub_tensordict(self, idx: IndexType) -> TensorDictBase:
-        """Returns a SubTensorDict with the desired index."""
-        return SubTensorDict(source=self, idx=idx)
-
-    def __iter__(self) -> Generator:
-        if not self.batch_dims:
-            raise StopIteration
-        length = self.batch_size[0]
-        for i in range(length):
-            yield self[i]
-
-    @cache  # noqa: B019
-    def flatten_keys(
-        self, separator: str = ".", inplace: bool = False
-    ) -> TensorDictBase:
-        to_flatten = []
-        existing_keys = self.keys(include_nested=True)
-        for key, value in self.items():
-            key_split = tuple(key.split(separator))
-            if isinstance(value, TensorDictBase):
-                to_flatten.append(key)
-            elif (
-                separator in key
-                and key_split in existing_keys
-                and not _is_tensor_collection(self.entry_class(key_split))
-            ):
-                raise KeyError(
-                    f"Flattening keys in tensordict collides with existing key '{key}'"
-                )
-
-        if inplace:
-            for key in to_flatten:
-                inner_tensordict = self.get(key).flatten_keys(
-                    separator=separator, inplace=inplace
-                )
-                for inner_key, inner_item in inner_tensordict.items():
-                    self.set(separator.join([key, inner_key]), inner_item)
-            for key in to_flatten:
-                del self[key]
-            return self
-        else:
-            tensordict_out = TensorDict(
-                {},
-                batch_size=self.batch_size,
-                device=self.device,
-                _run_checks=False,
-                _is_shared=self.is_shared(),
-                _is_memmap=self.is_memmap(),
-                names=self.names,
-            )
-            for key, value in self.items():
-                if key in to_flatten:
-                    inner_tensordict = self.get(key).flatten_keys(
-                        separator=separator, inplace=inplace
-                    )
-                    for inner_key, inner_item in inner_tensordict.items():
-                        tensordict_out.set(separator.join([key, inner_key]), inner_item)
-                else:
-                    tensordict_out.set(key, value)
-            return tensordict_out
-
-    @cache  # noqa: B019
-    def unflatten_keys(
-        self, separator: str = ".", inplace: bool = False
-    ) -> TensorDictBase:
-        to_unflatten = defaultdict(list)
-        for key in self.keys():
-            if separator in key[1:-1]:
-                split_key = key.split(separator)
-                to_unflatten[split_key[0]].append((key, separator.join(split_key[1:])))
-
-        if not inplace:
-            out = TensorDict(
-                {
-                    key: value
-                    for key, value in self.items()
-                    if separator not in key[1:-1]
-                },
-                batch_size=self.batch_size,
-                device=self.device,
-                _run_checks=False,
-                _is_shared=self.is_shared(),
-                _is_memmap=self.is_memmap(),
-                names=self.names,
-            )
-        else:
-            out = self
-
-        keys = set(out.keys())
-        for key, list_of_keys in to_unflatten.items():
-            if key in keys:
-                raise KeyError(
-                    "Unflattening key(s) in tensordict will override existing unflattened key"
-                )
-
-            tensordict = TensorDict(
-                {},
-                batch_size=self.batch_size,
-                device=self.device,
-                names=self.names,
-            )
-            if key in self.keys():
-                tensordict.update(self[key])
-            for old_key, new_key in list_of_keys:
-                value = self.get(old_key)
-                tensordict[new_key] = value
-                if inplace:
-                    del self[old_key]
-            out._set_str(
-                key,
-                tensordict.unflatten_keys(separator=separator),
-                inplace=False,
-                validated=True,
-            )
-        return out
-
-    def __len__(self) -> int:
-        """Returns the length of first dimension, if there is, otherwise 0."""
-        return self.shape[0] if self.batch_dims else 0
-
-    def __contains__(self, key: NestedKey) -> bool:
-        # by default a Mapping will implement __contains__ by calling __getitem__ and
-        # returning False if a KeyError is raised, True otherwise. TensorDict has a
-        # complex __getitem__ method since we support more than just retrieval of values
-        # by key, and so this can be quite inefficient, particularly if values are
-        # evaluated lazily on access. Hence we don't support use of __contains__ and
-        # direct the user to use TensorDict.keys() instead
-        raise NotImplementedError(
-            "TensorDict does not support membership checks with the `in` keyword. If "
-            "you want to check if a particular key is in your TensorDict, please use "
-            "`key in tensordict.keys()` instead."
-        )
-
-    def _get_names_idx(self, idx):
-        if not self._has_names():
-            names = None
-        else:
-
-            def is_boolean(idx):
-                if isinstance(idx, tuple) and len(idx) == 1:
-                    return is_boolean(idx[0])
-                if hasattr(idx, "dtype") and idx.dtype is torch.bool:
-                    return idx.ndim
-                return None
-
-            num_boolean_dim = is_boolean(idx)
-            names = self.names
-            if num_boolean_dim:
-                names = [None] + names[num_boolean_dim:]
-            else:
-                # def is_int(subidx):
-                #     if isinstance(subidx, Number):
-                #         return True
-                #     if isinstance(subidx, Tensor) and len(subidx.shape) == 0:
-                #         return True
-                #     return False
-
-                if not isinstance(idx, tuple):
-                    idx = (idx,)
-                if len(idx) < self.ndim:
-                    idx = (*idx, Ellipsis)
-                idx_names = convert_ellipsis_to_idx(idx, self.batch_size)
-                # this will convert a [None, :, :, 0, None, 0] in [None, 0, 1, None, 3]
-                count = 0
-                idx_to_take = []
-                for _idx in idx_names:
-                    if _idx is None:
-                        idx_to_take.append(None)
-                    elif _is_number(_idx):
-                        count += 1
-                    else:
-                        idx_to_take.append(count)
-                        count += 1
-                names = [names[i] if i is not None else None for i in idx_to_take]
-        return names
-
-    def _index_tensordict(self, index: IndexType) -> TensorDictBase:
-        batch_size = self.batch_size
-        if (
-            not batch_size
-            and index is not None
-            and (not isinstance(index, tuple) or any(idx is not None for idx in index))
-        ):
-            raise RuntimeError(
-                f"indexing a tensordict with td.batch_dims==0 is not permitted. Got index {index}."
-            )
-        names = self._get_names_idx(index)
-        batch_size = _getitem_batch_size(batch_size, index)
-        return TensorDict(
-            source={key: _get_item(item, index) for key, item in self.items()},
-            batch_size=batch_size,
-            device=self.device,
-            names=names,
-            _run_checks=False,
-            _is_shared=self.is_shared(),
-            _is_memmap=self.is_memmap(),
-        )
-
-    def __getitem__(self, index: IndexType) -> TensorDictBase:
-        """Indexes all tensors according to the provided index.
-
-        Returns a new tensordict where the values share the storage of the
-        original tensors (even when the index is a torch.Tensor).
-        Any in-place modification to the resulting tensordict will
-        impact the parent tensordict too.
-
-        Examples:
-            >>> td = TensorDict(source={'a': torch.zeros(3,4,5)},
-            ...     batch_size=torch.Size([3, 4]))
-            >>> subtd = td[torch.zeros(1, dtype=torch.long)]
-            >>> assert subtd.shape == torch.Size([1,4])
-            >>> subtd.set("a", torch.ones(1,4,5))
-            >>> print(td.get("a"))  # first row is full of 1
-            >>> # Warning: this will not work as expected
-            >>> subtd.get("a")[:] = 2.0
-            >>> print(td.get("a"))  # values have not changed
-
-        """
-        istuple = isinstance(index, tuple)
-        if istuple or isinstance(index, str):
-            idx_unravel = _unravel_key_to_tuple(index)
-            if idx_unravel:
-                return self._get_tuple(idx_unravel, NO_DEFAULT)
-        if (istuple and not index) or (not istuple and index is Ellipsis):
-            # empty tuple returns self
-            return self
-        if not istuple:
-            if isinstance(index, int):
-                return self._index_tensordict(index)
-            # we only want tuple indices
-            index = (index,)
-        # # convert range/np.ndarray to tensor: this is not cheap
-        # index = tuple(
-        #     torch.tensor(idx) if isinstance(idx, (np.ndarray, range)) else idx
-        #     for idx in index
-        # )
-        if istuple and any(idx is Ellipsis for idx in index):
-            index = convert_ellipsis_to_idx(index, self.batch_size)
-        if all(isinstance(idx, slice) and idx == slice(None) for idx in index):
-            return self
-
-        return self._index_tensordict(index)
-
-    __getitems__ = __getitem__
-
-    def __setitem__(
-        self,
-        index: IndexType,
-        value: TensorDictBase | dict | numbers.Number | CompatibleType,
-    ) -> None:
-        istuple = isinstance(index, tuple)
-        if istuple or isinstance(index, str):
-            # try:
-            index_unravel = _unravel_key_to_tuple(index)
-            if index_unravel:
-                self._set_tuple(
-                    index_unravel,
-                    value,
-                    inplace=BEST_ATTEMPT_INPLACE
-                    if isinstance(self, SubTensorDict)
-                    else False,
-                    validated=False,
-                )
-                return
-
-        if index is Ellipsis or (isinstance(index, tuple) and Ellipsis in index):
-            index = convert_ellipsis_to_idx(index, self.batch_size)
-        # elif isinstance(index, (list, range)):
-        #     index = torch.tensor(index, device=self.device)
-
-        if isinstance(value, (TensorDictBase, dict)):
-            indexed_bs = _getitem_batch_size(self.batch_size, index)
-            if isinstance(value, dict):
-                value = TensorDict(
-                    value, batch_size=indexed_bs, device=self.device, _run_checks=False
-                )
-            if value.batch_size != indexed_bs:
-                # try to expand
-                try:
-                    value = value.expand(indexed_bs)
-                except RuntimeError as err:
-                    raise RuntimeError(
-                        f"indexed destination TensorDict batch size is {indexed_bs} "
-                        f"(batch_size = {self.batch_size}, index={index}), "
-                        f"which differs from the source batch size {value.batch_size}"
-                    ) from err
-
-            keys = set(self.keys())
-            if any(key not in keys for key in value.keys()):
-                subtd = self.get_sub_tensordict(index)
-            for key, item in value.items():
-                if key in keys:
-                    self._set_at_str(key, item, index, validated=False)
-                else:
-                    subtd.set(key, item)
-        else:
-            for key in self.keys():
-                self.set_at_(key, value, index)
-
-    def __delitem__(self, index: IndexType) -> TensorDictBase:
-        # if isinstance(index, str):
-        return self.del_(index)
-        # raise IndexError(f"Index has to a string but received {index}.")
-
-    @abc.abstractmethod
-    def rename_key_(
-        self, old_key: str, new_key: str, safe: bool = False
-    ) -> TensorDictBase:
-        """Renames a key with a new string.
-
-        Args:
-            old_key (str): key to be renamed
-            new_key (str): new name
-            safe (bool, optional): if True, an error is thrown when the new
-                key is already present in the TensorDict.
-
-        Returns:
-            self
-
-        """
-        raise NotImplementedError
-
-    def fill_(self, key: NestedKey, value: float | bool) -> TensorDictBase:
-        """Fills a tensor pointed by the key with the a given value.
-
-        Args:
-            key (str): key to be remaned
-            value (Number, bool): value to use for the filling
-
-        Returns:
-            self
-
-        """
-        key = _unravel_key_to_tuple(key)
-        data = self._get_tuple(key, NO_DEFAULT)
-        if _is_tensor_collection(data.__class__):
-            data.apply_(lambda x: x.fill_(value))
-            # self._set(key, tensordict, inplace=True)
-        else:
-            data = data.fill_(value)
-            self._set_tuple(key, data, inplace=True, validated=True)
-        return self
-
-    def empty(self, recurse=False) -> TensorDictBase:
-        """Returns a new, empty tensordict with the same device and batch size.
-
-        Args:
-            recurse (bool, optional): if ``True``, the entire structure of the TensorDict
-                will be rerproduced without content. Otherwise, only the root
-                will be duplicated. Defaults to ``False``.
-
-        """
-        if not recurse:
-            return self.select()
-        return self.exclude(*self.keys(True, True))
-
-    def is_empty(self) -> bool:
-        for _ in self.keys():
-            return False
-        return True
-
-    def setdefault(
-        self, key: NestedKey, default: CompatibleType, inplace: bool = False
-    ) -> CompatibleType:
-        """Insert key with a value of default if key is not in the dictionary.
-
-        Return the value for key if key is in the dictionary, else default.
-
-        Args:
-            key (str): the name of the value.
-            default (torch.Tensor): value to be stored in the tensordict if the key is
-                not already present.
-
-        Returns:
-            The value of key in the tensordict. Will be default if the key was not
-            previously set.
-
-        """
-        if key not in self.keys(include_nested=isinstance(key, tuple)):
-            self.set(key, default, inplace=inplace)
-        return self.get(key)
-
-    @property
-    def is_locked(self) -> bool:
-        return self._is_locked
-
-    @is_locked.setter
-    def is_locked(self, value: bool) -> None:
-        if value:
-            self.lock_()
-        else:
-            self.unlock_()
-
-    def _lock_propagate(self, lock_ids=None):
-        """Registers the parent tensordict that handles the lock."""
-        self._is_locked = True
-        is_root = lock_ids is None
-        if is_root:
-            lock_ids = set()
-        self._lock_id = self._lock_id.union(lock_ids)
-        lock_ids = lock_ids.union({id(self)})
-        _locked_tensordicts = []
-        for key in self.keys():
-            if _is_tensor_collection(self.entry_class(key)):
-                dest = self.get(key)
-                dest._lock_propagate(lock_ids)
-                _locked_tensordicts.append(dest)
-        if is_root:
-            self._locked_tensordicts = _locked_tensordicts
-        else:
-            self._locked_tensordicts += _locked_tensordicts
-
-    @as_decorator("is_locked")
-    def lock_(self) -> TensorDictBase:
-        if self.is_locked:
-            return self
-        self._lock_propagate()
-        return self
-
-    lock = _renamed_inplace_method(lock_)
-
-    def _remove_lock(self, lock_id):
-        self._lock_id = self._lock_id - {lock_id}
-        if self._locked_tensordicts:
-            for td in self._locked_tensordicts:
-                td._remove_lock(lock_id)
-
-    @erase_cache
-    def _propagate_unlock(self, lock_ids=None):
-        if lock_ids is not None:
-            self._lock_id.difference_update(lock_ids)
-        else:
-            lock_ids = set()
-        self._is_locked = False
-
-        unlocked_tds = [self]
-        lock_ids.add(id(self))
-        for key in self.keys():
-            if _is_tensor_collection(self.entry_class(key)):
-                dest = self.get(key)
-                unlocked_tds.extend(dest._propagate_unlock(lock_ids))
-        self._locked_tensordicts = []
-
-        self._is_shared = False
-        self._is_memmap = False
-        return unlocked_tds
-
-    @as_decorator("is_locked")
-    def unlock_(self) -> TensorDictBase:
-        unlock_tds = self._propagate_unlock()
-        for td in unlock_tds:
-            if len(td._lock_id):
-                self.lock_()
-                raise RuntimeError(
-                    "Cannot unlock a tensordict that is part of a locked graph. "
-                    "Unlock the root tensordict first. If the tensordict is part of multiple graphs, "
-                    "group the graphs under a common tensordict an unlock this root. "
-                )
-        return self
-
-    unlock = _renamed_inplace_method(unlock_)
-
-    def __del__(self):
-        for td in self._locked_tensordicts:
-            td._remove_lock(id(self))
-
-    def is_floating_point(self):
-        for item in self.values(include_nested=True, leaves_only=True):
-            if not item.is_floating_point():
-                return False
-        else:
-            return True
-
-    def double(self):
-        r"""Casts all tensors to ``torch.bool``."""
-        return self.apply(lambda x: x.double())
-
-    def float(self):
-        r"""Casts all tensors to ``torch.float``."""
-        return self.apply(lambda x: x.float())
-
-    def int(self):
-        r"""Casts all tensors to ``torch.int``."""
-        return self.apply(lambda x: x.int())
-
-    def bool(self):
-        r"""Casts all tensors to ``torch.bool``."""
-        return self.apply(lambda x: x.bool())
-
-    def half(self):
-        r"""Casts all tensors to ``torch.half``."""
-        return self.apply(lambda x: x.half())
-
-    def bfloat16(self):
-        r"""Casts all tensors to ``torch.bfloat16``."""
-        return self.apply(lambda x: x.bfloat16())
-
-    def type(self, dst_type):
-        r"""Casts all tensors to :attr:`dst_type`.
-
-        Args:
-            dst_type (type or string): the desired type
-
-        """
-        return self.apply(lambda x: x.type(dst_type))
-
-
-_ACCEPTED_CLASSES = [
-    Tensor,
-    MemmapTensor,
-    TensorDictBase,
-]
-if _has_torchrec:
-    _ACCEPTED_CLASSES += [KeyedJaggedTensor]
-_ACCEPTED_CLASSES = tuple(_ACCEPTED_CLASSES)
-
-
-class TensorDict(TensorDictBase):
-    """A batched dictionary of tensors.
-
-    TensorDict is a tensor container where all tensors are stored in a
-    key-value pair fashion and where each element shares at least the
-    following features:
-    - memory location (shared, memory-mapped array, ...);
-    - batch size (i.e. n^th first dimensions).
-
-    Additionally, if the tensordict has a specified device, then each element
-    must share that device.
-
-    TensorDict instances support many regular tensor operations as long as
-    they are dtype-independent (as a TensorDict instance can contain tensors
-    of many different dtypes). Those operations include (but are not limited
-    to):
-
-    - operations on shape: when a shape operation is called (indexing,
-      reshape, view, expand, transpose, permute,
-      unsqueeze, squeeze, masking etc), the operations is done as if it
-      was done on a tensor of the same shape as the batch size then
-      expended to the right, e.g.:
-
-        >>> td = TensorDict({'a': torch.zeros(3,4,5)}, batch_size=[3, 4])
-        >>> # returns a TensorDict of batch size [3, 4, 1]
-        >>> td_unsqueeze = td.unsqueeze(-1)
-        >>> # returns a TensorDict of batch size [12]
-        >>> td_view = td.view(-1)
-        >>> # returns a tensor of batch size [12, 4]
-        >>> a_view = td.view(-1).get("a")
-
-    - casting operations: a TensorDict can be cast on a different device
-      or another TensorDict type using
-
-        >>> td_cpu = td.to("cpu")
-        >>> td_savec = td.to(SavedTensorDict)  # TensorDict saved on disk
-        >>> dictionary = td.to_dict()
-
-      A call of the `.to()` method with a dtype will return an error.
-
-    - Cloning, contiguous
-
-    - Reading: `td.get(key)`, `td.get_at(key, index)`
-
-    - Content modification: :obj:`td.set(key, value)`, :obj:`td.set_(key, value)`,
-      :obj:`td.update(td_or_dict)`, :obj:`td.update_(td_or_dict)`, :obj:`td.fill_(key,
-      value)`, :obj:`td.rename_key_(old_name, new_name)`, etc.
-
-    - Operations on multiple tensordicts: `torch.cat(tensordict_list, dim)`,
-      `torch.stack(tensordict_list, dim)`, `td1 == td2` etc.
-
-    Args:
-        source (TensorDict or dictionary): a data source. If empty, the
-            tensordict can be populated subsequently.
-        batch_size (iterable of int, optional): a batch size for the
-            tensordict. The batch size is immutable and can only be modified
-            by calling operations that create a new TensorDict. Unless the
-            source is another TensorDict, the batch_size argument must be
-            provided as it won't be inferred from the data.
-        device (torch.device or compatible type, optional): a device for the
-            TensorDict.
-        names (lsit of str, optional): the names of the dimensions of the
-            tensordict. If provided, its length must match the one of the
-            ``batch_size``. Defaults to ``None`` (no dimension name, or ``None``
-            for every dimension).
-
-    Examples:
-        >>> import torch
-        >>> from tensordict import TensorDict
-        >>> source = {'random': torch.randn(3, 4),
-        ...     'zeros': torch.zeros(3, 4, 5)}
-        >>> batch_size = [3]
-        >>> td = TensorDict(source, batch_size)
-        >>> print(td.shape)  # equivalent to td.batch_size
-        torch.Size([3])
-        >>> td_unqueeze = td.unsqueeze(-1)
-        >>> print(td_unqueeze.get("zeros").shape)
-        torch.Size([3, 1, 4, 5])
-        >>> print(td_unqueeze[0].shape)
-        torch.Size([1])
-        >>> print(td_unqueeze.view(-1).shape)
-        torch.Size([3])
-        >>> print((td.clone()==td).all())
-        True
-
-    """
-
-    __slots__ = (
-        "_tensordict",
-        "_batch_size",
-        "_is_shared",
-        "_is_memmap",
-        "_device",
-        "_is_locked",
-        "_td_dim_names",
-        "_lock_id",
-        "_locked_tensordicts",
-        "_cache",
-        "_last_op",
-        "__last_op_queue",
-    )
-
-    def __new__(cls, *args: Any, **kwargs: Any) -> TensorDict:
-        cls._is_shared = False
-        cls._is_memmap = False
-        cls._td_dim_names = None
-        return super().__new__(cls, *args, _safe=True, _lazy=False, **kwargs)
-
-    def __init__(
-        self,
-        source: TensorDictBase | dict[str, CompatibleType],
-        batch_size: Sequence[int] | torch.Size | int | None = None,
-        device: DeviceType | None = None,
-        names: Sequence[str] | None = None,
-        _run_checks: bool = True,
-        _is_shared: bool | None = False,
-        _is_memmap: bool | None = False,
-    ) -> None:
-        self._lock_id = set()
-        self._locked_tensordicts = []
-
-        self._is_shared = _is_shared
-        self._is_memmap = _is_memmap
-        if device is not None and isinstance(device, (int, str)):
-            device = torch.device(device)
-        self._device = device
-
-        if not _run_checks:
-            _tensordict: dict = _StringOnlyDict()
-            self._batch_size = batch_size
-            for key, value in source.items():
-                if isinstance(value, dict):
-                    value = TensorDict(
-                        value,
-                        batch_size=self._batch_size,
-                        device=self._device,
-                        _run_checks=_run_checks,
-                        _is_shared=_is_shared,
-                        _is_memmap=_is_memmap,
-                    )
-                _tensordict[key] = value
-            self._tensordict = _tensordict
-            self._td_dim_names = names
-        else:
-            self._tensordict = _StringOnlyDict()
-            if not isinstance(source, (TensorDictBase, dict)):
-                raise ValueError(
-                    "A TensorDict source is expected to be a TensorDictBase "
-                    f"sub-type or a dictionary, found type(source)={type(source)}."
-                )
-            self._batch_size = self._parse_batch_size(source, batch_size)
-            self.names = names
-
-            if source is not None:
-                for key, value in source.items():
-                    self.set(key, value)
-
-    # def __hash__(self):
-    #     return hash((self._tensordict, self._batch_size, self._device))
-
-    @classmethod
-    def from_dict(cls, input_dict, batch_size=None, device=None, batch_dims=None):
-        """Returns a TensorDict created from a dictionary or another :class:`TensorDict`.
-
-        If ``batch_size`` is not specified, returns the maximum batch size possible.
-
-        This function works on nested dictionaries too, or can be used to determine the
-        batch-size of a nested tensordict.
-
-        Args:
-            input_dict (dictionary, optional): a dictionary to use as a data source
-                (nested keys compatible).
-            batch_size (iterable of int, optional): a batch size for the tensordict.
-            device (torch.device or compatible type, optional): a device for the TensorDict.
-            batch_dims (int, optional): the ``batch_dims`` (ie number of leading dimensions
-                to be considered for ``batch_size``). Exclusinve with ``batch_size``.
-                Note that this is the __maximum__ number of batch dims of the tensordict,
-                a smaller number is tolerated.
-
-        Examples:
-            >>> input_dict = {"a": torch.randn(3, 4), "b": torch.randn(3)}
-            >>> print(TensorDict.from_dict(input_dict))
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([3]),
-                device=None,
-                is_shared=False)
-            >>> # nested dict: the nested TensorDict can have a different batch-size
-            >>> # as long as its leading dims match.
-            >>> input_dict = {"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}
-            >>> print(TensorDict.from_dict(input_dict))
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([3, 4]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([3]),
-                device=None,
-                is_shared=False)
-            >>> # we can also use this to work out the batch sie of a tensordict
-            >>> input_td = TensorDict({"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}, [])
-            >>> print(TensorDict.from_dict(input_td))
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([3, 4]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([3]),
-                device=None,
-                is_shared=False)
-
-        """
-        if batch_dims is not None and batch_size is not None:
-            raise ValueError(
-                "Cannot pass both batch_size and batch_dims to `from_dict`."
-            )
-
-        batch_size_set = [] if batch_size is None else batch_size
-        for key, value in list(input_dict.items()):
-            if isinstance(value, (dict,)):
-                # we don't know if another tensor of smaller size is coming
-                # so we can't be sure that the batch-size will still be valid later
-                input_dict[key] = TensorDict.from_dict(
-                    value, batch_size=[], device=device, batch_dims=None
-                )
-        # _run_checks=False breaks because a tensor may have the same batch-size as the tensordict
-        out = cls(
-            input_dict,
-            batch_size=batch_size_set,
-            device=device,
-        )
-        if batch_size is None:
-            _set_max_batch_size(out, batch_dims)
-        else:
-            out.batch_size = batch_size
-        return out
-
-    @staticmethod
-    def _parse_batch_size(
-        source: TensorDictBase | dict,
-        batch_size: Sequence[int] | torch.Size | int | None = None,
-    ) -> torch.Size:
-        try:
-            return torch.Size(batch_size)
-        except Exception as err:
-            if isinstance(batch_size, Number):
-                return torch.Size([batch_size])
-            elif isinstance(source, TensorDictBase):
-                return source.batch_size
-            raise ValueError(
-                "batch size was not specified when creating the TensorDict "
-                "instance and it could not be retrieved from source."
-            ) from err
-
-    @property
-    def batch_dims(self) -> int:
-        return len(self.batch_size)
-
-    @batch_dims.setter
-    def batch_dims(self, value: int) -> None:
-        raise RuntimeError(
-            f"Setting batch dims on {self.__class__.__name__} instances is "
-            f"not allowed."
-        )
-
-    def _has_names(self):
-        return self._td_dim_names is not None
-
-    def _erase_names(self):
-        self._td_dim_names = None
-
-    @property
-    def names(self):
-        names = self._td_dim_names
-        if names is None:
-            return [None for _ in range(self.batch_dims)]
-        return names
-
-    @names.setter
-    def names(self, value):
-        # we don't run checks on types for efficiency purposes
-        if value is None:
-            self._erase_names()
-            return
-        num_none = sum(v is None for v in value)
-        if num_none:
-            num_none -= 1
-        if len(set(value)) != len(value) - num_none:
-            raise ValueError(f"Some dimension names are non-unique: {value}.")
-        if len(value) != self.batch_dims:
-            raise ValueError(
-                "the length of the dimension names must equate the tensordict batch_dims attribute. "
-                f"Got {value} for batch_dims {self.batch_dims}."
-            )
-        self._rename_subtds(value)
-        self._td_dim_names = list(value)
-
-    def _rename_subtds(self, names):
-        if names is None:
-            for item in self._tensordict.values():
-                if _is_tensor_collection(item.__class__):
-                    item._erase_names()
-            return
-        for item in self._tensordict.values():
-            if _is_tensor_collection(item.__class__):
-                item_names = item.names
-                td_names = list(names) + item_names[len(names) :]
-                item.rename_(*td_names)
-
-    @property
-    def device(self) -> torch.device | None:
-        """Device of the tensordict.
-
-        Returns `None` if device hasn't been provided in the constructor or set via `tensordict.to(device)`.
-
-        """
-        return self._device
-
-    @device.setter
-    def device(self, value: DeviceType) -> None:
-        raise RuntimeError(
-            "device cannot be set using tensordict.device = device, "
-            "because device cannot be updated in-place. To update device, use "
-            "tensordict.to(new_device), which will return a new tensordict "
-            "on the new device."
-        )
-
-    @property
-    def batch_size(self) -> torch.Size:
-        return self._batch_size
-
-    @batch_size.setter
-    def batch_size(self, new_size: torch.Size) -> None:
-        self._batch_size_setter(new_size)
-
-    def _change_batch_size(self, new_size: torch.Size) -> None:
-        if not hasattr(self, "_orig_batch_size"):
-            self._orig_batch_size = self.batch_size
-        elif self._orig_batch_size == new_size:
-            del self._orig_batch_size
-        self._batch_size = new_size
-
-    # Checks
-    def _check_is_shared(self) -> bool:
-        share_list = [_is_shared(value) for value in self.values()]
-        if any(share_list) and not all(share_list):
-            shared_str = ", ".join(
-                [f"{key}: {_is_shared(value)}" for key, value in self.items()]
-            )
-            raise RuntimeError(
-                f"tensors must be either all shared or not, but mixed "
-                f"features is not allowed. "
-                f"Found: {shared_str}"
-            )
-        return all(share_list) and len(share_list) > 0
-
-    def _check_is_memmap(self) -> bool:
-        memmap_list = [is_memmap(self.entry_class(key)) for key in self.keys()]
-        if any(memmap_list) and not all(memmap_list):
-            memmap_str = ", ".join(
-                [f"{key}: {is_memmap(self.entry_class(key))}" for key in self.keys()]
-            )
-            raise RuntimeError(
-                f"tensors must be either all MemmapTensor or not, but mixed "
-                f"features is not allowed. "
-                f"Found: {memmap_str}"
-            )
-        return all(memmap_list) and len(memmap_list) > 0
-
-    def _check_device(self) -> None:
-        devices = {value.device for value in self.values()}
-        if self.device is not None and len(devices) >= 1 and devices != {self.device}:
-            raise RuntimeError(
-                f"TensorDict.device is {self._device}, but elements have "
-                f"device values {devices}. If TensorDict.device is set then "
-                "all elements must share that device."
-            )
-
-    # def _index_tensordict(self, idx: IndexType) -> TensorDictBase:
-    #     names = self._get_names_idx(idx)
-    #     self_copy = copy(self)
-    #     # self_copy = self.clone(False)
-    #     self_copy._tensordict = {
-    #         key: _get_item(item, idx) for key, item in self.items()
-    #     }
-    #     self_copy._batch_size = _getitem_batch_size(self_copy.batch_size, idx)
-    #     self_copy._device = self.device
-    #     self_copy.names = names
-    #     return self_copy
-
-    def pin_memory(self) -> TensorDictBase:
-        def pin_mem(tensor):
-            return tensor.pin_memory()
-
-        return self.apply(pin_mem)
-
-    def expand(self, *shape: int) -> TensorDictBase:
-        """Expands every tensor with `(*shape, *tensor.shape)` and returns the same tensordict with new tensors with expanded shapes.
-
-        Supports iterables to specify the shape.
-
-        """
-        d = {}
-        tensordict_dims = self.batch_dims
-
-        if len(shape) == 1 and isinstance(shape[0], Sequence):
-            shape = tuple(shape[0])
-
-        # new shape dim check
-        if len(shape) < len(self.shape):
-            raise RuntimeError(
-                "the number of sizes provided ({shape_dim}) must be greater or equal to the number of "
-                "dimensions in the TensorDict ({tensordict_dim})".format(
-                    shape_dim=len(shape), tensordict_dim=tensordict_dims
-                )
-            )
-
-        # new shape compatability check
-        for old_dim, new_dim in zip(self.batch_size, shape[-tensordict_dims:]):
-            if old_dim != 1 and new_dim != old_dim:
-                raise RuntimeError(
-                    "Incompatible expanded shape: The expanded shape length at non-singleton dimension should be same "
-                    "as the original length. target_shape = {new_shape}, existing_shape = {old_shape}".format(
-                        new_shape=shape, old_shape=self.batch_size
-                    )
-                )
-
-        for key, value in self.items():
-            tensor_dims = len(value.shape)
-            last_n_dims = tensor_dims - tensordict_dims
-            if last_n_dims > 0:
-                d[key] = value.expand(*shape, *value.shape[-last_n_dims:])
-            else:
-                d[key] = value.expand(*shape)
-        out = TensorDict(
-            source=d,
-            batch_size=torch.Size(shape),
-            device=self.device,
-            _run_checks=False,
-        )
-        if self._td_dim_names is not None:
-            out.refine_names(..., *self.names)
-        return out
-
-    def _set_str(
-        self,
-        key: NestedKey,
-        value: dict[str, CompatibleType] | CompatibleType,
-        *,
-        inplace: bool,
-        validated: bool,
-    ) -> TensorDictBase:
-        best_attempt = inplace is BEST_ATTEMPT_INPLACE
-        inplace = self._convert_inplace(inplace, key)
-        if not validated:
-            value = self._validate_value(value, check_shape=True)
-        if not inplace:
-            if self.is_locked:
-                raise RuntimeError(TensorDictBase.LOCK_ERROR)
-            self._tensordict[key] = value
-        else:
-            try:
-                dest = self._get_str(key, default=NO_DEFAULT)
-                if best_attempt and _is_tensor_collection(dest.__class__):
-                    dest.update(value, inplace=True)
-                else:
-                    dest.copy_(value)
-            except KeyError as err:
-                raise err
-            except Exception as err:
-                raise ValueError(
-                    f"Failed to update '{key}' in tensordict {self}"
-                ) from err
-        return self
-
-    def _set_tuple(
-        self,
-        key: NestedKey,
-        value: dict[str, CompatibleType] | CompatibleType,
-        *,
-        inplace: bool,
-        validated: bool,
-    ) -> TensorDictBase:
-        if len(key) == 1:
-            return self._set_str(key[0], value, inplace=inplace, validated=validated)
-        td = self._get_str(key[0], None)
-        if td is None:
-            self._create_nested_str(key[0])
-            td = self._get_str(key[0], NO_DEFAULT)
-            inplace = False
-        elif not _is_tensor_collection(td.__class__):
-            raise RuntimeError(
-                f"The entry {key[0]} is already present in " f"tensordict {self}."
-            )
-        td._set_tuple(key[1:], value, inplace=inplace, validated=validated)
-        return self
-
-    def _set_at_str(self, key, value, idx, *, validated):
-        if not validated:
-            value = self._validate_value(value, check_shape=False)
-            validated = True
-        tensor_in = self._get_str(key, NO_DEFAULT)
-
-        if isinstance(idx, tuple) and len(idx) and isinstance(idx[0], tuple):
-            warn(
-                "Multiple indexing can lead to unexpected behaviours when "
-                "setting items, for instance `td[idx1][idx2] = other` may "
-                "not write to the desired location if idx1 is a list/tensor."
-            )
-            tensor_in = _sub_index(tensor_in, idx)
-            tensor_in.copy_(value)
-        else:
-            _set_item(tensor_in, idx, value, validated=validated)
-
-        return self
-
-    def _set_at_tuple(self, key, value, idx, *, validated):
-        if len(key) == 1:
-            return self._set_at_str(key[0], value, idx, validated=validated)
-        if key[0] not in self.keys():
-            # this won't work
-            raise KeyError(f"key {key} not found in set_at_ with tensordict {self}.")
-        else:
-            td = self._get_str(key[0], NO_DEFAULT)
-        td._set_at_tuple(key[1:], value, idx, validated=validated)
-        return self
-
-    @lock_blocked
-    def del_(self, key: NestedKey) -> TensorDictBase:
-        key = _unravel_key_to_tuple(key)
-        if len(key) > 1:
-            td, subkey = _get_leaf_tensordict(self, key)
-            td.del_(subkey)
-            return self
-
-        del self._tensordict[key[0]]
-        return self
-
-    @lock_blocked
-    def rename_key_(
-        self, old_key: str, new_key: str, safe: bool = False
-    ) -> TensorDictBase:
-        # these checks are not perfect, tuples that are not tuples of strings or empty
-        # tuples could go through but (1) it will raise an error anyway and (2)
-        # those checks are expensive when repeated often.
-        if not isinstance(old_key, (str, tuple)):
-            raise TypeError(
-                f"Expected old_name to be a string or a tuple of strings but found {type(old_key)}"
-            )
-        if not isinstance(new_key, (str, tuple)):
-            raise TypeError(
-                f"Expected new_name to be a string or a tuple of strings but found {type(new_key)}"
-            )
-        if safe and (new_key in self.keys(include_nested=True)):
-            raise KeyError(f"key {new_key} already present in TensorDict.")
-
-        if isinstance(new_key, str):
-            self._set_str(new_key, self.get(old_key), inplace=False, validated=True)
-        else:
-            self._set_tuple(new_key, self.get(old_key), inplace=False, validated=True)
-        self.del_(old_key)
-        return self
-
-    rename_key = _renamed_inplace_method(rename_key_)
-
-    def _stack_onto_(self, list_item: list[CompatibleType], dim: int) -> TensorDict:
-        # if not isinstance(key, str):
-        #     raise ValueError("_stack_onto_ expects string keys.")
-        for key in self.keys():
-            vals = [item._get_str(key, None) for item in list_item]
-            if all(v is None for v in vals):
-                continue
-            dest = self._get_str(key, NO_DEFAULT)
-            torch.stack(
-                vals,
-                dim=dim,
-                out=dest,
-            )
-        return self
-
-    def entry_class(self, key: NestedKey) -> type:
-        return type(self.get(key))
-
-    def _stack_onto_at_(
-        self,
-        list_item: list[CompatibleType],
-        dim: int,
-        idx: IndexType,
-    ) -> TensorDict:
-        if not isinstance(idx, tuple):
-            idx = (idx,)
-        idx = convert_ellipsis_to_idx(idx, self.batch_size)
-        for key in self.keys():
-            vals = [td._get_str(key, NO_DEFAULT) for td in list_item]
-            if all(v is None for v in vals):
-                continue
-            v = self._get_str(key, NO_DEFAULT)
-            v_idx = v[idx]
-            if v.data_ptr() != v_idx.data_ptr():
-                raise IndexError(
-                    f"Index {idx} is incompatible with stack(..., out=data) as the storages of the indexed tensors differ."
-                )
-            torch.stack(vals, dim=dim, out=v_idx)
-            # raise ValueError(
-            #     f"Cannot stack onto an indexed tensor with index {idx} "
-            #     f"as its storage differs."
-            # )
-        return self
-
-    def _get_str(self, key, default):
-        first_key = key
-        out = self._tensordict.get(first_key, None)
-        if out is None:
-            return self._default_get(first_key, default)
-        return out
-
-    def _get_tuple(self, key, default):
-        first = self._get_str(key[0], default)
-        if len(key) == 1 or first is default:
-            return first
-        try:
-            if isinstance(first, KeyedJaggedTensor):
-                if len(key) != 2:
-                    raise ValueError(f"Got too many keys for a KJT: {key}.")
-                return first[key[1]]
-            else:
-                return first._get_tuple(key[1:], default=default)
-        except AttributeError as err:
-            if "has no attribute" in str(err):
-                raise ValueError(
-                    f"Expected a TensorDictBase instance but got {type(first)} instead"
-                    f" for key '{key[1:]}' in tensordict:\n{self}."
-                )
-
-    def share_memory_(self) -> TensorDictBase:
-        if self.is_memmap():
-            raise RuntimeError(
-                "memmap and shared memory are mutually exclusive features."
-            )
-        if self.device is not None and self.device.type == "cuda":
-            # cuda tensors are shared by default
-            return self
-        for value in self.values():
-            # no need to consider MemmapTensors here as we have checked that this is not a memmap-tensordict
-            if (
-                isinstance(value, Tensor)
-                and value.device.type == "cpu"
-                or _is_tensor_collection(value.__class__)
-            ):
-                value.share_memory_()
-        self._is_shared = True
-        self.lock_()
-        return self
-
-    def detach_(self) -> TensorDictBase:
-        for value in self.values():
-            value.detach_()
-        return self
-
-    def memmap_(
-        self,
-        prefix: str | None = None,
-        copy_existing: bool = False,
-    ) -> TensorDictBase:
-        if prefix is not None:
-            prefix = Path(prefix)
-            if not prefix.exists():
-                os.makedirs(prefix, exist_ok=True)
-            torch.save(
-                {"batch_size": self.batch_size, "device": self.device},
-                prefix / "meta.pt",
-            )
-        if self.is_shared() and self.device.type == "cpu":
-            raise RuntimeError(
-                "memmap and shared memory are mutually exclusive features."
-            )
-        if not self._tensordict.keys():
-            raise Exception(
-                "memmap_() must be called when the TensorDict is (partially) "
-                "populated. Set a tensor first."
-            )
-        for key, value in self.items():
-            if value.requires_grad:
-                raise Exception(
-                    "memmap is not compatible with gradients, one of Tensors has requires_grad equals True"
-                )
-            if _is_tensor_collection(value.__class__):
-                if prefix is not None:
-                    # ensure subdirectory exists
-                    os.makedirs(prefix / key, exist_ok=True)
-                    self._tensordict[key] = value.memmap_(
-                        prefix=prefix / key, copy_existing=copy_existing
-                    )
-                    torch.save(
-                        {"batch_size": value.batch_size, "device": value.device},
-                        prefix / key / "meta.pt",
-                    )
-                else:
-                    self._tensordict[key] = value.memmap_()
-                continue
-            elif isinstance(value, MemmapTensor):
-                if (
-                    # user didn't specify location
-                    prefix is None
-                    # file is already in the correct location
-                    or str(prefix / f"{key}.memmap") == value.filename
-                ):
-                    self._tensordict[key] = value
-                elif copy_existing:
-                    # user did specify location and memmap is in wrong place, so we copy
-                    self._tensordict[key] = MemmapTensor.from_tensor(
-                        value, filename=str(prefix / f"{key}.memmap")
-                    )
-                else:
-                    # memmap in wrong location and copy is disallowed
-                    raise RuntimeError(
-                        "TensorDict already contains MemmapTensors saved to a location "
-                        "incompatible with prefix. Either move the location of the "
-                        "MemmapTensors, or allow automatic copying with "
-                        "copy_existing=True"
-                    )
-            else:
-                self._tensordict[key] = MemmapTensor.from_tensor(
-                    value,
-                    filename=str(prefix / f"{key}.memmap")
-                    if prefix is not None
-                    else None,
-                )
-            if prefix is not None:
-                torch.save(
-                    {
-                        "shape": value.shape,
-                        "device": value.device,
-                        "dtype": value.dtype,
-                    },
-                    prefix / f"{key}.meta.pt",
-                )
-        self._is_memmap = True
-        self.lock_()
-        return self
-
-    @classmethod
-    def load_memmap(cls, prefix: str) -> TensorDictBase:
-        prefix = Path(prefix)
-        metadata = torch.load(prefix / "meta.pt")
-        out = cls({}, batch_size=metadata["batch_size"], device=metadata["device"])
-
-        for path in prefix.glob("**/*meta.pt"):
-            key = path.parts[len(prefix.parts) :]
-            if path.name == "meta.pt":
-                if path == prefix / "meta.pt":
-                    # skip prefix / "meta.pt" as we've already read it
-                    continue
-                key = key[:-1]  # drop "meta.pt" from key
-                metadata = torch.load(path)
-                if key in out.keys(include_nested=True):
-                    out[key].batch_size = metadata["batch_size"]
-                    device = metadata["device"]
-                    if device is not None:
-                        out[key] = out[key].to(device)
-                else:
-                    out[key] = cls(
-                        {}, batch_size=metadata["batch_size"], device=metadata["device"]
-                    )
-            else:
-                leaf, *_ = key[-1].rsplit(".", 2)  # remove .meta.pt suffix
-                key = (*key[:-1], leaf)
-                metadata = torch.load(path)
-                out[key] = MemmapTensor(
-                    *metadata["shape"],
-                    device=metadata["device"],
-                    dtype=metadata["dtype"],
-                    filename=str(path.parent / f"{leaf}.memmap"),
-                )
-
-        return out
-
-    def to(self, dest: DeviceType | torch.Size | type, **kwargs: Any) -> TensorDictBase:
-        if isinstance(dest, type) and issubclass(dest, TensorDictBase):
-            if isinstance(self, dest):
-                return self
-            td = dest(source=self, **kwargs)
-            if self._td_dim_names is not None:
-                td.names = self._td_dim_names
-            return td
-        elif isinstance(dest, (torch.device, str, int)):
-            # must be device
-            dest = torch.device(dest)
-            if self.device is not None and dest == self.device:
-                return self
-
-            def to(tensor):
-                return tensor.to(dest, **kwargs)
-
-            return self.apply(to, device=dest)
-        elif isinstance(dest, torch.Size):
-            self.batch_size = dest
-            return self
-        elif dest is None:
-            return self
-        else:
-            raise NotImplementedError(
-                f"dest must be a string, torch.device or a TensorDict "
-                f"instance, {dest} not allowed"
-            )
-
-    def masked_fill_(self, mask: Tensor, value: float | int | bool) -> TensorDictBase:
-        for item in self.values():
-            mask_expand = expand_as_right(mask, item)
-            item.masked_fill_(mask_expand, value)
-        return self
-
-    def masked_fill(self, mask: Tensor, value: float | bool) -> TensorDictBase:
-        td_copy = self.clone()
-        return td_copy.masked_fill_(mask, value)
-
-    def is_contiguous(self) -> bool:
-        return all([value.is_contiguous() for _, value in self.items()])
-
-    def clone(self, recurse: bool = True) -> TensorDictBase:
-        return TensorDict(
-            source={key: _clone_value(value, recurse) for key, value in self.items()},
-            batch_size=self.batch_size,
-            device=self.device,
-            names=copy(self._td_dim_names),
-            _run_checks=False,
-            _is_shared=self.is_shared() if not recurse else False,
-            _is_memmap=self.is_memmap() if not recurse else False,
-        )
-
-    def contiguous(self) -> TensorDictBase:
-        if not self.is_contiguous():
-            return self.clone()
-        return self
-
-    def select(
-        self, *keys: NestedKey, inplace: bool = False, strict: bool = True
-    ) -> TensorDictBase:
-        source = {}
-        if len(keys):
-            keys_to_select = None
-            for key in keys:
-                if isinstance(key, str):
-                    subkey = []
-                else:
-                    key, subkey = key[0], key[1:]
-                try:
-                    source[key] = self.get(key)
-                    if len(subkey):
-                        if keys_to_select is None:
-                            # delay creation of defaultdict
-                            keys_to_select = defaultdict(list)
-                        keys_to_select[key].append(subkey)
-                except KeyError as err:
-                    if not strict:
-                        continue
-                    else:
-                        raise KeyError(f"select failed to get key {key}") from err
-            if keys_to_select is not None:
-                for key, val in keys_to_select.items():
-                    source[key] = source[key].select(
-                        *val, strict=strict, inplace=inplace
-                    )
-
-        out = TensorDict(
-            device=self.device,
-            batch_size=self.batch_size,
-            source=source,
-            # names=self.names if self._has_names() else None,
-            names=self._td_dim_names,
-            _run_checks=False,
-            _is_memmap=self._is_memmap,
-            _is_shared=self._is_shared,
-        )
-        if inplace:
-            self._tensordict = out._tensordict
-            return self
-        return out
-
-    def keys(
-        self, include_nested: bool = False, leaves_only: bool = False
-    ) -> _TensorDictKeysView:
-        if not include_nested and not leaves_only:
-            return self._tensordict.keys()
-        else:
-            return self._nested_keys(
-                include_nested=include_nested, leaves_only=leaves_only
-            )
-
-    # @cache  # noqa: B019
-    def _nested_keys(
-        self, include_nested: bool = False, leaves_only: bool = False
-    ) -> _TensorDictKeysView:
-        return _TensorDictKeysView(
-            self, include_nested=include_nested, leaves_only=leaves_only
-        )
-
-    def __getstate__(self):
-        return {
-            slot: getattr(self, slot)
-            for slot in self.__slots__
-            if slot not in ("_last_op", "_cache", "__last_op_queue")
-        }
-
-    def __setstate__(self, state):
-        for slot, value in state.items():
-            setattr(self, slot, value)
-        self._cache = None
-        self.__last_op_queue = None
-        self._last_op = None
-
-    # some custom methods for efficiency
-    def items(
-        self, include_nested: bool = False, leaves_only: bool = False
-    ) -> Iterator[tuple[str, CompatibleType]]:
-        if not include_nested and not leaves_only:
-            return self._tensordict.items()
-        else:
-            return super().items(include_nested=include_nested, leaves_only=leaves_only)
-
-    def values(
-        self, include_nested: bool = False, leaves_only: bool = False
-    ) -> Iterator[tuple[str, CompatibleType]]:
-        if not include_nested and not leaves_only:
-            return self._tensordict.values()
-        else:
-            return super().values(
-                include_nested=include_nested, leaves_only=leaves_only
-            )
-
-
-class _ErrorInteceptor:
-    """Context manager for catching errors and modifying message.
-
-    Intended for use with stacking / concatenation operations applied to TensorDicts.
-
-    """
-
-    DEFAULT_EXC_MSG = "Expected all tensors to be on the same device"
-
-    def __init__(
-        self,
-        key: NestedKey,
-        prefix: str,
-        exc_msg: str | None = None,
-        exc_type: type[Exception] | None = None,
-    ) -> None:
-        self.exc_type = exc_type if exc_type is not None else RuntimeError
-        self.exc_msg = exc_msg if exc_msg is not None else self.DEFAULT_EXC_MSG
-        self.prefix = prefix
-        self.key = key
-
-    def _add_key_to_error_msg(self, msg: str) -> str:
-        if msg.startswith(self.prefix):
-            return f'{self.prefix} "{self.key}" /{msg[len(self.prefix):]}'
-        return f'{self.prefix} "{self.key}". {msg}'
-
-    def __enter__(self):
-        pass
-
-    def __exit__(self, exc_type, exc_value, _):
-        if exc_type is self.exc_type and (
-            self.exc_msg is None or self.exc_msg in str(exc_value)
-        ):
-            exc_value.args = (self._add_key_to_error_msg(str(exc_value)),)
-
-
-def _nested_keys_to_dict(keys: Iterator[NestedKey]) -> dict[str, Any]:
-    nested_keys = {}
-    for key in keys:
-        if isinstance(key, str):
-            nested_keys.setdefault(key, {})
-        else:
-            d = nested_keys
-            for subkey in key:
-                d = d.setdefault(subkey, {})
-    return nested_keys
-
-
-def _dict_to_nested_keys(
-    nested_keys: dict[NestedKey, NestedKey], prefix: tuple[str, ...] = ()
-) -> tuple[str, ...]:
-    for key, subkeys in nested_keys.items():
-        if subkeys:
-            yield from _dict_to_nested_keys(subkeys, prefix=(*prefix, key))
-        elif prefix:
-            yield (*prefix, key)
-        else:
-            yield key
-
-
-def _default_hook(td: TensorDictBase, key: tuple[str, ...]) -> None:
-    """Used to populate a tensordict.
-
-    For example, ``td.set(("a", "b"))`` may require to create ``"a"``.
-
-    """
-    out = td.get(key[0], None)
-    if out is None:
-        td._create_nested_str(key[0])
-        out = td._get_str(key[0], None)
-    return out
-
-
-def _get_leaf_tensordict(
-    tensordict: TensorDictBase, key: tuple[str, ...], hook: Callable = None
-) -> tuple[TensorDictBase, str]:
-    # utility function for traversing nested tensordicts
-    # hook should return the default value for tensordit.get(key)
-    while len(key) > 1:
-        if hook is not None:
-            tensordict = hook(tensordict, key)
-        else:
-            tensordict = tensordict.get(key[0])
-        key = key[1:]
-    return tensordict, key[0]
-
-
-def implements_for_td(torch_function: Callable) -> Callable[[Callable], Callable]:
-    """Register a torch function override for TensorDict."""
-
-    @functools.wraps(torch_function)
-    def decorator(func: Callable) -> Callable:
-        TD_HANDLED_FUNCTIONS[torch_function] = func
-        return func
-
-    return decorator
-
-
-def implements_for_lazy_td(torch_function: Callable) -> Callable[[Callable], Callable]:
-    """Register a torch function override for TensorDict."""
-
-    @functools.wraps(torch_function)
-    def decorator(func: Callable) -> Callable:
-        LAZY_TD_HANDLED_FUNCTIONS[torch_function] = func
-        return func
-
-    return decorator
-
-
-# @implements_for_td(torch.testing.assert_allclose) TODO
-def assert_allclose_td(
-    actual: TensorDictBase,
-    expected: TensorDictBase,
-    rtol: float | None = None,
-    atol: float | None = None,
-    equal_nan: bool = True,
-    msg: str = "",
-) -> bool:
-    """Compares two tensordicts and raise an exception if their content does not match exactly."""
-    if not _is_tensor_collection(actual.__class__) or not _is_tensor_collection(
-        expected.__class__
-    ):
-        raise TypeError("assert_allclose inputs must be of TensorDict type")
-    if isinstance(actual, LazyStackedTensorDict) and isinstance(
-        expected, LazyStackedTensorDict
-    ):
-        for sub_actual, sub_expected in zip(actual.tensordicts, expected.tensordicts):
-            assert_allclose_td(sub_actual, sub_expected, rtol=rtol, atol=atol)
-        return True
-
-    set1 = set(actual.keys())
-    set2 = set(expected.keys())
-    if not (len(set1.difference(set2)) == 0 and len(set2) == len(set1)):
-        raise KeyError(
-            "actual and expected tensordict keys mismatch, "
-            f"keys {(set1 - set2).union(set2 - set1)} appear in one but not "
-            f"the other."
-        )
-    keys = sorted(actual.keys(), key=str)
-    for key in keys:
-        input1 = actual.get(key)
-        input2 = expected.get(key)
-        if _is_tensor_collection(input1.__class__):
-            assert_allclose_td(input1, input2, rtol=rtol, atol=atol)
-            continue
-
-        mse = (input1.to(torch.float) - input2.to(torch.float)).pow(2).sum()
-        mse = mse.div(input1.numel()).sqrt().item()
-
-        default_msg = f"key {key} does not match, got mse = {mse:4.4f}"
-        msg = "\t".join([default_msg, msg]) if len(msg) else default_msg
-        if isinstance(input1, MemmapTensor):
-            input1 = input1._tensor
-        if isinstance(input2, MemmapTensor):
-            input2 = input2._tensor
-        torch.testing.assert_close(
-            input1, input2, rtol=rtol, atol=atol, equal_nan=equal_nan, msg=msg
-        )
-    return True
-
-
-@implements_for_td(torch.unbind)
-def _unbind(
-    td: TensorDictBase, *args: Any, **kwargs: Any
-) -> tuple[TensorDictBase, ...]:
-    return td.unbind(*args, **kwargs)
-
-
-@implements_for_td(torch.gather)
-def _gather(
-    input: TensorDictBase,
-    dim: int,
-    index: Tensor,
-    *,
-    sparse_grad: bool = False,
-    out: TensorDictBase | None = None,
-) -> TensorDictBase:
-    if sparse_grad:
-        raise NotImplementedError(
-            "sparse_grad=True not implemented for torch.gather(tensordict, ...)"
-        )
-    # the index must have as many dims as the tensordict
-    if not len(index):
-        raise RuntimeError("Cannot use torch.gather with an empty index")
-    dim_orig = dim
-    if dim < 0:
-        dim = input.batch_dims + dim
-    if dim > input.batch_dims - 1 or dim < 0:
-        raise RuntimeError(
-            f"Cannot gather tensordict with shape {input.shape} along dim {dim_orig}."
-        )
-
-    def _gather_tensor(tensor, dest=None):
-        index_expand = index
-        while index_expand.ndim < tensor.ndim:
-            index_expand = index_expand.unsqueeze(-1)
-        target_shape = list(tensor.shape)
-        target_shape[dim] = index_expand.shape[dim]
-        index_expand = index_expand.expand(target_shape)
-        out = torch.gather(tensor, dim, index_expand, out=dest)
-        return out
-
-    if out is None:
-        names = input.names if input._has_names() else None
-
-        return TensorDict(
-            {key: _gather_tensor(value) for key, value in input.items()},
-            batch_size=index.shape,
-            names=names,
-        )
-    TensorDict(
-        {key: _gather_tensor(value, out[key]) for key, value in input.items()},
-        batch_size=index.shape,
-    )
-    return out
-
-
-@implements_for_td(torch.full_like)
-def _full_like(td: TensorDictBase, fill_value: float, **kwargs: Any) -> TensorDictBase:
-    td_clone = td.clone()
-    for key in td_clone.keys():
-        td_clone.fill_(key, fill_value)
-    if "dtype" in kwargs:
-        raise ValueError("Cannot pass dtype to full_like with TensorDict")
-    if "device" in kwargs:
-        td_clone = td_clone.to(kwargs.pop("device"))
-    if len(kwargs):
-        raise RuntimeError(
-            f"keyword arguments {list(kwargs.keys())} are not "
-            f"supported with full_like with TensorDict"
-        )
-    return td_clone
-
-
-@implements_for_td(torch.zeros_like)
-def _zeros_like(td: TensorDictBase, **kwargs: Any) -> TensorDictBase:
-    td_clone = td.apply(torch.zeros_like)
-    if "dtype" in kwargs:
-        raise ValueError("Cannot pass dtype to full_like with TensorDict")
-    if "device" in kwargs:
-        td_clone = td_clone.to(kwargs.pop("device"))
-    if len(kwargs):
-        raise RuntimeError(
-            f"keyword arguments {list(kwargs.keys())} are not "
-            f"supported with full_like with TensorDict"
-        )
-    return td_clone
-
-
-@implements_for_td(torch.ones_like)
-def _ones_like(td: TensorDictBase, **kwargs: Any) -> TensorDictBase:
-    td_clone = td.clone()
-    for key in td_clone.keys():
-        td_clone.fill_(key, 1.0)
-    if "device" in kwargs:
-        td_clone = td_clone.to(kwargs.pop("device"))
-    if len(kwargs):
-        raise RuntimeError(
-            f"keyword arguments {list(kwargs.keys())} are not "
-            f"supported with full_like with TensorDict"
-        )
-    return td_clone
-
-
-@implements_for_td(torch.empty_like)
-def _empty_like(td: TensorDictBase, *args, **kwargs) -> TensorDictBase:
-    try:
-        tdclone = td.clone()
-    except Exception as err:
-        raise RuntimeError(
-            "The tensordict passed to torch.empty_like cannot be "
-            "cloned, preventing empty_like to be called. "
-            "Consider calling tensordict.to_tensordict() first."
-        ) from err
-    return tdclone.apply_(lambda x: torch.empty_like(x, *args, **kwargs))
-
-
-@implements_for_td(torch.clone)
-def _clone(td: TensorDictBase, *args: Any, **kwargs: Any) -> TensorDictBase:
-    return td.clone(*args, **kwargs)
-
-
-@implements_for_td(torch.squeeze)
-def _squeeze(td: TensorDictBase, *args: Any, **kwargs: Any) -> TensorDictBase:
-    return td.squeeze(*args, **kwargs)
-
-
-@implements_for_td(torch.unsqueeze)
-def _unsqueeze(td: TensorDictBase, *args: Any, **kwargs: Any) -> TensorDictBase:
-    return td.unsqueeze(*args, **kwargs)
-
-
-@implements_for_td(torch.masked_select)
-def _masked_select(td: TensorDictBase, *args: Any, **kwargs: Any) -> TensorDictBase:
-    return td.masked_select(*args, **kwargs)
-
-
-@implements_for_td(torch.permute)
-def _permute(td: TensorDictBase, dims: Sequence[int]) -> TensorDictBase:
-    return td.permute(*dims)
-
-
-@implements_for_td(torch.cat)
-def _cat(
-    list_of_tensordicts: Sequence[TensorDictBase],
-    dim: int = 0,
-    device: DeviceType | None = None,
-    out: TensorDictBase | None = None,
-) -> TensorDictBase:
-    if not list_of_tensordicts:
-        raise RuntimeError("list_of_tensordicts cannot be empty")
-
-    batch_size = list(list_of_tensordicts[0].batch_size)
-    if dim < 0:
-        dim = len(batch_size) + dim
-    if dim >= len(batch_size):
-        raise RuntimeError(
-            f"dim must be in the range 0 <= dim < len(batch_size), got dim"
-            f"={dim} and batch_size={batch_size}"
-        )
-    batch_size[dim] = sum([td.batch_size[dim] for td in list_of_tensordicts])
-    batch_size = torch.Size(batch_size)
-
-    # check that all tensordict match
-    keys = _check_keys(list_of_tensordicts, strict=True)
-    if out is None:
-        out = {}
-        for key in keys:
-            with _ErrorInteceptor(
-                key, "Attempted to concatenate tensors on different devices at key"
-            ):
-                out[key] = torch.cat(
-                    [td._get_str(key, NO_DEFAULT) for td in list_of_tensordicts], dim
-                )
-        if device is None:
-            device = list_of_tensordicts[0].device
-            for td in list_of_tensordicts[1:]:
-                if device == td.device:
-                    continue
-                else:
-                    device = None
-                    break
-        names = None
-        if list_of_tensordicts[0]._has_names():
-            names = list_of_tensordicts[0].names
-        return TensorDict(
-            out, device=device, batch_size=batch_size, _run_checks=False, names=names
-        )
-    else:
-        if out.batch_size != batch_size:
-            raise RuntimeError(
-                "out.batch_size and cat batch size must match, "
-                f"got out.batch_size={out.batch_size} and batch_size"
-                f"={batch_size}"
-            )
-
-        for key in keys:
-            with _ErrorInteceptor(
-                key, "Attempted to concatenate tensors on different devices at key"
-            ):
-                if isinstance(out, TensorDict):
-                    torch.cat(
-                        [td.get(key) for td in list_of_tensordicts],
-                        dim,
-                        out=out.get(key),
-                    )
-                else:
-                    out.set_(
-                        key, torch.cat([td.get(key) for td in list_of_tensordicts], dim)
-                    )
-        return out
-
-
-@implements_for_lazy_td(torch.cat)
-def _lazy_cat(
-    list_of_tensordicts: Sequence[LazyStackedTensorDict],
-    dim: int = 0,
-    out: LazyStackedTensorDict | None = None,
-) -> LazyStackedTensorDict:
-    if not list_of_tensordicts:
-        raise RuntimeError("list_of_tensordicts cannot be empty")
-
-    batch_size = list(list_of_tensordicts[0].batch_size)
-    if dim < 0:
-        dim = len(batch_size) + dim
-    if dim >= len(batch_size):
-        raise RuntimeError(
-            f"dim must be in the range 0 <= dim < len(batch_size), got dim"
-            f"={dim} and batch_size={batch_size}"
-        )
-    stack_dim = list_of_tensordicts[0].stack_dim
-    if any((td.stack_dim != stack_dim) for td in list_of_tensordicts):
-        raise RuntimeError("cat lazy stacked tds must have same stack dim")
-
-    batch_size[dim] = sum(td.batch_size[dim] for td in list_of_tensordicts)
-    batch_size = torch.Size(batch_size)
-
-    new_dim = dim
-    if dim > stack_dim:
-        new_dim = dim - 1
-
-    if out is None:
-        out = []
-        if dim == stack_dim:  # if dim is stack, just add all to the same list
-            for lazy_td in list_of_tensordicts:
-                out += lazy_td.tensordicts
-        else:
-            for i in range(len(list_of_tensordicts[0].tensordicts)):
-                out.append(
-                    torch.cat(
-                        [lazy_td.tensordicts[i] for lazy_td in list_of_tensordicts],
-                        new_dim,
-                    )
-                )
-        return LazyStackedTensorDict(*out, stack_dim=stack_dim)
-    else:
-        if not isinstance(out, LazyStackedTensorDict):
-            return _cat(list_of_tensordicts, dim=dim, out=out)
-
-        if out.batch_size != batch_size:
-            raise RuntimeError(
-                "out.batch_size and cat batch size must match, "
-                f"got out.batch_size={out.batch_size} and batch_size"
-                f"={batch_size}"
-            )
-        if out.stack_dim != dim:
-            index_base = (slice(None),) * out.stack_dim
-            for i, sub_dest in enumerate(out.tensordicts):
-                index = index_base + (i,)
-                tds_to_cat = [_td[index] for _td in list_of_tensordicts]
-                torch.cat(tds_to_cat, dim, out=sub_dest)
-        else:
-            init_idx = 0
-            for td_in in list_of_tensordicts:
-                sub_dest = out.tensordicts[init_idx : init_idx + td_in.shape[dim]]
-                init_idx += init_idx + td_in.shape[dim]
-                torch.stack(sub_dest, out.stack_dim).update(td_in, inplace=True)
-
-        return out
-
-
-@implements_for_td(torch.stack)
-def _stack(
-    list_of_tensordicts: Sequence[TensorDictBase],
-    dim: int = 0,
-    device: DeviceType | None = None,
-    out: TensorDictBase | None = None,
-    strict: bool = False,
-    contiguous: bool = False,
-) -> TensorDictBase:
-    if not list_of_tensordicts:
-        raise RuntimeError("list_of_tensordicts cannot be empty")
-    batch_size = list_of_tensordicts[0].batch_size
-    if dim < 0:
-        dim = len(batch_size) + dim + 1
-
-    for td in list_of_tensordicts[1:]:
-        if td.batch_size != list_of_tensordicts[0].batch_size:
-            raise RuntimeError(
-                "stacking tensordicts requires them to have congruent batch sizes, "
-                f"got td1.batch_size={td.batch_size} and td2.batch_size="
-                f"{list_of_tensordicts[0].batch_size}"
-            )
-
-    # check that all tensordict match
-    keys = _check_keys(list_of_tensordicts)
-
-    if out is None:
-        device = list_of_tensordicts[0].device
-        if contiguous:
-            out = {}
-            for key in keys:
-                with _ErrorInteceptor(
-                    key, "Attempted to stack tensors on different devices at key"
-                ):
-                    out[key] = torch.stack(
-                        [_tensordict.get(key) for _tensordict in list_of_tensordicts],
-                        dim,
-                    )
-
-            return TensorDict(
-                out,
-                batch_size=LazyStackedTensorDict._compute_batch_size(
-                    batch_size, dim, len(list_of_tensordicts)
-                ),
-                device=device,
-                _run_checks=False,
-            )
-        else:
-            out = LazyStackedTensorDict(
-                *list_of_tensordicts,
-                stack_dim=dim,
-            )
-    else:
-        batch_size = list(batch_size)
-        batch_size.insert(dim, len(list_of_tensordicts))
-        batch_size = torch.Size(batch_size)
-
-        if out.batch_size != batch_size:
-            raise RuntimeError(
-                "out.batch_size and stacked batch size must match, "
-                f"got out.batch_size={out.batch_size} and batch_size"
-                f"={batch_size}"
-            )
-
-        out_keys = set(out.keys())
-        if strict:
-            in_keys = set(keys)
-            if len(out_keys - in_keys) > 0:
-                raise RuntimeError(
-                    "The output tensordict has keys that are missing in the "
-                    "tensordict that has to be written: {out_keys - in_keys}. "
-                    "As per the call to `stack(..., strict=True)`, this "
-                    "is not permitted."
-                )
-            elif len(in_keys - out_keys) > 0:
-                raise RuntimeError(
-                    "The resulting tensordict has keys that are missing in "
-                    f"its destination: {in_keys - out_keys}. As per the call "
-                    "to `stack(..., strict=True)`, this is not permitted."
-                )
-
-        try:
-            out._stack_onto_(list_of_tensordicts, dim)
-        except KeyError as err:
-            raise err
-            # TODO fix this
-            # # if key in out_keys:
-            #     with _ErrorInteceptor(
-            #         key, "Attempted to stack tensors on different devices at key"
-            #     ):
-            #         out.set(
-            #             key,
-            #             torch.stack(
-            #                 [
-            #                     _tensordict.get(key)
-            #                     for _tensordict in list_of_tensordicts
-            #                 ],
-            #                 dim,
-            #             ),
-            #             inplace=True,
-            #         )
-
-    return out
-
-
-def pad(
-    tensordict: TensorDictBase, pad_size: Sequence[int], value: float = 0.0
-) -> TensorDictBase:
-    """Pads all tensors in a tensordict along the batch dimensions with a constant value, returning a new tensordict.
-
-    Args:
-         tensordict (TensorDict): The tensordict to pad
-         pad_size (Sequence[int]): The padding size by which to pad some batch
-            dimensions of the tensordict, starting from the first dimension and
-            moving forward. [len(pad_size) / 2] dimensions of the batch size will
-            be padded. For example to pad only the first dimension, pad has the form
-            (padding_left, padding_right). To pad two dimensions,
-            (padding_left, padding_right, padding_top, padding_bottom) and so on.
-            pad_size must be even and less than or equal to twice the number of batch dimensions.
-         value (float, optional): The fill value to pad by, default 0.0
-
-    Returns:
-        A new TensorDict padded along the batch dimensions
-
-    Examples:
-        >>> from tensordict import TensorDict
-        >>> from tensordict.tensordict import pad
-        >>> import torch
-        >>> td = TensorDict({'a': torch.ones(3, 4, 1),
-        ...     'b': torch.ones(3, 4, 1, 1)}, batch_size=[3, 4])
-        >>> dim0_left, dim0_right, dim1_left, dim1_right = [0, 1, 0, 2]
-        >>> padded_td = pad(td, [dim0_left, dim0_right, dim1_left, dim1_right], value=0.0)
-        >>> print(padded_td.batch_size)
-        torch.Size([4, 6])
-        >>> print(padded_td.get("a").shape)
-        torch.Size([4, 6, 1])
-        >>> print(padded_td.get("b").shape)
-        torch.Size([4, 6, 1, 1])
-
-    """
-    if len(pad_size) > 2 * len(tensordict.batch_size):
-        raise RuntimeError(
-            "The length of pad_size must be <= 2 * the number of batch dimensions"
-        )
-
-    if len(pad_size) % 2:
-        raise RuntimeError("pad_size must have an even number of dimensions")
-
-    new_batch_size = list(tensordict.batch_size)
-    for i in range(len(pad_size)):
-        new_batch_size[i // 2] += pad_size[i]
-
-    reverse_pad = pad_size[::-1]
-    for i in range(0, len(reverse_pad), 2):
-        reverse_pad[i], reverse_pad[i + 1] = reverse_pad[i + 1], reverse_pad[i]
-
-    out = TensorDict(
-        {}, torch.Size(new_batch_size), device=tensordict.device, _run_checks=False
-    )
-    for key, tensor in tensordict.items():
-        cur_pad = reverse_pad
-        if len(pad_size) < len(_shape(tensor)) * 2:
-            cur_pad = [0] * (len(_shape(tensor)) * 2 - len(pad_size)) + reverse_pad
-
-        if _is_tensor_collection(tensor.__class__):
-            padded = pad(tensor, pad_size, value)
-        else:
-            padded = torch.nn.functional.pad(tensor, cur_pad, value=value)
-        out.set(key, padded)
-
-    return out
-
-
-def pad_sequence(
-    list_of_tensordicts: Sequence[TensorDictBase],
-    batch_first: bool = True,
-    padding_value: float = 0.0,
-    out: TensorDictBase | None = None,
-    device: DeviceType | None = None,
-    return_mask: bool | None = False,
-) -> TensorDictBase:
-    """Pads a list of tensordicts in order for them to be stacked together in a contiguous format.
-
-    Args:
-        list_of_tensordicts (List[TensorDictBase]): the list of instances to pad and stack.
-        batch_first (bool, optional): the ``batch_first`` correspondant of :func:`torch.nn.utils.rnn.pad_sequence`.
-            Defaults to ``True``.
-        padding_value (number, optional): the padding value. Defaults to ``0.0``.
-        out (TensorDictBase, optional): if provided, the destination where the data will be
-            written.
-        device (device compatible type, optional): if provded, the device where the
-            TensorDict output will be created.
-        return_mask (bool, optional): if ``True``, a "mask" entry will be returned.
-            It contains the mask of valid values in the stacked tensordict.
-
-    Examples:
-        >>> list_td = [
-        ...     TensorDict({"a": torch.zeros((3,))}, []),
-        ...     TensorDict({"a": torch.zeros((4,))}, []),
-        ...     ]
-        >>> padded_td = pad_sequence(list_td)
-        >>> print(padded_td)
-        TensorDict(
-            fields={
-                a: Tensor(shape=torch.Size([2, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([]),
-            device=None,
-            is_shared=False)
-    """
-    if not list_of_tensordicts:
-        raise RuntimeError("list_of_tensordicts cannot be empty")
-    # check that all tensordict match
-    if return_mask:
-        list_of_tensordicts = [
-            td.clone(False).set("mask", torch.ones(td.shape, dtype=torch.bool))
-            for td in list_of_tensordicts
-        ]
-    keys = _check_keys(list_of_tensordicts, leaves_only=True, include_nested=True)
-    shape = max(len(td) for td in list_of_tensordicts)
-    if shape == 0:
-        shape = [
-            len(list_of_tensordicts),
-        ]
-    elif batch_first:
-        shape = [len(list_of_tensordicts), shape]
-    else:
-        shape = [shape, len(list_of_tensordicts)]
-    if out is None:
-        out = TensorDict(
-            {}, batch_size=torch.Size(shape), device=device, _run_checks=False
-        )
-        for key in keys:
-            try:
-                out.set(
-                    key,
-                    torch.nn.utils.rnn.pad_sequence(
-                        [td.get(key) for td in list_of_tensordicts],
-                        batch_first=batch_first,
-                        padding_value=padding_value,
-                    ),
-                )
-            except Exception as err:
-                raise RuntimeError(f"pad_sequence failed for key {key}") from err
-        return out
-    else:
-        for key in keys:
-            out.set_(
-                key,
-                torch.nn.utils.rnn.pad_sequence(
-                    [td.get(key) for td in list_of_tensordicts],
-                    batch_first=batch_first,
-                    padding_value=padding_value,
-                ),
-            )
-        return out
-
-
-@functools.wraps(pad_sequence)
-def pad_sequence_ts(*args, **kwargs):
-    """Warning: this function will soon be deprecated. Please use pad_sequence instead."""
-    warnings.warn(
-        "pad_sequence_ts will soon be deprecated in favour of pad_sequence. Please use the latter instead."
-    )
-    return pad_sequence(*args, **kwargs)
-
-
-@implements_for_td(torch.split)
-def _split(
-    td: TensorDict, split_size_or_sections: int | list[int], dim: int = 0
-) -> list[TensorDictBase]:
-    return td.split(split_size_or_sections, dim)
-
-
-class SubTensorDict(TensorDictBase):
-    """A TensorDict that only sees an index of the stored tensors.
-
-    By default, indexing a tensordict with an iterable will result in a
-    SubTensorDict. This is done such that a TensorDict indexed with
-    non-contiguous index (e.g. a Tensor) will still point to the original
-    memory location (unlike regular indexing of tensors).
-
-    Examples:
-        >>> from tensordict import TensorDict, SubTensorDict
-        >>> source = {'random': torch.randn(3, 4, 5, 6),
-        ...    'zeros': torch.zeros(3, 4, 1, dtype=torch.bool)}
-        >>> batch_size = torch.Size([3, 4])
-        >>> td = TensorDict(source, batch_size)
-        >>> td_index = td[:, 2]
-        >>> print(type(td_index), td_index.shape)
-        <class 'tensordict.tensordict.TensorDict'> \
-torch.Size([3])
-        >>> td_index = td[slice(None), slice(None)]
-        >>> print(type(td_index), td_index.shape)
-        <class 'tensordict.tensordict.TensorDict'> \
-torch.Size([3, 4])
-        >>> td_index = td.get_sub_tensordict((slice(None), torch.tensor([0, 2], dtype=torch.long)))
-        >>> print(type(td_index), td_index.shape)
-        <class 'tensordict.tensordict.SubTensorDict'> \
-torch.Size([3, 2])
-        >>> _ = td_index.fill_('zeros', 1)
-        >>> # the indexed tensors are updated with Trues
-        >>> print(td.get('zeros'))
-        tensor([[[ True],
-                 [False],
-                 [ True],
-                 [False]],
-        <BLANKLINE>
-                [[ True],
-                 [False],
-                 [ True],
-                 [False]],
-        <BLANKLINE>
-                [[ True],
-                 [False],
-                 [ True],
-                 [False]]])
-
-    """
-
-    def __new__(cls, *args: Any, **kwargs: Any) -> SubTensorDict:
-        cls._is_shared = False
-        cls._is_memmap = False
-        return super().__new__(cls, _safe=False, _lazy=True, _inplace_set=True)
-
-    def __init__(
-        self,
-        source: TensorDictBase,
-        idx: IndexType,
-        batch_size: Sequence[int] | None = None,
-    ) -> None:
-        if not _is_tensor_collection(source.__class__):
-            raise TypeError(
-                f"Expected source to be a subclass of TensorDictBase, "
-                f"got {type(source)}"
-            )
-        self._source = source
-        idx = (
-            (idx,)
-            if not isinstance(
-                idx,
-                (
-                    tuple,
-                    list,
-                ),
-            )
-            else tuple(idx)
-        )
-        if any(item is Ellipsis for item in idx):
-            idx = convert_ellipsis_to_idx(idx, self._source.batch_size)
-        self._batch_size = _getitem_batch_size(self._source.batch_size, idx)
-        self.idx = idx
-
-        if batch_size is not None and batch_size != self.batch_size:
-            raise RuntimeError("batch_size does not match self.batch_size.")
-
-    # def __hash__(self):
-    #     return hash((self._source, self.idx))
-
-    @staticmethod
-    def _convert_ellipsis(idx, shape):
-        if any(_idx is Ellipsis for _idx in idx):
-            new_idx = []
-            cursor = -1
-            for _idx in idx:
-                if _idx is Ellipsis:
-                    if cursor == len(idx) - 1:
-                        # then we can just skip
-                        continue
-                    n_upcoming = len(idx) - cursor - 1
-                    while cursor < len(shape) - n_upcoming:
-                        cursor += 1
-                        new_idx.append(slice(None))
-                else:
-                    new_idx.append(_idx)
-            return tuple(new_idx)
-        return idx
-
-    def exclude(self, *keys: str, inplace: bool = False) -> TensorDictBase:
-        if inplace:
-            return super().exclude(*keys, inplace=True)
-        return self.to_tensordict().exclude(*keys, inplace=True)
-
-    @property
-    def batch_size(self) -> torch.Size:
-        return self._batch_size
-
-    @batch_size.setter
-    def batch_size(self, new_size: torch.Size) -> None:
-        self._batch_size_setter(new_size)
-
-    @property
-    def names(self):
-        names = self._source._get_names_idx(self.idx)
-        if names is None:
-            return [None] * self.batch_dims
-        return names
-
-    @names.setter
-    def names(self, value):
-        raise RuntimeError(
-            "Names of a subtensordict cannot be modified. Instantiate the tensordict first."
-        )
-
-    def _has_names(self):
-        return self._source._has_names()
-
-    def _erase_names(self):
-        raise RuntimeError(
-            "Cannot erase names of a SubTensorDict. Erase source TensorDict's names instead."
-        )
-
-    def _rename_subtds(self, names):
-        for key in self.keys():
-            if _is_tensor_collection(self.entry_class(key)):
-                raise RuntimeError("Cannot rename nested sub-tensordict dimensions.")
-
-    @property
-    def device(self) -> None | torch.device:
-        return self._source.device
-
-    @device.setter
-    def device(self, value: DeviceType) -> None:
-        self._source.device = value
-
-    def _preallocate(self, key: str, value: CompatibleType) -> TensorDictBase:
-        return self._source.set(key, value)
-
-    def _convert_inplace(self, inplace, key):
-        has_key = key in self.keys()
-        if inplace is not False:
-            if inplace is True and not has_key:  # inplace could be None
-                raise KeyError(
-                    TensorDictBase.KEY_ERROR.format(
-                        key, self.__class__.__name__, sorted(self.keys())
-                    )
-                )
-            inplace = has_key
-        if not inplace and has_key:
-            raise RuntimeError(
-                "Calling `SubTensorDict.set(key, value, inplace=False)` is "
-                "prohibited for existing tensors. Consider calling "
-                "SubTensorDict.set_(...) or cloning your tensordict first."
-            )
-        elif not inplace and self.is_locked:
-            raise RuntimeError(TensorDictBase.LOCK_ERROR)
-        return inplace
-
-    def _set_str(
-        self,
-        key: NestedKey,
-        value: dict[str, CompatibleType] | CompatibleType,
-        *,
-        inplace: bool,
-        validated: bool,
-    ) -> TensorDictBase:
-        inplace = self._convert_inplace(inplace, key)
-        # it is assumed that if inplace=False then the key doesn't exist. This is
-        # checked in set method, but not here. responsibility lies with the caller
-        # so that this method can have minimal overhead from runtime checks
-        parent = self._source
-        if not validated:
-            value = self._validate_value(value, check_shape=True)
-            validated = True
-        if not inplace:
-            if _is_tensor_collection(value.__class__):
-                value_expand = _expand_to_match_shape(
-                    parent.batch_size, value, self.batch_dims, self.device
-                )
-                for _key, _tensor in value.items():
-                    value_expand[_key] = _expand_to_match_shape(
-                        parent.batch_size, _tensor, self.batch_dims, self.device
-                    )
-            else:
-                value_expand = torch.zeros(
-                    (
-                        *parent.batch_size,
-                        *_shape(value)[self.batch_dims :],
-                    ),
-                    dtype=value.dtype,
-                    device=self.device,
-                )
-                if self.is_shared() and self.device.type == "cpu":
-                    value_expand.share_memory_()
-                elif self.is_memmap():
-                    value_expand = MemmapTensor.from_tensor(value_expand)
-
-            parent._set_str(key, value_expand, inplace=False, validated=validated)
-
-        parent._set_at_str(key, value, self.idx, validated=validated)
-        return self
-
-    def _set_tuple(
-        self,
-        key: NestedKey,
-        value: dict[str, CompatibleType] | CompatibleType,
-        *,
-        inplace: bool,
-        validated: bool,
-    ) -> TensorDictBase:
-        if len(key) == 1:
-            return self._set_str(key[0], value, inplace=inplace, validated=validated)
-        parent = self._source
-        td = parent._get_str(key[0], None)
-        if td is None:
-            td = parent.select()
-            parent._set_str(key[0], td, inplace=False, validated=True)
-        SubTensorDict(td, self.idx)._set_tuple(
-            key[1:], value, inplace=inplace, validated=validated
-        )
-        return self
-
-    def _set_at_str(self, key, value, idx, *, validated):
-        tensor_in = self._get_str(key, NO_DEFAULT)
-        if not validated:
-            value = self._validate_value(value, check_shape=False)
-            validated = True
-        if isinstance(idx, tuple) and len(idx) and isinstance(idx[0], tuple):
-            warn(
-                "Multiple indexing can lead to unexpected behaviours when "
-                "setting items, for instance `td[idx1][idx2] = other` may "
-                "not write to the desired location if idx1 is a list/tensor."
-            )
-            tensor_in = _sub_index(tensor_in, idx)
-            tensor_in.copy_(value)
-        else:
-            _set_item(tensor_in, idx, value, validated=validated)
-        # make sure that the value is updated
-        self._source._set_at_str(key, tensor_in, self.idx, validated=validated)
-        return self
-
-    def _set_at_tuple(self, key, value, idx, *, validated):
-        if len(key) == 1:
-            return self._set_at_str(key[0], value, idx, validated=validated)
-        if key[0] not in self.keys():
-            # this won't work
-            raise KeyError(f"key {key} not found in set_at_ with tensordict {self}.")
-        else:
-            td = self._get_str(key[0], NO_DEFAULT)
-        td._set_at_tuple(key[1:], value, idx, validated=validated)
-        return self
-
-    # @cache  # noqa: B019
-    def keys(
-        self, include_nested: bool = False, leaves_only: bool = False
-    ) -> _TensorDictKeysView:
-        return self._source.keys(include_nested=include_nested, leaves_only=leaves_only)
-
-    def entry_class(self, key: NestedKey) -> type:
-        source_type = type(self._source.get(key))
-        if _is_tensor_collection(source_type):
-            return self.__class__
-        return source_type
-
-    def _stack_onto_(self, list_item: list[CompatibleType], dim: int) -> SubTensorDict:
-        self._source._stack_onto_at_(list_item, dim=dim, idx=self.idx)
-        return self
-
-    def to(self, dest: DeviceType | torch.Size | type, **kwargs: Any) -> TensorDictBase:
-        if isinstance(dest, type) and issubclass(dest, TensorDictBase):
-            if isinstance(self, dest):
-                return self
-            out = dest(
-                source=self.clone(),
-            )
-            if self._has_names():
-                out.names = self.names
-            return out
-        elif isinstance(dest, (torch.device, str, int)):
-            dest = torch.device(dest)
-            # try:
-            if self.device is not None and dest == self.device:
-                return self
-            td = self.to_tensordict().to(dest, **kwargs)
-            # must be device
-            return td
-
-        elif isinstance(dest, torch.Size):
-            self.batch_size = dest
-            return self
-        elif dest is None:
-            return self
-        else:
-            raise NotImplementedError(
-                f"dest must be a string, torch.device or a TensorDict "
-                f"instance, {dest} not allowed"
-            )
-
-    def _change_batch_size(self, new_size: torch.Size) -> None:
-        if not hasattr(self, "_orig_batch_size"):
-            self._orig_batch_size = self.batch_size
-        elif self._orig_batch_size == new_size:
-            del self._orig_batch_size
-        self._batch_size = new_size
-
-    def get(
-        self,
-        key: NestedKey,
-        default: Tensor | str | None = NO_DEFAULT,
-    ) -> CompatibleType:
-        return self._source.get_at(key, self.idx, default=default)
-
-    def _get_str(self, key, default):
-        if key in self.keys() and _is_tensor_collection(self.entry_class(key)):
-            return SubTensorDict(self._source._get_str(key, NO_DEFAULT), self.idx)
-        return self._source._get_at_str(key, self.idx, default=default)
-
-    def _get_tuple(self, key, default):
-        return self._source._get_at_tuple(key, self.idx, default=default)
-
-    def update(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
-        clone: bool = False,
-        inplace: bool = False,
-        **kwargs,
-    ) -> SubTensorDict:
-        if input_dict_or_td is self:
-            # no op
-            return self
-        keys = set(self.keys(False))
-        for key, value in input_dict_or_td.items():
-            if clone and hasattr(value, "clone"):
-                value = value.clone()
-            else:
-                value = tree_map(torch.clone, value)
-            key = _unravel_key_to_tuple(key)
-            firstkey, subkey = key[0], key[1:]
-            # the key must be a string by now. Let's check if it is present
-            if firstkey in keys:
-                target_class = self.entry_class(firstkey)
-                if _is_tensor_collection(target_class):
-                    target = self._source.get(firstkey).get_sub_tensordict(self.idx)
-                    if len(subkey):
-                        target._set_tuple(subkey, value, inplace=False, validated=False)
-                        continue
-                    elif isinstance(value, dict) or _is_tensor_collection(
-                        value.__class__
-                    ):
-                        target.update(value)
-                        continue
-                    raise ValueError(
-                        f"Tried to replace a tensordict with an incompatible object of type {type(value)}"
-                    )
-                else:
-                    self._set_tuple(key, value, inplace=True, validated=False)
-            else:
-                self._set_tuple(
-                    key,
-                    value,
-                    inplace=BEST_ATTEMPT_INPLACE if inplace else False,
-                    validated=False,
-                )
-        return self
-
-    def update_(
-        self,
-        input_dict: dict[str, CompatibleType] | TensorDictBase,
-        clone: bool = False,
-    ) -> SubTensorDict:
-        return self.update_at_(
-            input_dict, idx=self.idx, discard_idx_attr=True, clone=clone
-        )
-
-    def update_at_(
-        self,
-        input_dict: dict[str, CompatibleType] | TensorDictBase,
-        idx: IndexType,
-        discard_idx_attr: bool = False,
-        clone: bool = False,
-    ) -> SubTensorDict:
-        for key, value in input_dict.items():
-            if not isinstance(value, tuple(_ACCEPTED_CLASSES)):
-                raise TypeError(
-                    f"Expected value to be one of types {_ACCEPTED_CLASSES} "
-                    f"but got {type(value)}"
-                )
-            if clone:
-                value = value.clone()
-            key = _unravel_key_to_tuple(key)
-            if discard_idx_attr:
-                self._source._set_at_tuple(
-                    key,
-                    value,
-                    idx,
-                    validated=False,
-                )
-            else:
-                self._set_at_tuple(key, value, idx, validated=False)
-        return self
-
-    def get_parent_tensordict(self) -> TensorDictBase:
-        if not isinstance(self._source, TensorDictBase):
-            raise TypeError(
-                f"SubTensorDict was initialized with a source of type"
-                f" {self._source.__class__.__name__}, "
-                "parent tensordict not accessible"
-            )
-        return self._source
-
-    @lock_blocked
-    def del_(self, key: NestedKey) -> TensorDictBase:
-        self._source = self._source.del_(key)
-        return self
-
-    def clone(self, recurse: bool = True) -> SubTensorDict:
-        """Clones the SubTensorDict.
-
-        Args:
-            recurse (bool, optional): if ``True`` (default), a regular
-                :class:`TensorDict` instance will be created from the :class:`SubTensorDict`.
-                Otherwise, another :class:`SubTensorDict` with identical content
-                will be returned.
-
-        Examples:
-            >>> data = TensorDict({"a": torch.arange(4).reshape(2, 2,)}, batch_size=[2, 2])
-            >>> sub_data = data.get_sub_tensordict([0,])
-            >>> print(sub_data)
-            SubTensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
-                batch_size=torch.Size([2]),
-                device=None,
-                is_shared=False)
-            >>> # the data of both subtensordict is the same
-            >>> print(data.get("a").data_ptr(), sub_data.get("a").data_ptr())
-            140183705558208 140183705558208
-            >>> sub_data_clone = sub_data.clone(recurse=True)
-            >>> print(sub_data_clone)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
-                batch_size=torch.Size([2]),
-                device=None,
-                is_shared=False)
-            >>. print(sub_data.get("a").data_ptr())
-            140183705558208
-            >>> sub_data_clone = sub_data.clone(recurse=False)
-            >>> print(sub_data_clone)
-            SubTensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
-                batch_size=torch.Size([2]),
-                device=None,
-                is_shared=False)
-            >>> print(sub_data.get("a").data_ptr())
-            140183705558208
-        """
-        if not recurse:
-            return SubTensorDict(source=self._source.clone(recurse=False), idx=self.idx)
-        return self.to_tensordict()
-
-    def is_contiguous(self) -> bool:
-        return all(value.is_contiguous() for value in self.values())
-
-    def contiguous(self) -> TensorDictBase:
-        if self.is_contiguous():
-            return self
-        return TensorDict(
-            batch_size=self.batch_size,
-            source={key: value for key, value in self.items()},
-            device=self.device,
-            names=self.names,
-            _run_checks=False,
-        )
-
-    def select(
-        self, *keys: str, inplace: bool = False, strict: bool = True
-    ) -> TensorDictBase:
-        if inplace:
-            self._source = self._source.select(*keys, strict=strict)
-            return self
-        return self._source.select(*keys, strict=strict)[self.idx]
-
-    def expand(self, *shape: int, inplace: bool = False) -> TensorDictBase:
-        if len(shape) == 1 and isinstance(shape[0], Sequence):
-            shape = tuple(shape[0])
-        return self.apply(
-            lambda x: x.expand((*shape, *x.shape[self.ndim :])), batch_size=shape
-        )
-
-    def is_shared(self) -> bool:
-        return self._source.is_shared()
-
-    def is_memmap(self) -> bool:
-        return self._source.is_memmap()
-
-    def rename_key_(
-        self, old_key: str, new_key: str, safe: bool = False
-    ) -> SubTensorDict:
-        self._source.rename_key_(old_key, new_key, safe=safe)
-        return self
-
-    rename_key = _renamed_inplace_method(rename_key_)
-
-    def pin_memory(self) -> TensorDictBase:
-        self._source.pin_memory()
-        return self
-
-    def detach_(self) -> TensorDictBase:
-        raise RuntimeError("Detaching a sub-tensordict in-place cannot be done.")
-
-    def masked_fill_(self, mask: Tensor, value: float | bool) -> TensorDictBase:
-        for key, item in self.items():
-            self.set_(key, torch.full_like(item, value))
-        return self
-
-    def masked_fill(self, mask: Tensor, value: float | bool) -> TensorDictBase:
-        td_copy = self.clone()
-        return td_copy.masked_fill_(mask, value)
-
-    def memmap_(
-        self, prefix: str | None = None, copy_existing: bool = False
-    ) -> TensorDictBase:
-        raise RuntimeError(
-            "Converting a sub-tensordict values to memmap cannot be done."
-        )
-
-    def share_memory_(self) -> TensorDictBase:
-        raise RuntimeError(
-            "Casting a sub-tensordict values to shared memory cannot be done."
-        )
-
-    @property
-    def is_locked(self) -> bool:
-        return self._source.is_locked
-
-    @is_locked.setter
-    def is_locked(self, value) -> bool:
-        if value:
-            self.lock_()
-        else:
-            self.unlock_()
-
-    @as_decorator("is_locked")
-    def lock_(self) -> TensorDictBase:
-        # we can't lock sub-tensordicts because that would mean that the
-        # parent tensordict cannot be modified either.
-        if not self.is_locked:
-            raise RuntimeError(
-                "Cannot lock a SubTensorDict. Lock the parent tensordict instead."
-            )
-        return self
-
-    @as_decorator("is_locked")
-    def unlock_(self) -> TensorDictBase:
-        if self.is_locked:
-            raise RuntimeError(
-                "Cannot unlock a SubTensorDict. Unlock the parent tensordict instead."
-            )
-        return self
-
-    def _remove_lock(self, lock_id):
-        raise RuntimeError(
-            "Cannot unlock a SubTensorDict. Unlock the parent tensordict instead."
-        )
-
-    def _lock_propagate(self, lock_ids=None):
-        raise RuntimeError(
-            "Cannot lock a SubTensorDict. Lock the parent tensordict instead."
-        )
-
-    lock = _renamed_inplace_method(lock_)
-    unlock = _renamed_inplace_method(unlock_)
-
-    def __del__(self):
-        pass
-
-
-def merge_tensordicts(*tensordicts: TensorDictBase) -> TensorDictBase:
-    """Merges tensordicts together."""
-    if len(tensordicts) < 2:
-        raise RuntimeError(
-            f"at least 2 tensordicts must be provided, got" f" {len(tensordicts)}"
-        )
-    d = tensordicts[0].to_dict()
-    batch_size = tensordicts[0].batch_size
-    for td in tensordicts[1:]:
-        d.update(td.to_dict())
-        if td.batch_dims < len(batch_size):
-            batch_size = td.batch_size
-    return TensorDict(d, batch_size, device=td.device, _run_checks=False)
-
-
-class _LazyStackedTensorDictKeysView(_TensorDictKeysView):
-    tensordict: LazyStackedTensorDict
-
-    def __len__(self) -> int:
-        return len(self._keys())
-
-    def _keys(self) -> list[str]:
-        return self.tensordict._key_list()
-
-    def __contains__(self, item):
-        item = _unravel_key_to_tuple(item)
-        if item[0] in self.tensordict._iterate_over_keys():
-            if self.leaves_only:
-                return not _is_tensor_collection(self.tensordict.entry_class(item[0]))
-            has_first_key = True
-        else:
-            has_first_key = False
-        if not has_first_key or len(item) == 1:
-            return has_first_key
-        # otherwise take the long way
-        return all(
-            item[1:]
-            in tensordict.get(item[0]).keys(self.include_nested, self.leaves_only)
-            for tensordict in self.tensordict.tensordicts
-        )
-
-
-class LazyStackedTensorDict(TensorDictBase):
-    """A Lazy stack of TensorDicts.
-
-    When stacking TensorDicts together, the default behaviour is to put them
-    in a stack that is not instantiated.
-    This allows to seamlessly work with stacks of tensordicts with operations
-    that will affect the original tensordicts.
-
-    Args:
-         *tensordicts (TensorDict instances): a list of tensordict with
-            same batch size.
-         stack_dim (int): a dimension (between `-td.ndimension()` and
-            `td.ndimension()-1` along which the stack should be performed.
-         hook_out (callable, optional): a callable to execute after :meth:`~.get`.
-         hook_in (callable, optional): a callable to execute before :meth:`~.set`.
-
-    Examples:
-        >>> from tensordict import TensorDict
-        >>> import torch
-        >>> tds = [TensorDict({'a': torch.randn(3, 4)}, batch_size=[3])
-        ...     for _ in range(10)]
-        >>> td_stack = torch.stack(tds, -1)
-        >>> print(td_stack.shape)
-        torch.Size([3, 10])
-        >>> print(td_stack.get("a").shape)
-        torch.Size([3, 10, 4])
-        >>> print(td_stack[:, 0] is tds[0])
-        True
-
-    """
-
-    @classmethod
-    def __torch_function__(
-        cls,
-        func: Callable,
-        types: tuple[type, ...],
-        args: tuple[Any, ...] = (),
-        kwargs: dict[str, Any] | None = None,
-    ) -> Callable:
-        if func in LAZY_TD_HANDLED_FUNCTIONS:
-            if kwargs is None:
-                kwargs = {}
-            if func not in LAZY_TD_HANDLED_FUNCTIONS or not all(
-                issubclass(t, (Tensor, TensorDictBase)) for t in types
-            ):
-                return NotImplemented
-            return LAZY_TD_HANDLED_FUNCTIONS[func](*args, **kwargs)
-        else:
-            return super().__torch_function__(func, types, args, kwargs)
-
-    def __new__(cls, *args: Any, **kwargs: Any) -> LazyStackedTensorDict:
-        cls._td_dim_name = None
-        return super().__new__(cls, *args, _safe=False, _lazy=True, **kwargs)
-
-    def __init__(
-        self,
-        *tensordicts: TensorDictBase,
-        stack_dim: int = 0,
-        hook_out: callable | None = None,
-        hook_in: callable | None = None,
-        batch_size: Sequence[int] | None = None,  # TODO: remove
-    ) -> None:
-        self._is_shared = False
-        self._is_memmap = False
-        self._is_locked = None
-
-        # sanity check
-        N = len(tensordicts)
-        if not N:
-            raise RuntimeError(
-                "at least one tensordict must be provided to "
-                "StackedTensorDict to be instantiated"
-            )
-        if not isinstance(tensordicts[0], TensorDictBase):
-            raise TypeError(
-                f"Expected input to be TensorDictBase instance"
-                f" but got {type(tensordicts[0])} instead."
-            )
-        if stack_dim < 0:
-            raise RuntimeError(
-                f"stack_dim must be non negative, got stack_dim={stack_dim}"
-            )
-        _batch_size = tensordicts[0].batch_size
-        device = tensordicts[0].device
-
-        for td in tensordicts[1:]:
-            if not isinstance(td, TensorDictBase):
-                raise TypeError(
-                    "Expected all inputs to be TensorDictBase instances but got "
-                    f"{type(td)} instead."
-                )
-            _bs = td.batch_size
-            _device = td.device
-            if device != _device:
-                raise RuntimeError(f"devices differ, got {device} and {_device}")
-            if _bs != _batch_size:
-                raise RuntimeError(
-                    f"batch sizes in tensordicts differs, StackedTensorDict "
-                    f"cannot be created. Got td[0].batch_size={_batch_size} "
-                    f"and td[i].batch_size={_bs} "
-                )
-        self.tensordicts: list[TensorDictBase] = list(tensordicts)
-        self.stack_dim = stack_dim
-        self._batch_size = self._compute_batch_size(_batch_size, stack_dim, N)
-        self.hook_out = hook_out
-        self.hook_in = hook_in
-        if batch_size is not None and batch_size != self.batch_size:
-            raise RuntimeError("batch_size does not match self.batch_size.")
-
-    @property
-    def device(self) -> torch.device | None:
-        # devices might have changed, so we check that they're all the same
-        device_set = {td.device for td in self.tensordicts}
-        if len(device_set) != 1:
-            raise RuntimeError(
-                f"found multiple devices in {self.__class__.__name__}:" f" {device_set}"
-            )
-        device = self.tensordicts[0].device
-        return device
-
-    @device.setter
-    def device(self, value: DeviceType) -> None:
-        for t in self.tensordicts:
-            t.device = value
-
-    @property
-    def batch_size(self) -> torch.Size:
-        return self._batch_size
-
-    @batch_size.setter
-    def batch_size(self, new_size: torch.Size) -> None:
-        return self._batch_size_setter(new_size)
-
-    @property
-    @cache  # noqa
-    def names(self):
-        names = list(self.tensordicts[0].names)
-        for td in self.tensordicts[1:]:
-            if names != td.names:
-                raise ValueError(
-                    f"Not all dim names match, got {names} and {td.names}."
-                )
-        names.insert(self.stack_dim, self._td_dim_name)
-        return names
-
-    @names.setter
-    @erase_cache  # a nested lazy stacked tensordict is not apparent to the root
-    def names(self, value):
-        if value is None:
-            for td in self.tensordicts:
-                td.names = None
-            self._td_dim_name = None
-        else:
-            names_c = list(value)
-            name = names_c[self.stack_dim]
-            self._td_dim_name = name
-            del names_c[self.stack_dim]
-            for td in self.tensordicts:
-                if td._check_dim_name(name):
-                    # TODO: should reset names here
-                    raise ValueError(f"The dimension name {name} is already taken.")
-                td.rename_(*names_c)
-
-    def _rename_subtds(self, names):
-        # remove the name of the stack dim
-        names = list(names)
-        del names[self.stack_dim]
-        for td in self.tensordicts:
-            td.names = names
-
-    def _has_names(self):
-        return all(td._has_names() for td in self.tensordicts)
-
-    def _erase_names(self):
-        self._td_dim_name = None
-        for td in self.tensordicts:
-            td._erase_names()
-
-    def get_item_shape(self, key):
-        """Gets the shape of an item in the lazy stack.
-
-        Heterogeneous dimensions are returned as -1.
-
-        This implementation is inefficient as it will attempt to stack the items
-        to compute their shape, and should only be used for printing.
-        """
-        try:
-            item = self.get(key)
-            return item.shape
-        except RuntimeError as err:
-            if re.match(r"Found more than one unique shape in the tensors", str(err)):
-                shape = None
-                for td in self.tensordicts:
-                    if shape is None:
-                        shape = list(td.get_item_shape(key))
-                    else:
-                        _shape = td.get_item_shape(key)
-                        if len(shape) != len(_shape):
-                            shape = [-1]
-                            return torch.Size(shape)
-                        shape = [
-                            s1 if s1 == s2 else -1 for (s1, s2) in zip(shape, _shape)
-                        ]
-                shape.insert(self.stack_dim, len(self.tensordicts))
-                return torch.Size(shape)
-            else:
-                raise err
-
-    def is_shared(self) -> bool:
-        are_shared = [td.is_shared() for td in self.tensordicts]
-        are_shared = [value for value in are_shared if value is not None]
-        if not len(are_shared):
-            return None
-        if any(are_shared) and not all(are_shared):
-            raise RuntimeError(
-                f"tensordicts shared status mismatch, got {sum(are_shared)} "
-                f"shared tensordicts and "
-                f"{len(are_shared) - sum(are_shared)} non shared tensordict "
-            )
-        return all(are_shared)
-
-    def is_memmap(self) -> bool:
-        are_memmap = [td.is_memmap() for td in self.tensordicts]
-        if any(are_memmap) and not all(are_memmap):
-            raise RuntimeError(
-                f"tensordicts memmap status mismatch, got {sum(are_memmap)} "
-                f"memmap tensordicts and "
-                f"{len(are_memmap) - sum(are_memmap)} non memmap tensordict "
-            )
-        return all(are_memmap)
-
-    @staticmethod
-    def _compute_batch_size(
-        batch_size: torch.Size, stack_dim: int, N: int
-    ) -> torch.Size:
-        s = list(batch_size)
-        s.insert(stack_dim, N)
-        return torch.Size(s)
-
-    def _set_str(
-        self,
-        key: NestedKey,
-        value: dict[str, CompatibleType] | CompatibleType,
-        *,
-        inplace: bool,
-        validated: bool,
-    ) -> TensorDictBase:
-        try:
-            inplace = self._convert_inplace(inplace, key)
-        except KeyError as e:
-            raise KeyError(
-                "setting a value in-place on a stack of TensorDict is only "
-                "permitted if all members of the stack have this key in "
-                "their register."
-            ) from e
-        if not validated:
-            value = self._validate_value(value)
-            validated = True
-        if self.hook_in is not None:
-            value = self.hook_in(value)
-        values = value.unbind(self.stack_dim)
-        for tensordict, item in zip(self.tensordicts, values):
-            tensordict._set_str(key, item, inplace=inplace, validated=validated)
-        return self
-
-    def _set_tuple(
-        self,
-        key: NestedKey,
-        value: dict[str, CompatibleType] | CompatibleType,
-        *,
-        inplace: bool,
-        validated: bool,
-    ) -> TensorDictBase:
-        if len(key) == 1:
-            return self._set_str(key[0], value, inplace=inplace, validated=validated)
-        # if inplace is not False:  # inplace could be None
-        #     # we don't want to end up in the situation where one tensordict has
-        #     # inplace=True and another one inplace=False because inplace was loose.
-        #     # Worse could be writing with inplace=True up until some level then to
-        #     # realize the key is missing in one td, raising an exception and having
-        #     # messed up the data. Hence we must start by checking if the key
-        #     # is present.
-        #     has_key = key in self.keys(True)
-        #     if inplace is True and not has_key:  # inplace could be None
-        #         raise KeyError(
-        #             TensorDictBase.KEY_ERROR.format(
-        #                 key, self.__class__.__name__, sorted(self.keys())
-        #             )
-        #         )
-        #     inplace = has_key
-        if not validated:
-            value = self._validate_value(value)
-            validated = True
-        if self.hook_in is not None:
-            value = self.hook_in(value)
-        values = value.unbind(self.stack_dim)
-        for tensordict, item in zip(self.tensordicts, values):
-            tensordict._set_tuple(key, item, inplace=inplace, validated=validated)
-        return self
-
-    def _split_index(self, index):
-        """Given a tuple index, split it in as many indices as the number of tensordicts.
-
-        Returns:
-            a dictionary with {index-of-td: index-within-td}
-            the number of single dim indices until stack dim
-            a boolean indicating if the index along the stack dim is an integer
-        """
-        if not isinstance(index, tuple):
-            index = (index,)
-        index = convert_ellipsis_to_idx(index, self.batch_size)
-        index = _broadcast_tensors(index)
-        out = []
-        num_single = 0
-        num_none = 0
-        isinteger = False
-        is_nd_tensor = False
-        cursor = 0  # the dimension cursor
-        selected_td_idx = range(len(self.tensordicts))
-        has_bool = False
-        num_squash = 0
-        for i, idx in enumerate(index):  # noqa: B007
-            cursor_incr = 1
-            if idx is None:
-                out.append(None)
-                num_none += cursor <= self.stack_dim
-                continue
-            if cursor == self.stack_dim:
-                # we need to check which tds need to be indexed
-                if isinstance(idx, slice) or _is_number(idx):
-                    selected_td_idx = range(len(self.tensordicts))[idx]
-                    if not isinstance(selected_td_idx, range):
-                        isinteger = True
-                        selected_td_idx = [selected_td_idx]
-                elif isinstance(idx, (list, range)):
-                    selected_td_idx = idx
-                elif isinstance(idx, (torch.Tensor, np.ndarray)):
-                    if idx.dtype in (np.dtype("bool"), torch.bool):
-                        # we mark that we need to dispatch the indices across stack idx
-                        has_bool = True
-                        # split mask along dim
-                        individual_masks = idx = idx.unbind(0)
-                        selected_td_idx = range(len(self.tensordicts))
-                        out.append(idx)
-                        split_dim = self.stack_dim - num_single
-                        mask_loc = i
-                    else:
-                        if isinstance(idx, np.ndarray):
-                            idx = torch.tensor(idx)
-                        is_nd_tensor = True
-                        selected_td_idx = range(len(idx))
-                        out.append(idx.unbind(0))
-                else:
-                    raise TypeError(f"Invalid index type: {type(idx)}.")
-            else:
-                if _is_number(idx) and cursor < self.stack_dim:
-                    num_single += 1
-                if isinstance(
-                    idx,
-                    (
-                        int,
-                        slice,
-                        list,
-                        range,
-                    ),
-                ):
-                    out.append(idx)
-                elif isinstance(idx, (np.ndarray, torch.Tensor)):
-                    if idx.dtype in (np.dtype("bool"), torch.bool):
-                        cursor_incr = idx.ndim
-                        if cursor < self.stack_dim:
-                            num_squash += cursor_incr - 1
-                        if (
-                            cursor < self.stack_dim
-                            and cursor + cursor_incr > self.stack_dim
-                        ):
-                            # we mark that we need to dispatch the indices across stack idx
-                            has_bool = True
-                            # split mask along dim
-                            # relative_stack_dim = self.stack_dim - cursor - cursor_incr
-                            individual_masks = idx = idx.unbind(0)
-                            selected_td_idx = range(self.shape[i])
-                            split_dim = cursor - num_single
-                            mask_loc = i
-                    out.append(idx)
-                else:
-                    raise TypeError(f"Invalid index type: {type(idx)}.")
-            cursor += cursor_incr
-        if has_bool:
-            out = tuple(
-                tuple(idx if not isinstance(idx, tuple) else idx[i] for idx in out)
-                for i in selected_td_idx
-            )
-            return {
-                "index_dict": {i: out[i] for i in selected_td_idx},
-                "num_single": num_single,
-                "isinteger": isinteger,
-                "has_bool": has_bool,
-                "individual_masks": individual_masks,
-                "split_dim": split_dim,
-                "mask_loc": mask_loc,
-                "is_nd_tensor": is_nd_tensor,
-                "num_none": num_none,
-                "num_squash": num_squash,
-            }
-        elif is_nd_tensor:
-
-            def isindexable(idx):
-                if isinstance(idx, (torch.Tensor, np.ndarray)):
-                    if idx.dtype in (torch.bool, np.dtype("bool")):
-                        return False
-                    return True
-                if isinstance(idx, (tuple, list, range)):
-                    return True
-                return False
-
-            out = tuple(
-                tuple(idx if not isindexable(idx) else idx[i] for idx in out)
-                for i in selected_td_idx
-            )
-            return {
-                "index_dict": dict(enumerate(out)),
-                "num_single": num_single,
-                "isinteger": isinteger,
-                "has_bool": has_bool,
-                "is_nd_tensor": is_nd_tensor,
-                "num_none": num_none,
-                "num_squash": num_squash,
-            }
-        return {
-            "index_dict": {i: tuple(out) for i in selected_td_idx},
-            "num_single": num_single,
-            "isinteger": isinteger,
-            "has_bool": has_bool,
-            "is_nd_tensor": is_nd_tensor,
-            "num_none": num_none,
-            "num_squash": num_squash,
-        }
-
-    def _set_at_str(self, key, value, index, *, validated):
-        if not validated:
-            value = self._validate_value(value, check_shape=False)
-            validated = True
-        if self.hook_in is not None:
-            value = self.hook_in(value)
-        split_index = self._split_index(index)
-        converted_idx = split_index["index_dict"]
-        num_single = split_index["num_single"]
-        isinteger = split_index["isinteger"]
-        has_bool = split_index["has_bool"]
-        num_squash = split_index.get("num_squash", 0)
-        num_none = split_index.get("num_none", 0)
-        is_nd_tensor = split_index.get("is_nd_tensor", False)
-        if isinteger:
-            # this will break if the index along the stack dim is [0] or :1 or smth
-            for i, _idx in converted_idx.items():
-                self.tensordicts[i]._set_at_str(key, value, _idx, validated=validated)
-            return self
-        if is_nd_tensor:
-            unbind_dim = self.stack_dim - num_single + num_none - num_squash
-            value_unbind = value.unbind(unbind_dim)
-            for idx, _value in zip(converted_idx.values(), value_unbind):
-                self._set_at_str(key, _value, idx, validated=validated)
-            return self
-        elif not has_bool:
-            unbind_dim = self.stack_dim - num_single + num_none - num_squash
-            value_unbind = value.unbind(unbind_dim)
-            for (i, _idx), _value in zip(
-                converted_idx.items(),
-                value_unbind,
-            ):
-                self.tensordicts[i]._set_at_str(key, _value, _idx, validated=validated)
-        else:
-            # we must split, not unbind
-            mask_unbind = split_index["individual_masks"]
-            split_dim = split_index["split_dim"]
-            splits = [_mask_unbind.sum().item() for _mask_unbind in mask_unbind]
-            value_unbind = value.split(splits, split_dim)
-            if mask_unbind[0].ndim == 0:
-                # we can return a stack
-                for (i, _idx), mask, _value in zip(
-                    converted_idx.items(),
-                    mask_unbind,
-                    value_unbind,
-                ):
-                    if mask.any():
-                        self.tensordicts[i]._set_at_str(
-                            key, _value, _idx, validated=validated
-                        )
-            else:
-                for (i, _idx), _value in zip(converted_idx.items(), value_unbind):
-                    self_idx = (slice(None),) * split_index["mask_loc"] + (i,)
-                    self[self_idx]._set_at_str(key, _value, _idx, validated=validated)
-
-        # # it may be the case that we can't get the value
-        # # because it can't be stacked.
-        # # self[index]._set_str(key, value, validated=validated, inplace=True)
-        # # return self
-        # split_index = self._split_index(index)
-        # converted_idx = split_index["index_dict"]
-        # num_single = split_index["num_single"]
-        # isinteger = split_index["isinteger"]
-        # if isinteger:
-        #     for (i, _idx) in converted_idx.items():
-        #         if _idx:
-        #             self.tensordicts[i]._set_at_str(
-        #                 key, value, _idx, validated=validated
-        #             )
-        #         else:
-        #             self.tensordicts[i]._set_str(
-        #                 key,
-        #                 value,
-        #                 validated=validated,
-        #                 inplace=True,
-        #             )
-        #     return self
-        # unbind_dim = self.stack_dim - num_single
-        # for (i, _idx), _value in zip(converted_idx.items(), value.unbind(unbind_dim)):
-        #     self.tensordicts[i]._set_at_str(key, _value, _idx, validated=validated)
-        # return self
-
-    def _set_at_tuple(self, key, value, idx, *, validated):
-        if len(key) == 1:
-            return self._set_at_str(key[0], value, idx, validated=validated)
-        # get the "last" tds
-        tds = []
-        for td in self.tensordicts:
-            tds.append(td.get(key[:-1]))
-        # build only a single lazy stack from it
-        # (if the stack is a stack of stacks this won't be awesomely efficient
-        # but then we'd need to splut the value (which we can do) and recompute
-        # the sub-index for each td, which is a different story!
-        td = LazyStackedTensorDict(
-            *tds, stack_dim=self.stack_dim, hook_out=self.hook_out, hook_in=self.hook_in
-        )
-        if not validated:
-            value = self._validate_value(value, check_shape=False)
-            validated = True
-        if self.hook_in is not None:
-            value = self.hook_in(value)
-        item = td._get_str(key, NO_DEFAULT)
-        item[idx] = value
-        td._set_str(key, item, inplace=True, validated=True)
-        return self
-
-    def unsqueeze(self, dim: int) -> TensorDictBase:
-        if dim < 0:
-            dim = self.batch_dims + dim + 1
-
-        if (dim > self.batch_dims) or (dim < 0):
-            raise RuntimeError(
-                f"unsqueezing is allowed for dims comprised between "
-                f"`-td.batch_dims` and `td.batch_dims` only. Got "
-                f"dim={dim} with a batch size of {self.batch_size}."
-            )
-        if dim <= self.stack_dim:
-            stack_dim = self.stack_dim + 1
-        else:
-            dim = dim - 1
-            stack_dim = self.stack_dim
-        return LazyStackedTensorDict(
-            *(tensordict.unsqueeze(dim) for tensordict in self.tensordicts),
-            stack_dim=stack_dim,
-        )
-
-    def squeeze(self, dim: int | None = None) -> TensorDictBase:
-        """Squeezes all tensors for a dimension comprised in between `-td.batch_dims+1` and `td.batch_dims-1` and returns them in a new tensordict.
-
-        Args:
-            dim (Optional[int]): dimension along which to squeeze. If dim is None, all singleton dimensions will be squeezed. dim is None by default.
-
-        """
-        if dim is None:
-            size = self.size()
-            if len(self.size()) == 1 or size.count(1) == 0:
-                return self
-            first_singleton_dim = size.index(1)
-            return self.squeeze(first_singleton_dim).squeeze()
-
-        if dim < 0:
-            dim = self.batch_dims + dim
-
-        if self.batch_dims and (dim >= self.batch_dims or dim < 0):
-            raise RuntimeError(
-                f"squeezing is allowed for dims comprised between 0 and "
-                f"td.batch_dims only. Got dim={dim} and batch_size"
-                f"={self.batch_size}."
-            )
-
-        if dim >= self.batch_dims or self.batch_size[dim] != 1:
-            return self
-        if dim == self.stack_dim:
-            return self.tensordicts[0]
-        elif dim < self.stack_dim:
-            stack_dim = self.stack_dim - 1
-        else:
-            dim = dim - 1
-            stack_dim = self.stack_dim
-        return LazyStackedTensorDict(
-            *(tensordict.squeeze(dim) for tensordict in self.tensordicts),
-            stack_dim=stack_dim,
-        )
-
-    def unbind(self, dim: int) -> tuple[TensorDictBase, ...]:
-        if dim < 0:
-            dim = self.batch_dims + dim
-        if dim < 0 or dim >= self.ndim:
-            raise ValueError(
-                f"Cannot unbind along dimension {dim} with batch size {self.batch_size}."
-            )
-        if dim == self.stack_dim:
-            return tuple(self.tensordicts)
-        else:
-            # return a stack of unbound tensordicts
-            out = []
-            new_dim = dim if dim < self.stack_dim else dim - 1
-            new_stack_dim = (
-                self.stack_dim if dim > self.stack_dim else self.stack_dim - 1
-            )
-            for td in self.tensordicts:
-                out.append(td.unbind(new_dim))
-            return tuple(_stack(vals, new_stack_dim) for vals in zip(*out))
-
-    def _stack_onto_(
-        self,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> TensorDictBase:
-        if dim == self.stack_dim:
-            for source, tensordict_dest in zip(list_item, self.tensordicts):
-                tensordict_dest.update_(source)
-        else:
-            for i, td in enumerate(list_item):
-                idx = (slice(None),) * dim + (i,)
-                self.update_at_(td, idx)
-        return self
-
-    @cache  # noqa: B019
-    def _get_str(
-        self,
-        key: NestedKey,
-        default: str | CompatibleType = NO_DEFAULT,
-    ) -> CompatibleType:
-        # we can handle the case where the key is a tuple of length 1
-        tensors = []
-        for td in self.tensordicts:
-            tensors.append(td._get_str(key, default=default))
-            if (
-                tensors[-1] is default
-                and not isinstance(
-                    default, (MemmapTensor, KeyedJaggedTensor, torch.Tensor)
-                )
-                and not is_tensor_collection(default)
-            ):
-                # then we consider this default as non-stackable and return prematurly
-                return default
-        try:
-            out = torch.stack(tensors, self.stack_dim)
-            if _is_tensor_collection(out.__class__):
-                if self._td_dim_name is not None:
-                    out._td_dim_name = self._td_dim_name
-                if isinstance(out, LazyStackedTensorDict):
-                    # then it's a LazyStackedTD
-                    out.hook_out = self.hook_out
-                    out.hook_in = self.hook_in
-                else:
-                    # then it's a tensorclass
-                    out._tensordict.hook_out = self.hook_out
-                    out._tensordict.hook_in = self.hook_in
-            elif self.hook_out is not None:
-                out = self.hook_out(out)
-            return out
-        except RuntimeError as err:
-            if "stack expects each tensor to be equal size" in str(err):
-                shapes = {_shape(tensor) for tensor in tensors}
-                raise RuntimeError(
-                    f"Found more than one unique shape in the tensors to be "
-                    f"stacked ({shapes}). This is likely due to a modification "
-                    f"of one of the stacked TensorDicts, where a key has been "
-                    f"updated/created with an uncompatible shape. If the entries "
-                    f"are intended to have a different shape, use the get_nestedtensor "
-                    f"method instead."
-                )
-            else:
-                raise err
-
-    def _get_tuple(self, key, default):
-        first = self._get_str(key[0], None)
-        if first is None:
-            return self._default_get(first, default)
-        if len(key) == 1:
-            return first
-        try:
-            if isinstance(first, KeyedJaggedTensor):
-                if len(key) != 2:
-                    raise ValueError(f"Got too many keys for a KJT: {key}.")
-                return first[key[-1]]
-            else:
-                return first._get_tuple(key[1:], default=default)
-        except AttributeError as err:
-            if "has no attribute" in str(err):
-                raise ValueError(
-                    f"Expected a TensorDictBase instance but got {type(first)} instead"
-                    f" for key '{key[1:]}' in tensordict:\n{self}."
-                )
-
-    @cache  # noqa: B019
-    def _add_batch_dim(self, *, in_dim, vmap_level):
-        if self.is_memmap():
-            td = torch.stack([td.cpu().as_tensor() for td in self.tensordicts], 0)
-        else:
-            td = self
-        if in_dim < 0:
-            in_dim = self.ndim + in_dim
-        if in_dim == self.stack_dim:
-            return self._cached_add_batch_dims(td, in_dim=in_dim, vmap_level=vmap_level)
-        if in_dim < td.stack_dim:
-            # then we'll stack along a dim before
-            stack_dim = td.stack_dim - 1
-        else:
-            in_dim = in_dim - 1
-            stack_dim = td.stack_dim
-        tds = [
-            td.apply(
-                lambda _arg: _add_batch_dim(_arg, in_dim, vmap_level),
-                batch_size=[b for i, b in enumerate(td.batch_size) if i != in_dim],
-                names=[name for i, name in enumerate(td.names) if i != in_dim],
-            )
-            for td in td.tensordicts
-        ]
-        return LazyStackedTensorDict(*tds, stack_dim=stack_dim)
-
-    @classmethod
-    def _cached_add_batch_dims(cls, td, in_dim, vmap_level):
-        # we return a stack with hook_out, and hack the batch_size and names
-        # Per se it is still a LazyStack but the stacking dim is "hidden" from
-        # the outside
-        out = td.clone(False)
-
-        def hook_out(tensor, in_dim=in_dim, vmap_level=vmap_level):
-            return _add_batch_dim(tensor, in_dim, vmap_level)
-
-        n = len(td.tensordicts)
-
-        def hook_in(
-            tensor,
-            out_dim=in_dim,
-            batch_size=n,
-            vmap_level=vmap_level,
-        ):
-            return _remove_batch_dim(tensor, vmap_level, batch_size, out_dim)
-
-        out.hook_out = hook_out
-        out.hook_in = hook_in
-        out._batch_size = torch.Size(
-            [dim for i, dim in enumerate(out._batch_size) if i != out.stack_dim]
-        )
-        return out
-
-    @cache  # noqa: B019
-    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
-        if self.hook_out is not None:
-            # this is the hacked version. We just need to remove the hook_out and
-            # reset a proper batch size
-            return LazyStackedTensorDict(
-                *self.tensordicts,
-                stack_dim=out_dim,
-            )
-            # return self._cache_remove_batch_dim(vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim)
-        else:
-            # we must call _remove_batch_dim on all tensordicts
-            # batch_size: size of the batch when we unhide it.
-            # out_dim: dimension where the output will be found
-            new_batch_size = list(self.batch_size)
-            new_batch_size.insert(out_dim, batch_size)
-            new_names = list(self.names)
-            new_names.insert(out_dim, None)
-            # rebuild the lazy stack
-            # the stack dim is the same if the out_dim is past it, but it
-            # must be incremented by one otherwise.
-            # In the first case, the out_dim must be decremented by one
-            if out_dim > self.stack_dim:
-                stack_dim = self.stack_dim
-                out_dim = out_dim - 1
-            else:
-                stack_dim = self.stack_dim + 1
-            out = LazyStackedTensorDict(
-                *[
-                    td._remove_batch_dim(
-                        vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
-                    )
-                    for td in self.tensordicts
-                ],
-                stack_dim=stack_dim,
-            )
-        return out
-
-    def get_nestedtensor(
-        self,
-        key: NestedKey,
-        default: str | CompatibleType = NO_DEFAULT,
-    ) -> CompatibleType:
-        """Returns a nested tensor when stacking cannot be achieved.
-
-        Args:
-            key (NestedKey): the entry to nest.
-            default (Any, optiona): the default value to return in case the key
-                isn't in all sub-tensordicts.
-
-                .. note:: In case the default is a tensor, this method will attempt
-                  the construction of a nestedtensor with it. Otherwise, the default
-                  value will be returned.
-
-        Examples:
-            >>> td0 = TensorDict({"a": torch.zeros(4), "b": torch.zeros(4)}, [])
-            >>> td1 = TensorDict({"a": torch.ones(5)}, [])
-            >>> td = torch.stack([td0, td1], 0)
-            >>> a = td.get_nestedtensor("a")
-            >>> # using a tensor as default uses this default to build the nested tensor
-            >>> b = td.get_nestedtensor("b", default=torch.ones(4))
-            >>> assert (a == b).all()
-            >>> # using anything else as default returns the default
-            >>> b2 = td.get_nestedtensor("b", None)
-            >>> assert b2 is None
-
-        """
-        # disallow getting nested tensor if the stacking dimension is not 0
-        if self.stack_dim != 0:
-            raise RuntimeError(
-                "Because nested tensors can only be stacked along their first "
-                "dimension, LazyStackedTensorDict.get_nestedtensor can only be called "
-                "when the stack_dim is 0."
-            )
-
-        # we can handle the case where the key is a tuple of length 1
-        key = _unravel_key_to_tuple(key)
-        subkey = key[0]
-        if len(key) > 1:
-            tensordict = self.get(subkey, default)
-            if tensordict is default:
-                return default
-            return tensordict.get_nestedtensor(key[1:], default=default)
-        tensors = [td.get(subkey, default=default) for td in self.tensordicts]
-        if not isinstance(default, torch.Tensor) and any(
-            tensor is default for tensor in tensors
-        ):
-            # we don't stack but return the default
-            return default
-        return torch.nested.nested_tensor(tensors)
-
-    def is_contiguous(self) -> bool:
-        return False
-
-    def contiguous(self) -> TensorDictBase:
-        source = {key: value.contiguous() for key, value in self.items()}
-        batch_size = self.batch_size
-        device = self.device
-        out = TensorDict(
-            source=source,
-            batch_size=batch_size,
-            device=device,
-            names=self.names,
-            _run_checks=False,
-        )
-        return out
-
-    def clone(self, recurse: bool = True) -> TensorDictBase:
-        if recurse:
-            # This could be optimized using copy but we must be careful with
-            # metadata (_is_shared etc)
-            out = LazyStackedTensorDict(
-                *[td.clone() for td in self.tensordicts],
-                stack_dim=self.stack_dim,
-            )
-        else:
-            out = LazyStackedTensorDict(
-                *[td.clone(recurse=False) for td in self.tensordicts],
-                stack_dim=self.stack_dim,
-            )
-        if self._td_dim_name is not None:
-            out._td_dim_name = self._td_dim_name
-        return out
-
-    def pin_memory(self) -> TensorDictBase:
-        for td in self.tensordicts:
-            td.pin_memory()
-        return self
-
-    def to(self, dest: DeviceType | type, **kwargs) -> TensorDictBase:
-        if isinstance(dest, type) and issubclass(dest, TensorDictBase):
-            if isinstance(self, dest):
-                return self
-            kwargs.update({"batch_size": self.batch_size})
-            out = dest(source=self, **kwargs)
-            # if self._td_dim_name is not None:
-            # TODO: define a _has_names util to quickly check and avoid this
-            out.names = self.names
-            return out
-        elif isinstance(dest, (torch.device, str, int)):
-            dest = torch.device(dest)
-            if self.device is not None and dest == self.device:
-                return self
-            td = self.to_tensordict().to(dest, **kwargs)
-            return td
-
-        elif isinstance(dest, torch.Size):
-            self.batch_size = dest
-        elif dest is None:
-            return self
-        else:
-            raise NotImplementedError(
-                f"dest must be a string, torch.device or a TensorDict "
-                f"instance, {dest} not allowed"
-            )
-
-    def _check_new_batch_size(self, new_size: torch.Size) -> None:
-        if len(new_size) <= self.stack_dim:
-            raise RuntimeError(
-                "Changing the batch_size of a LazyStackedTensorDicts can only "
-                "be done with sizes that are at least as long as the "
-                "stacking dimension."
-            )
-        super()._check_new_batch_size(new_size)
-
-    def _change_batch_size(self, new_size: torch.Size) -> None:
-        if not hasattr(self, "_orig_batch_size"):
-            self._orig_batch_size = self.batch_size
-        elif self._orig_batch_size == new_size:
-            del self._orig_batch_size
-        self._batch_size = new_size
-
-    def keys(
-        self, include_nested: bool = False, leaves_only: bool = False
-    ) -> _LazyStackedTensorDictKeysView:
-        keys = _LazyStackedTensorDictKeysView(
-            self, include_nested=include_nested, leaves_only=leaves_only
-        )
-        return keys
-
-    valid_keys = keys
-
-    # def _iterate_over_keys(self) -> None:
-    #     for key in self.tensordicts[0].keys():
-    #         if all(key in td.keys() for td in self.tensordicts):
-    #             yield key
-    def _iterate_over_keys(self) -> None:
-        # this is about 20x faster than the version above
-        yield from self._key_list()
-
-    @cache  # noqa: B019
-    def _key_list(self):
-        keys = set(self.tensordicts[0].keys())
-        for td in self.tensordicts[1:]:
-            keys = keys.intersection(td.keys())
-        return sorted(keys, key=str)
-
-    def entry_class(self, key: NestedKey) -> type:
-        data_type = type(self.tensordicts[0].get(key))
-        if _is_tensor_collection(data_type):
-            return LazyStackedTensorDict
-        return data_type
-
-    def apply_(self, fn: Callable, *others):
-        for i, td in enumerate(self.tensordicts):
-            idx = (slice(None),) * self.stack_dim + (i,)
-            td.apply_(fn, *[other[idx] for other in others])
-        return self
-
-    def apply(
-        self,
-        fn: Callable,
-        *others: TensorDictBase,
-        batch_size: Sequence[int] | None = None,
-        device: torch.device | None = None,
-        names: Sequence[str] | None = None,
-        inplace: bool = False,
-        **constructor_kwargs,
-    ) -> TensorDictBase:
-        if inplace:
-            if any(arg for arg in (batch_size, device, names, constructor_kwargs)):
-                raise ValueError(
-                    "Cannot pass other arguments to LazyStackedTensorDict.apply when inplace=True."
-                )
-            return self.apply_(fn, *others)
-        else:
-            if batch_size is not None:
-                return super().apply(
-                    fn,
-                    *others,
-                    batch_size=batch_size,
-                    device=device,
-                    names=names,
-                    **constructor_kwargs,
-                )
-            others = (other.unbind(self.stack_dim) for other in others)
-            out = LazyStackedTensorDict(
-                *(
-                    td.apply(fn, *oth, device=device)
-                    for td, *oth in zip(self.tensordicts, *others)
-                ),
-                stack_dim=self.stack_dim,
-            )
-            if names is not None:
-                out.names = names
-            return out
-
-    def select(
-        self, *keys: str, inplace: bool = False, strict: bool = False
-    ) -> LazyStackedTensorDict:
-        # the following implementation keeps the hidden keys in the tensordicts
-        tensordicts = [
-            td.select(*keys, inplace=inplace, strict=strict) for td in self.tensordicts
-        ]
-        if inplace:
-            return self
-        return LazyStackedTensorDict(*tensordicts, stack_dim=self.stack_dim)
-
-    def exclude(self, *keys: str, inplace: bool = False) -> LazyStackedTensorDict:
-        tensordicts = [
-            tensordict.exclude(*keys, inplace=inplace)
-            for tensordict in self.tensordicts
-        ]
-        if inplace:
-            self.tensordicts = tensordicts
-            return self
-        return torch.stack(tensordicts, dim=self.stack_dim)
-
-    def __setitem__(self, index: IndexType, value: TensorDictBase) -> TensorDictBase:
-        if isinstance(index, (tuple, str)):
-            # try:
-            index_unravel = _unravel_key_to_tuple(index)
-            if index_unravel:
-                self._set_tuple(
-                    index_unravel,
-                    value,
-                    inplace=BEST_ATTEMPT_INPLACE
-                    if isinstance(self, SubTensorDict)
-                    else False,
-                    validated=False,
-                )
-                return
-
-            if any(isinstance(sub_index, (list, range)) for sub_index in index):
-                index = tuple(
-                    torch.tensor(sub_index, device=self.device)
-                    if isinstance(sub_index, (list, range))
-                    else sub_index
-                    for sub_index in index
-                )
-
-        if index is Ellipsis or (isinstance(index, tuple) and Ellipsis in index):
-            index = convert_ellipsis_to_idx(index, self.batch_size)
-        elif isinstance(index, (list, range)):
-            index = torch.tensor(index, device=self.device)
-
-        if isinstance(value, (TensorDictBase, dict)):
-            indexed_bs = _getitem_batch_size(self.batch_size, index)
-            if isinstance(value, dict):
-                value = TensorDict(
-                    value, batch_size=indexed_bs, device=self.device, _run_checks=False
-                )
-            if value.batch_size != indexed_bs:
-                # try to expand
-                try:
-                    value = value.expand(indexed_bs)
-                except RuntimeError as err:
-                    raise RuntimeError(
-                        f"indexed destination TensorDict batch size is {indexed_bs} "
-                        f"(batch_size = {self.batch_size}, index={index}), "
-                        f"which differs from the source batch size {value.batch_size}"
-                    ) from err
-            split_index = self._split_index(index)
-            converted_idx = split_index["index_dict"]
-            num_single = split_index["num_single"]
-            isinteger = split_index["isinteger"]
-            has_bool = split_index["has_bool"]
-            num_squash = split_index.get("num_squash", 0)
-            num_none = split_index.get("num_none", 0)
-            is_nd_tensor = split_index.get("is_nd_tensor", False)
-            if isinteger:
-                # this will break if the index along the stack dim is [0] or :1 or smth
-                for i, _idx in converted_idx.items():
-                    self.tensordicts[i][_idx] = value
-                return self
-            if is_nd_tensor:
-                raise RuntimeError(
-                    "Indexing along stack dim with a non-boolean tensor is not supported yet. "
-                    "Use SubTensorDict instead."
-                )
-            if not has_bool:
-                unbind_dim = self.stack_dim - num_single + num_none - num_squash
-                value_unbind = value.unbind(unbind_dim)
-                for (i, _idx), _value in zip(
-                    converted_idx.items(),
-                    value_unbind,
-                ):
-                    self.tensordicts[i][_idx] = _value
-            else:
-                # we must split, not unbind
-                mask_unbind = split_index["individual_masks"]
-                split_dim = split_index["split_dim"]
-                splits = [_mask_unbind.sum().item() for _mask_unbind in mask_unbind]
-                value_unbind = value.split(splits, split_dim)
-                if mask_unbind[0].ndim == 0:
-                    # we can return a stack
-                    for (i, _idx), mask, _value in zip(
-                        converted_idx.items(),
-                        mask_unbind,
-                        value_unbind,
-                    ):
-                        if mask.any():
-                            self.tensordicts[i][_idx] = _value
-                else:
-                    for (i, _idx), _value in zip(converted_idx.items(), value_unbind):
-                        self_idx = (slice(None),) * split_index["mask_loc"] + (i,)
-                        self[self_idx][_idx] = _value
-        else:
-            for key in self.keys():
-                self.set_at_(key, value, index)
-
-    def __contains__(self, item: IndexType) -> bool:
-        if isinstance(item, TensorDictBase):
-            return any(item is td for td in self.tensordicts)
-        return super().__contains__(item)
-
-    def __getitem__(self, index: IndexType) -> TensorDictBase:
-        if isinstance(index, (tuple, str)):
-            index_key = _unravel_key_to_tuple(index)
-            if index_key:
-                return self._get_tuple(index_key, NO_DEFAULT)
-        split_index = self._split_index(index)
-        converted_idx = split_index["index_dict"]
-        isinteger = split_index["isinteger"]
-        has_bool = split_index["has_bool"]
-        is_nd_tensor = split_index["is_nd_tensor"]
-        num_single = split_index.get("num_single", 0)
-        num_none = split_index.get("num_none", 0)
-        num_squash = split_index.get("num_squash", 0)
-        if has_bool:
-            mask_unbind = split_index["individual_masks"]
-            cat_dim = split_index["mask_loc"] - num_single
-            out = []
-            if mask_unbind[0].ndim == 0:
-                # we can return a stack
-                for (i, _idx), mask in zip(converted_idx.items(), mask_unbind):
-                    if mask.any():
-                        if mask.all() and self.tensordicts[i].ndim == 0:
-                            out.append(self.tensordicts[i])
-                        else:
-                            out.append(self.tensordicts[i][_idx])
-                            out[-1] = out[-1].squeeze(cat_dim)
-                return torch.stack(out, cat_dim)
-            else:
-                for i, _idx in converted_idx.items():
-                    self_idx = (slice(None),) * split_index["mask_loc"] + (i,)
-                    out.append(self[self_idx][_idx])
-                return torch.cat(out, cat_dim)
-        elif is_nd_tensor:
-            new_stack_dim = self.stack_dim - num_single + num_none
-            return torch.stack(
-                [self[idx] for idx in converted_idx.values()], new_stack_dim
-            )
-        else:
-            if isinteger:
-                for (
-                    i,
-                    _idx,
-                ) in (
-                    converted_idx.items()
-                ):  # for convenience but there's only one element
-                    out = self.tensordicts[i]
-                    if _idx is not None and _idx != ():
-                        out = out[_idx]
-                    return out
-            else:
-                out = []
-                new_stack_dim = self.stack_dim - num_single + num_none - num_squash
-                for i, _idx in converted_idx.items():
-                    out.append(self.tensordicts[i][_idx])
-                out = torch.stack(out, new_stack_dim)
-                out._td_dim_name = self._td_dim_name
-                return out
-
-        # index_dict = _convert_index_lazystack(index, self.stack_dim, self.batch_size)
-        # if index_dict is None:
-        #     # then we use a sub-tensordict
-        #     return self.get_sub_tensordict(index)
-        # td_index = index_dict["remaining_index"]
-        # stack_index = index_dict["stack_index"]
-        # new_stack_dim = index_dict["new_stack_dim"]
-        # if new_stack_dim is not None:
-        #     if isinstance(stack_index, slice):
-        #         # we can't iterate but we can index the list directly
-        #         out = LazyStackedTensorDict(
-        #             *[td[td_index] for td in self.tensordicts[stack_index]],
-        #             stack_dim=new_stack_dim,
-        #         )
-        #     elif isinstance(stack_index, (list, range)):
-        #         # then we can iterate
-        #         out = LazyStackedTensorDict(
-        #             *[self.tensordicts[idx][td_index] for idx in stack_index],
-        #             stack_dim=new_stack_dim,
-        #         )
-        #     elif isinstance(stack_index, Tensor):
-        #         # td_index is a nested tuple that mimics the shape of stack_index
-        #         def _nested_stack(t: list, stack_idx: Tensor, td_index):
-        #             if stack_idx.ndim:
-        #                 out = LazyStackedTensorDict(
-        #                     *[
-        #                         _nested_stack(t, _idx, td_index[i])
-        #                         for i, _idx in enumerate(stack_idx.unbind(0))
-        #                     ],
-        #                     stack_dim=new_stack_dim,
-        #                 )
-        #                 return out
-        #             return t[stack_idx][td_index]
-        #
-        #         # print(index, td_index, stack_index)
-        #         out = _nested_stack(self.tensordicts, stack_index, td_index)
-        #     else:
-        #         raise TypeError("Invalid index used for stack dimension.")
-        #     out._td_dim_name = self._td_dim_name
-        #     return out
-        # out = self.tensordicts[stack_index]
-        # if td_index:
-        #     return out[td_index]
-        # return out
-
-    # def __hash__(self):
-    #     return hash(self.tensordicts)
-
-    def __eq__(self, other):
-        if is_tensorclass(other):
-            return other == self
-        if isinstance(other, (dict,)):
-            other = TensorDict.from_dict(other)
-        if _is_tensor_collection(other.__class__):
-            out = []
-            for i, td in enumerate(self.tensordicts):
-                idx = (slice(None),) * self.stack_dim + (i,)
-                out.append(other[idx] == td)
-            return torch.stack(out, self.stack_dim)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return torch.stack(
-                [td == other for td in self.tensordicts],
-                self.stack_dim,
-            )
-        return False
-
-    def __ne__(self, other):
-        if is_tensorclass(other):
-            return other != self
-        if isinstance(other, (dict,)):
-            other = TensorDict.from_dict(other)
-        if _is_tensor_collection(other.__class__):
-            out = []
-            for i, td in enumerate(self.tensordicts):
-                idx = (slice(None),) * self.stack_dim + (i,)
-                out.append(other[idx] != td)
-            return torch.stack(out, self.stack_dim)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return torch.stack(
-                [td != other for td in self.tensordicts],
-                self.stack_dim,
-            )
-        return True
-
-    def all(self, dim: int = None) -> bool | TensorDictBase:
-        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
-            raise RuntimeError(
-                "dim must be greater than or equal to -tensordict.batch_dims and "
-                "smaller than tensordict.batch_dims"
-            )
-        if dim is not None:
-            # TODO: we need to adapt this to LazyStackedTensorDict too
-            if dim < 0:
-                dim = self.batch_dims + dim
-            return TensorDict(
-                source={key: value.all(dim=dim) for key, value in self.items()},
-                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
-                device=self.device,
-            )
-        return all(value.all() for value in self.tensordicts)
-
-    def any(self, dim: int = None) -> bool | TensorDictBase:
-        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
-            raise RuntimeError(
-                "dim must be greater than or equal to -tensordict.batch_dims and "
-                "smaller than tensordict.batch_dims"
-            )
-        if dim is not None:
-            # TODO: we need to adapt this to LazyStackedTensorDict too
-            if dim < 0:
-                dim = self.batch_dims + dim
-            return TensorDict(
-                source={key: value.any(dim=dim) for key, value in self.items()},
-                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
-                device=self.device,
-            )
-        return any(value.any() for value in self.tensordicts)
-
-    def _send(self, dst: int, _tag: int = -1, pseudo_rand: bool = False) -> int:
-        for td in self.tensordicts:
-            _tag = td._send(dst, _tag=_tag, pseudo_rand=pseudo_rand)
-        return _tag
-
-    def _isend(
-        self,
-        dst: int,
-        _tag: int = -1,
-        _futures: list[torch.Future] | None = None,
-        pseudo_rand: bool = False,
-    ) -> int:
-        if _futures is None:
-            is_root = True
-            _futures = []
-        else:
-            is_root = False
-        for td in self.tensordicts:
-            _tag = td._isend(dst, _tag=_tag, pseudo_rand=pseudo_rand, _futures=_futures)
-        if is_root:
-            for future in _futures:
-                future.wait()
-        return _tag
-
-    def _recv(self, src: int, _tag: int = -1, pseudo_rand: bool = False) -> int:
-        for td in self.tensordicts:
-            _tag = td._recv(src, _tag=_tag, pseudo_rand=pseudo_rand)
-        return _tag
-
-    def _irecv(
-        self,
-        src: int,
-        return_premature: bool = False,
-        _tag: int = -1,
-        _future_list: list[torch.Future] = None,
-        pseudo_rand: bool = False,
-    ) -> tuple[int, list[torch.Future]] | list[torch.Future] | None:
-        root = False
-        if _future_list is None:
-            _future_list = []
-            root = True
-        for td in self.tensordicts:
-            _tag, _future_list = td._irecv(
-                src=src,
-                return_premature=return_premature,
-                _tag=_tag,
-                _future_list=_future_list,
-                pseudo_rand=pseudo_rand,
-            )
-
-        if not root:
-            return _tag, _future_list
-        elif return_premature:
-            return _future_list
-        else:
-            for future in _future_list:
-                future.wait()
-            return
-
-    @lock_blocked
-    def del_(self, key: NestedKey, **kwargs: Any) -> TensorDictBase:
-        ids = set()
-        cur_len = len(ids)
-        is_deleted = False
-        error = None
-        for td in self.tensordicts:
-            # checking that the td has not been processed yet.
-            # It could be that not all sub-tensordicts have the appropriate
-            # entry but one must have it (or an error is thrown).
-            tdid = id(td)
-            ids.add(tdid)
-            new_cur_len = len(ids)
-            if new_cur_len == cur_len:
-                continue
-            cur_len = new_cur_len
-            try:
-                td.del_(key, **kwargs)
-                is_deleted = True
-            except KeyError as err:
-                error = err
-                continue
-        if not is_deleted:
-            # we know err is defined because LazyStackedTensorDict cannot be empty
-            raise error
-        return self
-
-    def pop(
-        self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
-    ) -> CompatibleType:
-        # using try/except for get/del is suboptimal, but
-        # this is faster that checkink if key in self keys
-        key = _unravel_key_to_tuple(key)
-        if len(key) == 1:
-            key = key[0]
-        present = False
-        if isinstance(key, tuple):
-            if key in self.keys(True):
-                present = True
-                value = self._get_tuple(key, NO_DEFAULT)
-        elif key in self.keys():
-            present = True
-            value = self._get_str(key, NO_DEFAULT)
-        if present:
-            self.del_(key)
-        elif default is not NO_DEFAULT:
-            value = default
-        else:
-            raise KeyError(
-                f"You are trying to pop key `{key}` which is not in dict "
-                f"without providing default value."
-            )
-        return value
-
-    def share_memory_(self) -> TensorDictBase:
-        for td in self.tensordicts:
-            td.share_memory_()
-        self._is_shared = True
-        self.lock_()
-        return self
-
-    def detach_(self) -> TensorDictBase:
-        for td in self.tensordicts:
-            td.detach_()
-        return self
-
-    def memmap_(
-        self, prefix: str | None = None, copy_existing: bool = False
-    ) -> TensorDictBase:
-        if prefix is not None:
-            prefix = Path(prefix)
-            if not prefix.exists():
-                os.makedirs(prefix, exist_ok=True)
-            torch.save({"stack_dim": self.stack_dim}, prefix / "meta.pt")
-        for i, td in enumerate(self.tensordicts):
-            td.memmap_(
-                prefix=(prefix / str(i)) if prefix is not None else None,
-                copy_existing=copy_existing,
-            )
-        self._is_memmap = True
-        self.lock_()
-        return self
-
-    def memmap_like(
-        self,
-        prefix: str | None = None,
-    ) -> TensorDictBase:
-        tds = []
-        if prefix is not None:
-            prefix = Path(prefix)
-            if not prefix.exists():
-                os.makedirs(prefix, exist_ok=True)
-            torch.save({"stack_dim": self.stack_dim}, prefix / "meta.pt")
-        for i, td in enumerate(self.tensordicts):
-            td_like = td.memmap_like(
-                prefix=(prefix / str(i)) if prefix is not None else None,
-            )
-            tds.append(td_like)
-        td_out = torch.stack(tds, self.stack_dim)
-        td_out._is_memmap = True
-        td_out.lock_()
-        return td_out
-
-    @classmethod
-    def load_memmap(cls, prefix: str) -> LazyStackedTensorDict:
-        prefix = Path(prefix)
-        tensordicts = []
-        i = 0
-        while (prefix / str(i)).exists():
-            tensordicts.append(TensorDict.load_memmap(prefix / str(i)))
-            i += 1
-
-        metadata = torch.load(prefix / "meta.pt")
-        return cls(*tensordicts, stack_dim=metadata["stack_dim"])
-
-    def expand(self, *shape: int, inplace: bool = False) -> TensorDictBase:
-        if len(shape) == 1 and isinstance(shape[0], Sequence):
-            shape = tuple(shape[0])
-        stack_dim = len(shape) + self.stack_dim - self.ndimension()
-        new_shape_tensordicts = [v for i, v in enumerate(shape) if i != stack_dim]
-        tensordicts = [td.expand(*new_shape_tensordicts) for td in self.tensordicts]
-        if inplace:
-            self.tensordicts = tensordicts
-            self.stack_dim = stack_dim
-            return self
-        return torch.stack(tensordicts, stack_dim)
-
-    def update(
-        self, input_dict_or_td: TensorDictBase, clone: bool = False, **kwargs: Any
-    ) -> TensorDictBase:
-        if input_dict_or_td is self:
-            # no op
-            return self
-
-        if (
-            isinstance(input_dict_or_td, LazyStackedTensorDict)
-            and input_dict_or_td.stack_dim == self.stack_dim
-        ):
-            if not input_dict_or_td.shape[self.stack_dim] == len(self.tensordicts):
-                raise ValueError(
-                    "cannot update stacked tensordicts with different shapes."
-                )
-            for td_dest, td_source in zip(
-                self.tensordicts, input_dict_or_td.tensordicts
-            ):
-                td_dest.update(td_source, clone=clone, **kwargs)
-            return self
-
-        keys = self.keys(False)
-        for key, value in input_dict_or_td.items():
-            if clone and hasattr(value, "clone"):
-                value = value.clone()
-            else:
-                value = tree_map(torch.clone, value)
-            if isinstance(key, tuple):
-                key, subkey = key[0], key[1:]
-            else:
-                subkey = ()
-            # the key must be a string by now. Let's check if it is present
-            if key in keys:
-                target_class = self.entry_class(key)
-                if _is_tensor_collection(target_class):
-                    if isinstance(value, dict):
-                        value_unbind = TensorDict(
-                            value, self.batch_size, _run_checks=False
-                        ).unbind(self.stack_dim)
-                    else:
-                        value_unbind = value.unbind(self.stack_dim)
-                    for t, _value in zip(self.tensordicts, value_unbind):
-                        if len(subkey):
-                            t.update({key: {subkey: _value}}, clone=clone, **kwargs)
-                        else:
-                            t.update({key: _value}, clone=clone, **kwargs)
-                    continue
-            if len(subkey):
-                self.set((key, *subkey), value, **kwargs)
-            else:
-                self.set(key, value, **kwargs)
-        return self
-
-    def update_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
-        clone: bool = False,
-        **kwargs: Any,
-    ) -> TensorDictBase:
-        if input_dict_or_td is self:
-            # no op
-            return self
-        if isinstance(input_dict_or_td, LazyStackedTensorDict):
-            if input_dict_or_td.stack_dim == self.stack_dim:
-                if not input_dict_or_td.shape[self.stack_dim] == len(self.tensordicts):
-                    raise ValueError(
-                        "cannot update stacked tensordicts with different shapes."
-                    )
-                for td_dest, td_source in zip(
-                    self.tensordicts, input_dict_or_td.tensordicts
-                ):
-                    td_dest.update_(td_source)
-                return self
-            else:
-                for i, td in enumerate(input_dict_or_td.tensordicts):
-                    idx = (slice(None),) * input_dict_or_td.stack_dim + (i,)
-                    self.update_at_(td, idx)
-        for key, value in input_dict_or_td.items():
-            if not isinstance(value, tuple(_ACCEPTED_CLASSES)):
-                raise TypeError(
-                    f"Expected value to be one of types {_ACCEPTED_CLASSES} "
-                    f"but got {type(value)}"
-                )
-            if clone:
-                value = value.clone()
-            self.set_(key, value, **kwargs)
-        return self
-
-    def update_at_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
-        index: IndexType,
-        clone: bool = False,
-    ) -> TensorDictBase:
-        if isinstance(input_dict_or_td, TensorDictBase):
-            split_index = self._split_index(index)
-            converted_idx = split_index["index_dict"]
-            num_single = split_index["num_single"]
-            isinteger = split_index["isinteger"]
-            if isinteger:
-                # this will break if the index along the stack dim is [0] or :1 or smth
-                for i, _idx in converted_idx.items():
-                    self.tensordicts[i].update_at_(
-                        input_dict_or_td,
-                        _idx,
-                    )
-                return self
-            unbind_dim = self.stack_dim - num_single
-            for (i, _idx), _value in zip(
-                converted_idx.items(),
-                input_dict_or_td.unbind(unbind_dim),
-            ):
-                self.tensordicts[i].update_at_(
-                    _value,
-                    _idx,
-                )
-            return self
-        for key, value in input_dict_or_td.items():
-            if not isinstance(value, _ACCEPTED_CLASSES):
-                raise TypeError(
-                    f"Expected value to be one of types {_ACCEPTED_CLASSES} "
-                    f"but got {type(value)}"
-                )
-            if clone:
-                value = value.clone()
-            self.set_at_(key, value, index)
-        return self
-
-    def rename_key_(
-        self, old_key: str, new_key: str, safe: bool = False
-    ) -> TensorDictBase:
-        def sort_keys(element):
-            if isinstance(element, tuple):
-                return "_-|-_".join(element)
-            return element
-
-        for td in self.tensordicts:
-            td.rename_key_(old_key, new_key, safe=safe)
-        return self
-
-    rename_key = _renamed_inplace_method(rename_key_)
-
-    def masked_fill_(self, mask: Tensor, value: float | bool) -> TensorDictBase:
-        mask_unbind = mask.unbind(dim=self.stack_dim)
-        for _mask, td in zip(mask_unbind, self.tensordicts):
-            td.masked_fill_(_mask, value)
-        return self
-
-    def masked_fill(self, mask: Tensor, value: float | bool) -> TensorDictBase:
-        td_copy = self.clone()
-        return td_copy.masked_fill_(mask, value)
-
-    @lock_blocked
-    def insert(self, index: int, tensordict: TensorDictBase) -> None:
-        """Insert a TensorDict into the stack at the specified index.
-
-        Analogous to list.insert. The inserted TensorDict must have compatible
-        batch_size and device. Insertion is in-place, nothing is returned.
-
-        Args:
-            index (int): The index at which the new TensorDict should be inserted.
-            tensordict (TensorDictBase): The TensorDict to be inserted into the stack.
-
-        """
-        if not isinstance(tensordict, TensorDictBase):
-            raise TypeError(
-                "Expected new value to be TensorDictBase instance but got "
-                f"{type(tensordict)} instead."
-            )
-
-        batch_size = self.tensordicts[0].batch_size
-        device = self.tensordicts[0].device
-
-        _batch_size = tensordict.batch_size
-        _device = tensordict.device
-
-        if device != _device:
-            raise ValueError(
-                f"Devices differ: stack has device={device}, new value has "
-                f"device={_device}."
-            )
-        if _batch_size != batch_size:
-            raise ValueError(
-                f"Batch sizes in tensordicts differs: stack has "
-                f"batch_size={batch_size}, new_value has batch_size={_batch_size}."
-            )
-
-        self.tensordicts.insert(index, tensordict)
-
-        N = len(self.tensordicts)
-        self._batch_size = self._compute_batch_size(batch_size, self.stack_dim, N)
-
-    @lock_blocked
-    def append(self, tensordict: TensorDictBase) -> None:
-        """Append a TensorDict onto the stack.
-
-        Analogous to list.append. The appended TensorDict must have compatible
-        batch_size and device. The append operation is in-place, nothing is returned.
-
-        Args:
-            tensordict (TensorDictBase): The TensorDict to be appended onto the stack.
-
-        """
-        self.insert(len(self.tensordicts), tensordict)
-
-    @property
-    def is_locked(self) -> bool:
-        if self._is_locked is not None:
-            # if tensordicts have been locked through this Lazy stack, then we can
-            # trust this lazy stack to contain the info.
-            # In all other cases we must check
-            return self._is_locked
-        # If any of the tensordicts is not locked, we assume that the lazy stack
-        # is not locked either. Caching is then disabled and
-        for td in self.tensordicts:
-            if not td.is_locked:
-                return False
-        else:
-            # In this case, all tensordicts were locked before the lazy stack
-            # was created and they were not locked through the lazy stack.
-            # This means we cannot cache the value because this lazy stack
-            # if not part of the graph. We don't want it to be part of the graph
-            # because this object being locked is only a side-effect.
-            # Calling self.lock_() here could however speed things up.
-            return True
-
-    @is_locked.setter
-    def is_locked(self, value: bool) -> None:
-        if value:
-            self.lock_()
-        else:
-            self.unlock_()
-
-    @property
-    def _lock_id(self):
-        """Ids of all tensordicts that need to be unlocked for this to be unlocked."""
-        _lock_id = set()
-        for tensordict in self.tensordicts:
-            _lock_id = _lock_id.union(tensordict._lock_id)
-        _lock_id = _lock_id - {id(self)}
-        return _lock_id
-
-    def _lock_propagate(self, lock_ids=None):
-        """Registers the parent tensordict that handles the lock."""
-        self._is_locked = True
-        _locked_tensordicts = []
-        is_root = lock_ids is None
-        if is_root:
-            lock_ids = set()
-        lock_ids = lock_ids.union({id(self)})
-        for dest in self.tensordicts:
-            dest._lock_propagate(lock_ids)
-            _locked_tensordicts.append(dest)
-
-    def _remove_lock(self, lock_id):
-        for td in self.tensordicts:
-            td._remove_lock(lock_id)
-
-    @erase_cache
-    def _propagate_unlock(self, lock_ids=None):
-        # we can't set _is_locked to False because after it's unlocked, anything
-        # can happen to a child tensordict.
-        self._is_locked = None
-        if lock_ids is None:
-            lock_ids = set()
-
-        unlocked_tds = [self]
-        lock_ids.add(id(self))
-        for dest in self.tensordicts:
-            unlocked_tds.extend(dest._propagate_unlock(lock_ids))
-
-        self._is_shared = False
-        self._is_memmap = False
-        return unlocked_tds
-
-    def __del__(self):
-        if self._is_locked is None:
-            # then we can reliably say that this lazy stack is not part of
-            # the tensordicts graphs
-            return
-        # this can be a perf bottleneck
-        for td in self.tensordicts:
-            td._remove_lock(id(self))
-
-    def __repr__(self):
-        fields = _td_fields(self)
-        field_str = indent(f"fields={{{fields}}}", 4 * " ")
-        exclusive_fields_str = indent(
-            f"exclusive_fields={{{self._repr_exclusive_fields()}}}", 4 * " "
-        )
-        batch_size_str = indent(f"batch_size={self.batch_size}", 4 * " ")
-        device_str = indent(f"device={self.device}", 4 * " ")
-        is_shared_str = indent(f"is_shared={self.is_shared()}", 4 * " ")
-        stack_dim = indent(f"stack_dim={self.stack_dim}", 4 * " ")
-        string = ",\n".join(
-            [
-                field_str,
-                exclusive_fields_str,
-                batch_size_str,
-                device_str,
-                is_shared_str,
-                stack_dim,
-            ]
-        )
-        return f"{type(self).__name__}(\n{string})"
-
-    def _repr_exclusive_fields(self):
-        keys = set(self.keys())
-        exclusive_keys = [
-            _td_fields(td, [k for k in td.keys() if k not in keys])
-            for td in self.tensordicts
-        ]
-        exclusive_key_str = ",\n".join(
-            [
-                indent(f"{i} ->{line}", 4 * " ")
-                for i, line in enumerate(exclusive_keys)
-                if line != "\n"
-            ]
-        )
-
-        return "\n" + exclusive_key_str
-
-    lock_ = TensorDictBase.lock_
-    lock = _renamed_inplace_method(lock_)
-
-    unlock_ = TensorDictBase.unlock_
-    unlock = _renamed_inplace_method(unlock_)
-
-
-class _CustomOpTensorDict(TensorDictBase):
-    """Encodes lazy operations on tensors contained in a TensorDict."""
-
-    def __new__(cls, *args: Any, **kwargs: Any) -> _CustomOpTensorDict:
-        return super().__new__(cls, *args, _safe=False, _lazy=True, **kwargs)
-
-    def __init__(
-        self,
-        source: TensorDictBase,
-        custom_op: str,
-        inv_op: str | None = None,
-        custom_op_kwargs: dict | None = None,
-        inv_op_kwargs: dict | None = None,
-        batch_size: Sequence[int] | None = None,
-    ) -> None:
-        self._is_shared = source.is_shared()
-        self._is_memmap = source.is_memmap()
-
-        if not isinstance(source, TensorDictBase):
-            raise TypeError(
-                f"Expected source to be a TensorDictBase isntance, "
-                f"but got {type(source)} instead."
-            )
-        self._source = source
-        self.custom_op = custom_op
-        self.inv_op = inv_op
-        self.custom_op_kwargs = custom_op_kwargs if custom_op_kwargs is not None else {}
-        self.inv_op_kwargs = inv_op_kwargs if inv_op_kwargs is not None else {}
-        self._batch_size = None
-        if batch_size is not None and batch_size != self.batch_size:
-            raise RuntimeError("batch_size does not match self.batch_size.")
-
-    # def __hash__(self):
-    #     return hash((self._source, self.custom_op, self.inv_op, self.custom_op_kwargs, self.inv_op_kwargs))
-
-    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
-        """Allows for a transformation to be customized for a certain shape, device or dtype.
-
-        By default, this is a no-op on self.custom_op_kwargs
-
-        Args:
-            source_tensor: corresponding Tensor
-
-        Returns:
-            a dictionary with the kwargs of the operation to execute
-            for the tensor
-
-        """
-        return self.custom_op_kwargs
-
-    def _update_inv_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
-        """Allows for an inverse transformation to be customized for a certain shape, device or dtype.
-
-        By default, this is a no-op on self.inv_op_kwargs
-
-        Args:
-            source_tensor: corresponding tensor
-
-        Returns:
-            a dictionary with the kwargs of the operation to execute for
-            the tensor
-
-        """
-        return self.inv_op_kwargs
-
-    def entry_class(self, key: NestedKey) -> type:
-        return type(self._source.get(key))
-
-    @property
-    def device(self) -> torch.device | None:
-        return self._source.device
-
-    @device.setter
-    def device(self, value: DeviceType) -> None:
-        self._source.device = value
-
-    @property
-    def batch_size(self) -> torch.Size:
-        if self._batch_size is None:
-            self._batch_size = getattr(
-                torch.zeros(self._source.batch_size, device="meta"), self.custom_op
-            )(**self.custom_op_kwargs).shape
-        return self._batch_size
-
-    @batch_size.setter
-    def batch_size(self, new_size: torch.Size) -> None:
-        self._batch_size_setter(new_size)
-
-    def _has_names(self):
-        return self._source._has_names()
-
-    def _erase_names(self):
-        raise RuntimeError(
-            f"Cannot erase names of a {type(self)}. "
-            f"Erase source TensorDict's names instead."
-        )
-
-    def _rename_subtds(self, names):
-        for key in self.keys():
-            if _is_tensor_collection(self.entry_class(key)):
-                raise RuntimeError(
-                    "Cannot rename dimensions of a lazy TensorDict with "
-                    "nested collections. Convert the instance to a regular "
-                    "tensordict by using the `to_tensordict()` method first."
-                )
-
-    def _change_batch_size(self, new_size: torch.Size) -> None:
-        if not hasattr(self, "_orig_batch_size"):
-            self._orig_batch_size = self.batch_size
-        elif self._orig_batch_size == new_size:
-            del self._orig_batch_size
-        self._batch_size = new_size
-
-    def _get_str(self, key, default):
-        tensor = self._source._get_str(key, default)
-        if tensor is default:
-            return tensor
-        return self._transform_value(tensor)
-
-    def _get_tuple(self, key, default):
-        tensor = self._source._get_tuple(key, default)
-        if tensor is default:
-            return tensor
-        return self._transform_value(tensor)
-
-    def _transform_value(self, item):
-        return getattr(item, self.custom_op)(**self._update_custom_op_kwargs(item))
-
-    def _set_str(self, key, value, *, inplace: bool, validated: bool):
-        if not validated:
-            value = self._validate_value(value, check_shape=True)
-            validated = True
-        value = getattr(value, self.inv_op)(**self._update_inv_op_kwargs(value))
-        self._source._set_str(key, value, inplace=inplace, validated=validated)
-        return self
-
-    def _set_tuple(self, key, value, *, inplace: bool, validated: bool):
-        if len(key) == 1:
-            return self._set_str(key[0], value, inplace=inplace, validated=validated)
-        source = self._source._get_str(key[0], None)
-        if source is None:
-            self._source._create_nested_str(key[0])
-            source = self._source._get_str(key[0], NO_DEFAULT)
-        nested = type(self)(
-            source,
-            custom_op=self.custom_op,
-            inv_op=self.inv_op,
-            custom_op_kwargs=self._update_custom_op_kwargs(source),
-            inv_op_kwargs=self._update_inv_op_kwargs(source),
-        )
-        nested._set_tuple(key[1:], value, inplace=inplace, validated=validated)
-        return self
-
-    def _set_at_str(self, key, value, idx, *, validated):
-        transformed_tensor, original_tensor = self._get_str(
-            key, NO_DEFAULT
-        ), self._source._get_str(key, NO_DEFAULT)
-        if transformed_tensor.data_ptr() != original_tensor.data_ptr():
-            raise RuntimeError(
-                f"{self} original tensor and transformed_in do not point to the "
-                f"same storage. Setting values in place is not currently "
-                f"supported in this setting, consider calling "
-                f"`td.clone()` before `td.set_at_(...)`"
-            )
-        transformed_tensor[idx] = value
-        return self
-
-    def _set_at_tuple(self, key, value, idx, *, validated):
-        transformed_tensor, original_tensor = self._get_tuple(
-            key, NO_DEFAULT
-        ), self._source._get_tuple(key, NO_DEFAULT)
-        if transformed_tensor.data_ptr() != original_tensor.data_ptr():
-            raise RuntimeError(
-                f"{self} original tensor and transformed_in do not point to the "
-                f"same storage. Setting values in place is not currently "
-                f"supported in this setting, consider calling "
-                f"`td.clone()` before `td.set_at_(...)`"
-            )
-        if not validated:
-            value = self._validate_value(value, check_shape=False)
-
-        transformed_tensor[idx] = value
-        return self
-
-    def _stack_onto_(
-        self,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> TensorDictBase:
-        raise RuntimeError(
-            f"stacking tensordicts is not allowed for type {type(self)}"
-            f"consider calling 'to_tensordict()` first"
-        )
-
-    def __repr__(self) -> str:
-        custom_op_kwargs_str = ", ".join(
-            [f"{key}={value}" for key, value in self.custom_op_kwargs.items()]
-        )
-        indented_source = textwrap.indent(f"source={self._source}", "\t")
-        return (
-            f"{self.__class__.__name__}(\n{indented_source}, "
-            f"\n\top={self.custom_op}({custom_op_kwargs_str}))"
-        )
-
-    # @cache  # noqa: B019
-    def keys(
-        self, include_nested: bool = False, leaves_only: bool = False
-    ) -> _TensorDictKeysView:
-        return self._source.keys(include_nested=include_nested, leaves_only=leaves_only)
-
-    def select(
-        self, *keys: str, inplace: bool = False, strict: bool = True
-    ) -> _CustomOpTensorDict:
-        if inplace:
-            self._source.select(*keys, inplace=inplace, strict=strict)
-            return self
-        self_copy = copy(self)
-        self_copy._source = self_copy._source.select(*keys, strict=strict)
-        return self_copy
-
-    def exclude(self, *keys: str, inplace: bool = False) -> TensorDictBase:
-        if inplace:
-            return super().exclude(*keys, inplace=True)
-        return TensorDict(
-            {key: value.clone() for key, value in self.items()},
-            batch_size=self.batch_size,
-            device=self.device,
-            _run_checks=False,
-            _is_memmap=self.is_memmap(),
-            _is_shared=self.is_shared(),
-        ).exclude(*keys, inplace=True)
-
-    def clone(self, recurse: bool = True) -> TensorDictBase:
-        """Clones the Lazy TensorDict.
-
-        Args:
-            recurse (bool, optional): if ``True`` (default), a regular
-                :class:`TensorDict` instance will be returned.
-                Otherwise, another :class:`SubTensorDict` with identical content
-                will be returned.
-        """
-        if not recurse:
-            return type(self)(
-                source=self._source.clone(False),
-                custom_op=self.custom_op,
-                inv_op=self.inv_op,
-                custom_op_kwargs=self.custom_op_kwargs,
-                inv_op_kwargs=self.inv_op_kwargs,
-                batch_size=self.batch_size,
-            )
-        return self.to_tensordict()
-
-    def is_contiguous(self) -> bool:
-        return all([value.is_contiguous() for _, value in self.items()])
-
-    def contiguous(self) -> TensorDictBase:
-        if self.is_contiguous():
-            return self
-        return self.to(TensorDict)
-
-    def rename_key_(
-        self, old_key: str, new_key: str, safe: bool = False
-    ) -> _CustomOpTensorDict:
-        self._source.rename_key_(old_key, new_key, safe=safe)
-        return self
-
-    rename_key = _renamed_inplace_method(rename_key_)
-
-    @lock_blocked
-    def del_(self, key: NestedKey) -> _CustomOpTensorDict:
-        self._source = self._source.del_(key)
-        return self
-
-    def to(self, dest: DeviceType | type, **kwargs) -> TensorDictBase:
-        if isinstance(dest, type) and issubclass(dest, TensorDictBase):
-            if isinstance(self, dest):
-                return self
-            out = dest(source=self)
-            if self._td_dim_names is not None:
-                out.names = self._td_dim_names
-            return out
-        elif isinstance(dest, (torch.device, str, int)):
-            if self.device is not None and torch.device(dest) == self.device:
-                return self
-            td = self._source.to(dest, **kwargs)
-            self_copy = copy(self)
-            self_copy._source = td
-            return self_copy
-        elif dest is None:
-            return self
-        else:
-            raise NotImplementedError(
-                f"dest must be a string, torch.device or a TensorDict "
-                f"instance, {dest} not allowed"
-            )
-
-    def pin_memory(self) -> _CustomOpTensorDict:
-        self._source.pin_memory()
-        return self
-
-    def detach_(self) -> _CustomOpTensorDict:
-        self._source.detach_()
-        return self
-
-    def masked_fill_(self, mask: Tensor, value: float | bool) -> _CustomOpTensorDict:
-        for key, item in self.items():
-            val = self._source.get(key)
-            mask_exp = expand_right(
-                mask, list(mask.shape) + list(val.shape[self._source.batch_dims :])
-            )
-            mask_proc_inv = getattr(mask_exp, self.inv_op)(
-                **self._update_inv_op_kwargs(item)
-            )
-            val[mask_proc_inv] = value
-            self._source.set(key, val)
-        return self
-
-    def masked_fill(self, mask: Tensor, value: float | bool) -> TensorDictBase:
-        td_copy = self.clone()
-        return td_copy.masked_fill_(mask, value)
-
-    def memmap_(
-        self, prefix: str | None = None, copy_existing: bool = False
-    ) -> _CustomOpTensorDict:
-        self._source.memmap_(prefix=prefix, copy_existing=copy_existing)
-        if prefix is not None:
-            prefix = Path(prefix)
-            metadata = torch.load(prefix / "meta.pt")
-            metadata["custom_op"] = self.custom_op
-            metadata["inv_op"] = self.inv_op
-            metadata["custom_op_kwargs"] = self.custom_op_kwargs
-            metadata["inv_op_kwargs"] = self.inv_op_kwargs
-            torch.save(metadata, prefix / "meta.pt")
-
-        self._is_memmap = True
-        self.lock_()
-        return self
-
-    @classmethod
-    def load_memmap(cls, prefix: str) -> _CustomOpTensorDict:
-        prefix = Path(prefix)
-        source = TensorDict.load_memmap(prefix)
-        metadata = torch.load(prefix / "meta.pt")
-        return cls(
-            source,
-            custom_op=metadata["custom_op"],
-            inv_op=metadata["inv_op"],
-            custom_op_kwargs=metadata["custom_op_kwargs"],
-            inv_op_kwargs=metadata["inv_op_kwargs"],
-        )
-
-    def share_memory_(self) -> _CustomOpTensorDict:
-        self._source.share_memory_()
-        self._is_shared = True
-        self.lock_()
-        return self
-
-    @property
-    def _td_dim_names(self):
-        # we also want for _td_dim_names to be accurate
-        if self._source._td_dim_names is None:
-            return None
-        return self.names
-
-    @property
-    def is_locked(self) -> bool:
-        return self._source.is_locked
-
-    @is_locked.setter
-    def is_locked(self, value) -> bool:
-        if value:
-            self.lock_()
-        else:
-            self.unlock_()
-
-    @as_decorator("is_locked")
-    def lock_(self) -> TensorDictBase:
-        self._source.lock_()
-        return self
-
-    @erase_cache
-    @as_decorator("is_locked")
-    def unlock_(self) -> TensorDictBase:
-        self._source.unlock_()
-        return self
-
-    def _remove_lock(self, lock_id):
-        return self._source._remove_lock(lock_id)
-
-    @erase_cache
-    def _lock_propagate(self, lock_ids):
-        return self._source._lock_propagate(lock_ids)
-
-    lock = _renamed_inplace_method(lock_)
-    unlock = _renamed_inplace_method(unlock_)
-
-    def __del__(self):
-        pass
-
-    @property
-    def sorted_keys(self):
-        return self._source.sorted_keys
-
-
-class _UnsqueezedTensorDict(_CustomOpTensorDict):
-    """A lazy view on an unsqueezed TensorDict.
-
-    When calling `tensordict.unsqueeze(dim)`, a lazy view of this operation is
-    returned such that the following code snippet works without raising an
-    exception:
-
-        >>> assert tensordict.unsqueeze(dim).squeeze(dim) is tensordict
-
-    Examples:
-        >>> from tensordict import TensorDict
-        >>> import torch
-        >>> td = TensorDict({'a': torch.randn(3, 4)}, batch_size=[3])
-        >>> td_unsqueeze = td.unsqueeze(-1)
-        >>> print(td_unsqueeze.shape)
-        torch.Size([3, 1])
-        >>> print(td_unsqueeze.squeeze(-1) is td)
-        True
-    """
-
-    def squeeze(self, dim: int | None) -> TensorDictBase:
-        if dim is not None and dim < 0:
-            dim = self.batch_dims + dim
-        if dim == self.custom_op_kwargs.get("dim"):
-            return self._source
-        return super().squeeze(dim)
-
-    def _stack_onto_(
-        self,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> TensorDictBase:
-        unsqueezed_dim = self.custom_op_kwargs["dim"]
-        diff_to_apply = 1 if dim < unsqueezed_dim else 0
-        list_item_unsqueeze = [
-            item.squeeze(unsqueezed_dim - diff_to_apply) for item in list_item
-        ]
-        return self._source._stack_onto_(list_item_unsqueeze, dim)
-
-    @property
-    def names(self):
-        names = copy(self._source.names)
-        dim = self.custom_op_kwargs.get("dim")
-        names.insert(dim, None)
-        return names
-
-    @names.setter
-    def names(self, value):
-        if value[: self.batch_dims] == self.names:
-            return
-        raise RuntimeError(
-            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
-        )
-
-
-class _SqueezedTensorDict(_CustomOpTensorDict):
-    """A lazy view on a squeezed TensorDict.
-
-    See the `UnsqueezedTensorDict` class documentation for more information.
-
-    """
-
-    def unsqueeze(self, dim: int) -> TensorDictBase:
-        if dim < 0:
-            dim = self.batch_dims + dim + 1
-        inv_op_dim = self.inv_op_kwargs.get("dim")
-        if inv_op_dim < 0:
-            inv_op_dim = self.batch_dims + inv_op_dim + 1
-        if dim == inv_op_dim:
-            return self._source
-        return super().unsqueeze(dim)
-
-    def _stack_onto_(
-        self,
-        # key: str,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> TensorDictBase:
-        squeezed_dim = self.custom_op_kwargs["dim"]
-        # dim=0, squeezed_dim=2, [3, 4, 5] [3, 4, 1, 5] [[4, 5], [4, 5], [4, 5]] => unsq 1
-        # dim=1, squeezed_dim=2, [3, 4, 5] [3, 4, 1, 5] [[3, 5], [3, 5], [3, 5], [3, 4]] => unsq 1
-        # dim=2, squeezed_dim=2, [3, 4, 5] [3, 4, 1, 5] [[3, 4], [3, 4], ...] => unsq 2
-        diff_to_apply = 1 if dim < squeezed_dim else 0
-        list_item_unsqueeze = [
-            item.unsqueeze(squeezed_dim - diff_to_apply) for item in list_item
-        ]
-        return self._source._stack_onto_(list_item_unsqueeze, dim)
-
-    @property
-    def names(self):
-        names = copy(self._source.names)
-        dim = self.custom_op_kwargs["dim"]
-        if self._source.batch_size[dim] == 1:
-            del names[dim]
-        return names
-
-    @names.setter
-    def names(self, value):
-        if value[: self.batch_dims] == self.names:
-            return
-        raise RuntimeError(
-            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
-        )
-
-
-class _ViewedTensorDict(_CustomOpTensorDict):
-    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
-        new_dim_list = list(self.custom_op_kwargs.get("size"))
-        new_dim_list += list(source_tensor.shape[self._source.batch_dims :])
-        new_dim = torch.Size(new_dim_list)
-        new_dict = deepcopy(self.custom_op_kwargs)
-        new_dict.update({"size": new_dim})
-        return new_dict
-
-    def _update_inv_op_kwargs(self, tensor: Tensor) -> dict:
-        size = list(self.inv_op_kwargs.get("size"))
-        size += list(_shape(tensor)[self.batch_dims :])
-        new_dim = torch.Size(size)
-        new_dict = deepcopy(self.inv_op_kwargs)
-        new_dict.update({"size": new_dim})
-        return new_dict
-
-    def view(
-        self, *shape: int, size: list | tuple | torch.Size | None = None
-    ) -> TensorDictBase:
-        if len(shape) == 0 and size is not None:
-            return self.view(*size)
-        elif len(shape) == 1 and isinstance(shape[0], (list, tuple, torch.Size)):
-            return self.view(*shape[0])
-        elif not isinstance(shape, torch.Size):
-            shape = infer_size_impl(shape, self.numel())
-            shape = torch.Size(shape)
-        if shape == self._source.batch_size:
-            return self._source
-        return super().view(*shape)
-
-    @property
-    def names(self):
-        return [None] * self.ndim
-
-    @names.setter
-    def names(self, value):
-        raise RuntimeError(
-            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
-        )
-
-
-class _TransposedTensorDict(_CustomOpTensorDict):
-    """A lazy view on a TensorDict with two batch dimensions transposed.
-
-    When calling `tensordict.permute(dims_list, dim)`, a lazy view of this operation is
-    returned such that the following code snippet works without raising an
-    exception:
-
-        >>> assert tensordict.transpose(dims_list, dim).transpose(dims_list, dim) is tensordict
-
-    """
-
-    def transpose(self, dim0, dim1) -> TensorDictBase:
-        if dim0 < 0:
-            dim0 = self.ndim + dim0
-        if dim1 < 0:
-            dim1 = self.ndim + dim1
-        if any((dim0 < 0, dim1 < 0)):
-            raise ValueError(
-                "The provided dimensions are incompatible with the tensordict batch-size."
-            )
-        if dim0 == dim1:
-            return self
-        dims = (self.inv_op_kwargs.get("dim0"), self.inv_op_kwargs.get("dim1"))
-        if dim0 in dims and dim1 in dims:
-            return self._source
-        return super().permute(dim0, dim1)
-
-    def add_missing_dims(
-        self, num_dims: int, batch_dims: tuple[int, ...]
-    ) -> tuple[int, ...]:
-        dim_diff = num_dims - len(batch_dims)
-        all_dims = list(range(num_dims))
-        for i, x in enumerate(batch_dims):
-            if x < 0:
-                x = x - dim_diff
-            all_dims[i] = x
-        return tuple(all_dims)
-
-    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
-        return self.custom_op_kwargs
-
-    def _update_inv_op_kwargs(self, tensor: Tensor) -> dict[str, Any]:
-        return self.custom_op_kwargs
-
-    def _stack_onto_(
-        self,
-        # key: str,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> TensorDictBase:
-        trsp = self.custom_op_kwargs["dim0"], self.custom_op_kwargs["dim1"]
-        if dim == trsp[0]:
-            dim = trsp[1]
-        elif dim == trsp[1]:
-            dim = trsp[0]
-
-        list_permuted_items = []
-        for item in list_item:
-            list_permuted_items.append(item.transpose(*trsp))
-        self._source._stack_onto_(list_permuted_items, dim)
-        return self
-
-    @property
-    def names(self):
-        names = copy(self._source.names)
-        dim0 = self.custom_op_kwargs["dim0"]
-        dim1 = self.custom_op_kwargs["dim1"]
-        names = [
-            names[dim0] if i == dim1 else names[dim1] if i == dim0 else name
-            for i, name in enumerate(names)
-        ]
-        return names
-
-    @names.setter
-    def names(self, value):
-        raise RuntimeError(
-            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
-        )
-
-
-class _PermutedTensorDict(_CustomOpTensorDict):
-    """A lazy view on a TensorDict with the batch dimensions permuted.
-
-    When calling `tensordict.permute(dims_list, dim)`, a lazy view of this operation is
-    returned such that the following code snippet works without raising an
-    exception:
-
-        >>> assert tensordict.permute(dims_list, dim).permute(dims_list, dim) is tensordict
-
-    Examples:
-        >>> from tensordict import TensorDict
-        >>> import torch
-        >>> td = TensorDict({'a': torch.randn(4, 5, 6, 9)}, batch_size=[3])
-        >>> td_permute = td.permute(dims=(2, 1, 0))
-        >>> print(td_permute.shape)
-        torch.Size([6, 5, 4])
-        >>> print(td_permute.permute(dims=(2, 1, 0)) is td)
-        True
-
-    """
-
-    def permute(
-        self,
-        *dims_list: int,
-        dims: Sequence[int] | None = None,
-    ) -> TensorDictBase:
-        if len(dims_list) == 0:
-            dims_list = dims
-        elif len(dims_list) == 1 and not isinstance(dims_list[0], int):
-            dims_list = dims_list[0]
-        if len(dims_list) != len(self.shape):
-            raise RuntimeError(
-                f"number of dims don't match in permute (got {len(dims_list)}, expected {len(self.shape)}"
-            )
-        if not len(dims_list) and not self.batch_dims:
-            return self
-        if np.array_equal(dims_list, range(self.batch_dims)):
-            return self
-        if np.array_equal(np.argsort(dims_list), self.inv_op_kwargs.get("dims")):
-            return self._source
-        return super().permute(*dims_list)
-
-    def add_missing_dims(
-        self, num_dims: int, batch_dims: tuple[int, ...]
-    ) -> tuple[int, ...]:
-        # Adds the feature dimensions to the permute dims
-        dim_diff = num_dims - len(batch_dims)
-        all_dims = list(range(num_dims))
-        for i, x in enumerate(batch_dims):
-            if x < 0:
-                x = x - dim_diff
-            all_dims[i] = x
-        return tuple(all_dims)
-
-    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
-        new_dims = self.add_missing_dims(
-            len(source_tensor.shape), self.custom_op_kwargs["dims"]
-        )
-        kwargs = deepcopy(self.custom_op_kwargs)
-        kwargs.update({"dims": new_dims})
-        return kwargs
-
-    def _update_inv_op_kwargs(self, tensor: Tensor) -> dict[str, Any]:
-        new_dims = self.add_missing_dims(
-            self._source.batch_dims + len(_shape(tensor)[self.batch_dims :]),
-            self.custom_op_kwargs["dims"],
-        )
-        kwargs = deepcopy(self.custom_op_kwargs)
-        kwargs.update({"dims": tuple(np.argsort(new_dims))})
-        return kwargs
-
-    def _stack_onto_(
-        self,
-        # key: str,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> TensorDictBase:
-        permute_dims = self.custom_op_kwargs["dims"]
-        inv_permute_dims = np.argsort(permute_dims)
-        new_dim = [i for i, v in enumerate(inv_permute_dims) if v == dim][0]
-        inv_permute_dims = [p for p in inv_permute_dims if p != dim]
-        inv_permute_dims = np.argsort(np.argsort(inv_permute_dims))
-
-        list_permuted_items = []
-        for item in list_item:
-            perm = list(inv_permute_dims) + list(
-                range(self.batch_dims - 1, item.ndimension())
-            )
-            list_permuted_items.append(item.permute(*perm))
-        self._source._stack_onto_(list_permuted_items, new_dim)
-        return self
-
-    @property
-    def names(self):
-        names = copy(self._source.names)
-        return [names[i] for i in self.custom_op_kwargs["dims"]]
-
-    @names.setter
-    def names(self, value):
-        if value[: self.batch_dims] == self.names:
-            return
-        raise RuntimeError(
-            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
-        )
-
-
-def _get_repr(tensor: Tensor) -> str:
-    s = ", ".join(
-        [
-            f"shape={_shape(tensor)}",
-            f"device={_device(tensor)}",
-            f"dtype={_dtype(tensor)}",
-            f"is_shared={_is_shared(tensor)}",
-        ]
-    )
-    return f"{tensor.__class__.__name__}({s})"
-
-
-def _get_repr_custom(cls, shape, device, dtype, is_shared) -> str:
-    s = ", ".join(
-        [
-            f"shape={shape}",
-            f"device={device}",
-            f"dtype={dtype}",
-            f"is_shared={is_shared}",
-        ]
-    )
-    return f"{cls.__name__}({s})"
-
-
-def _make_repr(key: str, item: CompatibleType, tensordict: TensorDictBase) -> str:
-    if _is_tensor_collection(type(item)):
-        return f"{key}: {repr(tensordict.get(key))}"
-    return f"{key}: {_get_repr(item)}"
-
-
-def _td_fields(td: TensorDictBase, keys=None) -> str:
-    strs = []
-    if keys is None:
-        keys = td.keys()
-    for key in keys:
-        shape = td.get_item_shape(key)
-        if -1 not in shape:
-            item = td.get(key)
-            strs.append(_make_repr(key, item, td))
-        else:
-            # we know td is lazy stacked and the key is a leaf
-            # so we can get the shape and escape the error
-            temp_td = td
-            while isinstance(
-                temp_td, LazyStackedTensorDict
-            ):  # we need to grab the het tensor from the inner nesting level
-                temp_td = temp_td.tensordicts[0]
-            tensor = temp_td.get(key)
-            if isinstance(tensor, TensorDictBase):
-                substr = _td_fields(tensor)
-            else:
-                substr = _get_repr_custom(
-                    tensor.__class__,
-                    shape=shape,
-                    device=tensor.device,
-                    dtype=tensor.dtype,
-                    is_shared=tensor.is_shared(),
-                )
-            strs.append(f"{key}: {substr}")
-
-    return indent(
-        "\n" + ",\n".join(sorted(strs)),
-        4 * " ",
-    )
-
-
-def _check_keys(
-    list_of_tensordicts: Sequence[TensorDictBase],
-    strict: bool = False,
-    include_nested: bool = False,
-    leaves_only: bool = False,
-) -> set[str]:
-    if not len(list_of_tensordicts):
-        return set()
-    keys: set[str] = set(
-        list_of_tensordicts[0].keys(
-            include_nested=include_nested, leaves_only=leaves_only
-        )
-    )
-    for td in list_of_tensordicts[1:]:
-        k = td.keys(include_nested=include_nested, leaves_only=leaves_only)
-        if not strict:
-            keys = keys.intersection(k)
-        else:
-            if set(k) != keys:
-                raise KeyError(
-                    f"got keys {keys} and {set(td.keys())} which are " f"incompatible"
-                )
-    return keys
-
-
-def _expand_to_match_shape(
-    parent_batch_size: torch.Size,
-    tensor: Tensor,
-    self_batch_dims: int,
-    self_device: DeviceType,
-) -> Tensor | TensorDictBase:
-    if hasattr(tensor, "dtype"):
-        return torch.zeros(
-            (
-                *parent_batch_size,
-                *_shape(tensor)[self_batch_dims:],
-            ),
-            dtype=tensor.dtype,
-            device=self_device,
-        )
-    else:
-        # tensordict
-        out = TensorDict(
-            {},
-            [*parent_batch_size, *_shape(tensor)[self_batch_dims:]],
-            device=self_device,
-        )
-        return out
-
-
-def make_tensordict(
-    input_dict: dict[str, CompatibleType] | None = None,
-    batch_size: Sequence[int] | torch.Size | int | None = None,
-    device: DeviceType | None = None,
-    **kwargs: CompatibleType,  # source
-) -> TensorDict:
-    """Returns a TensorDict created from the keyword arguments or an input dictionary.
-
-    If ``batch_size`` is not specified, returns the maximum batch size possible.
-
-    This function works on nested dictionaries too, or can be used to determine the
-    batch-size of a nested tensordict.
-
-    Args:
-        input_dict (dictionary, optional): a dictionary to use as a data source
-            (nested keys compatible).
-        **kwargs (TensorDict or torch.Tensor): keyword arguments as data source
-            (incompatible with nested keys).
-        batch_size (iterable of int, optional): a batch size for the tensordict.
-        device (torch.device or compatible type, optional): a device for the TensorDict.
-
-    Examples:
-        >>> input_dict = {"a": torch.randn(3, 4), "b": torch.randn(3)}
-        >>> print(make_tensordict(input_dict))
-        TensorDict(
-            fields={
-                a: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                b: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-        >>> # alternatively
-        >>> td = make_tensordict(**input_dict)
-        >>> # nested dict: the nested TensorDict can have a different batch-size
-        >>> # as long as its leading dims match.
-        >>> input_dict = {"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}
-        >>> print(make_tensordict(input_dict))
-        TensorDict(
-            fields={
-                a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
-                b: TensorDict(
-                    fields={
-                        c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([3, 4]),
-                    device=None,
-                    is_shared=False)},
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-        >>> # we can also use this to work out the batch sie of a tensordict
-        >>> input_td = TensorDict({"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}, [])
-        >>> print(make_tensordict(input_td))
-        TensorDict(
-            fields={
-                a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
-                b: TensorDict(
-                    fields={
-                        c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([3, 4]),
-                    device=None,
-                    is_shared=False)},
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-    """
-    if input_dict is not None:
-        kwargs.update(input_dict)
-    return TensorDict.from_dict(kwargs, batch_size=batch_size, device=device)
-
-
-def dense_stack_tds(
-    td_list: Sequence[TensorDictBase] | LazyStackedTensorDict,
-    dim: int = None,
-) -> TensorDictBase:
-    """Densely stack a list of :class:`tensordict.TensorDictBase` objects (or a :class:`tensordict.LazyStackedTensorDict`) given that they have the same structure.
-
-    This function is called with a list of :class:`tensordict.TensorDictBase` (either passed directly or obtrained from
-    a :class:`tensordict.LazyStackedTensorDict`).
-    Instead of calling ``torch.stack(td_list)``, which would return a :class:`tensordict.LazyStackedTensorDict`,
-    this function expands the first element of the input list and stacks the input list onto that element.
-    This works only when all the elements of the input list have the same structure.
-    The :class:`tensordict.TensorDictBase` returned will have the same type of the elements of the input list.
-
-    This function is useful when some of the :class:`tensordict.TensorDictBase` objects that need to be stacked
-    are :class:`tensordict.LazyStackedTensorDict` or have :class:`tensordict.LazyStackedTensorDict`
-    among entries (or nested entries).
-    In those cases, calling ``torch.stack(td_list).to_tensordict()`` is infeasible.
-    Thus, this function provides an alternative for densely stacking the list provided.
-
-    Args:
-        td_list (List of TensorDictBase or LazyStackedTensorDict): the tds to stack.
-        dim (int, optional): the dimension to stack them.
-            If td_list is a LazyStackedTensorDict, it will be retrieved automatically.
-
-    Examples:
-        >>> import torch
-        >>> from tensordict import TensorDict
-        >>> from tensordict import dense_stack_tds
-        >>> from tensordict.tensordict import assert_allclose_td
-        >>> td0 = TensorDict({"a": torch.zeros(3)},[])
-        >>> td1 = TensorDict({"a": torch.zeros(4), "b": torch.zeros(2)},[])
-        >>> td_lazy = torch.stack([td0, td1], dim=0)
-        >>> td_container = TensorDict({"lazy": td_lazy}, [])
-        >>> td_container_clone = td_container.clone()
-        >>> td_stack = torch.stack([td_container, td_container_clone], dim=0)
-        >>> td_stack
-        LazyStackedTensorDict(
-            fields={
-                lazy: LazyStackedTensorDict(
-                    fields={
-                        a: Tensor(shape=torch.Size([2, 2, -1]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    exclusive_fields={
-                    },
-                    batch_size=torch.Size([2, 2]),
-                    device=None,
-                    is_shared=False,
-                    stack_dim=0)},
-            exclusive_fields={
-            },
-            batch_size=torch.Size([2]),
-            device=None,
-            is_shared=False,
-            stack_dim=0)
-        >>> td_stack = dense_stack_tds(td_stack) # Automatically use the LazyStackedTensorDict stack_dim
-        TensorDict(
-            fields={
-                lazy: LazyStackedTensorDict(
-                    fields={
-                        a: Tensor(shape=torch.Size([2, 2, -1]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    exclusive_fields={
-                        1 ->
-                            b: Tensor(shape=torch.Size([2, 2]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([2, 2]),
-                    device=None,
-                    is_shared=False,
-                    stack_dim=1)},
-            batch_size=torch.Size([2]),
-            device=None,
-            is_shared=False)
-        # Note that
-        # (1) td_stack is now a TensorDict
-        # (2) this has pushed the stack_dim of "lazy" (0 -> 1)
-        # (3) this has revealed the exclusive keys.
-        >>> assert_allclose_td(td_stack, dense_stack_tds([td_container, td_container_clone], dim=0))
-        # This shows it is the same to pass a list or a LazyStackedTensorDict
-
-    """
-    if isinstance(td_list, LazyStackedTensorDict):
-        dim = td_list.stack_dim
-        td_list = td_list.tensordicts
-    elif dim is None:
-        raise ValueError(
-            "If a list of tensordicts is provided, stack_dim must not be None"
-        )
-    shape = list(td_list[0].shape)
-    shape.insert(dim, len(td_list))
-
-    out = td_list[0].unsqueeze(dim).expand(shape).clone()
-    return torch.stack(td_list, dim=dim, out=out)
-
-
-def _set_max_batch_size(source: TensorDictBase, batch_dims=None):
-    """Updates a tensordict with its maximium batch size."""
-    tensor_data = list(source.values())
-
-    for val in tensor_data:
-        if _is_tensor_collection(val.__class__):
-            _set_max_batch_size(val, batch_dims=batch_dims)
-    batch_size = []
-    if not tensor_data:  # when source is empty
-        source.batch_size = batch_size
-        return
-    curr_dim = 0
-    while True:
-        if tensor_data[0].dim() > curr_dim:
-            curr_dim_size = tensor_data[0].size(curr_dim)
-        else:
-            source.batch_size = batch_size
-            return
-        for tensor in tensor_data[1:]:
-            if tensor.dim() <= curr_dim or tensor.size(curr_dim) != curr_dim_size:
-                source.batch_size = batch_size
-                return
-        if batch_dims is None or len(batch_size) < batch_dims:
-            batch_size.append(curr_dim_size)
-        curr_dim += 1
-
-
-def _iter_items_lazystack(
-    tensordict: LazyStackedTensorDict,
-) -> Iterator[tuple[str, CompatibleType]]:
-    return tensordict.items()
-
-
-def _clone_value(value: CompatibleType, recurse: bool) -> CompatibleType:
-    if recurse:
-        return value.clone()
-    elif _is_tensor_collection(value.__class__):
-        return value.clone(recurse=False)
-    else:
-        return value
-
-
-def _is_number(item):
-    if isinstance(item, Number):
-        return True
-    if isinstance(item, Tensor) and item.ndim == 0:
-        return True
-    if isinstance(item, np.ndarray) and item.ndim == 0:
-        return True
-    return False
-
-
-def _expand_index(index, batch_size):
-    len_index = sum(True for idx in index if idx is not None)
-    if len_index > len(batch_size):
-        raise ValueError
-    if len_index < len(batch_size):
-        index = index + (slice(None),) * (len(batch_size) - len_index)
-    return index
-
-
-def _broadcast_tensors(index):
-    # tensors and range need to be broadcast
-    tensors = {
-        i: tensor if isinstance(tensor, Tensor) else torch.tensor(tensor)
-        for i, tensor in enumerate(index)
-        if isinstance(tensor, (range, list, np.ndarray, torch.Tensor))
-    }
-    if tensors:
-        shape = torch.broadcast_shapes(*[tensor.shape for tensor in tensors.values()])
-        tensors = {i: tensor.expand(shape) for i, tensor in tensors.items()}
-        index = tuple(
-            idx if i not in tensors else tensors[i] for i, idx in enumerate(index)
-        )
-    return index
-
-
-def _reduce_index(index):
-    if all(
-        idx is Ellipsis or (isinstance(idx, slice) and idx == slice(None))
-        for idx in index
-    ):
-        index = ()
-    return index
-
-
-def _convert_index_lazystack(index, stack_dim, batch_size):
-    out = {
-        "remaining_index": None,
-        "stack_index": None,
-        "new_stack_dim": None,
-    }
-    index = convert_ellipsis_to_idx(index, batch_size)
-    if not isinstance(index, tuple):
-        index = (index,)
-    if any(
-        isinstance(idx, (MemmapTensor, Tensor)) and idx.dtype == torch.bool
-        for idx in index
-    ):
-        return None
-    index = _expand_index(index, batch_size)
-    index = _broadcast_tensors(index)
-    # find the index corresponding to the stack dim
-    stack_dim_index = 0
-    new_stack_dim = 0
-    count = -1
-    has_first_tensor = False
-    while True:
-        if index[stack_dim_index] is None:
-            stack_dim_index += 1
-            new_stack_dim += 1
-            continue
-
-        count += 1
-        if count == stack_dim:
-            remaining_index = tuple(
-                idx for i, idx in enumerate(index) if i != stack_dim_index
-            )
-            stack_index = index[stack_dim_index]
-            break
-        if isinstance(index[stack_dim_index], int) or (
-            isinstance(index[stack_dim_index], (Tensor, np.ndarray))
-            and (not index[stack_dim_index].ndim or has_first_tensor)
-        ):
-            new_stack_dim_incr = 0
-        else:
-            new_stack_dim_incr = 1
-            has_first_tensor = has_first_tensor or isinstance(
-                index[stack_dim_index], (Tensor, np.ndarray)
-            )
-        new_stack_dim += new_stack_dim_incr
-        stack_dim_index += 1
-
-    out["stack_index"] = stack_index
-    if not isinstance(stack_index, int):
-        out["new_stack_dim"] = new_stack_dim
-    if isinstance(stack_index, Tensor):
-        # we build one index for each resulting item
-        # all the tensors are indexed along the same integer
-        def _dispatch(remaining_index, stack_index, i=None):
-            if i is not None:
-                remaining_index = tuple(
-                    idx[i] if isinstance(idx, Tensor) else idx
-                    for idx in remaining_index
-                )
-            if isinstance(stack_index, list):
-                out = []
-                for j, _stack_index in enumerate(stack_index):
-                    out.append(_dispatch(remaining_index, _stack_index, j))
-                return tuple(out)
-            return remaining_index
-
-        remaining_index = _dispatch(remaining_index, stack_index.tolist())
-    out["remaining_index"] = _reduce_index(remaining_index)
-    return out
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import abc
+import collections
+import functools
+import numbers
+import os
+import re
+import textwrap
+import warnings
+from collections import defaultdict
+from collections.abc import MutableMapping
+from copy import copy, deepcopy
+from numbers import Number
+from pathlib import Path
+from textwrap import indent
+from typing import (
+    Any,
+    Callable,
+    Generator,
+    Iterable,
+    Iterator,
+    OrderedDict,
+    Sequence,
+    Union,
+)
+from warnings import warn
+
+import numpy as np
+
+import torch
+from tensordict._tensordict import _unravel_key_to_tuple
+from tensordict.memmap import memmap_tensor_as_tensor, MemmapTensor
+from tensordict.utils import (
+    _device,
+    _dtype,
+    _GENERIC_NESTED_ERR,
+    _get_item,
+    _getitem_batch_size,
+    _is_shared,
+    _is_tensorclass,
+    _NON_STR_KEY_ERR,
+    _NON_STR_KEY_TUPLE_ERR,
+    _set_item,
+    _shape,
+    _StringOnlyDict,
+    _sub_index,
+    as_decorator,
+    cache,
+    convert_ellipsis_to_idx,
+    DeviceType,
+    erase_cache,
+    expand_as_right,
+    expand_right,
+    IndexType,
+    int_generator,
+    is_tensorclass,
+    lock_blocked,
+    NestedKey,
+    prod,
+)
+from torch import distributed as dist, Tensor
+from torch.utils._pytree import tree_map
+
+try:
+    from torch.jit._shape_functions import infer_size_impl
+except ImportError:
+    from tensordict.utils import infer_size_impl
+
+
+_has_functorch = False
+try:
+    try:
+        from functorch._C import is_batchedtensor
+    except ImportError:
+        from torch._C._functorch import (
+            _add_batch_dim,
+            _remove_batch_dim,
+            is_batchedtensor,
+        )
+
+    _has_functorch = True
+except ImportError:
+    _has_functorch = False
+
+    def is_batchedtensor(tensor: Tensor) -> bool:
+        """Placeholder for the functorch function."""
+        return False
+
+
+try:
+    from torchrec import KeyedJaggedTensor
+
+    _has_torchrec = True
+except ImportError as err:
+    _has_torchrec = False
+
+    class KeyedJaggedTensor:  # noqa: D103, D101
+        pass
+
+    TORCHREC_ERR = str(err)
+
+NO_DEFAULT = "_no_default_"
+
+
+class _BEST_ATTEMPT_INPLACE:
+    def __bool__(self):
+        raise NotImplementedError
+
+
+BEST_ATTEMPT_INPLACE = _BEST_ATTEMPT_INPLACE()
+
+# some complex string used as separator to concatenate and split keys in
+# distributed frameworks
+DIST_SEPARATOR = ".-|-."
+TD_HANDLED_FUNCTIONS: dict[Callable, Callable] = {}
+LAZY_TD_HANDLED_FUNCTIONS: dict[Callable, Callable] = {}
+CompatibleType = Union[
+    Tensor,
+    MemmapTensor,
+]  # None? # leaves space for TensorDictBase
+
+if _has_torchrec:
+    CompatibleType = Union[
+        Tensor,
+        MemmapTensor,
+        KeyedJaggedTensor,
+    ]
+_STR_MIXED_INDEX_ERROR = "Received a mixed string-non string index. Only string-only or string-free indices are supported."
+
+_HEURISTIC_EXCLUDED = (Tensor, tuple, list, set, dict, np.ndarray)
+
+_TENSOR_COLLECTION_MEMO = {}
+
+
+def _is_tensor_collection(datatype):
+    out = _TENSOR_COLLECTION_MEMO.get(datatype, None)
+    if out is None:
+        if issubclass(datatype, TensorDictBase):
+            out = True
+        elif _is_tensorclass(datatype):
+            out = True
+        else:
+            out = False
+        _TENSOR_COLLECTION_MEMO[datatype] = out
+    return out
+
+
+def is_tensor_collection(datatype: type | Any) -> bool:
+    """Checks if a data object or a type is a tensor container from the tensordict lib.
+
+    Returns:
+        ``True`` if the input is a TensorDictBase subclass, a tensorclass or an istance of these.
+        ``False`` otherwise.
+
+    Examples:
+        >>> is_tensor_collection(TensorDictBase)  # True
+        >>> is_tensor_collection(TensorDict({}, []))  # True
+        >>> @tensorclass
+        ... class MyClass:
+        ...     pass
+        ...
+        >>> is_tensor_collection(MyClass)  # True
+        >>> is_tensor_collection(MyClass(batch_size=[]))  # True
+
+    """
+    # memoizing is 2x faster
+    if not isinstance(datatype, type):
+        datatype = type(datatype)
+    return _is_tensor_collection(datatype)
+
+
+def is_memmap(datatype: type | Any) -> bool:
+    """Returns ``True`` if the class is a subclass of :class:`~.MemmapTensor` or the object an instance of it."""
+    return (
+        issubclass(datatype, MemmapTensor)
+        if isinstance(datatype, type)
+        else isinstance(datatype, MemmapTensor)
+    )
+
+
+class _TensorDictKeysView:
+    """A Key view for TensorDictBase instance.
+
+    _TensorDictKeysView is returned when accessing tensordict.keys() and holds a
+    reference to the original TensorDict. This class enables us to support nested keys
+    when performing membership checks and when iterating over keys.
+
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+
+        >>> td = TensorDict(
+        >>>     {"a": TensorDict({"b": torch.rand(1, 2)}, [1, 2]), "c": torch.rand(1)},
+        >>>     [1],
+        >>> )
+
+        >>> assert "a" in td.keys()
+        >>> assert ("a",) in td.keys()
+        >>> assert ("a", "b") in td.keys()
+        >>> assert ("a", "c") not in td.keys()
+
+        >>> assert set(td.keys()) == {("a", "b"), "c"}
+    """
+
+    def __init__(
+        self,
+        tensordict: TensorDictBase,
+        include_nested: bool,
+        leaves_only: bool,
+    ) -> None:
+        self.tensordict = tensordict
+        self.include_nested = include_nested
+        self.leaves_only = leaves_only
+
+    def __iter__(self) -> Iterable[str] | Iterable[tuple[str, ...]]:
+        if not self.include_nested:
+            if self.leaves_only:
+                for key in self._keys():
+                    target_class = self.tensordict.entry_class(key)
+                    if _is_tensor_collection(target_class):
+                        continue
+                    yield key
+            else:
+                yield from self._keys()
+        else:
+            yield from (
+                key if len(key) > 1 else key[0]
+                for key in self._iter_helper(self.tensordict)
+            )
+
+    def _iter_helper(
+        self, tensordict: TensorDictBase, prefix: str | None = None
+    ) -> Iterable[str] | Iterable[tuple[str, ...]]:
+        for key, value in self._items(tensordict):
+            full_key = self._combine_keys(prefix, key)
+            cls = value.__class__
+            if self.include_nested and (
+                _is_tensor_collection(cls) or issubclass(cls, KeyedJaggedTensor)
+            ):
+                subkeys = tuple(self._iter_helper(value, prefix=full_key))
+                yield from subkeys
+            if not self.leaves_only or not _is_tensor_collection(cls):
+                yield full_key
+
+    def _combine_keys(self, prefix: tuple | None, key: str) -> tuple:
+        if prefix is not None:
+            return prefix + (key,)
+        return (key,)
+
+    def __len__(self) -> int:
+        return sum(1 for _ in self)
+
+    def _items(
+        self, tensordict: TensorDict | None = None
+    ) -> Iterable[tuple[NestedKey, CompatibleType]]:
+        if tensordict is None:
+            tensordict = self.tensordict
+        if isinstance(tensordict, TensorDict) or is_tensorclass(tensordict):
+            return tensordict._tensordict.items()
+        elif isinstance(tensordict, LazyStackedTensorDict):
+            return _iter_items_lazystack(tensordict)
+        elif isinstance(tensordict, KeyedJaggedTensor):
+            return tuple((key, tensordict[key]) for key in tensordict.keys())
+        elif isinstance(tensordict, _CustomOpTensorDict):
+            # it's possible that a TensorDict contains a nested LazyStackedTensorDict,
+            # or _CustomOpTensorDict, so as we iterate through the contents we need to
+            # be careful to not rely on tensordict._tensordict existing.
+            return (
+                (key, tensordict._get_str(key, NO_DEFAULT))
+                for key in tensordict._source.keys()
+            )
+
+    def _keys(self) -> _TensorDictKeysView:
+        return self.tensordict._tensordict.keys()
+
+    def __contains__(self, key: NestedKey) -> bool:
+        key = _unravel_key_to_tuple(key)
+        if not key:
+            raise TypeError(_NON_STR_KEY_ERR)
+
+        if isinstance(key, str):
+            if key in self._keys():
+                if self.leaves_only:
+                    return not _is_tensor_collection(self.tensordict.entry_class(key))
+                return True
+            return False
+        else:
+            # thanks to _unravel_key_to_tuple we know the key is a tuple
+            if len(key) == 1:
+                return key[0] in self._keys()
+            elif self.include_nested:
+                if key[0] in self._keys():
+                    entry_type = self.tensordict.entry_class(key[0])
+                    if entry_type in (Tensor, MemmapTensor):
+                        return False
+                    if entry_type is KeyedJaggedTensor:
+                        if len(key) > 2:
+                            return False
+                        return key[1] in self.tensordict.get(key[0]).keys()
+                    _is_tensordict = _is_tensor_collection(entry_type)
+                    if _is_tensordict:
+                        # # this will call _unravel_key_to_tuple many times
+                        # return key[1:] in self.tensordict._get_str(key[0], NO_DEFAULT).keys(include_nested=self.include_nested)
+                        # this won't call _unravel_key_to_tuple but requires to get the default which can be suboptimal
+                        leaf_td = self.tensordict._get_tuple(key[:-1], None)
+                        if leaf_td is None or (
+                            not _is_tensor_collection(leaf_td.__class__)
+                            and not isinstance(leaf_td, KeyedJaggedTensor)
+                        ):
+                            return False
+                        return key[-1] in leaf_td.keys()
+                return False
+            # this is reached whenever there is more than one key but include_nested is False
+            if all(isinstance(subkey, str) for subkey in key):
+                raise TypeError(_NON_STR_KEY_TUPLE_ERR)
+
+    def __repr__(self):
+        include_nested = f"include_nested={self.include_nested}"
+        leaves_only = f"leaves_only={self.leaves_only}"
+        return f"{self.__class__.__name__}({list(self)},\n{indent(include_nested, 4*' ')},\n{indent(leaves_only, 4*' ')})"
+
+
+def _renamed_inplace_method(fn):
+    def wrapper(*args, **kwargs):
+        warn(
+            f"{fn.__name__.rstrip('_')} has been deprecated, use {fn.__name__} instead"
+        )
+        return fn(*args, **kwargs)
+
+    return wrapper
+
+
+class TensorDictBase(MutableMapping):
+    """TensorDictBase is an abstract parent class for TensorDicts, a torch.Tensor data container."""
+
+    LOCK_ERROR = (
+        "Cannot modify locked TensorDict. For in-place modification, consider "
+        "using the `set_()` method and make sure the key is present."
+    )
+    KEY_ERROR = 'key "{}" not found in {} with ' "keys {}"
+
+    def __new__(cls, *args: Any, **kwargs: Any) -> TensorDictBase:
+        self = super().__new__(cls)
+        self._safe = kwargs.get("_safe", False)
+        self._lazy = kwargs.get("_lazy", False)
+        self._inplace_set = kwargs.get("_inplace_set", False)
+        self.is_meta = kwargs.get("is_meta", False)
+        self._is_locked = kwargs.get("_is_locked", False)
+        self._cache = None
+        self._last_op = None
+        self.__last_op_queue = None
+        return self
+
+    def __getstate__(self) -> dict[str, Any]:
+        state = self.__dict__.copy()
+        return state
+
+    def __setstate__(self, state: dict[str, Any]) -> dict[str, Any]:
+        self.__dict__.update(state)
+
+    @staticmethod
+    def from_module(module):
+        """Copies the params and buffers of a module in a tensordict.
+
+        Examples:
+            >>> from torch import nn
+            >>> module = nn.TransformerDecoder(
+            ...     decoder_layer=nn.TransformerDecoderLayer(nhead=4, d_model=4),
+            ...     num_layers=1)
+            >>> params = TensorDict.from_module(module)
+            >>> print(params["layers", "0", "linear1"])
+            TensorDict(
+                fields={
+                    bias: Parameter(shape=torch.Size([2048]), device=cpu, dtype=torch.float32, is_shared=False),
+                    weight: Parameter(shape=torch.Size([2048, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+        """
+        td = TensorDict(dict(module.named_parameters()), [])
+        td.update(dict(module.named_buffers()))
+        td = td.unflatten_keys(".")
+        td.lock_()
+        return td
+
+    @property
+    def shape(self) -> torch.Size:
+        """See :obj:`TensorDictBase.batch_size`."""
+        return self.batch_size
+
+    @property
+    @abc.abstractmethod
+    def batch_size(self) -> torch.Size:
+        """Shape of (or batch_size) of a TensorDict.
+
+        The shape of a tensordict corresponds to the common N first
+        dimensions of the tensors it contains, where N is an arbitrary
+        number. The TensorDict shape is controlled by the user upon
+        initialization (i.e. it is not inferred from the tensor shapes) and
+        it should not be changed dynamically.
+
+        Returns:
+            a torch.Size object describing the TensorDict batch size.
+
+        """
+        raise NotImplementedError
+
+    def _erase_cache(self):
+        self._cache = None
+
+    @property
+    @abc.abstractmethod
+    def names(self):
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def _erase_names(self):
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def _rename_subtds(self, value):
+        # renames all the sub-tensordicts dimension according to value.
+        # If value has less dimensions than the TD, the rest is just assumed to be None
+        raise NotImplementedError
+
+    def _check_dim_name(self, name):
+        if name is None:
+            return False
+        if self._has_names() and name in self.names:
+            return True
+        for key in self.keys():
+            if _is_tensor_collection(self.entry_class(key)):
+                if self._get_str(key, NO_DEFAULT)._check_dim_name(name):
+                    return True
+        else:
+            return False
+
+    def refine_names(self, *names):
+        """Refines the dimension names of self according to names.
+
+        Refining is a special case of renaming that “lifts” unnamed dimensions.
+        A None dim can be refined to have any name; a named dim can only be
+        refined to have the same name.
+
+        Because named tensors can coexist with unnamed tensors, refining names
+        gives a nice way to write named-tensor-aware code that works with both
+        named and unnamed tensors.
+
+        names may contain up to one Ellipsis (...). The Ellipsis is expanded
+        greedily; it is expanded in-place to fill names to the same length as
+        self.dim() using names from the corresponding indices of self.names.
+
+        Returns: the tensordict with dimensions named accordingly.
+
+        """
+        # replace ellipsis if any
+        names_copy = copy(names)
+        if any(name is Ellipsis for name in names):
+            ellipsis_name = [NO_DEFAULT for _ in range(self.ndim - len(names) + 1)]
+            names = []
+            for name in names_copy:
+                if name is Ellipsis:
+                    names += ellipsis_name
+                else:
+                    names.append(name)
+        # check that the names that are set are either None or identical
+        curr_names = self.names
+        for i, name in enumerate(names):
+            if name is NO_DEFAULT:
+                # whatever value is ok
+                names[i] = curr_names[i]
+                continue
+            else:
+                if curr_names[i] is None:
+                    continue
+                if self.names[i] == name:
+                    continue
+                else:
+                    raise RuntimeError(
+                        f"refine_names: cannot coerce TensorDict names {self.names} with {names_copy}."
+                    )
+        self.names = names
+        # we also need to rename the sub-tensordicts
+        # self._rename_subtds(self.names)
+        return self
+
+    def rename(self, *names, **rename_map):
+        clone = self.clone(recurse=False)
+        if len(names) == 1 and names[0] is None:
+            clone.names = None
+        if rename_map and names:
+            raise ValueError(
+                "Passed both a name map and a name list. Only one is accepted."
+            )
+        elif not rename_map and not names:
+            raise ValueError(
+                "Neither a name map nor a name list was passed. "
+                "Only one is accepted."
+            )
+        elif rename_map:
+            cnames = list(clone.names)
+            for i, name in enumerate(cnames):
+                new_name = rename_map.pop(name, NO_DEFAULT)
+                if new_name is not NO_DEFAULT:
+                    cnames[i] = new_name
+            clone.names = cnames
+            if rename_map:
+                raise ValueError(
+                    f"Some names to be renamed were not part of the tensordict names: {rename_map.keys()} vs {self.names}."
+                )
+        else:
+            clone.names = names
+        return clone
+
+    def rename_(self, *names, **rename_map):
+        if len(names) == 1 and names[0] is None:
+            self.names = None
+        if rename_map and names:
+            raise ValueError(
+                "Passed both a name map and a name list. " "Only one is accepted."
+            )
+        elif not rename_map and not names and self.batch_dims:
+            raise ValueError(
+                "Neither a name map nor a name list was passed. "
+                "Only one is accepted."
+            )
+        elif rename_map:
+            cnames = list(self.names)
+            for i, name in enumerate(cnames):
+                new_name = rename_map.pop(name, NO_DEFAULT)
+                if new_name is not NO_DEFAULT:
+                    cnames[i] = new_name
+            if rename_map:
+                raise ValueError(
+                    f"Some names to be renamed were not part of the tensordict names: {rename_map.keys()} vs {self.names}."
+                )
+            self.names = cnames
+        else:
+            self.names = names
+        return self
+
+    @abc.abstractmethod
+    def _has_names(self):
+        raise NotImplementedError
+
+    @property
+    def _last_op_queue(self):
+        last_op_queue = self.__last_op_queue
+        if last_op_queue is None:
+            last_op_queue = self.__last_op_queue = collections.deque()
+        return last_op_queue
+
+    def size(self, dim: int | None = None) -> torch.Size | int:
+        """Returns the size of the dimension indicated by :obj:`dim`.
+
+        If dim is not specified, returns the batch_size (or shape) of the TensorDict.
+
+        """
+        if dim is None:
+            return self.batch_size
+        return self.batch_size[dim]
+
+    @property
+    def requires_grad(self) -> bool:
+        return any(v.requires_grad for v in self.values())
+
+    def _batch_size_setter(self, new_batch_size: torch.Size) -> None:
+        if new_batch_size == self.batch_size:
+            return
+        if self._lazy:
+            raise RuntimeError(
+                "modifying the batch size of a lazy repesentation of a "
+                "tensordict is not permitted. Consider instantiating the "
+                "tensordict first by calling `td = td.to_tensordict()` before "
+                "resetting the batch size."
+            )
+        if not isinstance(new_batch_size, torch.Size):
+            new_batch_size = torch.Size(new_batch_size)
+        for key in self.keys():
+            if _is_tensor_collection(self.entry_class(key)):
+                tensordict = self.get(key)
+                if len(tensordict.batch_size) < len(new_batch_size):
+                    # document as edge case
+                    tensordict.batch_size = new_batch_size
+                    self._set_str(key, tensordict, inplace=True, validated=True)
+        self._check_new_batch_size(new_batch_size)
+        self._change_batch_size(new_batch_size)
+        if self._has_names():
+            names = self.names
+            if len(names) < len(new_batch_size):
+                self.names = names + [None] * (len(new_batch_size) - len(names))
+            else:
+                self.names = names[: self.batch_dims]
+
+    @property
+    def batch_dims(self) -> int:
+        """Length of the tensordict batch size.
+
+        Returns:
+            int describing the number of dimensions of the tensordict.
+
+        """
+        return len(self.batch_size)
+
+    def ndimension(self) -> int:
+        return self.batch_dims
+
+    @property
+    def ndim(self) -> int:
+        return self.batch_dims
+
+    def dim(self) -> int:
+        return self.batch_dims
+
+    @property
+    @abc.abstractmethod
+    def device(self) -> torch.device | None:
+        """Device of a TensorDict.
+
+        If the TensorDict has a specified device, all
+        tensors of a tensordict must live on the same device. If the TensorDict device
+        is None, then different values can be located on different devices.
+
+        Returns:
+            torch.device object indicating the device where the tensors
+            are placed, or None if TensorDict does not have a device.
+
+        """
+        raise NotImplementedError
+
+    @device.setter
+    @abc.abstractmethod
+    def device(self, value: DeviceType) -> None:
+        raise NotImplementedError
+
+    def clear_device_(self) -> TensorDictBase:
+        """Clears the device of the tensordict.
+
+        Returns: self
+
+        """
+        self._device = None
+        for value in self.values():
+            if _is_tensor_collection(value.__class__):
+                value.clear_device_()
+        return self
+
+    clear_device = _renamed_inplace_method(clear_device_)
+
+    def is_shared(self) -> bool:
+        """Checks if tensordict is in shared memory.
+
+        If a TensorDict instance is in shared memory, any new tensor written
+        in it will be placed in shared memory. If a TensorDict is created with
+        tensors that are all in shared memory, this does not mean that it will be
+        in shared memory (as a new tensor may not be in shared memory).
+        Only if one calls `tensordict.share_memory_()` or places the tensordict
+        on a device where the content is shared will the tensordict be considered
+        in shared memory.
+
+        This is always True for CUDA tensordicts, except when stored as
+        MemmapTensors.
+
+        """
+        if self.device and not self._is_memmap:
+            return self.device.type == "cuda" or self._is_shared
+        return self._is_shared
+
+    def state_dict(self) -> OrderedDict[str, Any]:
+        out = collections.OrderedDict()
+        for key, item in self.apply(memmap_tensor_as_tensor).items():
+            out[key] = (
+                item if not _is_tensor_collection(item.__class__) else item.state_dict()
+            )
+        if "__batch_size" in out:
+            raise KeyError(
+                "Cannot retrieve the state_dict of a TensorDict with `'__batch_size'` key"
+            )
+        if "__device" in out:
+            raise KeyError(
+                "Cannot retrieve the state_dict of a TensorDict with `'__batch_size'` key"
+            )
+        out["__batch_size"] = self.batch_size
+        out["__device"] = self.device
+        return out
+
+    def load_state_dict(self, state_dict: OrderedDict[str, Any]) -> TensorDictBase:
+        # copy since we'll be using pop
+        state_dict = copy(state_dict)
+        self.batch_size = state_dict.pop("__batch_size")
+        device = state_dict.pop("__device")
+        if device is not None:
+            self.to(device)
+        for key, item in state_dict.items():
+            if isinstance(item, dict):
+                self.set(
+                    key,
+                    self.get(key, default=TensorDict({}, [])).load_state_dict(item),
+                    inplace=True,
+                )
+            else:
+                self.set(key, item, inplace=True)
+        return self
+
+    def is_memmap(self) -> bool:
+        """Checks if tensordict is stored with MemmapTensors."""
+        return self._is_memmap
+
+    def numel(self) -> int:
+        """Total number of elements in the batch."""
+        return max(1, prod(self.batch_size))
+
+    def _check_batch_size(self) -> None:
+        bs = [value.shape[: self.batch_dims] for value in self.values()] + [
+            self.batch_size
+        ]
+        if len(set(bs)) > 1:
+            raise RuntimeError(
+                f"batch_size are incongruent, got {list(set(bs))}, "
+                f"-- expected {self.batch_size}"
+            )
+
+    def _check_is_shared(self) -> bool:
+        raise NotImplementedError(f"{self.__class__.__name__}")
+
+    def _check_device(self) -> None:
+        raise NotImplementedError(f"{self.__class__.__name__}")
+
+    @abc.abstractmethod
+    def entry_class(self, key: NestedKey) -> type:
+        """Returns the class of an entry, avoiding a call to `isinstance(td.get(key), type)`."""
+        raise NotImplementedError(f"{self.__class__.__name__}")
+
+    def set(
+        self, key: NestedKey, item: CompatibleType, inplace: bool = False, **kwargs: Any
+    ) -> TensorDictBase:
+        """Sets a new key-value pair.
+
+        Args:
+            key (str, tuple of str): name of the key to be set.
+                If tuple of str it is equivalent to chained calls of getattr
+            item (torch.Tensor): value to be stored in the tensordict
+            inplace (bool, optional): if True and if a key matches an existing
+                key in the tensordict, then the update will occur in-place
+                for that key-value pair. Default is :obj:`False`.
+
+        Returns:
+            self
+
+        """
+        key = _unravel_key_to_tuple(key)
+        # inplace is loose here, but for set_ it is constraining. We translate it
+        # to None to tell _set_str and others to drop it if the key isn't found
+        inplace = BEST_ATTEMPT_INPLACE if inplace else False
+        return self._set_tuple(key, item, inplace=inplace, validated=False)
+
+    def _convert_inplace(self, inplace, key):
+        if inplace is not False:
+            has_key = key in self.keys()
+            if inplace is True and not has_key:  # inplace could be None
+                raise KeyError(
+                    TensorDictBase.KEY_ERROR.format(
+                        key, self.__class__.__name__, sorted(self.keys())
+                    )
+                )
+            inplace = has_key
+        return inplace
+
+    @abc.abstractmethod
+    def _set_str(self, key, value, *, inplace, validated):
+        ...
+
+    @abc.abstractmethod
+    def _set_tuple(self, key, value, *, inplace, validated):
+        ...
+
+    def set_at_(
+        self, key: NestedKey, value: CompatibleType, index: IndexType
+    ) -> TensorDictBase:
+        """Sets the values in-place at the index indicated by :obj:`idx`.
+
+        Args:
+            key (str, tuple of str): key to be modified.
+            value (torch.Tensor): value to be set at the index `idx`
+            index (int, tensor or tuple): index where to write the values.
+
+        Returns:
+            self
+
+        """
+        key = _unravel_key_to_tuple(key)
+        return self._set_at_tuple(key, value, index, validated=False)
+
+    @abc.abstractmethod
+    def _set_at_str(self, key, value, idx, *, validated):
+        ...
+
+    @abc.abstractmethod
+    def _set_at_tuple(self, key, value, idx, *, validated):
+        ...
+
+    def set_(
+        self,
+        key: NestedKey,
+        item: CompatibleType,
+    ) -> TensorDictBase:
+        """Sets a value to an existing key while keeping the original storage.
+
+        Args:
+            key (str): name of the value
+            item (torch.Tensor): value to be stored in the tensordict
+
+        Returns:
+            self
+
+        """
+        key = _unravel_key_to_tuple(key)
+        return self._set_tuple(key, item, inplace=True, validated=False)
+
+    @abc.abstractmethod
+    def _stack_onto_(
+        self,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> TensorDictBase:
+        """Stacks a list of values onto an existing key while keeping the original storage.
+
+        Args:
+            key (str): name of the value
+            list_item (list of torch.Tensor): value to be stacked and stored in the tensordict.
+            dim (int): dimension along which the tensors should be stacked.
+
+        Returns:
+            self
+
+        """
+        raise NotImplementedError(f"{self.__class__.__name__}")
+
+    def gather_and_stack(self, dst: int) -> TensorDictBase | None:
+        """Gathers tensordicts from various workers and stacks them onto self in the destination worker.
+
+        Args:
+            dst (int): the rank of the destination worker where :func:`gather_and_stack` will be called.
+
+        Example:
+            >>> from torch import multiprocessing as mp
+            >>> from tensordict import TensorDict
+            >>> import torch
+            >>>
+            >>> def client():
+            ...     torch.distributed.init_process_group(
+            ...         "gloo",
+            ...         rank=1,
+            ...         world_size=2,
+            ...         init_method=f"tcp://localhost:10003",
+            ...     )
+            ...     # Create a single tensordict to be sent to server
+            ...     td = TensorDict(
+            ...         {("a", "b"): torch.randn(2),
+            ...          "c": torch.randn(2)}, [2]
+            ...     )
+            ...     td.gather_and_stack(0)
+            ...
+            >>> def server():
+            ...     torch.distributed.init_process_group(
+            ...         "gloo",
+            ...         rank=0,
+            ...         world_size=2,
+            ...         init_method=f"tcp://localhost:10003",
+            ...     )
+            ...     # Creates the destination tensordict on server.
+            ...     # The first dim must be equal to world_size-1
+            ...     td = TensorDict(
+            ...         {("a", "b"): torch.zeros(2),
+            ...          "c": torch.zeros(2)}, [2]
+            ...     ).expand(1, 2).contiguous()
+            ...     td.gather_and_stack(0)
+            ...     assert td["a", "b"] != 0
+            ...     print("yuppie")
+            ...
+            >>> if __name__ == "__main__":
+            ...     mp.set_start_method("spawn")
+            ...
+            ...     main_worker = mp.Process(target=server)
+            ...     secondary_worker = mp.Process(target=client)
+            ...
+            ...     main_worker.start()
+            ...     secondary_worker.start()
+            ...
+            ...     main_worker.join()
+            ...     secondary_worker.join()
+        """
+        output = (
+            [None for _ in range(dist.get_world_size())]
+            if dst == dist.get_rank()
+            else None
+        )
+        dist.gather_object(self, output, dst=dst)
+        if dst == dist.get_rank():
+            # remove self from output
+            output = [item for i, item in enumerate(output) if i != dst]
+            self.update(torch.stack(output, 0), inplace=True)
+            return self
+        return None
+
+    def send(self, dst: int, init_tag: int = 0, pseudo_rand: bool = False) -> None:
+        """Sends the content of a tensordict to a distant worker.
+
+        Args:
+            dst (int): the rank of the destination worker where the content
+                should be sent.
+            init_tag (int): the initial tag to be used to mark the tensors.
+                Note that this will be incremented by as much as the number of
+                tensors contained in the TensorDict.
+            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
+                random, allowing to send multiple data from different nodes
+                without overlap. Notice that the generation of these pseudo-random
+                numbers is expensive (1e-5 sec/number), meaning that it could
+                slow down the runtime of your algorithm.
+                Defaults to ``False``.
+
+        Example:
+            >>> from torch import multiprocessing as mp
+            >>> from tensordict import TensorDict
+            >>> import torch
+            >>>
+            >>>
+            >>> def client():
+            ...     torch.distributed.init_process_group(
+            ...         "gloo",
+            ...         rank=1,
+            ...         world_size=2,
+            ...         init_method=f"tcp://localhost:10003",
+            ...     )
+            ...
+            ...     td = TensorDict(
+            ...         {
+            ...             ("a", "b"): torch.randn(2),
+            ...             "c": torch.randn(2, 3),
+            ...             "_": torch.ones(2, 1, 5),
+            ...         },
+            ...         [2],
+            ...     )
+            ...     td.send(0)
+            ...
+            >>>
+            >>> def server(queue):
+            ...     torch.distributed.init_process_group(
+            ...         "gloo",
+            ...         rank=0,
+            ...         world_size=2,
+            ...         init_method=f"tcp://localhost:10003",
+            ...     )
+            ...     td = TensorDict(
+            ...         {
+            ...             ("a", "b"): torch.zeros(2),
+            ...             "c": torch.zeros(2, 3),
+            ...             "_": torch.zeros(2, 1, 5),
+            ...         },
+            ...         [2],
+            ...     )
+            ...     td.recv(1)
+            ...     assert (td != 0).all()
+            ...     queue.put("yuppie")
+            ...
+            >>>
+            >>> if __name__=="__main__":
+            ...     queue = mp.Queue(1)
+            ...     main_worker = mp.Process(target=server, args=(queue,))
+            ...     secondary_worker = mp.Process(target=client)
+            ...
+            ...     main_worker.start()
+            ...     secondary_worker.start()
+            ...     out = queue.get(timeout=10)
+            ...     assert out == "yuppie"
+            ...     main_worker.join()
+            ...     secondary_worker.join()
+
+        """
+        self._send(dst, _tag=init_tag - 1, pseudo_rand=pseudo_rand)
+
+    def _send(self, dst: int, _tag: int = -1, pseudo_rand: bool = False) -> int:
+        for key in self.sorted_keys:
+            value = self._get_str(key, NO_DEFAULT)
+            if isinstance(value, Tensor):
+                pass
+            elif _is_tensor_collection(value.__class__):
+                _tag = value._send(dst, _tag=_tag, pseudo_rand=pseudo_rand)
+                continue
+            elif isinstance(value, MemmapTensor):
+                value = value.as_tensor()
+            else:
+                raise NotImplementedError(f"Type {type(value)} is not supported.")
+            if not pseudo_rand:
+                _tag += 1
+            else:
+                _tag = int_generator(_tag + 1)
+            dist.send(value, dst=dst, tag=_tag)
+
+        return _tag
+
+    def recv(self, src: int, init_tag: int = 0, pseudo_rand: bool = False) -> int:
+        """Receives the content of a tensordict and updates content with it.
+
+        Check the example in the `send` method for context.
+
+        Args:
+            src (int): the rank of the source worker.
+            init_tag (int): the ``init_tag`` used by the source worker.
+            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
+                random, allowing to send multiple data from different nodes
+                without overlap. Notice that the generation of these pseudo-random
+                numbers is expensive (1e-5 sec/number), meaning that it could
+                slow down the runtime of your algorithm.
+                This value must match the one passed to :func:`send`.
+                Defaults to ``False``.
+
+        """
+        return self._recv(src, _tag=init_tag - 1, pseudo_rand=pseudo_rand)
+
+    def _recv(self, src: int, _tag: int = -1, pseudo_rand: bool = False) -> int:
+        for key in self.sorted_keys:
+            value = self._get_str(key, NO_DEFAULT)
+            if isinstance(value, Tensor):
+                pass
+            elif _is_tensor_collection(value.__class__):
+                _tag = value._recv(src, _tag=_tag, pseudo_rand=pseudo_rand)
+                continue
+            elif isinstance(value, MemmapTensor):
+                value = value.as_tensor()
+            else:
+                raise NotImplementedError(f"Type {type(value)} is not supported.")
+            if not pseudo_rand:
+                _tag += 1
+            else:
+                _tag = int_generator(_tag + 1)
+            dist.recv(value, src=src, tag=_tag)
+            self._set_str(key, value, inplace=True, validated=True)
+
+        return _tag
+
+    def isend(self, dst: int, init_tag: int = 0, pseudo_rand: bool = False) -> int:
+        """Sends the content of the tensordict asynchronously.
+
+        Args:
+            dst (int): the rank of the destination worker where the content
+                should be sent.
+            init_tag (int): the initial tag to be used to mark the tensors.
+                Note that this will be incremented by as much as the number of
+                tensors contained in the TensorDict.
+            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
+                random, allowing to send multiple data from different nodes
+                without overlap. Notice that the generation of these pseudo-random
+                numbers is expensive (1e-5 sec/number), meaning that it could
+                slow down the runtime of your algorithm.
+                Defaults to ``False``.
+
+        Example:
+            >>> import torch
+            >>> from tensordict import TensorDict
+            >>> from torch import multiprocessing as mp
+            >>> def client():
+            ...     torch.distributed.init_process_group(
+            ...         "gloo",
+            ...         rank=1,
+            ...         world_size=2,
+            ...         init_method=f"tcp://localhost:10003",
+            ...     )
+            ...
+            ...     td = TensorDict(
+            ...         {
+            ...             ("a", "b"): torch.randn(2),
+            ...             "c": torch.randn(2, 3),
+            ...             "_": torch.ones(2, 1, 5),
+            ...         },
+            ...         [2],
+            ...     )
+            ...     td.isend(0)
+            ...
+            >>>
+            >>> def server(queue, return_premature=True):
+            ...     torch.distributed.init_process_group(
+            ...         "gloo",
+            ...         rank=0,
+            ...         world_size=2,
+            ...         init_method=f"tcp://localhost:10003",
+            ...     )
+            ...     td = TensorDict(
+            ...         {
+            ...             ("a", "b"): torch.zeros(2),
+            ...             "c": torch.zeros(2, 3),
+            ...             "_": torch.zeros(2, 1, 5),
+            ...         },
+            ...         [2],
+            ...     )
+            ...     out = td.irecv(1, return_premature=return_premature)
+            ...     if return_premature:
+            ...         for fut in out:
+            ...             fut.wait()
+            ...     assert (td != 0).all()
+            ...     queue.put("yuppie")
+            ...
+            >>>
+            >>> if __name__ == "__main__":
+            ...     queue = mp.Queue(1)
+            ...     main_worker = mp.Process(
+            ...         target=server,
+            ...         args=(queue, )
+            ...         )
+            ...     secondary_worker = mp.Process(target=client)
+            ...
+            ...     main_worker.start()
+            ...     secondary_worker.start()
+            ...     out = queue.get(timeout=10)
+            ...     assert out == "yuppie"
+            ...     main_worker.join()
+            ...     secondary_worker.join()
+
+        """
+        return self._isend(dst, init_tag - 1, pseudo_rand=pseudo_rand)
+
+    def _isend(
+        self,
+        dst: int,
+        _tag: int = -1,
+        _futures: list[torch.Future] | None = None,
+        pseudo_rand: bool = False,
+    ) -> int:
+        root = False
+        if _futures is None:
+            root = True
+            _futures = []
+        for key in self.sorted_keys:
+            value = self._get_str(key, NO_DEFAULT)
+            if _is_tensor_collection(value.__class__):
+                _tag = value._isend(
+                    dst, _tag=_tag, pseudo_rand=pseudo_rand, _futures=_futures
+                )
+                continue
+            elif isinstance(value, Tensor):
+                pass
+            elif isinstance(value, MemmapTensor):
+                value = value.as_tensor()
+            else:
+                raise NotImplementedError(f"Type {type(value)} is not supported.")
+            if not pseudo_rand:
+                _tag += 1
+            else:
+                _tag = int_generator(_tag + 1)
+            _future = dist.isend(value, dst=dst, tag=_tag)
+            _futures.append(_future)
+        if root:
+            for _future in _futures:
+                _future.wait()
+        return _tag
+
+    def irecv(
+        self,
+        src: int,
+        return_premature: bool = False,
+        init_tag: int = 0,
+        pseudo_rand: bool = False,
+    ) -> tuple[int, list[torch.Future]] | list[torch.Future] | None:
+        """Receives the content of a tensordict and updates content with it asynchronously.
+
+        Check the example in the `isend` method for context.
+
+        Args:
+            src (int): the rank of the source worker.
+            return_premature (bool): if ``True``, returns a list of futures to wait
+                upon until the tensordict is updated. Defaults to ``False``,
+                i.e. waits until update is completed withing the call.
+            init_tag (int): the ``init_tag`` used by the source worker.
+            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
+                random, allowing to send multiple data from different nodes
+                without overlap. Notice that the generation of these pseudo-random
+                numbers is expensive (1e-5 sec/number), meaning that it could
+                slow down the runtime of your algorithm.
+                This value must match the one passed to :func:`isend`.
+                Defaults to ``False``.
+
+        Returns:
+            if ``return_premature=True``, a list of futures to wait
+                upon until the tensordict is updated.
+        """
+        return self._irecv(
+            src, return_premature, _tag=init_tag - 1, pseudo_rand=pseudo_rand
+        )
+
+    def _irecv(
+        self,
+        src: int,
+        return_premature: bool = False,
+        _tag: int = -1,
+        _future_list: list[torch.Future] = None,
+        pseudo_rand: bool = False,
+    ) -> tuple[int, list[torch.Future]] | list[torch.Future] | None:
+        root = False
+        if _future_list is None:
+            _future_list = []
+            root = True
+
+        for key in self.sorted_keys:
+            value = self._get_str(key, NO_DEFAULT)
+            if _is_tensor_collection(value.__class__):
+                _tag, _future_list = value._irecv(
+                    src,
+                    _tag=_tag,
+                    _future_list=_future_list,
+                    pseudo_rand=pseudo_rand,
+                )
+                continue
+            elif isinstance(value, MemmapTensor):
+                value = value.as_tensor()
+            elif isinstance(value, Tensor):
+                pass
+            else:
+                raise NotImplementedError(f"Type {type(value)} is not supported.")
+            if not pseudo_rand:
+                _tag += 1
+            else:
+                _tag = int_generator(_tag + 1)
+            _future_list.append(dist.irecv(value, src=src, tag=_tag))
+        if not root:
+            return _tag, _future_list
+        elif return_premature:
+            return _future_list
+        else:
+            for future in _future_list:
+                future.wait()
+            return
+
+    def reduce(self, dst, op=dist.ReduceOp.SUM, async_op=False, return_premature=False):
+        """Reduces the tensordict across all machines.
+
+        Only the process with ``rank`` dst is going to receive the final result.
+
+        """
+        return self._reduce(dst, op, async_op, return_premature)
+
+    def _reduce(
+        self,
+        dst,
+        op=dist.ReduceOp.SUM,
+        async_op=False,
+        return_premature=False,
+        _future_list=None,
+    ):
+        root = False
+        if _future_list is None:
+            _future_list = []
+            root = True
+        for key in self.sorted_keys:
+            value = self._get_str(key, NO_DEFAULT)
+            if _is_tensor_collection(value.__class__):
+                _future_list = value._reduce(
+                    dst=dst,
+                    op=op,
+                    async_op=async_op,
+                    _future_list=_future_list,
+                )
+                continue
+            elif isinstance(value, MemmapTensor):
+                value = value.as_tensor()
+            elif isinstance(value, Tensor):
+                pass
+            else:
+                raise NotImplementedError(f"Type {type(value)} is not supported.")
+            _future_list.append(dist.reduce(value, dst=dst, op=op, async_op=async_op))
+        if not root:
+            return _future_list
+        elif async_op and return_premature:
+            return _future_list
+        elif async_op:
+            for future in _future_list:
+                future.wait()
+            return
+
+    def _stack_onto_at_(
+        self,
+        key: str,
+        list_item: list[CompatibleType],
+        dim: int,
+        idx: IndexType,
+    ) -> TensorDictBase:
+        """Similar to _stack_onto_ but on a specific index. Only works with regular TensorDicts."""
+        raise RuntimeError(
+            f"Cannot call _stack_onto_at_ with {self.__class__.__name__}. "
+            "This error is probably caused by a call to a lazy operation before stacking. "
+            "Make sure your sub-classed tensordicts are turned into regular tensordicts by calling to_tensordict() "
+            "before calling __getindex__ and stack."
+        )
+
+    def _default_get(
+        self, key: str, default: str | CompatibleType = NO_DEFAULT
+    ) -> CompatibleType:
+        if default is not NO_DEFAULT:
+            return default
+        else:
+            # raise KeyError
+            raise KeyError(
+                TensorDictBase.KEY_ERROR.format(
+                    key, self.__class__.__name__, sorted(self.keys())
+                )
+            )
+
+    def get(
+        self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
+    ) -> CompatibleType:
+        """Gets the value stored with the input key.
+
+        Args:
+            key (str, tuple of str): key to be queried. If tuple of str it is
+                equivalent to chained calls of getattr.
+            default: default value if the key is not found in the tensordict.
+
+        """
+        key = _unravel_key_to_tuple(key)
+        if not key:
+            raise KeyError(_GENERIC_NESTED_ERR)
+        return self._get_tuple(key, default=default)
+
+    @abc.abstractmethod
+    def _get_str(self, key, default):
+        ...
+
+    @abc.abstractmethod
+    def _get_tuple(self, key, default):
+        ...
+
+    def get_item_shape(self, key: NestedKey):
+        """Returns the shape of the entry."""
+        return self.get(key).shape
+
+    def pop(
+        self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
+    ) -> CompatibleType:
+        key = _unravel_key_to_tuple(key)
+        if not key:
+            raise KeyError(_GENERIC_NESTED_ERR)
+        try:
+            # using try/except for get/del is suboptimal, but
+            # this is faster that checkink if key in self keys
+            out = self.get(key, default)
+            self.del_(key)
+        except KeyError as err:
+            # if default provided, 'out' value will return, else raise error
+            if default == NO_DEFAULT:
+                raise KeyError(
+                    f"You are trying to pop key `{key}` which is not in dict "
+                    f"without providing default value."
+                ) from err
+        return out
+
+    def apply_(self, fn: Callable, *others) -> TensorDictBase:
+        """Applies a callable to all values stored in the tensordict and re-writes them in-place.
+
+        Args:
+            fn (Callable): function to be applied to the tensors in the
+                tensordict.
+            *others (sequence of TensorDictBase, optional): the other
+                tensordicts to be used.
+
+        Returns:
+            self or a copy of self with the function applied
+
+        """
+        return self.apply(fn, *others, inplace=True)
+
+    def apply(
+        self,
+        fn: Callable,
+        *others: TensorDictBase,
+        batch_size: Sequence[int] | None = None,
+        device: torch.device | None = None,
+        names: Sequence[str] | None = None,
+        inplace: bool = False,
+        **constructor_kwargs,
+    ) -> TensorDictBase:
+        """Applies a callable to all values stored in the tensordict and sets them in a new tensordict.
+
+        The apply method will return an TensorDict instance, regardless of the
+        input type. To keep the same type, one can execute
+
+          >>> out = td.clone(False).update(td.apply(...))
+
+        Args:
+            fn (Callable): function to be applied to the tensors in the
+                tensordict.
+            *others (TensorDictBase instances, optional): if provided, these
+                tensordicts should have a structure matching the one of the
+                current tensordict. The :obj:`fn` argument should receive as many
+                inputs as the number of tensordicts, including the one where apply is
+                being called.
+            batch_size (sequence of int, optional): if provided,
+                the resulting TensorDict will have the desired batch_size.
+                The :obj:`batch_size` argument should match the batch_size after
+                the transformation. This is a keyword only argument.
+            device (torch.device, optional): the resulting device, if any.
+            names (list of str, optional): the new dimension names, in case the
+                batch_size is modified.
+            inplace (bool, optional): if True, changes are made in-place.
+                Default is False. This is a keyword only argument.
+            **constructor_kwargs: additional keyword arguments to be passed to the
+                TensorDict constructor.
+
+        Returns:
+            a new tensordict with transformed_in tensors.
+
+        Example:
+            >>> td = TensorDict({"a": -torch.ones(3), "b": {"c": torch.ones(3)}}, batch_size=[3])
+            >>> td_1 = td.apply(lambda x: x+1)
+            >>> assert (td["a"] == 0).all()
+            >>> assert (td["b", "c"] == 2).all()
+            >>> td_2 = td.apply(lambda x, y: x+y, td)
+            >>> assert (td_2["a"] == -2).all()
+            >>> assert (td_2["b", "c"] == 2).all()
+        """
+        if inplace:
+            out = self
+        elif batch_size is not None:
+            out = TensorDict(
+                {},
+                batch_size=torch.Size(batch_size),
+                names=names,
+                device=self.device if not device else device,
+                _run_checks=False,
+                **constructor_kwargs,
+            )
+        else:
+            out = TensorDict(
+                {},
+                batch_size=self.batch_size,
+                device=self.device if not device else device,
+                names=self.names if self._has_names() else None,
+                _run_checks=False,
+                **constructor_kwargs,
+            )
+
+        is_locked = out.is_locked
+        if not inplace and is_locked:
+            out.unlock_()
+
+        for key, item in self.items():
+            _others = [_other.get(key) for _other in others]
+            if _is_tensor_collection(item.__class__):
+                item_trsf = item.apply(
+                    fn,
+                    *_others,
+                    inplace=inplace,
+                    batch_size=batch_size,
+                    device=device,
+                    **constructor_kwargs,
+                )
+            else:
+                item_trsf = fn(item, *_others)
+            if item_trsf is not None:
+                # if `self` is a `SubTensorDict` we want to process the input,
+                # hence we call `set` rather than `_set_str`.
+                if isinstance(self, SubTensorDict):
+                    out.set(key, item_trsf, inplace=inplace)
+                else:
+                    out._set_str(
+                        key,
+                        item_trsf,
+                        inplace=BEST_ATTEMPT_INPLACE if inplace else False,
+                        validated=False,
+                    )
+
+        if not inplace and is_locked:
+            out.lock_()
+        return out
+
+    @cache  # noqa: B019
+    def _add_batch_dim(self, *, in_dim, vmap_level):
+        if self.is_memmap():
+            td = self.cpu().as_tensor()
+        else:
+            td = self
+        out = TensorDict(
+            {
+                key: value._add_batch_dim(in_dim=in_dim, vmap_level=vmap_level)
+                if is_tensor_collection(value)
+                else _add_batch_dim(value, in_dim, vmap_level)
+                for key, value in td.items()
+            },
+            batch_size=[b for i, b in enumerate(td.batch_size) if i != in_dim],
+            names=[name for i, name in enumerate(td.names) if i != in_dim],
+        )
+        return out
+
+    @cache  # noqa: B019
+    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
+        new_batch_size = list(self.batch_size)
+        new_batch_size.insert(out_dim, batch_size)
+        new_names = list(self.names)
+        new_names.insert(out_dim, None)
+        out = TensorDict(
+            {
+                key: value._remove_batch_dim(
+                    vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
+                )
+                if is_tensor_collection(value)
+                else _remove_batch_dim(value, vmap_level, batch_size, out_dim)
+                for key, value in self.items()
+            },
+            batch_size=new_batch_size,
+            names=new_names,
+        )
+        return out
+
+    def as_tensor(self):
+        """Calls as_tensor on all the tensors contained in the object.
+
+        This is reserved to classes that contain exclusively MemmapTensors,
+        and will raise an exception in all other cases.
+
+        """
+        try:
+            return self.apply(lambda x: x.as_tensor())
+        except AttributeError as err:
+            raise AttributeError(
+                f"{self.__class__.__name__} does not have an 'as_tensor' method "
+                f"because at least one of its tensors does not support this method."
+            ) from err
+
+    def update(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        clone: bool = False,
+        inplace: bool = False,
+    ) -> TensorDictBase:
+        """Updates the TensorDict with values from either a dictionary or another TensorDict.
+
+        Args:
+            input_dict_or_td (TensorDictBase or dict): Does not keyword arguments
+                (unlike :obj:`dict.update()`).
+            clone (bool, optional): whether the tensors in the input (
+                tensor) dict should be cloned before being set. Default is
+                `False`.
+            inplace (bool, optional): if True and if a key matches an existing
+                key in the tensordict, then the update will occur in-place
+                for that key-value pair. Default is :obj:`False`.
+            **kwargs: keyword arguments for the :obj:`TensorDict.set` method
+
+        Returns:
+            self
+
+        """
+        if input_dict_or_td is self:
+            # no op
+            return self
+        keys = set(self.keys(False))
+        for key, value in list(input_dict_or_td.items()):
+            if clone and hasattr(value, "clone"):
+                value = value.clone()
+            if isinstance(key, tuple):
+                key, subkey = key[0], key[1:]
+            else:
+                subkey = []
+            # the key must be a string by now. Let's check if it is present
+            if key in keys:
+                target_type = self.entry_class(key)
+                if _is_tensor_collection(target_type):
+                    target = self.get(key)
+                    if len(subkey):
+                        target.update({subkey: value}, inplace=inplace, clone=clone)
+                        continue
+                    elif isinstance(value, (dict,)) or _is_tensor_collection(
+                        value.__class__
+                    ):
+                        if isinstance(value, LazyStackedTensorDict) and not isinstance(
+                            target, LazyStackedTensorDict
+                        ):
+                            self.set(
+                                key,
+                                LazyStackedTensorDict(
+                                    *target.unbind(value.stack_dim),
+                                    stack_dim=value.stack_dim,
+                                ).update(value, inplace=inplace, clone=clone),
+                            )
+                        else:
+                            target.update(value, inplace=inplace, clone=clone)
+                        continue
+            if len(subkey):
+                self.set((key, *subkey), value, inplace=inplace)
+            else:
+                self.set(key, value, inplace=inplace)
+        return self
+
+    def update_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        clone: bool = False,
+    ) -> TensorDictBase:
+        """Updates the TensorDict in-place with values from either a dictionary or another TensorDict.
+
+        Unlike TensorDict.update, this function will
+        throw an error if the key is unknown to the TensorDict
+
+        Args:
+            input_dict_or_td (TensorDictBase or dict): Does not keyword
+                arguments (unlike :obj:`dict.update()`).
+            clone (bool, optional): whether the tensors in the input (
+                tensor) dict should be cloned before being set. Default is
+                `False`.
+
+        Returns:
+            self
+
+        """
+        if input_dict_or_td is self:
+            # no op
+            return self
+        for key, value in input_dict_or_td.items():
+            # if not isinstance(value, _accepted_classes):
+            #     raise TypeError(
+            #         f"Expected value to be one of types {_accepted_classes} "
+            #         f"but got {type(value)}"
+            #     )
+            if clone:
+                value = value.clone()
+            self.set_(key, value)
+        return self
+
+    def update_at_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        idx: IndexType,
+        clone: bool = False,
+    ) -> TensorDictBase:
+        """Updates the TensorDict in-place at the specified index with values from either a dictionary or another TensorDict.
+
+        Unlike  TensorDict.update, this function will throw an error if the key is unknown to the TensorDict.
+
+        Args:
+            input_dict_or_td (TensorDictBase or dict): Does not keyword arguments
+                (unlike :obj:`dict.update()`).
+            idx (int, torch.Tensor, iterable, slice): index of the tensordict
+                where the update should occur.
+            clone (bool, optional): whether the tensors in the input (
+                tensor) dict should be cloned before being set. Default is
+                `False`.
+
+        Returns:
+            self
+
+        Examples:
+            >>> td = TensorDict(source={'a': torch.zeros(3, 4, 5),
+            ...    'b': torch.zeros(3, 4, 10)}, batch_size=[3, 4])
+            >>> td.update_at_(
+            ...    TensorDict(source={'a': torch.ones(1, 4, 5),
+            ...        'b': torch.ones(1, 4, 10)}, batch_size=[1, 4]),
+            ...    slice(1, 2))
+            TensorDict(
+                fields={
+                    a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32),
+                    b: Tensor(torch.Size([3, 4, 10]), dtype=torch.float32)},
+                batch_size=torch.Size([3, 4]),
+                device=None,
+                is_shared=False)
+
+        """
+        for key, value in input_dict_or_td.items():
+            if not isinstance(value, _ACCEPTED_CLASSES):
+                raise TypeError(
+                    f"Expected value to be one of types {_ACCEPTED_CLASSES} "
+                    f"but got {type(value)}"
+                )
+            if clone:
+                value = value.clone()
+            self.set_at_(key, value, idx)
+        return self
+
+    def _convert_to_tensor(self, array: np.ndarray) -> Tensor | MemmapTensor:
+        if isinstance(array, np.bool_):
+            array = array.item()
+        if isinstance(array, list):
+            array = np.asarray(array)
+        return torch.as_tensor(array, device=self.device)
+
+    def _convert_to_tensordict(self, dict_value: dict[str, Any]) -> TensorDictBase:
+        return TensorDict(
+            dict_value,
+            batch_size=self.batch_size,
+            device=self.device,
+            _is_shared=self._is_shared,
+            _is_memmap=self._is_memmap,
+        )
+
+    def _validate_key(self, key: NestedKey) -> NestedKey:
+        key = _unravel_key_to_tuple(key)
+        if not key:
+            raise KeyError(_GENERIC_NESTED_ERR)
+        return key
+
+    def _validate_value(
+        self,
+        value: CompatibleType | dict[str, CompatibleType],
+        *,
+        check_shape: bool = True,
+    ) -> CompatibleType | dict[str, CompatibleType]:
+        cls = value.__class__
+        is_tc = _is_tensor_collection(cls)
+        if is_tc or issubclass(cls, _ACCEPTED_CLASSES):
+            pass
+        elif issubclass(cls, dict):
+            value = self._convert_to_tensordict(value)
+            is_tc = True
+        else:
+            try:
+                value = self._convert_to_tensor(value)
+            except ValueError as err:
+                raise ValueError(
+                    f"TensorDict conversion only supports tensorclasses, tensordicts,"
+                    f" numeric scalars and tensors. Got {type(value)}"
+                ) from err
+        bs = self.batch_size
+        if check_shape and bs and _shape(value)[: len(bs)] != bs:
+            # if TensorDict, let's try to map it to the desired shape
+            if is_tc:
+                value = value.clone(recurse=False)
+                value.batch_size = self.batch_size
+            else:
+                raise RuntimeError(
+                    f"batch dimension mismatch, got self.batch_size"
+                    f"={self.batch_size} and value.shape[:self.batch_dims]"
+                    f"={_shape(value)[: self.batch_dims]} with value {value}"
+                )
+        device = self.device
+        if device is not None and value.device != device:
+            value = value.to(device, non_blocking=True)
+        if is_tc and check_shape:
+            has_names = self._has_names()
+            if has_names and value.names[: self.ndim] != self.names:
+                value = value.clone(False).refine_names(*self.names)
+            elif not has_names and value._has_names():
+                self.names = value.names[: self.batch_dims]
+
+        return value
+
+    @abc.abstractmethod
+    def pin_memory(self) -> TensorDictBase:
+        """Calls :obj:`pin_memory` on the stored tensors."""
+        raise NotImplementedError(f"{self.__class__.__name__}")
+
+    def items(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> Iterator[tuple[str, CompatibleType]]:
+        """Returns a generator of key-value pairs for the tensordict."""
+        # check the conditions once only
+        if include_nested and leaves_only:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                if _is_tensor_collection(val.__class__):
+                    yield from (
+                        (_unravel_key_to_tuple((k, _key)), _val)
+                        for _key, _val in val.items(
+                            include_nested=include_nested, leaves_only=leaves_only
+                        )
+                    )
+                else:
+                    yield k, val
+        elif include_nested:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                yield k, val
+                if _is_tensor_collection(val.__class__):
+                    yield from (
+                        (_unravel_key_to_tuple((k, _key)), _val)
+                        for _key, _val in val.items(
+                            include_nested=include_nested, leaves_only=leaves_only
+                        )
+                    )
+        elif leaves_only:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                if not _is_tensor_collection(val.__class__):
+                    yield k, val
+        else:
+            for k in self.keys():
+                yield k, self._get_str(k, NO_DEFAULT)
+
+    def values(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> Iterator[CompatibleType]:
+        """Returns a generator representing the values for the tensordict."""
+        # check the conditions once only
+        if include_nested and leaves_only:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                if _is_tensor_collection(val.__class__):
+                    yield from val.values(
+                        include_nested=include_nested, leaves_only=leaves_only
+                    )
+                else:
+                    yield val
+        elif include_nested:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                yield val
+                if _is_tensor_collection(val.__class__):
+                    yield from val.values(
+                        include_nested=include_nested, leaves_only=leaves_only
+                    )
+        elif leaves_only:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                if not _is_tensor_collection(val.__class__):
+                    yield val
+        else:
+            for k in self.keys():
+                yield self._get_str(k, NO_DEFAULT)
+
+    @abc.abstractmethod
+    def keys(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> _TensorDictKeysView:
+        """Returns a generator of tensordict keys."""
+        raise NotImplementedError(f"{self.__class__.__name__}")
+
+    @property
+    @cache  # noqa: B019
+    def sorted_keys(self) -> list[NestedKey]:
+        """Returns the keys sorted in alphabetical order.
+
+        Does not support extra argument.
+
+        If the TensorDict is locked, the keys are cached until the tensordict
+        is unlocked.
+
+        """
+        return sorted(self.keys())
+
+    def expand(self, *shape: int) -> TensorDictBase:
+        """Expands each tensors of the tensordict according to the torch.expand function.
+
+        In practice, this amends to: :obj:`tensor.expand(*shape, *tensor.shape)`.
+
+        Supports iterables to specify the shape
+
+        Examples:
+            >>> td = TensorDict(source={'a': torch.zeros(3, 4, 5),
+            ...     'b': torch.zeros(3, 4, 10)}, batch_size=[3, 4])
+            >>> td_expand = td.expand(10, 3, 4)
+            >>> assert td_expand.shape == torch.Size([10, 3, 4])
+            >>> assert td_expand.get("a").shape == torch.Size([10, 3, 4, 5])
+
+        """
+        d = {}
+        tensordict_dims = self.batch_dims
+
+        if len(shape) == 1 and isinstance(shape[0], Sequence):
+            shape = tuple(shape[0])
+
+        # new shape dim check
+        if len(shape) < len(self.shape):
+            raise RuntimeError(
+                "the number of sizes provided ({shape_dim}) must be greater or equal to the number of "
+                "dimensions in the TensorDict ({tensordict_dim})".format(
+                    shape_dim=len(shape), tensordict_dim=tensordict_dims
+                )
+            )
+
+        # new shape compatability check
+        for old_dim, new_dim in zip(self.batch_size, shape[-tensordict_dims:]):
+            if old_dim != 1 and new_dim != old_dim:
+                raise RuntimeError(
+                    "Incompatible expanded shape: The expanded shape length at non-singleton dimension should be same "
+                    "as the original length. target_shape = {new_shape}, existing_shape = {old_shape}".format(
+                        new_shape=shape, old_shape=self.batch_size
+                    )
+                )
+        for key, value in self.items():
+            tensor_dims = len(value.shape)
+            last_n_dims = tensor_dims - tensordict_dims
+            if last_n_dims > 0:
+                d[key] = value.expand((*shape, *value.shape[-last_n_dims:]))
+            else:
+                d[key] = value.expand(shape)
+        return TensorDict(
+            source=d,
+            batch_size=torch.Size(shape),
+            device=self.device,
+            _run_checks=False,
+        )
+
+    def flatten(self, start_dim=0, end_dim=-1):
+        """Flattens all the tensors of a tensordict.
+
+        Args:
+            start_dim (int) – the first dim to flatten
+            end_dim (int) – the last dim to flatten
+
+        Examples:
+            >>> td = TensorDict({"a": torch.arange(60).view(3, 4, 5), "b": torch.arange(12).view(3, 4)}, [3, 4])
+            >>> td_flat = td.flatten(0, 1)
+            >>> td_flat.batch_size
+            torch.Size([12])
+            >>> td_flat["a"]
+            tensor([[ 0,  1,  2,  3,  4],
+                    [ 5,  6,  7,  8,  9],
+                    [10, 11, 12, 13, 14],
+                    [15, 16, 17, 18, 19],
+                    [20, 21, 22, 23, 24],
+                    [25, 26, 27, 28, 29],
+                    [30, 31, 32, 33, 34],
+                    [35, 36, 37, 38, 39],
+                    [40, 41, 42, 43, 44],
+                    [45, 46, 47, 48, 49],
+                    [50, 51, 52, 53, 54],
+                    [55, 56, 57, 58, 59]])
+            >>> td_flat["b"]
+            tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
+
+        """
+        if end_dim < 0:
+            end_dim = self.ndim + end_dim
+            if end_dim < 0:
+                raise ValueError(
+                    f"Incompatible end_dim {end_dim} for tensordict with shape {self.shape}."
+                )
+        if end_dim <= start_dim:
+            raise ValueError(
+                "The end dimension must be strictly greater than the start dim."
+            )
+
+        def flatten(tensor):
+            return torch.flatten(tensor, start_dim, end_dim)
+
+        nelt = prod(self.batch_size[start_dim : end_dim + 1])
+        if start_dim > 0:
+            batch_size = (
+                list(self.batch_size)[:start_dim]
+                + [nelt]
+                + list(self.batch_size[end_dim + 1 :])
+            )
+        else:
+            batch_size = [nelt] + list(self.batch_size[end_dim + 1 :])
+        out = self.apply(flatten, batch_size=batch_size)
+        if self._has_names():
+            names = [
+                name
+                for i, name in enumerate(self.names)
+                if (i < start_dim or i > end_dim)
+            ]
+            names.insert(start_dim, None)
+            out.names = names
+        return out
+
+    def unflatten(self, dim, unflattened_size):
+        """Unflattens a tensordict dim expanding it to a desired shape.
+
+        Args:
+            dim (int): specifies the dimension of the input tensor to be unflattened.
+            unflattened_size (shape): is the new shape of the unflattened dimension of the tensordict.
+
+        Examples:
+            >>> td = TensorDict({"a": torch.arange(60).view(3, 4, 5), "b": torch.arange(12).view(3, 4)}, [3, 4])
+            >>> td_flat = td.flatten(0, 1)
+            >>> td_unflat = td_flat.unflatten(0, [3, 4])
+            >>> assert (td == td_unflat).all()
+        """
+        if dim < 0:
+            dim = self.ndim + dim
+            if dim < 0:
+                raise ValueError(
+                    f"Incompatible dim {dim} for tensordict with shape {self.shape}."
+                )
+
+        def unflatten(tensor):
+            return torch.unflatten(
+                tensor,
+                dim,
+                unflattened_size,
+            )
+
+        if dim > 0:
+            batch_size = (
+                list(self.batch_size)[:dim]
+                + list(unflattened_size)
+                + list(self.batch_size[dim + 1 :])
+            )
+        else:
+            batch_size = list(unflattened_size) + list(self.batch_size[1:])
+        out = self.apply(unflatten, batch_size=batch_size)
+        if self._has_names():
+            names = copy(self.names)
+            for _ in range(len(unflattened_size) - 1):
+                names.insert(dim, None)
+            out.names = names
+        return out
+
+    def __enter__(self):
+        self._last_op_queue.append(self._last_op)
+        return self
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        if exc_type is not None and issubclass(exc_type, Exception):
+            return False
+        _last_op = self._last_op_queue.pop()
+        if _last_op is not None:
+            last_op, (args, kwargs) = _last_op
+            if last_op is self.__class__.lock_.__name__:
+                return self.unlock_()
+            elif last_op is self.__class__.unlock_.__name__:
+                return self.lock_()
+            else:
+                raise NotImplementedError(f"Unrecognised function {last_op}.")
+        return self
+
+    def __bool__(self) -> bool:
+        raise ValueError("Converting a tensordict to boolean value is not permitted")
+
+    def __ne__(self, other: object) -> TensorDictBase:
+        """XOR operation over two tensordicts, for evey key.
+
+        The two tensordicts must have the same key set.
+
+        Args:
+            other (TensorDictBase, dict, or float): the value to compare against.
+
+        Returns:
+            a new TensorDict instance with all tensors are boolean
+            tensors of the same shape as the original tensors.
+
+        """
+        if _is_tensorclass(other.__class__):
+            return other != self
+        if isinstance(other, (dict,)) or _is_tensor_collection(other.__class__):
+            keys1 = set(self.keys())
+            keys2 = set(other.keys())
+            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                raise KeyError(
+                    f"keys in {self} and {other} mismatch, got {keys1} and {keys2}"
+                )
+            d = {}
+            for key, item1 in self.items():
+                d[key] = item1 != other.get(key)
+            return TensorDict(batch_size=self.batch_size, source=d, device=self.device)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return TensorDict(
+                {key: value != other for key, value in self.items()},
+                self.batch_size,
+                device=self.device,
+            )
+        return True
+
+    # @abc.abstractmethod
+    # def __hash__(self):
+    #     ...
+
+    def __eq__(self, other: object) -> TensorDictBase:
+        """Compares two tensordicts against each other, for every key. The two tensordicts must have the same key set.
+
+        Returns:
+            a new TensorDict instance with all tensors are boolean
+            tensors of the same shape as the original tensors.
+
+        """
+        if is_tensorclass(other):
+            return other == self
+        if isinstance(other, (dict,)) or _is_tensor_collection(other.__class__):
+            keys1 = set(self.keys())
+            keys2 = set(other.keys())
+            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
+            d = {}
+            for key, item1 in self.items():
+                d[key] = item1 == other.get(key)
+            return TensorDict(batch_size=self.batch_size, source=d, device=self.device)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return TensorDict(
+                {key: value == other for key, value in self.items()},
+                self.batch_size,
+                device=self.device,
+            )
+        return False
+
+    @abc.abstractmethod
+    def del_(self, key: NestedKey) -> TensorDictBase:
+        """Deletes a key of the tensordict.
+
+        Args:
+            key (NestedKey): key to be deleted
+
+        Returns:
+            self
+
+        """
+        raise NotImplementedError(f"{self.__class__.__name__}")
+
+    @abc.abstractmethod
+    def select(
+        self, *keys: str, inplace: bool = False, strict: bool = True
+    ) -> TensorDictBase:
+        """Selects the keys of the tensordict and returns an new tensordict with only the selected keys.
+
+        The values are not copied: in-place modifications a tensor of either
+        of the original or new tensordict will result in a change in both
+        tensordicts.
+
+        Args:
+            *keys (str): keys to select
+            inplace (bool): if True, the tensordict is pruned in place.
+                Default is :obj:`False`.
+            strict (bool, optional): whether selecting a key that is not present
+                will return an error or not. Default: :obj:`True`.
+
+        Returns:
+            A new tensordict with the selected keys only.
+
+        """
+        raise NotImplementedError(f"{self.__class__.__name__}")
+
+    def exclude(self, *keys: str, inplace: bool = False) -> TensorDictBase:
+        target = self if inplace else self.clone(recurse=False)
+        for key in keys:
+            if key in self.keys(True):
+                del target[key]
+        return target
+
+    def copy_(self, tensordict: TensorDictBase) -> TensorDictBase:
+        """See :obj:`TensorDictBase.update_`."""
+        return self.update_(tensordict)
+
+    def copy_at_(self, tensordict: TensorDictBase, idx: IndexType) -> TensorDictBase:
+        """See :obj:`TensorDictBase.update_at_`."""
+        return self.update_at_(tensordict, idx)
+
+    def get_at(
+        self, key: NestedKey, idx: IndexType, default: CompatibleType = NO_DEFAULT
+    ) -> CompatibleType:
+        """Get the value of a tensordict from the key `key` at the index `idx`.
+
+        Args:
+            key (str, tuple of str): key to be retrieved.
+            idx (int, slice, torch.Tensor, iterable): index of the tensor.
+            default (torch.Tensor): default value to return if the key is
+                not present in the tensordict.
+
+        Returns:
+            indexed tensor.
+
+        """
+        key = _unravel_key_to_tuple(key)
+        if not key:
+            raise KeyError(_GENERIC_NESTED_ERR)
+        # must be a tuple
+        return self._get_at_tuple(key, idx, default)
+
+    def _get_at_str(self, key, idx, default):
+        out = self._get_str(key, default)
+        if out is default:
+            return out
+        return out[idx]
+
+    def _get_at_tuple(self, key, idx, default):
+        out = self._get_tuple(key, default)
+        if out is default:
+            return out
+        return out[idx]
+
+    @abc.abstractmethod
+    def share_memory_(self) -> TensorDictBase:
+        """Places all the tensors in shared memory.
+
+        The TensorDict is then locked, meaning that the only writing operations that
+        can be executed must be done in-place.
+        Once the tensordict is unlocked, the share_memory attribute is turned to False,
+        because cross-process identity is not guaranteed anymore.
+
+        Returns:
+            self.
+
+        """
+        raise NotImplementedError(f"{self.__class__.__name__}")
+
+    @abc.abstractmethod
+    def memmap_(
+        self, prefix: str | None = None, copy_existing: bool = False
+    ) -> TensorDictBase:
+        """Writes all tensors onto a MemmapTensor.
+
+        Args:
+            prefix (str): directory prefix where the memmap tensors will have to
+                be stored.
+            copy_existing (bool): If False (default), an exception will be raised if an
+                entry in the tensordict is already a MemmapTensor but is not saved in
+                the correct location according to prefix. If True, any MemmapTensors
+                that are not in the correct location are copied to the new location.
+
+        The TensorDict is then locked, meaning that the only writing operations that
+        can be executed must be done in-place.
+        Once the tensordict is unlocked, the memmap attribute is turned to False,
+        because cross-process identity is not guaranteed anymore.
+
+        Returns:
+            self.
+
+        Note:
+            Serialising in this fashion might be slow with deeply nested tensordicts, so
+            we do not recommend calling this method inside a training loop.
+        """
+        raise NotImplementedError(f"{self.__class__.__name__}")
+
+    def memmap_like(self, prefix: str | None = None) -> TensorDictBase:
+        """Creates an empty Memory-mapped tensordict with the same content shape as the current one.
+
+        Args:
+            prefix (str): directory prefix where the memmap tensors will have to
+                be stored.
+
+        The resulting TensorDict will be locked and ``is_memmap() = True``,
+        meaning that the only writing operations that can be executed must be done in-place.
+        Once the tensordict is unlocked, the memmap attribute is turned to False,
+        because cross-process identity is not guaranteed anymore.
+
+        Returns:
+            a new ``TensorDict`` instance with data stored as memory-mapped tensors.
+
+        """
+        if prefix is not None:
+            prefix = Path(prefix)
+            if not prefix.exists():
+                os.makedirs(prefix, exist_ok=True)
+            torch.save(
+                {"batch_size": self.batch_size, "device": self.device},
+                prefix / "meta.pt",
+            )
+        if not self.keys():
+            raise Exception(
+                "memmap_like() must be called when the TensorDict is (partially) "
+                "populated. Set a tensor first."
+            )
+        tensordict = TensorDict(
+            {},
+            self.batch_size,
+            device=self.device,
+            names=self.names if self._has_names() else None,
+        )
+        for key, value in self.items():
+            if _is_tensor_collection(value.__class__):
+                if prefix is not None:
+                    # ensure subdirectory exists
+                    os.makedirs(prefix / key, exist_ok=True)
+                    tensordict[key] = value.memmap_like(
+                        prefix=prefix / key,
+                    )
+                    torch.save(
+                        {"batch_size": value.batch_size, "device": value.device},
+                        prefix / key / "meta.pt",
+                    )
+                else:
+                    tensordict[key] = value.memmap_like()
+                continue
+            else:
+                tensordict[key] = MemmapTensor.empty_like(
+                    value,
+                    filename=str(prefix / f"{key}.memmap")
+                    if prefix is not None
+                    else None,
+                )
+            if prefix is not None:
+                torch.save(
+                    {
+                        "shape": value.shape,
+                        "device": value.device,
+                        "dtype": value.dtype,
+                    },
+                    prefix / f"{key}.meta.pt",
+                )
+        tensordict._is_memmap = True
+        tensordict.lock_()
+        return tensordict
+
+    @abc.abstractmethod
+    def detach_(self) -> TensorDictBase:
+        """Detach the tensors in the tensordict in-place.
+
+        Returns:
+            self.
+
+        """
+        raise NotImplementedError(f"{self.__class__.__name__}")
+
+    def detach(self) -> TensorDictBase:
+        """Detach the tensors in the tensordict.
+
+        Returns:
+            a new tensordict with no tensor requiring gradient.
+
+        """
+        return self.apply(lambda x: x.detach())
+
+    def to_h5(
+        self,
+        filename,
+        **kwargs,
+    ):
+        """Converts a tensordict to a PersistentTensorDict with the h5 backend.
+
+        Args:
+            filename (str or path): path to the h5 file.
+            device (torch.device or compatible, optional): the device where to
+                expect the tensor once they are returned. Defaults to ``None``
+                (on cpu by default).
+            **kwargs: kwargs to be passed to :meth:`h5py.File.create_dataset`.
+
+        Returns:
+            A :class:`PersitentTensorDict` instance linked to the newly created file.
+
+        Examples:
+            >>> import tempfile
+            >>> import timeit
+            >>>
+            >>> from tensordict import TensorDict, MemmapTensor
+            >>> td = TensorDict({
+            ...     "a": MemmapTensor(1_000_000),
+            ...     "b": {"c": MemmapTensor(1_000_000, 3)},
+            ... }, [1_000_000])
+            >>>
+            >>> file = tempfile.NamedTemporaryFile()
+            >>> td_h5 = td.to_h5(file.name, compression="gzip", compression_opts=9)
+            >>> print(td_h5)
+            PersistentTensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([1000000]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: PersistentTensorDict(
+                        fields={
+                            c: Tensor(shape=torch.Size([1000000, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([1000000]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([1000000]),
+                device=None,
+                is_shared=False)
+
+
+        """
+        from .persistent import PersistentTensorDict
+
+        out = PersistentTensorDict.from_dict(
+            self,
+            filename=filename,
+            **kwargs,
+        )
+        if self._has_names():
+            out.names = self.names
+        return out
+
+    def to_tensordict(self):
+        """Returns a regular TensorDict instance from the TensorDictBase.
+
+        Returns:
+            a new TensorDict object containing the same values.
+
+        """
+        return TensorDict(
+            {
+                key: value.clone()
+                if not _is_tensor_collection(value.__class__)
+                else value.to_tensordict()
+                for key, value in self.items()
+            },
+            device=self.device,
+            batch_size=self.batch_size,
+            names=self.names if self._has_names() else None,
+        )
+
+    def zero_(self) -> TensorDictBase:
+        """Zeros all tensors in the tensordict in-place."""
+        for key in self.keys():
+            self.fill_(key, 0)
+        return self
+
+    def unbind(self, dim: int) -> tuple[TensorDictBase, ...]:
+        """Returns a tuple of indexed tensordicts unbound along the indicated dimension.
+
+        Resulting tensordicts will share the storage of the initial tensordict.
+
+        """
+        if dim < 0:
+            dim = self.batch_dims + dim
+        batch_size = torch.Size([s for i, s in enumerate(self.batch_size) if i != dim])
+        names = None
+        if self._has_names():
+            names = copy(self.names)
+            names = [name for i, name in enumerate(names) if i != dim]
+        out = []
+        unbind_self_dict = {key: tensor.unbind(dim) for key, tensor in self.items()}
+        for _idx in range(self.batch_size[dim]):
+            td = TensorDict(
+                {key: tensor[_idx] for key, tensor in unbind_self_dict.items()},
+                batch_size=batch_size,
+                _run_checks=False,
+                device=self.device,
+                _is_memmap=False,
+                _is_shared=False,
+                names=names,
+            )
+            out.append(td)
+            if self.is_shared():
+                out[-1].share_memory_()
+            elif self.is_memmap():
+                out[-1].memmap_()
+        return tuple(out)
+
+    def chunk(self, chunks: int, dim: int = 0) -> tuple[TensorDictBase, ...]:
+        """Splits a tendordict into the specified number of chunks, if possible.
+
+        Each chunk is a view of the input tensordict.
+
+        Args:
+            chunks (int): number of chunks to return
+            dim (int, optional): dimension along which to split the
+                tensordict. Default is 0.
+
+        """
+        if chunks < 1:
+            raise ValueError(
+                f"chunks must be a strictly positive integer, got {chunks}."
+            )
+        indices = []
+        _idx_start = 0
+        if chunks > 1:
+            interval = _idx_end = self.batch_size[dim] // chunks
+        else:
+            interval = _idx_end = self.batch_size[dim]
+        for c in range(chunks):
+            indices.append(slice(_idx_start, _idx_end))
+            _idx_start = _idx_end
+            _idx_end = _idx_end + interval if c < chunks - 2 else self.batch_size[dim]
+        if dim < 0:
+            dim = len(self.batch_size) + dim
+        return tuple(self[(*[slice(None) for _ in range(dim)], idx)] for idx in indices)
+
+    def clone(self, recurse: bool = True) -> TensorDictBase:
+        """Clones a TensorDictBase subclass instance onto a new TensorDictBase subclass of the same type.
+
+        To create a TensorDict instance from any other TensorDictBase subtype, call the :meth:`~.to_tensordict` method
+        instead.
+
+        Args:
+            recurse (bool, optional): if True, each tensor contained in the
+                TensorDict will be copied too. Default is `True`.
+
+        .. note::
+          For some TensorDictBase subtypes, such as :class:`SubTensorDict`, cloning
+          recursively makes little sense (in this specific case it would involve
+          copying the parent tensordict too). In those cases, :meth:`~.clone` will
+          fall back onto :meth:`~.to_tensordict`.
+
+        """
+        raise NotImplementedError
+
+    @classmethod
+    def __torch_function__(
+        cls,
+        func: Callable,
+        types: tuple[type, ...],
+        args: tuple[Any, ...] = (),
+        kwargs: dict[str, Any] | None = None,
+    ) -> Callable:
+        if kwargs is None:
+            kwargs = {}
+        if func not in TD_HANDLED_FUNCTIONS or not all(
+            issubclass(t, (Tensor, TensorDictBase)) for t in types
+        ):
+            return NotImplemented
+        return TD_HANDLED_FUNCTIONS[func](*args, **kwargs)
+
+    @abc.abstractmethod
+    def to(self, dest: DeviceType | type | torch.Size, **kwargs) -> TensorDictBase:
+        """Maps a TensorDictBase subclass either on a new device or to another TensorDictBase subclass (if permitted).
+
+        Casting tensors to a new dtype is not allowed, as tensordicts are not bound to contain a single
+        tensor dtype.
+
+        Args:
+            dest (device, size or TensorDictBase subclass): destination of the
+                tensordict. If it is a torch.Size object, the batch_size
+                will be updated provided that it is compatible with the
+                stored tensors.
+
+        Returns:
+            a new tensordict. If device indicated by dest differs from
+            the tensordict device, this is a no-op.
+
+        """
+        raise NotImplementedError
+
+    def _check_new_batch_size(self, new_size: torch.Size) -> None:
+        n = len(new_size)
+        for key, tensor in self.items():
+            if _shape(tensor)[:n] != new_size:
+                raise RuntimeError(
+                    f"the tensor {key} has shape {_shape(tensor)} which "
+                    f"is incompatible with the new shape {new_size}."
+                )
+
+    @abc.abstractmethod
+    def _change_batch_size(self, new_size: torch.Size) -> None:
+        raise NotImplementedError
+
+    def cpu(self) -> TensorDictBase:
+        """Casts a tensordict to CPU."""
+        return self.to("cpu")
+
+    def cuda(self, device: int = None) -> TensorDictBase:
+        """Casts a tensordict to a cuda device (if not already on it)."""
+        if device is None:
+            return self.to(torch.device("cuda"))
+        return self.to(f"cuda:{device}")
+
+    def _create_nested_str(self, key):
+        self._set_str(key, self.select(), inplace=False, validated=True)
+
+    def _create_nested_tuple(self, key):
+        self._create_nested_str(key[0])
+        if len(key) > 1:
+            td = self._get_str(key[0], NO_DEFAULT)
+            td._create_nested_tuple(key[1:])
+
+    @lock_blocked
+    def create_nested(self, key):
+        """Creates a nested tensordict of the same shape, device and dim names as the current tensordict.
+
+        If the value already exists, it will be overwritten by this operation.
+        This operation is blocked in locked tensordicts.
+
+        Examples:
+            >>> data = TensorDict({}, [3, 4, 5])
+            >>> data.create_nested("root")
+            >>> data.create_nested(("some", "nested", "value"))
+            >>> nested = data.get(("some", "nested", "value"))
+        """
+        key = _unravel_key_to_tuple(key)
+        self._create_nested_tuple(key)
+        return self
+
+    @abc.abstractmethod
+    def masked_fill_(self, mask: Tensor, value: float | bool) -> TensorDictBase:
+        """Fills the values corresponding to the mask with the desired value.
+
+        Args:
+            mask (boolean torch.Tensor): mask of values to be filled. Shape
+                must match tensordict shape.
+            value: value to used to fill the tensors.
+
+        Returns:
+            self
+
+        Examples:
+            >>> td = TensorDict(source={'a': torch.zeros(3, 4)},
+            ...     batch_size=[3])
+            >>> mask = torch.tensor([True, False, False])
+            >>> _ = td.masked_fill_(mask, 1.0)
+            >>> td.get("a")
+            tensor([[1., 1., 1., 1.],
+                    [0., 0., 0., 0.],
+                    [0., 0., 0., 0.]])
+        """
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def masked_fill(self, mask: Tensor, value: float | bool) -> TensorDictBase:
+        """Out-of-place version of masked_fill.
+
+        Args:
+            mask (boolean torch.Tensor): mask of values to be filled. Shape
+                must match tensordict shape.
+            value: value to used to fill the tensors.
+
+        Returns:
+            self
+
+        Examples:
+            >>> td = TensorDict(source={'a': torch.zeros(3, 4)},
+            ...     batch_size=[3])
+            >>> mask = torch.tensor([True, False, False])
+            >>> td1 = td.masked_fill(mask, 1.0)
+            >>> td1.get("a")
+            tensor([[1., 1., 1., 1.],
+                    [0., 0., 0., 0.],
+                    [0., 0., 0., 0.]])
+        """
+        raise NotImplementedError
+
+    def where(self, condition, other, *, out=None):
+        """Return a ``TensorDict`` of elements selected from either self or other, depending on condition.
+
+        Args:
+            condition (BoolTensor): When ``True`` (nonzero), yields ``self``,
+                otherwise yields ``other``.
+            other (TensorDictBase or Scalar): value (if ``other`` is a scalar)
+                or values selected at indices where condition is ``False``.
+            out (Tensor, optional): the output ``TensorDictBase`` instance.
+
+        """
+        raise NotImplementedError
+
+    def masked_select(self, mask: Tensor) -> TensorDictBase:
+        """Masks all tensors of the TensorDict and return a new TensorDict instance with similar keys pointing to masked values.
+
+        Args:
+            mask (torch.Tensor): boolean mask to be used for the tensors.
+                Shape must match the TensorDict batch_size.
+
+        Examples:
+            >>> td = TensorDict(source={'a': torch.zeros(3, 4)},
+            ...    batch_size=[3])
+            >>> mask = torch.tensor([True, False, False])
+            >>> td_mask = td.masked_select(mask)
+            >>> td_mask.get("a")
+            tensor([[0., 0., 0., 0.]])
+
+        """
+        d = {}
+        mask_expand = mask
+        while mask_expand.ndimension() > self.batch_dims:
+            mndim = mask_expand.ndimension()
+            mask_expand = mask_expand.squeeze(-1)
+            if mndim == mask_expand.ndimension():  # no more squeeze
+                break
+        for key, value in self.items():
+            d[key] = value[mask_expand]
+        dim = int(mask.sum().item())
+        other_dim = self.shape[mask.ndim :]
+        return TensorDict(
+            device=self.device, source=d, batch_size=torch.Size([dim, *other_dim])
+        )
+
+    @abc.abstractmethod
+    def is_contiguous(self) -> bool:
+        """Returns a boolean indicating if all the tensors are contiguous."""
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def contiguous(self) -> TensorDictBase:
+        """Returns a new tensordict of the same type with contiguous values (or self if values are already contiguous)."""
+        raise NotImplementedError
+
+    def to_dict(self) -> dict[str, Any]:
+        """Returns a dictionary with key-value pairs matching those of the tensordict."""
+        return {
+            key: value.to_dict() if _is_tensor_collection(value.__class__) else value
+            for key, value in self.items()
+        }
+
+    def unsqueeze(self, dim: int) -> TensorDictBase:
+        """Unsqueeze all tensors for a dimension comprised in between `-td.batch_dims` and `td.batch_dims` and returns them in a new tensordict.
+
+        Args:
+            dim (int): dimension along which to unsqueeze
+
+        """
+        if dim < 0:
+            dim = self.batch_dims + dim + 1
+
+        if (dim > self.batch_dims) or (dim < 0):
+            raise RuntimeError(
+                f"unsqueezing is allowed for dims comprised between "
+                f"`-td.batch_dims` and `td.batch_dims` only. Got "
+                f"dim={dim} with a batch size of {self.batch_size}."
+            )
+        return _UnsqueezedTensorDict(
+            source=self,
+            custom_op="unsqueeze",
+            inv_op="squeeze",
+            custom_op_kwargs={"dim": dim},
+            inv_op_kwargs={"dim": dim},
+        )
+
+    def squeeze(self, dim: int | None = None) -> TensorDictBase:
+        """Squeezes all tensors for a dimension comprised in between `-td.batch_dims+1` and `td.batch_dims-1` and returns them in a new tensordict.
+
+        Args:
+            dim (Optional[int]): dimension along which to squeeze. If dim is None, all singleton dimensions will be squeezed. dim is None by default.
+
+        """
+        if dim is None:
+            size = self.size()
+            if len(self.size()) == 1 or size.count(1) == 0:
+                return self
+            first_singleton_dim = size.index(1)
+
+            squeezed_dict = _SqueezedTensorDict(
+                source=self,
+                custom_op="squeeze",
+                inv_op="unsqueeze",
+                custom_op_kwargs={"dim": first_singleton_dim},
+                inv_op_kwargs={"dim": first_singleton_dim},
+            )
+            return squeezed_dict.squeeze(dim=None)
+
+        if dim < 0:
+            dim = self.batch_dims + dim
+
+        if self.batch_dims and (dim >= self.batch_dims or dim < 0):
+            raise RuntimeError(
+                f"squeezing is allowed for dims comprised between 0 and "
+                f"td.batch_dims only. Got dim={dim} and batch_size"
+                f"={self.batch_size}."
+            )
+
+        if dim >= self.batch_dims or self.batch_size[dim] != 1:
+            return self
+        return _SqueezedTensorDict(
+            source=self,
+            custom_op="squeeze",
+            inv_op="unsqueeze",
+            custom_op_kwargs={"dim": dim},
+            inv_op_kwargs={"dim": dim},
+        )
+
+    def reshape(
+        self,
+        *shape: int,
+        size: list | tuple | torch.Size | None = None,
+    ) -> TensorDictBase:
+        """Returns a contiguous, reshaped tensor of the desired shape.
+
+        Args:
+            *shape (int): new shape of the resulting tensordict.
+            size: iterable
+
+        Returns:
+            A TensorDict with reshaped keys
+
+        """
+        if len(shape) == 0 and size is not None:
+            return self.reshape(*size)
+        elif len(shape) == 1 and isinstance(shape[0], (list, tuple, torch.Size)):
+            return self.reshape(*shape[0])
+        elif not isinstance(shape, torch.Size):
+            shape = torch.Size(shape)
+
+        d = {}
+        for key, item in self.items():
+            d[key] = item.reshape((*shape, *item.shape[self.ndimension() :]))
+        if d:
+            batch_size = d[key].shape[: len(shape)]
+        else:
+            if any(not isinstance(i, int) or i < 0 for i in shape):
+                raise RuntimeError(
+                    "Implicit reshaping is not permitted with empty " "tensordicts"
+                )
+            batch_size = torch.Size(shape)
+        return TensorDict(d, batch_size, device=self.device, _run_checks=False)
+
+    def split(self, split_size: int | list[int], dim: int = 0) -> list[TensorDictBase]:
+        """Splits each tensor in the TensorDict with the specified size in the given dimension, like `torch.split`.
+
+        Returns a list of TensorDict with the view of split chunks of items. Nested TensorDicts will remain nested.
+
+        The list of TensorDict maintains the original order of the tensor chunks.
+
+        Args:
+            split_size (int or List(int)): size of a single chunk or list of sizes for each chunk
+            dim (int): dimension along which to split the tensor
+
+        Returns:
+            A list of TensorDict with specified size in given dimension.
+
+        """
+        batch_sizes = []
+        if self.batch_dims == 0:
+            raise RuntimeError("TensorDict with empty batch size is not splittable")
+        if not (-self.batch_dims <= dim < self.batch_dims):
+            raise IndexError(
+                f"Dimension out of range (expected to be in range of [-{self.batch_dims}, {self.batch_dims - 1}], but got {dim})"
+            )
+        if dim < 0:
+            dim += self.batch_dims
+        if isinstance(split_size, int):
+            rep, remainder = divmod(self.batch_size[dim], split_size)
+            rep_shape = torch.Size(
+                [
+                    split_size if idx == dim else size
+                    for (idx, size) in enumerate(self.batch_size)
+                ]
+            )
+            batch_sizes = [rep_shape for _ in range(rep)]
+            if remainder:
+                batch_sizes.append(
+                    torch.Size(
+                        [
+                            remainder if dim_idx == dim else dim_size
+                            for (dim_idx, dim_size) in enumerate(self.batch_size)
+                        ]
+                    )
+                )
+        elif isinstance(split_size, list) and all(
+            isinstance(element, int) for element in split_size
+        ):
+            if sum(split_size) != self.batch_size[dim]:
+                raise RuntimeError(
+                    f"Split method expects split_size to sum exactly to {self.batch_size[dim]} (tensor's size at dimension {dim}), but got split_size={split_size}"
+                )
+            for i in split_size:
+                batch_sizes.append(
+                    torch.Size(
+                        [
+                            i if dim_idx == dim else dim_size
+                            for (dim_idx, dim_size) in enumerate(self.batch_size)
+                        ]
+                    )
+                )
+        else:
+            raise TypeError(
+                "split(): argument 'split_size' must be int or list of ints"
+            )
+        dictionaries = [{} for _ in range(len(batch_sizes))]
+        for key, item in self.items():
+            split_tensors = torch.split(item, split_size, dim)
+            for idx, split_tensor in enumerate(split_tensors):
+                dictionaries[idx][key] = split_tensor
+        names = None
+        if self._has_names():
+            names = copy(self.names)
+        return [
+            TensorDict(
+                dictionaries[i],
+                batch_sizes[i],
+                device=self.device,
+                names=names,
+                _run_checks=False,
+                _is_shared=self.is_shared(),
+                _is_memmap=self.is_memmap(),
+            )
+            for i in range(len(dictionaries))
+        ]
+
+    def gather(
+        self, dim: int, index: Tensor, out: TensorDictBase | None = None
+    ) -> TensorDictBase:
+        """Gathers values along an axis specified by `dim`.
+
+        Args:
+            dim (int): the dimension along which collect the elements
+            index (torch.Tensor): a long tensor which number of dimension matches
+                the one of the tensordict with only one dimension differring between
+                the two (the gathering dimension). Its elements refer to the
+                index to be gathered along the required dimension.
+            out (TensorDictBase, optional): a destination tensordict. It must
+                have the same shape as the index.
+
+        Examples:
+            >>> td = TensorDict(
+            ...     {"a": torch.randn(3, 4, 5),
+            ...      "b": TensorDict({"c": torch.zeros(3, 4, 5)}, [3, 4, 5])},
+            ...     [3, 4])
+            >>> index = torch.randint(4, (3, 2))
+            >>> td_gather = td.gather(dim=1, index=index)
+            >>> print(td_gather)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([3, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            c: Tensor(shape=torch.Size([3, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([3, 2, 5]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([3, 2]),
+                device=None,
+                is_shared=False)
+
+        Gather keeps the dimension names.
+
+        Examples:
+            >>> td.names = ["a", "b"]
+            >>> td_gather = td.gather(dim=1, index=index)
+            >>> td_gather.names
+            ["a", "b"]
+        """
+        return torch.gather(self, dim, index, out=out)
+
+    def view(
+        self,
+        *shape: int,
+        size: list | tuple | torch.Size | None = None,
+    ) -> TensorDictBase:
+        """Returns a tensordict with views of the tensors according to a new shape, compatible with the tensordict batch_size.
+
+        Args:
+            *shape (int): new shape of the resulting tensordict.
+            size: iterable
+
+        Returns:
+            a new tensordict with the desired batch_size.
+
+        Examples:
+            >>> td = TensorDict(source={'a': torch.zeros(3,4,5),
+            ...    'b': torch.zeros(3,4,10,1)}, batch_size=torch.Size([3, 4]))
+            >>> td_view = td.view(12)
+            >>> print(td_view.get("a").shape)  # torch.Size([12, 5])
+            >>> print(td_view.get("b").shape)  # torch.Size([12, 10, 1])
+            >>> td_view = td.view(-1, 4, 3)
+            >>> print(td_view.get("a").shape)  # torch.Size([1, 4, 3, 5])
+            >>> print(td_view.get("b").shape)  # torch.Size([1, 4, 3, 10, 1])
+
+        """
+        if len(shape) == 0 and size is not None:
+            return self.view(*size)
+        elif len(shape) == 1 and isinstance(shape[0], (list, tuple, torch.Size)):
+            return self.view(*shape[0])
+        elif not isinstance(shape, torch.Size):
+            shape = infer_size_impl(shape, self.numel())
+            shape = torch.Size(shape)
+        if shape == self.shape:
+            return self
+        return _ViewedTensorDict(
+            source=self,
+            custom_op="view",
+            inv_op="view",
+            custom_op_kwargs={"size": shape},
+            inv_op_kwargs={"size": self.batch_size},
+        )
+
+    def transpose(self, dim0, dim1):
+        """Returns a tensordit that is a transposed version of input. The given dimensions ``dim0`` and ``dim1`` are swapped.
+
+        In-place or out-place modifications of the transposed tensordict will
+        impact the original tensordict too as the memory is shared and the operations
+        are mapped back on the original tensordict.
+
+        Examples:
+            >>> tensordict = TensorDict({"a": torch.randn(3, 4, 5)}, [3, 4])
+            >>> tensordict_transpose = tensordict.transpose(0, 1)
+            >>> print(tensordict_transpose.shape)
+            torch.Size([4, 3])
+            >>> tensordict_transpose.set("b",, torch.randn(4, 3))
+            >>> print(tensordict.get("b").shape)
+            torch.Size([3, 4])
+        """
+        if dim0 < 0:
+            dim0 = self.ndim + dim0
+        if dim1 < 0:
+            dim1 = self.ndim + dim1
+        if any((dim0 < 0, dim1 < 0)):
+            raise ValueError(
+                "The provided dimensions are incompatible with the tensordict batch-size."
+            )
+        if dim0 == dim1:
+            return self
+        return _TransposedTensorDict(
+            source=self,
+            custom_op="transpose",
+            inv_op="transpose",
+            custom_op_kwargs={"dim0": dim0, "dim1": dim1},
+            inv_op_kwargs={"dim0": dim0, "dim1": dim1},
+        )
+
+    def permute(
+        self,
+        *dims_list: int,
+        dims: list[int] | None = None,
+    ) -> TensorDictBase:
+        """Returns a view of a tensordict with the batch dimensions permuted according to dims.
+
+        Args:
+            *dims_list (int): the new ordering of the batch dims of the tensordict. Alternatively,
+                a single iterable of integers can be provided.
+            dims (list of int): alternative way of calling permute(...).
+
+        Returns:
+            a new tensordict with the batch dimensions in the desired order.
+
+        Examples:
+            >>> tensordict = TensorDict({"a": torch.randn(3, 4, 5)}, [3, 4])
+            >>> print(tensordict.permute([1, 0]))
+            PermutedTensorDict(
+                source=TensorDict(
+                    fields={
+                        a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32)},
+                    batch_size=torch.Size([3, 4]),
+                    device=cpu,
+                    is_shared=False),
+                op=permute(dims=[1, 0]))
+            >>> print(tensordict.permute(1, 0))
+            PermutedTensorDict(
+                source=TensorDict(
+                    fields={
+                        a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32)},
+                    batch_size=torch.Size([3, 4]),
+                    device=cpu,
+                    is_shared=False),
+                op=permute(dims=[1, 0]))
+            >>> print(tensordict.permute(dims=[1, 0]))
+            PermutedTensorDict(
+                source=TensorDict(
+                    fields={
+                        a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32)},
+                    batch_size=torch.Size([3, 4]),
+                    device=cpu,
+                    is_shared=False),
+                op=permute(dims=[1, 0]))
+        """
+        if len(dims_list) == 0:
+            dims_list = dims
+        elif len(dims_list) == 1 and not isinstance(dims_list[0], int):
+            dims_list = dims_list[0]
+        if len(dims_list) != len(self.shape):
+            raise RuntimeError(
+                f"number of dims don't match in permute (got {len(dims_list)}, expected {len(self.shape)}"
+            )
+
+        if not len(dims_list) and not self.batch_dims:
+            return self
+        if np.array_equal(dims_list, range(self.batch_dims)):
+            return self
+        min_dim, max_dim = -self.batch_dims, self.batch_dims - 1
+        seen = [False for dim in range(max_dim + 1)]
+        for idx in dims_list:
+            if idx < min_dim or idx > max_dim:
+                raise IndexError(
+                    f"dimension out of range (expected to be in range of [{min_dim}, {max_dim}], but got {idx})"
+                )
+            if seen[idx]:
+                raise RuntimeError("repeated dim in permute")
+            seen[idx] = True
+
+        return _PermutedTensorDict(
+            source=self,
+            custom_op="permute",
+            inv_op="permute",
+            custom_op_kwargs={"dims": dims_list},
+            inv_op_kwargs={"dims": dims_list},
+        )
+
+    def __repr__(self) -> str:
+        fields = _td_fields(self)
+        field_str = indent(f"fields={{{fields}}}", 4 * " ")
+        batch_size_str = indent(f"batch_size={self.batch_size}", 4 * " ")
+        device_str = indent(f"device={self.device}", 4 * " ")
+        is_shared_str = indent(f"is_shared={self.is_shared()}", 4 * " ")
+        string = ",\n".join([field_str, batch_size_str, device_str, is_shared_str])
+        return f"{type(self).__name__}(\n{string})"
+
+    def all(self, dim: int = None) -> bool | TensorDictBase:
+        """Checks if all values are True/non-null in the tensordict.
+
+        Args:
+            dim (int, optional): if None, returns a boolean indicating
+                whether all tensors return `tensor.all() == True`
+                If integer, all is called upon the dimension specified if
+                and only if this dimension is compatible with the tensordict
+                shape.
+
+        """
+        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
+            raise RuntimeError(
+                "dim must be greater than or equal to -tensordict.batch_dims and "
+                "smaller than tensordict.batch_dims"
+            )
+        if dim is not None:
+            if dim < 0:
+                dim = self.batch_dims + dim
+
+            names = None
+            if self._has_names():
+                names = copy(self.names)
+                names = [name for i, name in enumerate(names) if i != dim]
+
+            return TensorDict(
+                source={key: value.all(dim=dim) for key, value in self.items()},
+                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
+                device=self.device,
+                names=names,
+            )
+        return all(value.all() for value in self.values())
+
+    def any(self, dim: int = None) -> bool | TensorDictBase:
+        """Checks if any value is True/non-null in the tensordict.
+
+        Args:
+            dim (int, optional): if None, returns a boolean indicating
+                whether all tensors return `tensor.any() == True`.
+                If integer, all is called upon the dimension specified if
+                and only if this dimension is compatible with
+                the tensordict shape.
+
+        """
+        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
+            raise RuntimeError(
+                "dim must be greater than or equal to -tensordict.batch_dims and "
+                "smaller than tensordict.batch_dims"
+            )
+        if dim is not None:
+            if dim < 0:
+                dim = self.batch_dims + dim
+
+            names = None
+            if self._has_names():
+                names = copy(self.names)
+                names = [name for i, name in enumerate(names) if i != dim]
+
+            return TensorDict(
+                source={key: value.any(dim=dim) for key, value in self.items()},
+                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
+                device=self.device,
+                names=names,
+            )
+        return any([value.any() for value in self.values()])
+
+    def get_sub_tensordict(self, idx: IndexType) -> TensorDictBase:
+        """Returns a SubTensorDict with the desired index."""
+        return SubTensorDict(source=self, idx=idx)
+
+    def __iter__(self) -> Generator:
+        if not self.batch_dims:
+            raise StopIteration
+        length = self.batch_size[0]
+        for i in range(length):
+            yield self[i]
+
+    @cache  # noqa: B019
+    def flatten_keys(
+        self, separator: str = ".", inplace: bool = False
+    ) -> TensorDictBase:
+        to_flatten = []
+        existing_keys = self.keys(include_nested=True)
+        for key, value in self.items():
+            key_split = tuple(key.split(separator))
+            if isinstance(value, TensorDictBase):
+                to_flatten.append(key)
+            elif (
+                separator in key
+                and key_split in existing_keys
+                and not _is_tensor_collection(self.entry_class(key_split))
+            ):
+                raise KeyError(
+                    f"Flattening keys in tensordict collides with existing key '{key}'"
+                )
+
+        if inplace:
+            for key in to_flatten:
+                inner_tensordict = self.get(key).flatten_keys(
+                    separator=separator, inplace=inplace
+                )
+                for inner_key, inner_item in inner_tensordict.items():
+                    self.set(separator.join([key, inner_key]), inner_item)
+            for key in to_flatten:
+                del self[key]
+            return self
+        else:
+            tensordict_out = TensorDict(
+                {},
+                batch_size=self.batch_size,
+                device=self.device,
+                _run_checks=False,
+                _is_shared=self.is_shared(),
+                _is_memmap=self.is_memmap(),
+                names=self.names,
+            )
+            for key, value in self.items():
+                if key in to_flatten:
+                    inner_tensordict = self.get(key).flatten_keys(
+                        separator=separator, inplace=inplace
+                    )
+                    for inner_key, inner_item in inner_tensordict.items():
+                        tensordict_out.set(separator.join([key, inner_key]), inner_item)
+                else:
+                    tensordict_out.set(key, value)
+            return tensordict_out
+
+    @cache  # noqa: B019
+    def unflatten_keys(
+        self, separator: str = ".", inplace: bool = False
+    ) -> TensorDictBase:
+        to_unflatten = defaultdict(list)
+        for key in self.keys():
+            if separator in key[1:-1]:
+                split_key = key.split(separator)
+                to_unflatten[split_key[0]].append((key, separator.join(split_key[1:])))
+
+        if not inplace:
+            out = TensorDict(
+                {
+                    key: value
+                    for key, value in self.items()
+                    if separator not in key[1:-1]
+                },
+                batch_size=self.batch_size,
+                device=self.device,
+                _run_checks=False,
+                _is_shared=self.is_shared(),
+                _is_memmap=self.is_memmap(),
+                names=self.names,
+            )
+        else:
+            out = self
+
+        keys = set(out.keys())
+        for key, list_of_keys in to_unflatten.items():
+            if key in keys:
+                raise KeyError(
+                    "Unflattening key(s) in tensordict will override existing unflattened key"
+                )
+
+            tensordict = TensorDict(
+                {},
+                batch_size=self.batch_size,
+                device=self.device,
+                names=self.names,
+            )
+            if key in self.keys():
+                tensordict.update(self[key])
+            for old_key, new_key in list_of_keys:
+                value = self.get(old_key)
+                tensordict[new_key] = value
+                if inplace:
+                    del self[old_key]
+            out._set_str(
+                key,
+                tensordict.unflatten_keys(separator=separator),
+                inplace=False,
+                validated=True,
+            )
+        return out
+
+    def __len__(self) -> int:
+        """Returns the length of first dimension, if there is, otherwise 0."""
+        return self.shape[0] if self.batch_dims else 0
+
+    def __contains__(self, key: NestedKey) -> bool:
+        # by default a Mapping will implement __contains__ by calling __getitem__ and
+        # returning False if a KeyError is raised, True otherwise. TensorDict has a
+        # complex __getitem__ method since we support more than just retrieval of values
+        # by key, and so this can be quite inefficient, particularly if values are
+        # evaluated lazily on access. Hence we don't support use of __contains__ and
+        # direct the user to use TensorDict.keys() instead
+        raise NotImplementedError(
+            "TensorDict does not support membership checks with the `in` keyword. If "
+            "you want to check if a particular key is in your TensorDict, please use "
+            "`key in tensordict.keys()` instead."
+        )
+
+    def _get_names_idx(self, idx):
+        if not self._has_names():
+            names = None
+        else:
+
+            def is_boolean(idx):
+                if isinstance(idx, tuple) and len(idx) == 1:
+                    return is_boolean(idx[0])
+                if hasattr(idx, "dtype") and idx.dtype is torch.bool:
+                    return idx.ndim
+                return None
+
+            num_boolean_dim = is_boolean(idx)
+            names = self.names
+            if num_boolean_dim:
+                names = [None] + names[num_boolean_dim:]
+            else:
+                # def is_int(subidx):
+                #     if isinstance(subidx, Number):
+                #         return True
+                #     if isinstance(subidx, Tensor) and len(subidx.shape) == 0:
+                #         return True
+                #     return False
+
+                if not isinstance(idx, tuple):
+                    idx = (idx,)
+                if len(idx) < self.ndim:
+                    idx = (*idx, Ellipsis)
+                idx_names = convert_ellipsis_to_idx(idx, self.batch_size)
+                # this will convert a [None, :, :, 0, None, 0] in [None, 0, 1, None, 3]
+                count = 0
+                idx_to_take = []
+                for _idx in idx_names:
+                    if _idx is None:
+                        idx_to_take.append(None)
+                    elif _is_number(_idx):
+                        count += 1
+                    else:
+                        idx_to_take.append(count)
+                        count += 1
+                names = [names[i] if i is not None else None for i in idx_to_take]
+        return names
+
+    def _index_tensordict(self, index: IndexType) -> TensorDictBase:
+        batch_size = self.batch_size
+        if (
+            not batch_size
+            and index is not None
+            and (not isinstance(index, tuple) or any(idx is not None for idx in index))
+        ):
+            raise RuntimeError(
+                f"indexing a tensordict with td.batch_dims==0 is not permitted. Got index {index}."
+            )
+        names = self._get_names_idx(index)
+        batch_size = _getitem_batch_size(batch_size, index)
+        return TensorDict(
+            source={key: _get_item(item, index) for key, item in self.items()},
+            batch_size=batch_size,
+            device=self.device,
+            names=names,
+            _run_checks=False,
+            _is_shared=self.is_shared(),
+            _is_memmap=self.is_memmap(),
+        )
+
+    def __getitem__(self, index: IndexType) -> TensorDictBase:
+        """Indexes all tensors according to the provided index.
+
+        Returns a new tensordict where the values share the storage of the
+        original tensors (even when the index is a torch.Tensor).
+        Any in-place modification to the resulting tensordict will
+        impact the parent tensordict too.
+
+        Examples:
+            >>> td = TensorDict(source={'a': torch.zeros(3,4,5)},
+            ...     batch_size=torch.Size([3, 4]))
+            >>> subtd = td[torch.zeros(1, dtype=torch.long)]
+            >>> assert subtd.shape == torch.Size([1,4])
+            >>> subtd.set("a", torch.ones(1,4,5))
+            >>> print(td.get("a"))  # first row is full of 1
+            >>> # Warning: this will not work as expected
+            >>> subtd.get("a")[:] = 2.0
+            >>> print(td.get("a"))  # values have not changed
+
+        """
+        istuple = isinstance(index, tuple)
+        if istuple or isinstance(index, str):
+            idx_unravel = _unravel_key_to_tuple(index)
+            if idx_unravel:
+                return self._get_tuple(idx_unravel, NO_DEFAULT)
+        if (istuple and not index) or (not istuple and index is Ellipsis):
+            # empty tuple returns self
+            return self
+        if not istuple:
+            if isinstance(index, int):
+                return self._index_tensordict(index)
+            # we only want tuple indices
+            index = (index,)
+        # # convert range/np.ndarray to tensor: this is not cheap
+        # index = tuple(
+        #     torch.tensor(idx) if isinstance(idx, (np.ndarray, range)) else idx
+        #     for idx in index
+        # )
+        if istuple and any(idx is Ellipsis for idx in index):
+            index = convert_ellipsis_to_idx(index, self.batch_size)
+        if all(isinstance(idx, slice) and idx == slice(None) for idx in index):
+            return self
+
+        return self._index_tensordict(index)
+
+    __getitems__ = __getitem__
+
+    def __setitem__(
+        self,
+        index: IndexType,
+        value: TensorDictBase | dict | numbers.Number | CompatibleType,
+    ) -> None:
+        istuple = isinstance(index, tuple)
+        if istuple or isinstance(index, str):
+            # try:
+            index_unravel = _unravel_key_to_tuple(index)
+            if index_unravel:
+                self._set_tuple(
+                    index_unravel,
+                    value,
+                    inplace=BEST_ATTEMPT_INPLACE
+                    if isinstance(self, SubTensorDict)
+                    else False,
+                    validated=False,
+                )
+                return
+
+        if index is Ellipsis or (isinstance(index, tuple) and Ellipsis in index):
+            index = convert_ellipsis_to_idx(index, self.batch_size)
+        # elif isinstance(index, (list, range)):
+        #     index = torch.tensor(index, device=self.device)
+
+        if isinstance(value, (TensorDictBase, dict)):
+            indexed_bs = _getitem_batch_size(self.batch_size, index)
+            if isinstance(value, dict):
+                value = TensorDict(
+                    value, batch_size=indexed_bs, device=self.device, _run_checks=False
+                )
+            if value.batch_size != indexed_bs:
+                # try to expand
+                try:
+                    value = value.expand(indexed_bs)
+                except RuntimeError as err:
+                    raise RuntimeError(
+                        f"indexed destination TensorDict batch size is {indexed_bs} "
+                        f"(batch_size = {self.batch_size}, index={index}), "
+                        f"which differs from the source batch size {value.batch_size}"
+                    ) from err
+
+            keys = set(self.keys())
+            if any(key not in keys for key in value.keys()):
+                subtd = self.get_sub_tensordict(index)
+            for key, item in value.items():
+                if key in keys:
+                    self._set_at_str(key, item, index, validated=False)
+                else:
+                    subtd.set(key, item)
+        else:
+            for key in self.keys():
+                self.set_at_(key, value, index)
+
+    def __delitem__(self, index: IndexType) -> TensorDictBase:
+        # if isinstance(index, str):
+        return self.del_(index)
+        # raise IndexError(f"Index has to a string but received {index}.")
+
+    @abc.abstractmethod
+    def rename_key_(
+        self, old_key: str, new_key: str, safe: bool = False
+    ) -> TensorDictBase:
+        """Renames a key with a new string.
+
+        Args:
+            old_key (str): key to be renamed
+            new_key (str): new name
+            safe (bool, optional): if True, an error is thrown when the new
+                key is already present in the TensorDict.
+
+        Returns:
+            self
+
+        """
+        raise NotImplementedError
+
+    def fill_(self, key: NestedKey, value: float | bool) -> TensorDictBase:
+        """Fills a tensor pointed by the key with the a given value.
+
+        Args:
+            key (str): key to be remaned
+            value (Number, bool): value to use for the filling
+
+        Returns:
+            self
+
+        """
+        key = _unravel_key_to_tuple(key)
+        data = self._get_tuple(key, NO_DEFAULT)
+        if _is_tensor_collection(data.__class__):
+            data.apply_(lambda x: x.fill_(value))
+            # self._set(key, tensordict, inplace=True)
+        else:
+            data = data.fill_(value)
+            self._set_tuple(key, data, inplace=True, validated=True)
+        return self
+
+    def empty(self, recurse=False) -> TensorDictBase:
+        """Returns a new, empty tensordict with the same device and batch size.
+
+        Args:
+            recurse (bool, optional): if ``True``, the entire structure of the TensorDict
+                will be rerproduced without content. Otherwise, only the root
+                will be duplicated. Defaults to ``False``.
+
+        """
+        if not recurse:
+            return self.select()
+        return self.exclude(*self.keys(True, True))
+
+    def is_empty(self) -> bool:
+        for _ in self.keys():
+            return False
+        return True
+
+    def setdefault(
+        self, key: NestedKey, default: CompatibleType, inplace: bool = False
+    ) -> CompatibleType:
+        """Insert key with a value of default if key is not in the dictionary.
+
+        Return the value for key if key is in the dictionary, else default.
+
+        Args:
+            key (str): the name of the value.
+            default (torch.Tensor): value to be stored in the tensordict if the key is
+                not already present.
+
+        Returns:
+            The value of key in the tensordict. Will be default if the key was not
+            previously set.
+
+        """
+        if key not in self.keys(include_nested=isinstance(key, tuple)):
+            self.set(key, default, inplace=inplace)
+        return self.get(key)
+
+    @property
+    def is_locked(self) -> bool:
+        return self._is_locked
+
+    @is_locked.setter
+    def is_locked(self, value: bool) -> None:
+        if value:
+            self.lock_()
+        else:
+            self.unlock_()
+
+    def _lock_propagate(self, lock_ids=None):
+        """Registers the parent tensordict that handles the lock."""
+        self._is_locked = True
+        is_root = lock_ids is None
+        if is_root:
+            lock_ids = set()
+        self._lock_id = self._lock_id.union(lock_ids)
+        lock_ids = lock_ids.union({id(self)})
+        _locked_tensordicts = []
+        for key in self.keys():
+            if _is_tensor_collection(self.entry_class(key)):
+                dest = self.get(key)
+                dest._lock_propagate(lock_ids)
+                _locked_tensordicts.append(dest)
+        if is_root:
+            self._locked_tensordicts = _locked_tensordicts
+        else:
+            self._locked_tensordicts += _locked_tensordicts
+
+    @as_decorator("is_locked")
+    def lock_(self) -> TensorDictBase:
+        if self.is_locked:
+            return self
+        self._lock_propagate()
+        return self
+
+    lock = _renamed_inplace_method(lock_)
+
+    def _remove_lock(self, lock_id):
+        self._lock_id = self._lock_id - {lock_id}
+        if self._locked_tensordicts:
+            for td in self._locked_tensordicts:
+                td._remove_lock(lock_id)
+
+    @erase_cache
+    def _propagate_unlock(self, lock_ids=None):
+        if lock_ids is not None:
+            self._lock_id.difference_update(lock_ids)
+        else:
+            lock_ids = set()
+        self._is_locked = False
+
+        unlocked_tds = [self]
+        lock_ids.add(id(self))
+        for key in self.keys():
+            if _is_tensor_collection(self.entry_class(key)):
+                dest = self.get(key)
+                unlocked_tds.extend(dest._propagate_unlock(lock_ids))
+        self._locked_tensordicts = []
+
+        self._is_shared = False
+        self._is_memmap = False
+        return unlocked_tds
+
+    @as_decorator("is_locked")
+    def unlock_(self) -> TensorDictBase:
+        unlock_tds = self._propagate_unlock()
+        for td in unlock_tds:
+            if len(td._lock_id):
+                self.lock_()
+                raise RuntimeError(
+                    "Cannot unlock a tensordict that is part of a locked graph. "
+                    "Unlock the root tensordict first. If the tensordict is part of multiple graphs, "
+                    "group the graphs under a common tensordict an unlock this root. "
+                )
+        return self
+
+    unlock = _renamed_inplace_method(unlock_)
+
+    def __del__(self):
+        for td in self._locked_tensordicts:
+            td._remove_lock(id(self))
+
+    def is_floating_point(self):
+        for item in self.values(include_nested=True, leaves_only=True):
+            if not item.is_floating_point():
+                return False
+        else:
+            return True
+
+    def double(self):
+        r"""Casts all tensors to ``torch.bool``."""
+        return self.apply(lambda x: x.double())
+
+    def float(self):
+        r"""Casts all tensors to ``torch.float``."""
+        return self.apply(lambda x: x.float())
+
+    def int(self):
+        r"""Casts all tensors to ``torch.int``."""
+        return self.apply(lambda x: x.int())
+
+    def bool(self):
+        r"""Casts all tensors to ``torch.bool``."""
+        return self.apply(lambda x: x.bool())
+
+    def half(self):
+        r"""Casts all tensors to ``torch.half``."""
+        return self.apply(lambda x: x.half())
+
+    def bfloat16(self):
+        r"""Casts all tensors to ``torch.bfloat16``."""
+        return self.apply(lambda x: x.bfloat16())
+
+    def type(self, dst_type):
+        r"""Casts all tensors to :attr:`dst_type`.
+
+        Args:
+            dst_type (type or string): the desired type
+
+        """
+        return self.apply(lambda x: x.type(dst_type))
+
+
+_ACCEPTED_CLASSES = [
+    Tensor,
+    MemmapTensor,
+    TensorDictBase,
+]
+if _has_torchrec:
+    _ACCEPTED_CLASSES += [KeyedJaggedTensor]
+_ACCEPTED_CLASSES = tuple(_ACCEPTED_CLASSES)
+
+
+class TensorDict(TensorDictBase):
+    """A batched dictionary of tensors.
+
+    TensorDict is a tensor container where all tensors are stored in a
+    key-value pair fashion and where each element shares at least the
+    following features:
+    - memory location (shared, memory-mapped array, ...);
+    - batch size (i.e. n^th first dimensions).
+
+    Additionally, if the tensordict has a specified device, then each element
+    must share that device.
+
+    TensorDict instances support many regular tensor operations as long as
+    they are dtype-independent (as a TensorDict instance can contain tensors
+    of many different dtypes). Those operations include (but are not limited
+    to):
+
+    - operations on shape: when a shape operation is called (indexing,
+      reshape, view, expand, transpose, permute,
+      unsqueeze, squeeze, masking etc), the operations is done as if it
+      was done on a tensor of the same shape as the batch size then
+      expended to the right, e.g.:
+
+        >>> td = TensorDict({'a': torch.zeros(3,4,5)}, batch_size=[3, 4])
+        >>> # returns a TensorDict of batch size [3, 4, 1]
+        >>> td_unsqueeze = td.unsqueeze(-1)
+        >>> # returns a TensorDict of batch size [12]
+        >>> td_view = td.view(-1)
+        >>> # returns a tensor of batch size [12, 4]
+        >>> a_view = td.view(-1).get("a")
+
+    - casting operations: a TensorDict can be cast on a different device
+      or another TensorDict type using
+
+        >>> td_cpu = td.to("cpu")
+        >>> td_savec = td.to(SavedTensorDict)  # TensorDict saved on disk
+        >>> dictionary = td.to_dict()
+
+      A call of the `.to()` method with a dtype will return an error.
+
+    - Cloning, contiguous
+
+    - Reading: `td.get(key)`, `td.get_at(key, index)`
+
+    - Content modification: :obj:`td.set(key, value)`, :obj:`td.set_(key, value)`,
+      :obj:`td.update(td_or_dict)`, :obj:`td.update_(td_or_dict)`, :obj:`td.fill_(key,
+      value)`, :obj:`td.rename_key_(old_name, new_name)`, etc.
+
+    - Operations on multiple tensordicts: `torch.cat(tensordict_list, dim)`,
+      `torch.stack(tensordict_list, dim)`, `td1 == td2` etc.
+
+    Args:
+        source (TensorDict or dictionary): a data source. If empty, the
+            tensordict can be populated subsequently.
+        batch_size (iterable of int, optional): a batch size for the
+            tensordict. The batch size is immutable and can only be modified
+            by calling operations that create a new TensorDict. Unless the
+            source is another TensorDict, the batch_size argument must be
+            provided as it won't be inferred from the data.
+        device (torch.device or compatible type, optional): a device for the
+            TensorDict.
+        names (lsit of str, optional): the names of the dimensions of the
+            tensordict. If provided, its length must match the one of the
+            ``batch_size``. Defaults to ``None`` (no dimension name, or ``None``
+            for every dimension).
+
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+        >>> source = {'random': torch.randn(3, 4),
+        ...     'zeros': torch.zeros(3, 4, 5)}
+        >>> batch_size = [3]
+        >>> td = TensorDict(source, batch_size)
+        >>> print(td.shape)  # equivalent to td.batch_size
+        torch.Size([3])
+        >>> td_unqueeze = td.unsqueeze(-1)
+        >>> print(td_unqueeze.get("zeros").shape)
+        torch.Size([3, 1, 4, 5])
+        >>> print(td_unqueeze[0].shape)
+        torch.Size([1])
+        >>> print(td_unqueeze.view(-1).shape)
+        torch.Size([3])
+        >>> print((td.clone()==td).all())
+        True
+
+    """
+
+    __slots__ = (
+        "_tensordict",
+        "_batch_size",
+        "_is_shared",
+        "_is_memmap",
+        "_device",
+        "_is_locked",
+        "_td_dim_names",
+        "_lock_id",
+        "_locked_tensordicts",
+        "_cache",
+        "_last_op",
+        "__last_op_queue",
+    )
+
+    def __new__(cls, *args: Any, **kwargs: Any) -> TensorDict:
+        cls._is_shared = False
+        cls._is_memmap = False
+        cls._td_dim_names = None
+        return super().__new__(cls, *args, _safe=True, _lazy=False, **kwargs)
+
+    def __init__(
+        self,
+        source: TensorDictBase | dict[str, CompatibleType],
+        batch_size: Sequence[int] | torch.Size | int | None = None,
+        device: DeviceType | None = None,
+        names: Sequence[str] | None = None,
+        _run_checks: bool = True,
+        _is_shared: bool | None = False,
+        _is_memmap: bool | None = False,
+    ) -> None:
+        self._lock_id = set()
+        self._locked_tensordicts = []
+
+        self._is_shared = _is_shared
+        self._is_memmap = _is_memmap
+        if device is not None and isinstance(device, (int, str)):
+            device = torch.device(device)
+        self._device = device
+
+        if not _run_checks:
+            _tensordict: dict = _StringOnlyDict()
+            self._batch_size = batch_size
+            for key, value in source.items():
+                if isinstance(value, dict):
+                    value = TensorDict(
+                        value,
+                        batch_size=self._batch_size,
+                        device=self._device,
+                        _run_checks=_run_checks,
+                        _is_shared=_is_shared,
+                        _is_memmap=_is_memmap,
+                    )
+                _tensordict[key] = value
+            self._tensordict = _tensordict
+            self._td_dim_names = names
+        else:
+            self._tensordict = _StringOnlyDict()
+            if not isinstance(source, (TensorDictBase, dict)):
+                raise ValueError(
+                    "A TensorDict source is expected to be a TensorDictBase "
+                    f"sub-type or a dictionary, found type(source)={type(source)}."
+                )
+            self._batch_size = self._parse_batch_size(source, batch_size)
+            self.names = names
+
+            if source is not None:
+                for key, value in source.items():
+                    self.set(key, value)
+
+    # def __hash__(self):
+    #     return hash((self._tensordict, self._batch_size, self._device))
+
+    @classmethod
+    def from_dict(cls, input_dict, batch_size=None, device=None, batch_dims=None):
+        """Returns a TensorDict created from a dictionary or another :class:`TensorDict`.
+
+        If ``batch_size`` is not specified, returns the maximum batch size possible.
+
+        This function works on nested dictionaries too, or can be used to determine the
+        batch-size of a nested tensordict.
+
+        Args:
+            input_dict (dictionary, optional): a dictionary to use as a data source
+                (nested keys compatible).
+            batch_size (iterable of int, optional): a batch size for the tensordict.
+            device (torch.device or compatible type, optional): a device for the TensorDict.
+            batch_dims (int, optional): the ``batch_dims`` (ie number of leading dimensions
+                to be considered for ``batch_size``). Exclusinve with ``batch_size``.
+                Note that this is the __maximum__ number of batch dims of the tensordict,
+                a smaller number is tolerated.
+
+        Examples:
+            >>> input_dict = {"a": torch.randn(3, 4), "b": torch.randn(3)}
+            >>> print(TensorDict.from_dict(input_dict))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([3]),
+                device=None,
+                is_shared=False)
+            >>> # nested dict: the nested TensorDict can have a different batch-size
+            >>> # as long as its leading dims match.
+            >>> input_dict = {"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}
+            >>> print(TensorDict.from_dict(input_dict))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([3, 4]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([3]),
+                device=None,
+                is_shared=False)
+            >>> # we can also use this to work out the batch sie of a tensordict
+            >>> input_td = TensorDict({"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}, [])
+            >>> print(TensorDict.from_dict(input_td))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([3, 4]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([3]),
+                device=None,
+                is_shared=False)
+
+        """
+        if batch_dims is not None and batch_size is not None:
+            raise ValueError(
+                "Cannot pass both batch_size and batch_dims to `from_dict`."
+            )
+
+        batch_size_set = [] if batch_size is None else batch_size
+        for key, value in list(input_dict.items()):
+            if isinstance(value, (dict,)):
+                # we don't know if another tensor of smaller size is coming
+                # so we can't be sure that the batch-size will still be valid later
+                input_dict[key] = TensorDict.from_dict(
+                    value, batch_size=[], device=device, batch_dims=None
+                )
+        # _run_checks=False breaks because a tensor may have the same batch-size as the tensordict
+        out = cls(
+            input_dict,
+            batch_size=batch_size_set,
+            device=device,
+        )
+        if batch_size is None:
+            _set_max_batch_size(out, batch_dims)
+        else:
+            out.batch_size = batch_size
+        return out
+
+    @staticmethod
+    def _parse_batch_size(
+        source: TensorDictBase | dict,
+        batch_size: Sequence[int] | torch.Size | int | None = None,
+    ) -> torch.Size:
+        try:
+            return torch.Size(batch_size)
+        except Exception as err:
+            if isinstance(batch_size, Number):
+                return torch.Size([batch_size])
+            elif isinstance(source, TensorDictBase):
+                return source.batch_size
+            raise ValueError(
+                "batch size was not specified when creating the TensorDict "
+                "instance and it could not be retrieved from source."
+            ) from err
+
+    @property
+    def batch_dims(self) -> int:
+        return len(self.batch_size)
+
+    @batch_dims.setter
+    def batch_dims(self, value: int) -> None:
+        raise RuntimeError(
+            f"Setting batch dims on {self.__class__.__name__} instances is "
+            f"not allowed."
+        )
+
+    def _has_names(self):
+        return self._td_dim_names is not None
+
+    def _erase_names(self):
+        self._td_dim_names = None
+
+    @property
+    def names(self):
+        names = self._td_dim_names
+        if names is None:
+            return [None for _ in range(self.batch_dims)]
+        return names
+
+    @names.setter
+    def names(self, value):
+        # we don't run checks on types for efficiency purposes
+        if value is None:
+            self._erase_names()
+            return
+        num_none = sum(v is None for v in value)
+        if num_none:
+            num_none -= 1
+        if len(set(value)) != len(value) - num_none:
+            raise ValueError(f"Some dimension names are non-unique: {value}.")
+        if len(value) != self.batch_dims:
+            raise ValueError(
+                "the length of the dimension names must equate the tensordict batch_dims attribute. "
+                f"Got {value} for batch_dims {self.batch_dims}."
+            )
+        self._rename_subtds(value)
+        self._td_dim_names = list(value)
+
+    def _rename_subtds(self, names):
+        if names is None:
+            for item in self._tensordict.values():
+                if _is_tensor_collection(item.__class__):
+                    item._erase_names()
+            return
+        for item in self._tensordict.values():
+            if _is_tensor_collection(item.__class__):
+                item_names = item.names
+                td_names = list(names) + item_names[len(names) :]
+                item.rename_(*td_names)
+
+    @property
+    def device(self) -> torch.device | None:
+        """Device of the tensordict.
+
+        Returns `None` if device hasn't been provided in the constructor or set via `tensordict.to(device)`.
+
+        """
+        return self._device
+
+    @device.setter
+    def device(self, value: DeviceType) -> None:
+        raise RuntimeError(
+            "device cannot be set using tensordict.device = device, "
+            "because device cannot be updated in-place. To update device, use "
+            "tensordict.to(new_device), which will return a new tensordict "
+            "on the new device."
+        )
+
+    @property
+    def batch_size(self) -> torch.Size:
+        return self._batch_size
+
+    @batch_size.setter
+    def batch_size(self, new_size: torch.Size) -> None:
+        self._batch_size_setter(new_size)
+
+    def _change_batch_size(self, new_size: torch.Size) -> None:
+        if not hasattr(self, "_orig_batch_size"):
+            self._orig_batch_size = self.batch_size
+        elif self._orig_batch_size == new_size:
+            del self._orig_batch_size
+        self._batch_size = new_size
+
+    # Checks
+    def _check_is_shared(self) -> bool:
+        share_list = [_is_shared(value) for value in self.values()]
+        if any(share_list) and not all(share_list):
+            shared_str = ", ".join(
+                [f"{key}: {_is_shared(value)}" for key, value in self.items()]
+            )
+            raise RuntimeError(
+                f"tensors must be either all shared or not, but mixed "
+                f"features is not allowed. "
+                f"Found: {shared_str}"
+            )
+        return all(share_list) and len(share_list) > 0
+
+    def _check_is_memmap(self) -> bool:
+        memmap_list = [is_memmap(self.entry_class(key)) for key in self.keys()]
+        if any(memmap_list) and not all(memmap_list):
+            memmap_str = ", ".join(
+                [f"{key}: {is_memmap(self.entry_class(key))}" for key in self.keys()]
+            )
+            raise RuntimeError(
+                f"tensors must be either all MemmapTensor or not, but mixed "
+                f"features is not allowed. "
+                f"Found: {memmap_str}"
+            )
+        return all(memmap_list) and len(memmap_list) > 0
+
+    def _check_device(self) -> None:
+        devices = {value.device for value in self.values()}
+        if self.device is not None and len(devices) >= 1 and devices != {self.device}:
+            raise RuntimeError(
+                f"TensorDict.device is {self._device}, but elements have "
+                f"device values {devices}. If TensorDict.device is set then "
+                "all elements must share that device."
+            )
+
+    # def _index_tensordict(self, idx: IndexType) -> TensorDictBase:
+    #     names = self._get_names_idx(idx)
+    #     self_copy = copy(self)
+    #     # self_copy = self.clone(False)
+    #     self_copy._tensordict = {
+    #         key: _get_item(item, idx) for key, item in self.items()
+    #     }
+    #     self_copy._batch_size = _getitem_batch_size(self_copy.batch_size, idx)
+    #     self_copy._device = self.device
+    #     self_copy.names = names
+    #     return self_copy
+
+    def pin_memory(self) -> TensorDictBase:
+        def pin_mem(tensor):
+            return tensor.pin_memory()
+
+        return self.apply(pin_mem)
+
+    def expand(self, *shape: int) -> TensorDictBase:
+        """Expands every tensor with `(*shape, *tensor.shape)` and returns the same tensordict with new tensors with expanded shapes.
+
+        Supports iterables to specify the shape.
+
+        """
+        d = {}
+        tensordict_dims = self.batch_dims
+
+        if len(shape) == 1 and isinstance(shape[0], Sequence):
+            shape = tuple(shape[0])
+
+        # new shape dim check
+        if len(shape) < len(self.shape):
+            raise RuntimeError(
+                "the number of sizes provided ({shape_dim}) must be greater or equal to the number of "
+                "dimensions in the TensorDict ({tensordict_dim})".format(
+                    shape_dim=len(shape), tensordict_dim=tensordict_dims
+                )
+            )
+
+        # new shape compatability check
+        for old_dim, new_dim in zip(self.batch_size, shape[-tensordict_dims:]):
+            if old_dim != 1 and new_dim != old_dim:
+                raise RuntimeError(
+                    "Incompatible expanded shape: The expanded shape length at non-singleton dimension should be same "
+                    "as the original length. target_shape = {new_shape}, existing_shape = {old_shape}".format(
+                        new_shape=shape, old_shape=self.batch_size
+                    )
+                )
+
+        for key, value in self.items():
+            tensor_dims = len(value.shape)
+            last_n_dims = tensor_dims - tensordict_dims
+            if last_n_dims > 0:
+                d[key] = value.expand(*shape, *value.shape[-last_n_dims:])
+            else:
+                d[key] = value.expand(*shape)
+        out = TensorDict(
+            source=d,
+            batch_size=torch.Size(shape),
+            device=self.device,
+            _run_checks=False,
+        )
+        if self._td_dim_names is not None:
+            out.refine_names(..., *self.names)
+        return out
+
+    def _set_str(
+        self,
+        key: NestedKey,
+        value: dict[str, CompatibleType] | CompatibleType,
+        *,
+        inplace: bool,
+        validated: bool,
+    ) -> TensorDictBase:
+        best_attempt = inplace is BEST_ATTEMPT_INPLACE
+        inplace = self._convert_inplace(inplace, key)
+        if not validated:
+            value = self._validate_value(value, check_shape=True)
+        if not inplace:
+            if self.is_locked:
+                raise RuntimeError(TensorDictBase.LOCK_ERROR)
+            self._tensordict[key] = value
+        else:
+            try:
+                dest = self._get_str(key, default=NO_DEFAULT)
+                if best_attempt and _is_tensor_collection(dest.__class__):
+                    dest.update(value, inplace=True)
+                else:
+                    dest.copy_(value)
+            except KeyError as err:
+                raise err
+            except Exception as err:
+                raise ValueError(
+                    f"Failed to update '{key}' in tensordict {self}"
+                ) from err
+        return self
+
+    def _set_tuple(
+        self,
+        key: NestedKey,
+        value: dict[str, CompatibleType] | CompatibleType,
+        *,
+        inplace: bool,
+        validated: bool,
+    ) -> TensorDictBase:
+        if len(key) == 1:
+            return self._set_str(key[0], value, inplace=inplace, validated=validated)
+        td = self._get_str(key[0], None)
+        if td is None:
+            self._create_nested_str(key[0])
+            td = self._get_str(key[0], NO_DEFAULT)
+            inplace = False
+        elif not _is_tensor_collection(td.__class__):
+            raise RuntimeError(
+                f"The entry {key[0]} is already present in " f"tensordict {self}."
+            )
+        td._set_tuple(key[1:], value, inplace=inplace, validated=validated)
+        return self
+
+    def _set_at_str(self, key, value, idx, *, validated):
+        if not validated:
+            value = self._validate_value(value, check_shape=False)
+            validated = True
+        tensor_in = self._get_str(key, NO_DEFAULT)
+
+        if isinstance(idx, tuple) and len(idx) and isinstance(idx[0], tuple):
+            warn(
+                "Multiple indexing can lead to unexpected behaviours when "
+                "setting items, for instance `td[idx1][idx2] = other` may "
+                "not write to the desired location if idx1 is a list/tensor."
+            )
+            tensor_in = _sub_index(tensor_in, idx)
+            tensor_in.copy_(value)
+        else:
+            _set_item(tensor_in, idx, value, validated=validated)
+
+        return self
+
+    def _set_at_tuple(self, key, value, idx, *, validated):
+        if len(key) == 1:
+            return self._set_at_str(key[0], value, idx, validated=validated)
+        if key[0] not in self.keys():
+            # this won't work
+            raise KeyError(f"key {key} not found in set_at_ with tensordict {self}.")
+        else:
+            td = self._get_str(key[0], NO_DEFAULT)
+        td._set_at_tuple(key[1:], value, idx, validated=validated)
+        return self
+
+    @lock_blocked
+    def del_(self, key: NestedKey) -> TensorDictBase:
+        key = _unravel_key_to_tuple(key)
+        if len(key) > 1:
+            td, subkey = _get_leaf_tensordict(self, key)
+            td.del_(subkey)
+            return self
+
+        del self._tensordict[key[0]]
+        return self
+
+    @lock_blocked
+    def rename_key_(
+        self, old_key: str, new_key: str, safe: bool = False
+    ) -> TensorDictBase:
+        # these checks are not perfect, tuples that are not tuples of strings or empty
+        # tuples could go through but (1) it will raise an error anyway and (2)
+        # those checks are expensive when repeated often.
+        if not isinstance(old_key, (str, tuple)):
+            raise TypeError(
+                f"Expected old_name to be a string or a tuple of strings but found {type(old_key)}"
+            )
+        if not isinstance(new_key, (str, tuple)):
+            raise TypeError(
+                f"Expected new_name to be a string or a tuple of strings but found {type(new_key)}"
+            )
+        if safe and (new_key in self.keys(include_nested=True)):
+            raise KeyError(f"key {new_key} already present in TensorDict.")
+
+        if isinstance(new_key, str):
+            self._set_str(new_key, self.get(old_key), inplace=False, validated=True)
+        else:
+            self._set_tuple(new_key, self.get(old_key), inplace=False, validated=True)
+        self.del_(old_key)
+        return self
+
+    rename_key = _renamed_inplace_method(rename_key_)
+
+    def _stack_onto_(self, list_item: list[CompatibleType], dim: int) -> TensorDict:
+        # if not isinstance(key, str):
+        #     raise ValueError("_stack_onto_ expects string keys.")
+        for key in self.keys():
+            vals = [item._get_str(key, None) for item in list_item]
+            if all(v is None for v in vals):
+                continue
+            dest = self._get_str(key, NO_DEFAULT)
+            torch.stack(
+                vals,
+                dim=dim,
+                out=dest,
+            )
+        return self
+
+    def entry_class(self, key: NestedKey) -> type:
+        return type(self.get(key))
+
+    def _stack_onto_at_(
+        self,
+        list_item: list[CompatibleType],
+        dim: int,
+        idx: IndexType,
+    ) -> TensorDict:
+        if not isinstance(idx, tuple):
+            idx = (idx,)
+        idx = convert_ellipsis_to_idx(idx, self.batch_size)
+        for key in self.keys():
+            vals = [td._get_str(key, NO_DEFAULT) for td in list_item]
+            if all(v is None for v in vals):
+                continue
+            v = self._get_str(key, NO_DEFAULT)
+            v_idx = v[idx]
+            if v.data_ptr() != v_idx.data_ptr():
+                raise IndexError(
+                    f"Index {idx} is incompatible with stack(..., out=data) as the storages of the indexed tensors differ."
+                )
+            torch.stack(vals, dim=dim, out=v_idx)
+            # raise ValueError(
+            #     f"Cannot stack onto an indexed tensor with index {idx} "
+            #     f"as its storage differs."
+            # )
+        return self
+
+    def _get_str(self, key, default):
+        first_key = key
+        out = self._tensordict.get(first_key, None)
+        if out is None:
+            return self._default_get(first_key, default)
+        return out
+
+    def _get_tuple(self, key, default):
+        first = self._get_str(key[0], default)
+        if len(key) == 1 or first is default:
+            return first
+        try:
+            if isinstance(first, KeyedJaggedTensor):
+                if len(key) != 2:
+                    raise ValueError(f"Got too many keys for a KJT: {key}.")
+                return first[key[1]]
+            else:
+                return first._get_tuple(key[1:], default=default)
+        except AttributeError as err:
+            if "has no attribute" in str(err):
+                raise ValueError(
+                    f"Expected a TensorDictBase instance but got {type(first)} instead"
+                    f" for key '{key[1:]}' in tensordict:\n{self}."
+                )
+
+    def share_memory_(self) -> TensorDictBase:
+        if self.is_memmap():
+            raise RuntimeError(
+                "memmap and shared memory are mutually exclusive features."
+            )
+        if self.device is not None and self.device.type == "cuda":
+            # cuda tensors are shared by default
+            return self
+        for value in self.values():
+            # no need to consider MemmapTensors here as we have checked that this is not a memmap-tensordict
+            if (
+                isinstance(value, Tensor)
+                and value.device.type == "cpu"
+                or _is_tensor_collection(value.__class__)
+            ):
+                value.share_memory_()
+        self._is_shared = True
+        self.lock_()
+        return self
+
+    def detach_(self) -> TensorDictBase:
+        for value in self.values():
+            value.detach_()
+        return self
+
+    def memmap_(
+        self,
+        prefix: str | None = None,
+        copy_existing: bool = False,
+    ) -> TensorDictBase:
+        if prefix is not None:
+            prefix = Path(prefix)
+            if not prefix.exists():
+                os.makedirs(prefix, exist_ok=True)
+            torch.save(
+                {"batch_size": self.batch_size, "device": self.device},
+                prefix / "meta.pt",
+            )
+        if self.is_shared() and self.device.type == "cpu":
+            raise RuntimeError(
+                "memmap and shared memory are mutually exclusive features."
+            )
+        if not self._tensordict.keys():
+            raise Exception(
+                "memmap_() must be called when the TensorDict is (partially) "
+                "populated. Set a tensor first."
+            )
+        for key, value in self.items():
+            if value.requires_grad:
+                raise Exception(
+                    "memmap is not compatible with gradients, one of Tensors has requires_grad equals True"
+                )
+            if _is_tensor_collection(value.__class__):
+                if prefix is not None:
+                    # ensure subdirectory exists
+                    os.makedirs(prefix / key, exist_ok=True)
+                    self._tensordict[key] = value.memmap_(
+                        prefix=prefix / key, copy_existing=copy_existing
+                    )
+                    torch.save(
+                        {"batch_size": value.batch_size, "device": value.device},
+                        prefix / key / "meta.pt",
+                    )
+                else:
+                    self._tensordict[key] = value.memmap_()
+                continue
+            elif isinstance(value, MemmapTensor):
+                if (
+                    # user didn't specify location
+                    prefix is None
+                    # file is already in the correct location
+                    or str(prefix / f"{key}.memmap") == value.filename
+                ):
+                    self._tensordict[key] = value
+                elif copy_existing:
+                    # user did specify location and memmap is in wrong place, so we copy
+                    self._tensordict[key] = MemmapTensor.from_tensor(
+                        value, filename=str(prefix / f"{key}.memmap")
+                    )
+                else:
+                    # memmap in wrong location and copy is disallowed
+                    raise RuntimeError(
+                        "TensorDict already contains MemmapTensors saved to a location "
+                        "incompatible with prefix. Either move the location of the "
+                        "MemmapTensors, or allow automatic copying with "
+                        "copy_existing=True"
+                    )
+            else:
+                self._tensordict[key] = MemmapTensor.from_tensor(
+                    value,
+                    filename=str(prefix / f"{key}.memmap")
+                    if prefix is not None
+                    else None,
+                )
+            if prefix is not None:
+                torch.save(
+                    {
+                        "shape": value.shape,
+                        "device": value.device,
+                        "dtype": value.dtype,
+                    },
+                    prefix / f"{key}.meta.pt",
+                )
+        self._is_memmap = True
+        self.lock_()
+        return self
+
+    @classmethod
+    def load_memmap(cls, prefix: str) -> TensorDictBase:
+        prefix = Path(prefix)
+        metadata = torch.load(prefix / "meta.pt")
+        out = cls({}, batch_size=metadata["batch_size"], device=metadata["device"])
+
+        for path in prefix.glob("**/*meta.pt"):
+            key = path.parts[len(prefix.parts) :]
+            if path.name == "meta.pt":
+                if path == prefix / "meta.pt":
+                    # skip prefix / "meta.pt" as we've already read it
+                    continue
+                key = key[:-1]  # drop "meta.pt" from key
+                metadata = torch.load(path)
+                if key in out.keys(include_nested=True):
+                    out[key].batch_size = metadata["batch_size"]
+                    device = metadata["device"]
+                    if device is not None:
+                        out[key] = out[key].to(device)
+                else:
+                    out[key] = cls(
+                        {}, batch_size=metadata["batch_size"], device=metadata["device"]
+                    )
+            else:
+                leaf, *_ = key[-1].rsplit(".", 2)  # remove .meta.pt suffix
+                key = (*key[:-1], leaf)
+                metadata = torch.load(path)
+                out[key] = MemmapTensor(
+                    *metadata["shape"],
+                    device=metadata["device"],
+                    dtype=metadata["dtype"],
+                    filename=str(path.parent / f"{leaf}.memmap"),
+                )
+
+        return out
+
+    def to(self, dest: DeviceType | torch.Size | type, **kwargs: Any) -> TensorDictBase:
+        if isinstance(dest, type) and issubclass(dest, TensorDictBase):
+            if isinstance(self, dest):
+                return self
+            td = dest(source=self, **kwargs)
+            if self._td_dim_names is not None:
+                td.names = self._td_dim_names
+            return td
+        elif isinstance(dest, (torch.device, str, int)):
+            # must be device
+            dest = torch.device(dest)
+            if self.device is not None and dest == self.device:
+                return self
+
+            def to(tensor):
+                return tensor.to(dest, **kwargs)
+
+            return self.apply(to, device=dest)
+        elif isinstance(dest, torch.Size):
+            self.batch_size = dest
+            return self
+        elif dest is None:
+            return self
+        else:
+            raise NotImplementedError(
+                f"dest must be a string, torch.device or a TensorDict "
+                f"instance, {dest} not allowed"
+            )
+
+    def where(self, condition, other, *, out=None):
+        if out is None:
+            if _is_tensor_collection(other.__class__):
+
+                def func(tensor, _other):
+                    return torch.where(
+                        expand_as_right(condition, tensor), tensor, _other
+                    )
+
+                return self.apply(func, other)
+            else:
+
+                def func(tensor):
+                    return torch.where(
+                        expand_as_right(condition, tensor), tensor, other
+                    )
+
+                return self.apply(func)
+        else:
+            if _is_tensor_collection(other.__class__):
+
+                def func(tensor, _other, _out):
+                    return torch.where(
+                        expand_as_right(condition, tensor), tensor, _other, out=_out
+                    )
+
+                return self.apply(func, other, out)
+            else:
+
+                def func(tensor, _out):
+                    return torch.where(
+                        expand_as_right(condition, tensor), tensor, other, out=_out
+                    )
+
+                return self.apply(func, out)
+
+    def masked_fill_(self, mask: Tensor, value: float | int | bool) -> TensorDictBase:
+        for item in self.values():
+            mask_expand = expand_as_right(mask, item)
+            item.masked_fill_(mask_expand, value)
+        return self
+
+    def masked_fill(self, mask: Tensor, value: float | bool) -> TensorDictBase:
+        td_copy = self.clone()
+        return td_copy.masked_fill_(mask, value)
+
+    def is_contiguous(self) -> bool:
+        return all([value.is_contiguous() for _, value in self.items()])
+
+    def clone(self, recurse: bool = True) -> TensorDictBase:
+        return TensorDict(
+            source={key: _clone_value(value, recurse) for key, value in self.items()},
+            batch_size=self.batch_size,
+            device=self.device,
+            names=copy(self._td_dim_names),
+            _run_checks=False,
+            _is_shared=self.is_shared() if not recurse else False,
+            _is_memmap=self.is_memmap() if not recurse else False,
+        )
+
+    def contiguous(self) -> TensorDictBase:
+        if not self.is_contiguous():
+            return self.clone()
+        return self
+
+    def select(
+        self, *keys: NestedKey, inplace: bool = False, strict: bool = True
+    ) -> TensorDictBase:
+        source = {}
+        if len(keys):
+            keys_to_select = None
+            for key in keys:
+                if isinstance(key, str):
+                    subkey = []
+                else:
+                    key, subkey = key[0], key[1:]
+                try:
+                    source[key] = self.get(key)
+                    if len(subkey):
+                        if keys_to_select is None:
+                            # delay creation of defaultdict
+                            keys_to_select = defaultdict(list)
+                        keys_to_select[key].append(subkey)
+                except KeyError as err:
+                    if not strict:
+                        continue
+                    else:
+                        raise KeyError(f"select failed to get key {key}") from err
+            if keys_to_select is not None:
+                for key, val in keys_to_select.items():
+                    source[key] = source[key].select(
+                        *val, strict=strict, inplace=inplace
+                    )
+
+        out = TensorDict(
+            device=self.device,
+            batch_size=self.batch_size,
+            source=source,
+            # names=self.names if self._has_names() else None,
+            names=self._td_dim_names,
+            _run_checks=False,
+            _is_memmap=self._is_memmap,
+            _is_shared=self._is_shared,
+        )
+        if inplace:
+            self._tensordict = out._tensordict
+            return self
+        return out
+
+    def keys(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> _TensorDictKeysView:
+        if not include_nested and not leaves_only:
+            return self._tensordict.keys()
+        else:
+            return self._nested_keys(
+                include_nested=include_nested, leaves_only=leaves_only
+            )
+
+    # @cache  # noqa: B019
+    def _nested_keys(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> _TensorDictKeysView:
+        return _TensorDictKeysView(
+            self, include_nested=include_nested, leaves_only=leaves_only
+        )
+
+    def __getstate__(self):
+        return {
+            slot: getattr(self, slot)
+            for slot in self.__slots__
+            if slot not in ("_last_op", "_cache", "__last_op_queue")
+        }
+
+    def __setstate__(self, state):
+        for slot, value in state.items():
+            setattr(self, slot, value)
+        self._cache = None
+        self.__last_op_queue = None
+        self._last_op = None
+
+    # some custom methods for efficiency
+    def items(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> Iterator[tuple[str, CompatibleType]]:
+        if not include_nested and not leaves_only:
+            return self._tensordict.items()
+        else:
+            return super().items(include_nested=include_nested, leaves_only=leaves_only)
+
+    def values(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> Iterator[tuple[str, CompatibleType]]:
+        if not include_nested and not leaves_only:
+            return self._tensordict.values()
+        else:
+            return super().values(
+                include_nested=include_nested, leaves_only=leaves_only
+            )
+
+
+class _ErrorInteceptor:
+    """Context manager for catching errors and modifying message.
+
+    Intended for use with stacking / concatenation operations applied to TensorDicts.
+
+    """
+
+    DEFAULT_EXC_MSG = "Expected all tensors to be on the same device"
+
+    def __init__(
+        self,
+        key: NestedKey,
+        prefix: str,
+        exc_msg: str | None = None,
+        exc_type: type[Exception] | None = None,
+    ) -> None:
+        self.exc_type = exc_type if exc_type is not None else RuntimeError
+        self.exc_msg = exc_msg if exc_msg is not None else self.DEFAULT_EXC_MSG
+        self.prefix = prefix
+        self.key = key
+
+    def _add_key_to_error_msg(self, msg: str) -> str:
+        if msg.startswith(self.prefix):
+            return f'{self.prefix} "{self.key}" /{msg[len(self.prefix):]}'
+        return f'{self.prefix} "{self.key}". {msg}'
+
+    def __enter__(self):
+        pass
+
+    def __exit__(self, exc_type, exc_value, _):
+        if exc_type is self.exc_type and (
+            self.exc_msg is None or self.exc_msg in str(exc_value)
+        ):
+            exc_value.args = (self._add_key_to_error_msg(str(exc_value)),)
+
+
+def _nested_keys_to_dict(keys: Iterator[NestedKey]) -> dict[str, Any]:
+    nested_keys = {}
+    for key in keys:
+        if isinstance(key, str):
+            nested_keys.setdefault(key, {})
+        else:
+            d = nested_keys
+            for subkey in key:
+                d = d.setdefault(subkey, {})
+    return nested_keys
+
+
+def _dict_to_nested_keys(
+    nested_keys: dict[NestedKey, NestedKey], prefix: tuple[str, ...] = ()
+) -> tuple[str, ...]:
+    for key, subkeys in nested_keys.items():
+        if subkeys:
+            yield from _dict_to_nested_keys(subkeys, prefix=(*prefix, key))
+        elif prefix:
+            yield (*prefix, key)
+        else:
+            yield key
+
+
+def _default_hook(td: TensorDictBase, key: tuple[str, ...]) -> None:
+    """Used to populate a tensordict.
+
+    For example, ``td.set(("a", "b"))`` may require to create ``"a"``.
+
+    """
+    out = td.get(key[0], None)
+    if out is None:
+        td._create_nested_str(key[0])
+        out = td._get_str(key[0], None)
+    return out
+
+
+def _get_leaf_tensordict(
+    tensordict: TensorDictBase, key: tuple[str, ...], hook: Callable = None
+) -> tuple[TensorDictBase, str]:
+    # utility function for traversing nested tensordicts
+    # hook should return the default value for tensordit.get(key)
+    while len(key) > 1:
+        if hook is not None:
+            tensordict = hook(tensordict, key)
+        else:
+            tensordict = tensordict.get(key[0])
+        key = key[1:]
+    return tensordict, key[0]
+
+
+def implements_for_td(torch_function: Callable) -> Callable[[Callable], Callable]:
+    """Register a torch function override for TensorDict."""
+
+    @functools.wraps(torch_function)
+    def decorator(func: Callable) -> Callable:
+        TD_HANDLED_FUNCTIONS[torch_function] = func
+        return func
+
+    return decorator
+
+
+def implements_for_lazy_td(torch_function: Callable) -> Callable[[Callable], Callable]:
+    """Register a torch function override for TensorDict."""
+
+    @functools.wraps(torch_function)
+    def decorator(func: Callable) -> Callable:
+        LAZY_TD_HANDLED_FUNCTIONS[torch_function] = func
+        return func
+
+    return decorator
+
+
+# @implements_for_td(torch.testing.assert_allclose) TODO
+def assert_allclose_td(
+    actual: TensorDictBase,
+    expected: TensorDictBase,
+    rtol: float | None = None,
+    atol: float | None = None,
+    equal_nan: bool = True,
+    msg: str = "",
+) -> bool:
+    """Compares two tensordicts and raise an exception if their content does not match exactly."""
+    if not _is_tensor_collection(actual.__class__) or not _is_tensor_collection(
+        expected.__class__
+    ):
+        raise TypeError("assert_allclose inputs must be of TensorDict type")
+    if isinstance(actual, LazyStackedTensorDict) and isinstance(
+        expected, LazyStackedTensorDict
+    ):
+        for sub_actual, sub_expected in zip(actual.tensordicts, expected.tensordicts):
+            assert_allclose_td(sub_actual, sub_expected, rtol=rtol, atol=atol)
+        return True
+
+    set1 = set(actual.keys())
+    set2 = set(expected.keys())
+    if not (len(set1.difference(set2)) == 0 and len(set2) == len(set1)):
+        raise KeyError(
+            "actual and expected tensordict keys mismatch, "
+            f"keys {(set1 - set2).union(set2 - set1)} appear in one but not "
+            f"the other."
+        )
+    keys = sorted(actual.keys(), key=str)
+    for key in keys:
+        input1 = actual.get(key)
+        input2 = expected.get(key)
+        if _is_tensor_collection(input1.__class__):
+            assert_allclose_td(input1, input2, rtol=rtol, atol=atol)
+            continue
+
+        mse = (input1.to(torch.float) - input2.to(torch.float)).pow(2).sum()
+        mse = mse.div(input1.numel()).sqrt().item()
+
+        default_msg = f"key {key} does not match, got mse = {mse:4.4f}"
+        msg = "\t".join([default_msg, msg]) if len(msg) else default_msg
+        if isinstance(input1, MemmapTensor):
+            input1 = input1._tensor
+        if isinstance(input2, MemmapTensor):
+            input2 = input2._tensor
+        torch.testing.assert_close(
+            input1, input2, rtol=rtol, atol=atol, equal_nan=equal_nan, msg=msg
+        )
+    return True
+
+
+@implements_for_td(torch.unbind)
+def _unbind(
+    td: TensorDictBase, *args: Any, **kwargs: Any
+) -> tuple[TensorDictBase, ...]:
+    return td.unbind(*args, **kwargs)
+
+
+@implements_for_td(torch.gather)
+def _gather(
+    input: TensorDictBase,
+    dim: int,
+    index: Tensor,
+    *,
+    sparse_grad: bool = False,
+    out: TensorDictBase | None = None,
+) -> TensorDictBase:
+    if sparse_grad:
+        raise NotImplementedError(
+            "sparse_grad=True not implemented for torch.gather(tensordict, ...)"
+        )
+    # the index must have as many dims as the tensordict
+    if not len(index):
+        raise RuntimeError("Cannot use torch.gather with an empty index")
+    dim_orig = dim
+    if dim < 0:
+        dim = input.batch_dims + dim
+    if dim > input.batch_dims - 1 or dim < 0:
+        raise RuntimeError(
+            f"Cannot gather tensordict with shape {input.shape} along dim {dim_orig}."
+        )
+
+    def _gather_tensor(tensor, dest=None):
+        index_expand = index
+        while index_expand.ndim < tensor.ndim:
+            index_expand = index_expand.unsqueeze(-1)
+        target_shape = list(tensor.shape)
+        target_shape[dim] = index_expand.shape[dim]
+        index_expand = index_expand.expand(target_shape)
+        out = torch.gather(tensor, dim, index_expand, out=dest)
+        return out
+
+    if out is None:
+        names = input.names if input._has_names() else None
+
+        return TensorDict(
+            {key: _gather_tensor(value) for key, value in input.items()},
+            batch_size=index.shape,
+            names=names,
+        )
+    TensorDict(
+        {key: _gather_tensor(value, out[key]) for key, value in input.items()},
+        batch_size=index.shape,
+    )
+    return out
+
+
+@implements_for_td(torch.full_like)
+def _full_like(td: TensorDictBase, fill_value: float, **kwargs: Any) -> TensorDictBase:
+    td_clone = td.clone()
+    for key in td_clone.keys():
+        td_clone.fill_(key, fill_value)
+    if "dtype" in kwargs:
+        raise ValueError("Cannot pass dtype to full_like with TensorDict")
+    if "device" in kwargs:
+        td_clone = td_clone.to(kwargs.pop("device"))
+    if len(kwargs):
+        raise RuntimeError(
+            f"keyword arguments {list(kwargs.keys())} are not "
+            f"supported with full_like with TensorDict"
+        )
+    return td_clone
+
+
+@implements_for_td(torch.zeros_like)
+def _zeros_like(td: TensorDictBase, **kwargs: Any) -> TensorDictBase:
+    td_clone = td.apply(torch.zeros_like)
+    if "dtype" in kwargs:
+        raise ValueError("Cannot pass dtype to full_like with TensorDict")
+    if "device" in kwargs:
+        td_clone = td_clone.to(kwargs.pop("device"))
+    if len(kwargs):
+        raise RuntimeError(
+            f"keyword arguments {list(kwargs.keys())} are not "
+            f"supported with full_like with TensorDict"
+        )
+    return td_clone
+
+
+@implements_for_td(torch.ones_like)
+def _ones_like(td: TensorDictBase, **kwargs: Any) -> TensorDictBase:
+    td_clone = td.apply(lambda x: torch.ones_like(x))
+    if "device" in kwargs:
+        td_clone = td_clone.to(kwargs.pop("device"))
+    if len(kwargs):
+        raise RuntimeError(
+            f"keyword arguments {list(kwargs.keys())} are not "
+            f"supported with full_like with TensorDict"
+        )
+    return td_clone
+
+
+@implements_for_td(torch.empty_like)
+def _empty_like(td: TensorDictBase, *args, **kwargs) -> TensorDictBase:
+    try:
+        tdclone = td.clone()
+    except Exception as err:
+        raise RuntimeError(
+            "The tensordict passed to torch.empty_like cannot be "
+            "cloned, preventing empty_like to be called. "
+            "Consider calling tensordict.to_tensordict() first."
+        ) from err
+    return tdclone.apply_(lambda x: torch.empty_like(x, *args, **kwargs))
+
+
+@implements_for_td(torch.clone)
+def _clone(td: TensorDictBase, *args: Any, **kwargs: Any) -> TensorDictBase:
+    return td.clone(*args, **kwargs)
+
+
+@implements_for_td(torch.squeeze)
+def _squeeze(td: TensorDictBase, *args: Any, **kwargs: Any) -> TensorDictBase:
+    return td.squeeze(*args, **kwargs)
+
+
+@implements_for_td(torch.unsqueeze)
+def _unsqueeze(td: TensorDictBase, *args: Any, **kwargs: Any) -> TensorDictBase:
+    return td.unsqueeze(*args, **kwargs)
+
+
+@implements_for_td(torch.masked_select)
+def _masked_select(td: TensorDictBase, *args: Any, **kwargs: Any) -> TensorDictBase:
+    return td.masked_select(*args, **kwargs)
+
+
+@implements_for_td(torch.permute)
+def _permute(td: TensorDictBase, dims: Sequence[int]) -> TensorDictBase:
+    return td.permute(*dims)
+
+
+@implements_for_td(torch.cat)
+def _cat(
+    list_of_tensordicts: Sequence[TensorDictBase],
+    dim: int = 0,
+    device: DeviceType | None = None,
+    out: TensorDictBase | None = None,
+) -> TensorDictBase:
+    if not list_of_tensordicts:
+        raise RuntimeError("list_of_tensordicts cannot be empty")
+
+    batch_size = list(list_of_tensordicts[0].batch_size)
+    if dim < 0:
+        dim = len(batch_size) + dim
+    if dim >= len(batch_size):
+        raise RuntimeError(
+            f"dim must be in the range 0 <= dim < len(batch_size), got dim"
+            f"={dim} and batch_size={batch_size}"
+        )
+    batch_size[dim] = sum([td.batch_size[dim] for td in list_of_tensordicts])
+    batch_size = torch.Size(batch_size)
+
+    # check that all tensordict match
+    keys = _check_keys(list_of_tensordicts, strict=True)
+    if out is None:
+        out = {}
+        for key in keys:
+            with _ErrorInteceptor(
+                key, "Attempted to concatenate tensors on different devices at key"
+            ):
+                out[key] = torch.cat(
+                    [td._get_str(key, NO_DEFAULT) for td in list_of_tensordicts], dim
+                )
+        if device is None:
+            device = list_of_tensordicts[0].device
+            for td in list_of_tensordicts[1:]:
+                if device == td.device:
+                    continue
+                else:
+                    device = None
+                    break
+        names = None
+        if list_of_tensordicts[0]._has_names():
+            names = list_of_tensordicts[0].names
+        return TensorDict(
+            out, device=device, batch_size=batch_size, _run_checks=False, names=names
+        )
+    else:
+        if out.batch_size != batch_size:
+            raise RuntimeError(
+                "out.batch_size and cat batch size must match, "
+                f"got out.batch_size={out.batch_size} and batch_size"
+                f"={batch_size}"
+            )
+
+        for key in keys:
+            with _ErrorInteceptor(
+                key, "Attempted to concatenate tensors on different devices at key"
+            ):
+                if isinstance(out, TensorDict):
+                    torch.cat(
+                        [td.get(key) for td in list_of_tensordicts],
+                        dim,
+                        out=out.get(key),
+                    )
+                else:
+                    out.set_(
+                        key, torch.cat([td.get(key) for td in list_of_tensordicts], dim)
+                    )
+        return out
+
+
+@implements_for_lazy_td(torch.cat)
+def _lazy_cat(
+    list_of_tensordicts: Sequence[LazyStackedTensorDict],
+    dim: int = 0,
+    out: LazyStackedTensorDict | None = None,
+) -> LazyStackedTensorDict:
+    if not list_of_tensordicts:
+        raise RuntimeError("list_of_tensordicts cannot be empty")
+
+    batch_size = list(list_of_tensordicts[0].batch_size)
+    if dim < 0:
+        dim = len(batch_size) + dim
+    if dim >= len(batch_size):
+        raise RuntimeError(
+            f"dim must be in the range 0 <= dim < len(batch_size), got dim"
+            f"={dim} and batch_size={batch_size}"
+        )
+    stack_dim = list_of_tensordicts[0].stack_dim
+    if any((td.stack_dim != stack_dim) for td in list_of_tensordicts):
+        raise RuntimeError("cat lazy stacked tds must have same stack dim")
+
+    batch_size[dim] = sum(td.batch_size[dim] for td in list_of_tensordicts)
+    batch_size = torch.Size(batch_size)
+
+    new_dim = dim
+    if dim > stack_dim:
+        new_dim = dim - 1
+
+    if out is None:
+        out = []
+        if dim == stack_dim:  # if dim is stack, just add all to the same list
+            for lazy_td in list_of_tensordicts:
+                out += lazy_td.tensordicts
+        else:
+            for i in range(len(list_of_tensordicts[0].tensordicts)):
+                out.append(
+                    torch.cat(
+                        [lazy_td.tensordicts[i] for lazy_td in list_of_tensordicts],
+                        new_dim,
+                    )
+                )
+        return LazyStackedTensorDict(*out, stack_dim=stack_dim)
+    else:
+        if not isinstance(out, LazyStackedTensorDict):
+            return _cat(list_of_tensordicts, dim=dim, out=out)
+
+        if out.batch_size != batch_size:
+            raise RuntimeError(
+                "out.batch_size and cat batch size must match, "
+                f"got out.batch_size={out.batch_size} and batch_size"
+                f"={batch_size}"
+            )
+        if out.stack_dim != dim:
+            index_base = (slice(None),) * out.stack_dim
+            for i, sub_dest in enumerate(out.tensordicts):
+                index = index_base + (i,)
+                tds_to_cat = [_td[index] for _td in list_of_tensordicts]
+                torch.cat(tds_to_cat, dim, out=sub_dest)
+        else:
+            init_idx = 0
+            for td_in in list_of_tensordicts:
+                sub_dest = out.tensordicts[init_idx : init_idx + td_in.shape[dim]]
+                init_idx += init_idx + td_in.shape[dim]
+                torch.stack(sub_dest, out.stack_dim).update(td_in, inplace=True)
+
+        return out
+
+
+@implements_for_td(torch.stack)
+def _stack(
+    list_of_tensordicts: Sequence[TensorDictBase],
+    dim: int = 0,
+    device: DeviceType | None = None,
+    out: TensorDictBase | None = None,
+    strict: bool = False,
+    contiguous: bool = False,
+) -> TensorDictBase:
+    if not list_of_tensordicts:
+        raise RuntimeError("list_of_tensordicts cannot be empty")
+    batch_size = list_of_tensordicts[0].batch_size
+    if dim < 0:
+        dim = len(batch_size) + dim + 1
+
+    for td in list_of_tensordicts[1:]:
+        if td.batch_size != list_of_tensordicts[0].batch_size:
+            raise RuntimeError(
+                "stacking tensordicts requires them to have congruent batch sizes, "
+                f"got td1.batch_size={td.batch_size} and td2.batch_size="
+                f"{list_of_tensordicts[0].batch_size}"
+            )
+
+    # check that all tensordict match
+    keys = _check_keys(list_of_tensordicts)
+
+    if out is None:
+        device = list_of_tensordicts[0].device
+        if contiguous:
+            out = {}
+            for key in keys:
+                with _ErrorInteceptor(
+                    key, "Attempted to stack tensors on different devices at key"
+                ):
+                    out[key] = torch.stack(
+                        [_tensordict.get(key) for _tensordict in list_of_tensordicts],
+                        dim,
+                    )
+
+            return TensorDict(
+                out,
+                batch_size=LazyStackedTensorDict._compute_batch_size(
+                    batch_size, dim, len(list_of_tensordicts)
+                ),
+                device=device,
+                _run_checks=False,
+            )
+        else:
+            out = LazyStackedTensorDict(
+                *list_of_tensordicts,
+                stack_dim=dim,
+            )
+    else:
+        batch_size = list(batch_size)
+        batch_size.insert(dim, len(list_of_tensordicts))
+        batch_size = torch.Size(batch_size)
+
+        if out.batch_size != batch_size:
+            raise RuntimeError(
+                "out.batch_size and stacked batch size must match, "
+                f"got out.batch_size={out.batch_size} and batch_size"
+                f"={batch_size}"
+            )
+
+        out_keys = set(out.keys())
+        if strict:
+            in_keys = set(keys)
+            if len(out_keys - in_keys) > 0:
+                raise RuntimeError(
+                    "The output tensordict has keys that are missing in the "
+                    "tensordict that has to be written: {out_keys - in_keys}. "
+                    "As per the call to `stack(..., strict=True)`, this "
+                    "is not permitted."
+                )
+            elif len(in_keys - out_keys) > 0:
+                raise RuntimeError(
+                    "The resulting tensordict has keys that are missing in "
+                    f"its destination: {in_keys - out_keys}. As per the call "
+                    "to `stack(..., strict=True)`, this is not permitted."
+                )
+
+        try:
+            out._stack_onto_(list_of_tensordicts, dim)
+        except KeyError as err:
+            raise err
+            # TODO fix this
+            # # if key in out_keys:
+            #     with _ErrorInteceptor(
+            #         key, "Attempted to stack tensors on different devices at key"
+            #     ):
+            #         out.set(
+            #             key,
+            #             torch.stack(
+            #                 [
+            #                     _tensordict.get(key)
+            #                     for _tensordict in list_of_tensordicts
+            #                 ],
+            #                 dim,
+            #             ),
+            #             inplace=True,
+            #         )
+
+    return out
+
+
+def pad(
+    tensordict: TensorDictBase, pad_size: Sequence[int], value: float = 0.0
+) -> TensorDictBase:
+    """Pads all tensors in a tensordict along the batch dimensions with a constant value, returning a new tensordict.
+
+    Args:
+         tensordict (TensorDict): The tensordict to pad
+         pad_size (Sequence[int]): The padding size by which to pad some batch
+            dimensions of the tensordict, starting from the first dimension and
+            moving forward. [len(pad_size) / 2] dimensions of the batch size will
+            be padded. For example to pad only the first dimension, pad has the form
+            (padding_left, padding_right). To pad two dimensions,
+            (padding_left, padding_right, padding_top, padding_bottom) and so on.
+            pad_size must be even and less than or equal to twice the number of batch dimensions.
+         value (float, optional): The fill value to pad by, default 0.0
+
+    Returns:
+        A new TensorDict padded along the batch dimensions
+
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> from tensordict.tensordict import pad
+        >>> import torch
+        >>> td = TensorDict({'a': torch.ones(3, 4, 1),
+        ...     'b': torch.ones(3, 4, 1, 1)}, batch_size=[3, 4])
+        >>> dim0_left, dim0_right, dim1_left, dim1_right = [0, 1, 0, 2]
+        >>> padded_td = pad(td, [dim0_left, dim0_right, dim1_left, dim1_right], value=0.0)
+        >>> print(padded_td.batch_size)
+        torch.Size([4, 6])
+        >>> print(padded_td.get("a").shape)
+        torch.Size([4, 6, 1])
+        >>> print(padded_td.get("b").shape)
+        torch.Size([4, 6, 1, 1])
+
+    """
+    if len(pad_size) > 2 * len(tensordict.batch_size):
+        raise RuntimeError(
+            "The length of pad_size must be <= 2 * the number of batch dimensions"
+        )
+
+    if len(pad_size) % 2:
+        raise RuntimeError("pad_size must have an even number of dimensions")
+
+    new_batch_size = list(tensordict.batch_size)
+    for i in range(len(pad_size)):
+        new_batch_size[i // 2] += pad_size[i]
+
+    reverse_pad = pad_size[::-1]
+    for i in range(0, len(reverse_pad), 2):
+        reverse_pad[i], reverse_pad[i + 1] = reverse_pad[i + 1], reverse_pad[i]
+
+    out = TensorDict(
+        {}, torch.Size(new_batch_size), device=tensordict.device, _run_checks=False
+    )
+    for key, tensor in tensordict.items():
+        cur_pad = reverse_pad
+        if len(pad_size) < len(_shape(tensor)) * 2:
+            cur_pad = [0] * (len(_shape(tensor)) * 2 - len(pad_size)) + reverse_pad
+
+        if _is_tensor_collection(tensor.__class__):
+            padded = pad(tensor, pad_size, value)
+        else:
+            padded = torch.nn.functional.pad(tensor, cur_pad, value=value)
+        out.set(key, padded)
+
+    return out
+
+
+def pad_sequence(
+    list_of_tensordicts: Sequence[TensorDictBase],
+    batch_first: bool = True,
+    padding_value: float = 0.0,
+    out: TensorDictBase | None = None,
+    device: DeviceType | None = None,
+    return_mask: bool | None = False,
+) -> TensorDictBase:
+    """Pads a list of tensordicts in order for them to be stacked together in a contiguous format.
+
+    Args:
+        list_of_tensordicts (List[TensorDictBase]): the list of instances to pad and stack.
+        batch_first (bool, optional): the ``batch_first`` correspondant of :func:`torch.nn.utils.rnn.pad_sequence`.
+            Defaults to ``True``.
+        padding_value (number, optional): the padding value. Defaults to ``0.0``.
+        out (TensorDictBase, optional): if provided, the destination where the data will be
+            written.
+        device (device compatible type, optional): if provded, the device where the
+            TensorDict output will be created.
+        return_mask (bool, optional): if ``True``, a "mask" entry will be returned.
+            It contains the mask of valid values in the stacked tensordict.
+
+    Examples:
+        >>> list_td = [
+        ...     TensorDict({"a": torch.zeros((3,))}, []),
+        ...     TensorDict({"a": torch.zeros((4,))}, []),
+        ...     ]
+        >>> padded_td = pad_sequence(list_td)
+        >>> print(padded_td)
+        TensorDict(
+            fields={
+                a: Tensor(shape=torch.Size([2, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+    """
+    if not list_of_tensordicts:
+        raise RuntimeError("list_of_tensordicts cannot be empty")
+    # check that all tensordict match
+    if return_mask:
+        list_of_tensordicts = [
+            td.clone(False).set("mask", torch.ones(td.shape, dtype=torch.bool))
+            for td in list_of_tensordicts
+        ]
+    keys = _check_keys(list_of_tensordicts, leaves_only=True, include_nested=True)
+    shape = max(len(td) for td in list_of_tensordicts)
+    if shape == 0:
+        shape = [
+            len(list_of_tensordicts),
+        ]
+    elif batch_first:
+        shape = [len(list_of_tensordicts), shape]
+    else:
+        shape = [shape, len(list_of_tensordicts)]
+    if out is None:
+        out = TensorDict(
+            {}, batch_size=torch.Size(shape), device=device, _run_checks=False
+        )
+        for key in keys:
+            try:
+                out.set(
+                    key,
+                    torch.nn.utils.rnn.pad_sequence(
+                        [td.get(key) for td in list_of_tensordicts],
+                        batch_first=batch_first,
+                        padding_value=padding_value,
+                    ),
+                )
+            except Exception as err:
+                raise RuntimeError(f"pad_sequence failed for key {key}") from err
+        return out
+    else:
+        for key in keys:
+            out.set_(
+                key,
+                torch.nn.utils.rnn.pad_sequence(
+                    [td.get(key) for td in list_of_tensordicts],
+                    batch_first=batch_first,
+                    padding_value=padding_value,
+                ),
+            )
+        return out
+
+
+@functools.wraps(pad_sequence)
+def pad_sequence_ts(*args, **kwargs):
+    """Warning: this function will soon be deprecated. Please use pad_sequence instead."""
+    warnings.warn(
+        "pad_sequence_ts will soon be deprecated in favour of pad_sequence. Please use the latter instead."
+    )
+    return pad_sequence(*args, **kwargs)
+
+
+@implements_for_td(torch.split)
+def _split(
+    td: TensorDict, split_size_or_sections: int | list[int], dim: int = 0
+) -> list[TensorDictBase]:
+    return td.split(split_size_or_sections, dim)
+
+
+class SubTensorDict(TensorDictBase):
+    """A TensorDict that only sees an index of the stored tensors.
+
+    By default, indexing a tensordict with an iterable will result in a
+    SubTensorDict. This is done such that a TensorDict indexed with
+    non-contiguous index (e.g. a Tensor) will still point to the original
+    memory location (unlike regular indexing of tensors).
+
+    Examples:
+        >>> from tensordict import TensorDict, SubTensorDict
+        >>> source = {'random': torch.randn(3, 4, 5, 6),
+        ...    'zeros': torch.zeros(3, 4, 1, dtype=torch.bool)}
+        >>> batch_size = torch.Size([3, 4])
+        >>> td = TensorDict(source, batch_size)
+        >>> td_index = td[:, 2]
+        >>> print(type(td_index), td_index.shape)
+        <class 'tensordict.tensordict.TensorDict'> \
+torch.Size([3])
+        >>> td_index = td[slice(None), slice(None)]
+        >>> print(type(td_index), td_index.shape)
+        <class 'tensordict.tensordict.TensorDict'> \
+torch.Size([3, 4])
+        >>> td_index = td.get_sub_tensordict((slice(None), torch.tensor([0, 2], dtype=torch.long)))
+        >>> print(type(td_index), td_index.shape)
+        <class 'tensordict.tensordict.SubTensorDict'> \
+torch.Size([3, 2])
+        >>> _ = td_index.fill_('zeros', 1)
+        >>> # the indexed tensors are updated with Trues
+        >>> print(td.get('zeros'))
+        tensor([[[ True],
+                 [False],
+                 [ True],
+                 [False]],
+        <BLANKLINE>
+                [[ True],
+                 [False],
+                 [ True],
+                 [False]],
+        <BLANKLINE>
+                [[ True],
+                 [False],
+                 [ True],
+                 [False]]])
+
+    """
+
+    def __new__(cls, *args: Any, **kwargs: Any) -> SubTensorDict:
+        cls._is_shared = False
+        cls._is_memmap = False
+        return super().__new__(cls, _safe=False, _lazy=True, _inplace_set=True)
+
+    def __init__(
+        self,
+        source: TensorDictBase,
+        idx: IndexType,
+        batch_size: Sequence[int] | None = None,
+    ) -> None:
+        if not _is_tensor_collection(source.__class__):
+            raise TypeError(
+                f"Expected source to be a subclass of TensorDictBase, "
+                f"got {type(source)}"
+            )
+        self._source = source
+        idx = (
+            (idx,)
+            if not isinstance(
+                idx,
+                (
+                    tuple,
+                    list,
+                ),
+            )
+            else tuple(idx)
+        )
+        if any(item is Ellipsis for item in idx):
+            idx = convert_ellipsis_to_idx(idx, self._source.batch_size)
+        self._batch_size = _getitem_batch_size(self._source.batch_size, idx)
+        self.idx = idx
+
+        if batch_size is not None and batch_size != self.batch_size:
+            raise RuntimeError("batch_size does not match self.batch_size.")
+
+    # def __hash__(self):
+    #     return hash((self._source, self.idx))
+
+    @staticmethod
+    def _convert_ellipsis(idx, shape):
+        if any(_idx is Ellipsis for _idx in idx):
+            new_idx = []
+            cursor = -1
+            for _idx in idx:
+                if _idx is Ellipsis:
+                    if cursor == len(idx) - 1:
+                        # then we can just skip
+                        continue
+                    n_upcoming = len(idx) - cursor - 1
+                    while cursor < len(shape) - n_upcoming:
+                        cursor += 1
+                        new_idx.append(slice(None))
+                else:
+                    new_idx.append(_idx)
+            return tuple(new_idx)
+        return idx
+
+    def exclude(self, *keys: str, inplace: bool = False) -> TensorDictBase:
+        if inplace:
+            return super().exclude(*keys, inplace=True)
+        return self.to_tensordict().exclude(*keys, inplace=True)
+
+    @property
+    def batch_size(self) -> torch.Size:
+        return self._batch_size
+
+    @batch_size.setter
+    def batch_size(self, new_size: torch.Size) -> None:
+        self._batch_size_setter(new_size)
+
+    @property
+    def names(self):
+        names = self._source._get_names_idx(self.idx)
+        if names is None:
+            return [None] * self.batch_dims
+        return names
+
+    @names.setter
+    def names(self, value):
+        raise RuntimeError(
+            "Names of a subtensordict cannot be modified. Instantiate the tensordict first."
+        )
+
+    def _has_names(self):
+        return self._source._has_names()
+
+    def _erase_names(self):
+        raise RuntimeError(
+            "Cannot erase names of a SubTensorDict. Erase source TensorDict's names instead."
+        )
+
+    def _rename_subtds(self, names):
+        for key in self.keys():
+            if _is_tensor_collection(self.entry_class(key)):
+                raise RuntimeError("Cannot rename nested sub-tensordict dimensions.")
+
+    @property
+    def device(self) -> None | torch.device:
+        return self._source.device
+
+    @device.setter
+    def device(self, value: DeviceType) -> None:
+        self._source.device = value
+
+    def _preallocate(self, key: str, value: CompatibleType) -> TensorDictBase:
+        return self._source.set(key, value)
+
+    def _convert_inplace(self, inplace, key):
+        has_key = key in self.keys()
+        if inplace is not False:
+            if inplace is True and not has_key:  # inplace could be None
+                raise KeyError(
+                    TensorDictBase.KEY_ERROR.format(
+                        key, self.__class__.__name__, sorted(self.keys())
+                    )
+                )
+            inplace = has_key
+        if not inplace and has_key:
+            raise RuntimeError(
+                "Calling `SubTensorDict.set(key, value, inplace=False)` is "
+                "prohibited for existing tensors. Consider calling "
+                "SubTensorDict.set_(...) or cloning your tensordict first."
+            )
+        elif not inplace and self.is_locked:
+            raise RuntimeError(TensorDictBase.LOCK_ERROR)
+        return inplace
+
+    def _set_str(
+        self,
+        key: NestedKey,
+        value: dict[str, CompatibleType] | CompatibleType,
+        *,
+        inplace: bool,
+        validated: bool,
+    ) -> TensorDictBase:
+        inplace = self._convert_inplace(inplace, key)
+        # it is assumed that if inplace=False then the key doesn't exist. This is
+        # checked in set method, but not here. responsibility lies with the caller
+        # so that this method can have minimal overhead from runtime checks
+        parent = self._source
+        if not validated:
+            value = self._validate_value(value, check_shape=True)
+            validated = True
+        if not inplace:
+            if _is_tensor_collection(value.__class__):
+                value_expand = _expand_to_match_shape(
+                    parent.batch_size, value, self.batch_dims, self.device
+                )
+                for _key, _tensor in value.items():
+                    value_expand[_key] = _expand_to_match_shape(
+                        parent.batch_size, _tensor, self.batch_dims, self.device
+                    )
+            else:
+                value_expand = torch.zeros(
+                    (
+                        *parent.batch_size,
+                        *_shape(value)[self.batch_dims :],
+                    ),
+                    dtype=value.dtype,
+                    device=self.device,
+                )
+                if self.is_shared() and self.device.type == "cpu":
+                    value_expand.share_memory_()
+                elif self.is_memmap():
+                    value_expand = MemmapTensor.from_tensor(value_expand)
+
+            parent._set_str(key, value_expand, inplace=False, validated=validated)
+
+        parent._set_at_str(key, value, self.idx, validated=validated)
+        return self
+
+    def _set_tuple(
+        self,
+        key: NestedKey,
+        value: dict[str, CompatibleType] | CompatibleType,
+        *,
+        inplace: bool,
+        validated: bool,
+    ) -> TensorDictBase:
+        if len(key) == 1:
+            return self._set_str(key[0], value, inplace=inplace, validated=validated)
+        parent = self._source
+        td = parent._get_str(key[0], None)
+        if td is None:
+            td = parent.select()
+            parent._set_str(key[0], td, inplace=False, validated=True)
+        SubTensorDict(td, self.idx)._set_tuple(
+            key[1:], value, inplace=inplace, validated=validated
+        )
+        return self
+
+    def _set_at_str(self, key, value, idx, *, validated):
+        tensor_in = self._get_str(key, NO_DEFAULT)
+        if not validated:
+            value = self._validate_value(value, check_shape=False)
+            validated = True
+        if isinstance(idx, tuple) and len(idx) and isinstance(idx[0], tuple):
+            warn(
+                "Multiple indexing can lead to unexpected behaviours when "
+                "setting items, for instance `td[idx1][idx2] = other` may "
+                "not write to the desired location if idx1 is a list/tensor."
+            )
+            tensor_in = _sub_index(tensor_in, idx)
+            tensor_in.copy_(value)
+        else:
+            _set_item(tensor_in, idx, value, validated=validated)
+        # make sure that the value is updated
+        self._source._set_at_str(key, tensor_in, self.idx, validated=validated)
+        return self
+
+    def _set_at_tuple(self, key, value, idx, *, validated):
+        if len(key) == 1:
+            return self._set_at_str(key[0], value, idx, validated=validated)
+        if key[0] not in self.keys():
+            # this won't work
+            raise KeyError(f"key {key} not found in set_at_ with tensordict {self}.")
+        else:
+            td = self._get_str(key[0], NO_DEFAULT)
+        td._set_at_tuple(key[1:], value, idx, validated=validated)
+        return self
+
+    # @cache  # noqa: B019
+    def keys(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> _TensorDictKeysView:
+        return self._source.keys(include_nested=include_nested, leaves_only=leaves_only)
+
+    def entry_class(self, key: NestedKey) -> type:
+        source_type = type(self._source.get(key))
+        if _is_tensor_collection(source_type):
+            return self.__class__
+        return source_type
+
+    def _stack_onto_(self, list_item: list[CompatibleType], dim: int) -> SubTensorDict:
+        self._source._stack_onto_at_(list_item, dim=dim, idx=self.idx)
+        return self
+
+    def to(self, dest: DeviceType | torch.Size | type, **kwargs: Any) -> TensorDictBase:
+        if isinstance(dest, type) and issubclass(dest, TensorDictBase):
+            if isinstance(self, dest):
+                return self
+            out = dest(
+                source=self.clone(),
+            )
+            if self._has_names():
+                out.names = self.names
+            return out
+        elif isinstance(dest, (torch.device, str, int)):
+            dest = torch.device(dest)
+            # try:
+            if self.device is not None and dest == self.device:
+                return self
+            td = self.to_tensordict().to(dest, **kwargs)
+            # must be device
+            return td
+
+        elif isinstance(dest, torch.Size):
+            self.batch_size = dest
+            return self
+        elif dest is None:
+            return self
+        else:
+            raise NotImplementedError(
+                f"dest must be a string, torch.device or a TensorDict "
+                f"instance, {dest} not allowed"
+            )
+
+    def _change_batch_size(self, new_size: torch.Size) -> None:
+        if not hasattr(self, "_orig_batch_size"):
+            self._orig_batch_size = self.batch_size
+        elif self._orig_batch_size == new_size:
+            del self._orig_batch_size
+        self._batch_size = new_size
+
+    def get(
+        self,
+        key: NestedKey,
+        default: Tensor | str | None = NO_DEFAULT,
+    ) -> CompatibleType:
+        return self._source.get_at(key, self.idx, default=default)
+
+    def _get_str(self, key, default):
+        if key in self.keys() and _is_tensor_collection(self.entry_class(key)):
+            return SubTensorDict(self._source._get_str(key, NO_DEFAULT), self.idx)
+        return self._source._get_at_str(key, self.idx, default=default)
+
+    def _get_tuple(self, key, default):
+        return self._source._get_at_tuple(key, self.idx, default=default)
+
+    def update(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        clone: bool = False,
+        inplace: bool = False,
+        **kwargs,
+    ) -> SubTensorDict:
+        if input_dict_or_td is self:
+            # no op
+            return self
+        keys = set(self.keys(False))
+        for key, value in input_dict_or_td.items():
+            if clone and hasattr(value, "clone"):
+                value = value.clone()
+            else:
+                value = tree_map(torch.clone, value)
+            key = _unravel_key_to_tuple(key)
+            firstkey, subkey = key[0], key[1:]
+            # the key must be a string by now. Let's check if it is present
+            if firstkey in keys:
+                target_class = self.entry_class(firstkey)
+                if _is_tensor_collection(target_class):
+                    target = self._source.get(firstkey).get_sub_tensordict(self.idx)
+                    if len(subkey):
+                        target._set_tuple(subkey, value, inplace=False, validated=False)
+                        continue
+                    elif isinstance(value, dict) or _is_tensor_collection(
+                        value.__class__
+                    ):
+                        target.update(value)
+                        continue
+                    raise ValueError(
+                        f"Tried to replace a tensordict with an incompatible object of type {type(value)}"
+                    )
+                else:
+                    self._set_tuple(key, value, inplace=True, validated=False)
+            else:
+                self._set_tuple(
+                    key,
+                    value,
+                    inplace=BEST_ATTEMPT_INPLACE if inplace else False,
+                    validated=False,
+                )
+        return self
+
+    def update_(
+        self,
+        input_dict: dict[str, CompatibleType] | TensorDictBase,
+        clone: bool = False,
+    ) -> SubTensorDict:
+        return self.update_at_(
+            input_dict, idx=self.idx, discard_idx_attr=True, clone=clone
+        )
+
+    def update_at_(
+        self,
+        input_dict: dict[str, CompatibleType] | TensorDictBase,
+        idx: IndexType,
+        discard_idx_attr: bool = False,
+        clone: bool = False,
+    ) -> SubTensorDict:
+        for key, value in input_dict.items():
+            if not isinstance(value, tuple(_ACCEPTED_CLASSES)):
+                raise TypeError(
+                    f"Expected value to be one of types {_ACCEPTED_CLASSES} "
+                    f"but got {type(value)}"
+                )
+            if clone:
+                value = value.clone()
+            key = _unravel_key_to_tuple(key)
+            if discard_idx_attr:
+                self._source._set_at_tuple(
+                    key,
+                    value,
+                    idx,
+                    validated=False,
+                )
+            else:
+                self._set_at_tuple(key, value, idx, validated=False)
+        return self
+
+    def get_parent_tensordict(self) -> TensorDictBase:
+        if not isinstance(self._source, TensorDictBase):
+            raise TypeError(
+                f"SubTensorDict was initialized with a source of type"
+                f" {self._source.__class__.__name__}, "
+                "parent tensordict not accessible"
+            )
+        return self._source
+
+    @lock_blocked
+    def del_(self, key: NestedKey) -> TensorDictBase:
+        self._source = self._source.del_(key)
+        return self
+
+    def clone(self, recurse: bool = True) -> SubTensorDict:
+        """Clones the SubTensorDict.
+
+        Args:
+            recurse (bool, optional): if ``True`` (default), a regular
+                :class:`TensorDict` instance will be created from the :class:`SubTensorDict`.
+                Otherwise, another :class:`SubTensorDict` with identical content
+                will be returned.
+
+        Examples:
+            >>> data = TensorDict({"a": torch.arange(4).reshape(2, 2,)}, batch_size=[2, 2])
+            >>> sub_data = data.get_sub_tensordict([0,])
+            >>> print(sub_data)
+            SubTensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
+                batch_size=torch.Size([2]),
+                device=None,
+                is_shared=False)
+            >>> # the data of both subtensordict is the same
+            >>> print(data.get("a").data_ptr(), sub_data.get("a").data_ptr())
+            140183705558208 140183705558208
+            >>> sub_data_clone = sub_data.clone(recurse=True)
+            >>> print(sub_data_clone)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
+                batch_size=torch.Size([2]),
+                device=None,
+                is_shared=False)
+            >>. print(sub_data.get("a").data_ptr())
+            140183705558208
+            >>> sub_data_clone = sub_data.clone(recurse=False)
+            >>> print(sub_data_clone)
+            SubTensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
+                batch_size=torch.Size([2]),
+                device=None,
+                is_shared=False)
+            >>> print(sub_data.get("a").data_ptr())
+            140183705558208
+        """
+        if not recurse:
+            return SubTensorDict(source=self._source.clone(recurse=False), idx=self.idx)
+        return self.to_tensordict()
+
+    def is_contiguous(self) -> bool:
+        return all(value.is_contiguous() for value in self.values())
+
+    def contiguous(self) -> TensorDictBase:
+        if self.is_contiguous():
+            return self
+        return TensorDict(
+            batch_size=self.batch_size,
+            source={key: value for key, value in self.items()},
+            device=self.device,
+            names=self.names,
+            _run_checks=False,
+        )
+
+    def select(
+        self, *keys: str, inplace: bool = False, strict: bool = True
+    ) -> TensorDictBase:
+        if inplace:
+            self._source = self._source.select(*keys, strict=strict)
+            return self
+        return self._source.select(*keys, strict=strict)[self.idx]
+
+    def expand(self, *shape: int, inplace: bool = False) -> TensorDictBase:
+        if len(shape) == 1 and isinstance(shape[0], Sequence):
+            shape = tuple(shape[0])
+        return self.apply(
+            lambda x: x.expand((*shape, *x.shape[self.ndim :])), batch_size=shape
+        )
+
+    def is_shared(self) -> bool:
+        return self._source.is_shared()
+
+    def is_memmap(self) -> bool:
+        return self._source.is_memmap()
+
+    def rename_key_(
+        self, old_key: str, new_key: str, safe: bool = False
+    ) -> SubTensorDict:
+        self._source.rename_key_(old_key, new_key, safe=safe)
+        return self
+
+    rename_key = _renamed_inplace_method(rename_key_)
+
+    def pin_memory(self) -> TensorDictBase:
+        self._source.pin_memory()
+        return self
+
+    def detach_(self) -> TensorDictBase:
+        raise RuntimeError("Detaching a sub-tensordict in-place cannot be done.")
+
+    def where(self, condition, other, *, out=None):
+        return self.to_tensordict().where(condition=condition, other=other, out=out)
+
+    def masked_fill_(self, mask: Tensor, value: float | bool) -> TensorDictBase:
+        for key, item in self.items():
+            self.set_(key, torch.full_like(item, value))
+        return self
+
+    def masked_fill(self, mask: Tensor, value: float | bool) -> TensorDictBase:
+        td_copy = self.clone()
+        return td_copy.masked_fill_(mask, value)
+
+    def memmap_(
+        self, prefix: str | None = None, copy_existing: bool = False
+    ) -> TensorDictBase:
+        raise RuntimeError(
+            "Converting a sub-tensordict values to memmap cannot be done."
+        )
+
+    def share_memory_(self) -> TensorDictBase:
+        raise RuntimeError(
+            "Casting a sub-tensordict values to shared memory cannot be done."
+        )
+
+    @property
+    def is_locked(self) -> bool:
+        return self._source.is_locked
+
+    @is_locked.setter
+    def is_locked(self, value) -> bool:
+        if value:
+            self.lock_()
+        else:
+            self.unlock_()
+
+    @as_decorator("is_locked")
+    def lock_(self) -> TensorDictBase:
+        # we can't lock sub-tensordicts because that would mean that the
+        # parent tensordict cannot be modified either.
+        if not self.is_locked:
+            raise RuntimeError(
+                "Cannot lock a SubTensorDict. Lock the parent tensordict instead."
+            )
+        return self
+
+    @as_decorator("is_locked")
+    def unlock_(self) -> TensorDictBase:
+        if self.is_locked:
+            raise RuntimeError(
+                "Cannot unlock a SubTensorDict. Unlock the parent tensordict instead."
+            )
+        return self
+
+    def _remove_lock(self, lock_id):
+        raise RuntimeError(
+            "Cannot unlock a SubTensorDict. Unlock the parent tensordict instead."
+        )
+
+    def _lock_propagate(self, lock_ids=None):
+        raise RuntimeError(
+            "Cannot lock a SubTensorDict. Lock the parent tensordict instead."
+        )
+
+    lock = _renamed_inplace_method(lock_)
+    unlock = _renamed_inplace_method(unlock_)
+
+    def __del__(self):
+        pass
+
+
+def merge_tensordicts(*tensordicts: TensorDictBase) -> TensorDictBase:
+    """Merges tensordicts together."""
+    if len(tensordicts) < 2:
+        raise RuntimeError(
+            f"at least 2 tensordicts must be provided, got" f" {len(tensordicts)}"
+        )
+    d = tensordicts[0].to_dict()
+    batch_size = tensordicts[0].batch_size
+    for td in tensordicts[1:]:
+        d.update(td.to_dict())
+        if td.batch_dims < len(batch_size):
+            batch_size = td.batch_size
+    return TensorDict(d, batch_size, device=td.device, _run_checks=False)
+
+
+class _LazyStackedTensorDictKeysView(_TensorDictKeysView):
+    tensordict: LazyStackedTensorDict
+
+    def __len__(self) -> int:
+        return len(self._keys())
+
+    def _keys(self) -> list[str]:
+        return self.tensordict._key_list()
+
+    def __contains__(self, item):
+        item = _unravel_key_to_tuple(item)
+        if item[0] in self.tensordict._iterate_over_keys():
+            if self.leaves_only:
+                return not _is_tensor_collection(self.tensordict.entry_class(item[0]))
+            has_first_key = True
+        else:
+            has_first_key = False
+        if not has_first_key or len(item) == 1:
+            return has_first_key
+        # otherwise take the long way
+        return all(
+            item[1:]
+            in tensordict.get(item[0]).keys(self.include_nested, self.leaves_only)
+            for tensordict in self.tensordict.tensordicts
+        )
+
+
+class LazyStackedTensorDict(TensorDictBase):
+    """A Lazy stack of TensorDicts.
+
+    When stacking TensorDicts together, the default behaviour is to put them
+    in a stack that is not instantiated.
+    This allows to seamlessly work with stacks of tensordicts with operations
+    that will affect the original tensordicts.
+
+    Args:
+         *tensordicts (TensorDict instances): a list of tensordict with
+            same batch size.
+         stack_dim (int): a dimension (between `-td.ndimension()` and
+            `td.ndimension()-1` along which the stack should be performed.
+         hook_out (callable, optional): a callable to execute after :meth:`~.get`.
+         hook_in (callable, optional): a callable to execute before :meth:`~.set`.
+
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> import torch
+        >>> tds = [TensorDict({'a': torch.randn(3, 4)}, batch_size=[3])
+        ...     for _ in range(10)]
+        >>> td_stack = torch.stack(tds, -1)
+        >>> print(td_stack.shape)
+        torch.Size([3, 10])
+        >>> print(td_stack.get("a").shape)
+        torch.Size([3, 10, 4])
+        >>> print(td_stack[:, 0] is tds[0])
+        True
+
+    """
+
+    @classmethod
+    def __torch_function__(
+        cls,
+        func: Callable,
+        types: tuple[type, ...],
+        args: tuple[Any, ...] = (),
+        kwargs: dict[str, Any] | None = None,
+    ) -> Callable:
+        if func in LAZY_TD_HANDLED_FUNCTIONS:
+            if kwargs is None:
+                kwargs = {}
+            if func not in LAZY_TD_HANDLED_FUNCTIONS or not all(
+                issubclass(t, (Tensor, TensorDictBase)) for t in types
+            ):
+                return NotImplemented
+            return LAZY_TD_HANDLED_FUNCTIONS[func](*args, **kwargs)
+        else:
+            return super().__torch_function__(func, types, args, kwargs)
+
+    def __new__(cls, *args: Any, **kwargs: Any) -> LazyStackedTensorDict:
+        cls._td_dim_name = None
+        return super().__new__(cls, *args, _safe=False, _lazy=True, **kwargs)
+
+    def __init__(
+        self,
+        *tensordicts: TensorDictBase,
+        stack_dim: int = 0,
+        hook_out: callable | None = None,
+        hook_in: callable | None = None,
+        batch_size: Sequence[int] | None = None,  # TODO: remove
+    ) -> None:
+        self._is_shared = False
+        self._is_memmap = False
+        self._is_locked = None
+
+        # sanity check
+        N = len(tensordicts)
+        if not N:
+            raise RuntimeError(
+                "at least one tensordict must be provided to "
+                "StackedTensorDict to be instantiated"
+            )
+        if not isinstance(tensordicts[0], TensorDictBase):
+            raise TypeError(
+                f"Expected input to be TensorDictBase instance"
+                f" but got {type(tensordicts[0])} instead."
+            )
+        if stack_dim < 0:
+            raise RuntimeError(
+                f"stack_dim must be non negative, got stack_dim={stack_dim}"
+            )
+        _batch_size = tensordicts[0].batch_size
+        device = tensordicts[0].device
+
+        for td in tensordicts[1:]:
+            if not isinstance(td, TensorDictBase):
+                raise TypeError(
+                    "Expected all inputs to be TensorDictBase instances but got "
+                    f"{type(td)} instead."
+                )
+            _bs = td.batch_size
+            _device = td.device
+            if device != _device:
+                raise RuntimeError(f"devices differ, got {device} and {_device}")
+            if _bs != _batch_size:
+                raise RuntimeError(
+                    f"batch sizes in tensordicts differs, StackedTensorDict "
+                    f"cannot be created. Got td[0].batch_size={_batch_size} "
+                    f"and td[i].batch_size={_bs} "
+                )
+        self.tensordicts: list[TensorDictBase] = list(tensordicts)
+        self.stack_dim = stack_dim
+        self._batch_size = self._compute_batch_size(_batch_size, stack_dim, N)
+        self.hook_out = hook_out
+        self.hook_in = hook_in
+        if batch_size is not None and batch_size != self.batch_size:
+            raise RuntimeError("batch_size does not match self.batch_size.")
+
+    @property
+    def device(self) -> torch.device | None:
+        # devices might have changed, so we check that they're all the same
+        device_set = {td.device for td in self.tensordicts}
+        if len(device_set) != 1:
+            raise RuntimeError(
+                f"found multiple devices in {self.__class__.__name__}:" f" {device_set}"
+            )
+        device = self.tensordicts[0].device
+        return device
+
+    @device.setter
+    def device(self, value: DeviceType) -> None:
+        for t in self.tensordicts:
+            t.device = value
+
+    @property
+    def batch_size(self) -> torch.Size:
+        return self._batch_size
+
+    @batch_size.setter
+    def batch_size(self, new_size: torch.Size) -> None:
+        return self._batch_size_setter(new_size)
+
+    @property
+    @cache  # noqa
+    def names(self):
+        names = list(self.tensordicts[0].names)
+        for td in self.tensordicts[1:]:
+            if names != td.names:
+                raise ValueError(
+                    f"Not all dim names match, got {names} and {td.names}."
+                )
+        names.insert(self.stack_dim, self._td_dim_name)
+        return names
+
+    @names.setter
+    @erase_cache  # a nested lazy stacked tensordict is not apparent to the root
+    def names(self, value):
+        if value is None:
+            for td in self.tensordicts:
+                td.names = None
+            self._td_dim_name = None
+        else:
+            names_c = list(value)
+            name = names_c[self.stack_dim]
+            self._td_dim_name = name
+            del names_c[self.stack_dim]
+            for td in self.tensordicts:
+                if td._check_dim_name(name):
+                    # TODO: should reset names here
+                    raise ValueError(f"The dimension name {name} is already taken.")
+                td.rename_(*names_c)
+
+    def _rename_subtds(self, names):
+        # remove the name of the stack dim
+        names = list(names)
+        del names[self.stack_dim]
+        for td in self.tensordicts:
+            td.names = names
+
+    def _has_names(self):
+        return all(td._has_names() for td in self.tensordicts)
+
+    def _erase_names(self):
+        self._td_dim_name = None
+        for td in self.tensordicts:
+            td._erase_names()
+
+    def get_item_shape(self, key):
+        """Gets the shape of an item in the lazy stack.
+
+        Heterogeneous dimensions are returned as -1.
+
+        This implementation is inefficient as it will attempt to stack the items
+        to compute their shape, and should only be used for printing.
+        """
+        try:
+            item = self.get(key)
+            return item.shape
+        except RuntimeError as err:
+            if re.match(r"Found more than one unique shape in the tensors", str(err)):
+                shape = None
+                for td in self.tensordicts:
+                    if shape is None:
+                        shape = list(td.get_item_shape(key))
+                    else:
+                        _shape = td.get_item_shape(key)
+                        if len(shape) != len(_shape):
+                            shape = [-1]
+                            return torch.Size(shape)
+                        shape = [
+                            s1 if s1 == s2 else -1 for (s1, s2) in zip(shape, _shape)
+                        ]
+                shape.insert(self.stack_dim, len(self.tensordicts))
+                return torch.Size(shape)
+            else:
+                raise err
+
+    def is_shared(self) -> bool:
+        are_shared = [td.is_shared() for td in self.tensordicts]
+        are_shared = [value for value in are_shared if value is not None]
+        if not len(are_shared):
+            return None
+        if any(are_shared) and not all(are_shared):
+            raise RuntimeError(
+                f"tensordicts shared status mismatch, got {sum(are_shared)} "
+                f"shared tensordicts and "
+                f"{len(are_shared) - sum(are_shared)} non shared tensordict "
+            )
+        return all(are_shared)
+
+    def is_memmap(self) -> bool:
+        are_memmap = [td.is_memmap() for td in self.tensordicts]
+        if any(are_memmap) and not all(are_memmap):
+            raise RuntimeError(
+                f"tensordicts memmap status mismatch, got {sum(are_memmap)} "
+                f"memmap tensordicts and "
+                f"{len(are_memmap) - sum(are_memmap)} non memmap tensordict "
+            )
+        return all(are_memmap)
+
+    @staticmethod
+    def _compute_batch_size(
+        batch_size: torch.Size, stack_dim: int, N: int
+    ) -> torch.Size:
+        s = list(batch_size)
+        s.insert(stack_dim, N)
+        return torch.Size(s)
+
+    def _set_str(
+        self,
+        key: NestedKey,
+        value: dict[str, CompatibleType] | CompatibleType,
+        *,
+        inplace: bool,
+        validated: bool,
+    ) -> TensorDictBase:
+        try:
+            inplace = self._convert_inplace(inplace, key)
+        except KeyError as e:
+            raise KeyError(
+                "setting a value in-place on a stack of TensorDict is only "
+                "permitted if all members of the stack have this key in "
+                "their register."
+            ) from e
+        if not validated:
+            value = self._validate_value(value)
+            validated = True
+        if self.hook_in is not None:
+            value = self.hook_in(value)
+        values = value.unbind(self.stack_dim)
+        for tensordict, item in zip(self.tensordicts, values):
+            tensordict._set_str(key, item, inplace=inplace, validated=validated)
+        return self
+
+    def _set_tuple(
+        self,
+        key: NestedKey,
+        value: dict[str, CompatibleType] | CompatibleType,
+        *,
+        inplace: bool,
+        validated: bool,
+    ) -> TensorDictBase:
+        if len(key) == 1:
+            return self._set_str(key[0], value, inplace=inplace, validated=validated)
+        # if inplace is not False:  # inplace could be None
+        #     # we don't want to end up in the situation where one tensordict has
+        #     # inplace=True and another one inplace=False because inplace was loose.
+        #     # Worse could be writing with inplace=True up until some level then to
+        #     # realize the key is missing in one td, raising an exception and having
+        #     # messed up the data. Hence we must start by checking if the key
+        #     # is present.
+        #     has_key = key in self.keys(True)
+        #     if inplace is True and not has_key:  # inplace could be None
+        #         raise KeyError(
+        #             TensorDictBase.KEY_ERROR.format(
+        #                 key, self.__class__.__name__, sorted(self.keys())
+        #             )
+        #         )
+        #     inplace = has_key
+        if not validated:
+            value = self._validate_value(value)
+            validated = True
+        if self.hook_in is not None:
+            value = self.hook_in(value)
+        values = value.unbind(self.stack_dim)
+        for tensordict, item in zip(self.tensordicts, values):
+            tensordict._set_tuple(key, item, inplace=inplace, validated=validated)
+        return self
+
+    def _split_index(self, index):
+        """Given a tuple index, split it in as many indices as the number of tensordicts.
+
+        Returns:
+            a dictionary with {index-of-td: index-within-td}
+            the number of single dim indices until stack dim
+            a boolean indicating if the index along the stack dim is an integer
+        """
+        if not isinstance(index, tuple):
+            index = (index,)
+        index = convert_ellipsis_to_idx(index, self.batch_size)
+        index = _broadcast_tensors(index)
+        out = []
+        num_single = 0
+        num_none = 0
+        isinteger = False
+        is_nd_tensor = False
+        cursor = 0  # the dimension cursor
+        selected_td_idx = range(len(self.tensordicts))
+        has_bool = False
+        num_squash = 0
+        for i, idx in enumerate(index):  # noqa: B007
+            cursor_incr = 1
+            if idx is None:
+                out.append(None)
+                num_none += cursor <= self.stack_dim
+                continue
+            if cursor == self.stack_dim:
+                # we need to check which tds need to be indexed
+                if isinstance(idx, slice) or _is_number(idx):
+                    selected_td_idx = range(len(self.tensordicts))[idx]
+                    if not isinstance(selected_td_idx, range):
+                        isinteger = True
+                        selected_td_idx = [selected_td_idx]
+                elif isinstance(idx, (list, range)):
+                    selected_td_idx = idx
+                elif isinstance(idx, (torch.Tensor, np.ndarray)):
+                    if idx.dtype in (np.dtype("bool"), torch.bool):
+                        # we mark that we need to dispatch the indices across stack idx
+                        has_bool = True
+                        # split mask along dim
+                        individual_masks = idx = idx.unbind(0)
+                        selected_td_idx = range(len(self.tensordicts))
+                        out.append(idx)
+                        split_dim = self.stack_dim - num_single
+                        mask_loc = i
+                    else:
+                        if isinstance(idx, np.ndarray):
+                            idx = torch.tensor(idx)
+                        is_nd_tensor = True
+                        selected_td_idx = range(len(idx))
+                        out.append(idx.unbind(0))
+                else:
+                    raise TypeError(f"Invalid index type: {type(idx)}.")
+            else:
+                if _is_number(idx) and cursor < self.stack_dim:
+                    num_single += 1
+                if isinstance(
+                    idx,
+                    (
+                        int,
+                        slice,
+                        list,
+                        range,
+                    ),
+                ):
+                    out.append(idx)
+                elif isinstance(idx, (np.ndarray, torch.Tensor)):
+                    if idx.dtype in (np.dtype("bool"), torch.bool):
+                        cursor_incr = idx.ndim
+                        if cursor < self.stack_dim:
+                            num_squash += cursor_incr - 1
+                        if (
+                            cursor < self.stack_dim
+                            and cursor + cursor_incr > self.stack_dim
+                        ):
+                            # we mark that we need to dispatch the indices across stack idx
+                            has_bool = True
+                            # split mask along dim
+                            # relative_stack_dim = self.stack_dim - cursor - cursor_incr
+                            individual_masks = idx = idx.unbind(0)
+                            selected_td_idx = range(self.shape[i])
+                            split_dim = cursor - num_single
+                            mask_loc = i
+                    out.append(idx)
+                else:
+                    raise TypeError(f"Invalid index type: {type(idx)}.")
+            cursor += cursor_incr
+        if has_bool:
+            out = tuple(
+                tuple(idx if not isinstance(idx, tuple) else idx[i] for idx in out)
+                for i in selected_td_idx
+            )
+            return {
+                "index_dict": {i: out[i] for i in selected_td_idx},
+                "num_single": num_single,
+                "isinteger": isinteger,
+                "has_bool": has_bool,
+                "individual_masks": individual_masks,
+                "split_dim": split_dim,
+                "mask_loc": mask_loc,
+                "is_nd_tensor": is_nd_tensor,
+                "num_none": num_none,
+                "num_squash": num_squash,
+            }
+        elif is_nd_tensor:
+
+            def isindexable(idx):
+                if isinstance(idx, (torch.Tensor, np.ndarray)):
+                    if idx.dtype in (torch.bool, np.dtype("bool")):
+                        return False
+                    return True
+                if isinstance(idx, (tuple, list, range)):
+                    return True
+                return False
+
+            out = tuple(
+                tuple(idx if not isindexable(idx) else idx[i] for idx in out)
+                for i in selected_td_idx
+            )
+            return {
+                "index_dict": dict(enumerate(out)),
+                "num_single": num_single,
+                "isinteger": isinteger,
+                "has_bool": has_bool,
+                "is_nd_tensor": is_nd_tensor,
+                "num_none": num_none,
+                "num_squash": num_squash,
+            }
+        return {
+            "index_dict": {i: tuple(out) for i in selected_td_idx},
+            "num_single": num_single,
+            "isinteger": isinteger,
+            "has_bool": has_bool,
+            "is_nd_tensor": is_nd_tensor,
+            "num_none": num_none,
+            "num_squash": num_squash,
+        }
+
+    def _set_at_str(self, key, value, index, *, validated):
+        if not validated:
+            value = self._validate_value(value, check_shape=False)
+            validated = True
+        if self.hook_in is not None:
+            value = self.hook_in(value)
+        split_index = self._split_index(index)
+        converted_idx = split_index["index_dict"]
+        num_single = split_index["num_single"]
+        isinteger = split_index["isinteger"]
+        has_bool = split_index["has_bool"]
+        num_squash = split_index.get("num_squash", 0)
+        num_none = split_index.get("num_none", 0)
+        is_nd_tensor = split_index.get("is_nd_tensor", False)
+        if isinteger:
+            # this will break if the index along the stack dim is [0] or :1 or smth
+            for i, _idx in converted_idx.items():
+                self.tensordicts[i]._set_at_str(key, value, _idx, validated=validated)
+            return self
+        if is_nd_tensor:
+            unbind_dim = self.stack_dim - num_single + num_none - num_squash
+            value_unbind = value.unbind(unbind_dim)
+            for idx, _value in zip(converted_idx.values(), value_unbind):
+                self._set_at_str(key, _value, idx, validated=validated)
+            return self
+        elif not has_bool:
+            unbind_dim = self.stack_dim - num_single + num_none - num_squash
+            value_unbind = value.unbind(unbind_dim)
+            for (i, _idx), _value in zip(
+                converted_idx.items(),
+                value_unbind,
+            ):
+                self.tensordicts[i]._set_at_str(key, _value, _idx, validated=validated)
+        else:
+            # we must split, not unbind
+            mask_unbind = split_index["individual_masks"]
+            split_dim = split_index["split_dim"]
+            splits = [_mask_unbind.sum().item() for _mask_unbind in mask_unbind]
+            value_unbind = value.split(splits, split_dim)
+            if mask_unbind[0].ndim == 0:
+                # we can return a stack
+                for (i, _idx), mask, _value in zip(
+                    converted_idx.items(),
+                    mask_unbind,
+                    value_unbind,
+                ):
+                    if mask.any():
+                        self.tensordicts[i]._set_at_str(
+                            key, _value, _idx, validated=validated
+                        )
+            else:
+                for (i, _idx), _value in zip(converted_idx.items(), value_unbind):
+                    self_idx = (slice(None),) * split_index["mask_loc"] + (i,)
+                    self[self_idx]._set_at_str(key, _value, _idx, validated=validated)
+
+        # # it may be the case that we can't get the value
+        # # because it can't be stacked.
+        # # self[index]._set_str(key, value, validated=validated, inplace=True)
+        # # return self
+        # split_index = self._split_index(index)
+        # converted_idx = split_index["index_dict"]
+        # num_single = split_index["num_single"]
+        # isinteger = split_index["isinteger"]
+        # if isinteger:
+        #     for (i, _idx) in converted_idx.items():
+        #         if _idx:
+        #             self.tensordicts[i]._set_at_str(
+        #                 key, value, _idx, validated=validated
+        #             )
+        #         else:
+        #             self.tensordicts[i]._set_str(
+        #                 key,
+        #                 value,
+        #                 validated=validated,
+        #                 inplace=True,
+        #             )
+        #     return self
+        # unbind_dim = self.stack_dim - num_single
+        # for (i, _idx), _value in zip(converted_idx.items(), value.unbind(unbind_dim)):
+        #     self.tensordicts[i]._set_at_str(key, _value, _idx, validated=validated)
+        # return self
+
+    def _set_at_tuple(self, key, value, idx, *, validated):
+        if len(key) == 1:
+            return self._set_at_str(key[0], value, idx, validated=validated)
+        # get the "last" tds
+        tds = []
+        for td in self.tensordicts:
+            tds.append(td.get(key[:-1]))
+        # build only a single lazy stack from it
+        # (if the stack is a stack of stacks this won't be awesomely efficient
+        # but then we'd need to splut the value (which we can do) and recompute
+        # the sub-index for each td, which is a different story!
+        td = LazyStackedTensorDict(
+            *tds, stack_dim=self.stack_dim, hook_out=self.hook_out, hook_in=self.hook_in
+        )
+        if not validated:
+            value = self._validate_value(value, check_shape=False)
+            validated = True
+        if self.hook_in is not None:
+            value = self.hook_in(value)
+        item = td._get_str(key, NO_DEFAULT)
+        item[idx] = value
+        td._set_str(key, item, inplace=True, validated=True)
+        return self
+
+    def unsqueeze(self, dim: int) -> TensorDictBase:
+        if dim < 0:
+            dim = self.batch_dims + dim + 1
+
+        if (dim > self.batch_dims) or (dim < 0):
+            raise RuntimeError(
+                f"unsqueezing is allowed for dims comprised between "
+                f"`-td.batch_dims` and `td.batch_dims` only. Got "
+                f"dim={dim} with a batch size of {self.batch_size}."
+            )
+        if dim <= self.stack_dim:
+            stack_dim = self.stack_dim + 1
+        else:
+            dim = dim - 1
+            stack_dim = self.stack_dim
+        return LazyStackedTensorDict(
+            *(tensordict.unsqueeze(dim) for tensordict in self.tensordicts),
+            stack_dim=stack_dim,
+        )
+
+    def squeeze(self, dim: int | None = None) -> TensorDictBase:
+        """Squeezes all tensors for a dimension comprised in between `-td.batch_dims+1` and `td.batch_dims-1` and returns them in a new tensordict.
+
+        Args:
+            dim (Optional[int]): dimension along which to squeeze. If dim is None, all singleton dimensions will be squeezed. dim is None by default.
+
+        """
+        if dim is None:
+            size = self.size()
+            if len(self.size()) == 1 or size.count(1) == 0:
+                return self
+            first_singleton_dim = size.index(1)
+            return self.squeeze(first_singleton_dim).squeeze()
+
+        if dim < 0:
+            dim = self.batch_dims + dim
+
+        if self.batch_dims and (dim >= self.batch_dims or dim < 0):
+            raise RuntimeError(
+                f"squeezing is allowed for dims comprised between 0 and "
+                f"td.batch_dims only. Got dim={dim} and batch_size"
+                f"={self.batch_size}."
+            )
+
+        if dim >= self.batch_dims or self.batch_size[dim] != 1:
+            return self
+        if dim == self.stack_dim:
+            return self.tensordicts[0]
+        elif dim < self.stack_dim:
+            stack_dim = self.stack_dim - 1
+        else:
+            dim = dim - 1
+            stack_dim = self.stack_dim
+        return LazyStackedTensorDict(
+            *(tensordict.squeeze(dim) for tensordict in self.tensordicts),
+            stack_dim=stack_dim,
+        )
+
+    def unbind(self, dim: int) -> tuple[TensorDictBase, ...]:
+        if dim < 0:
+            dim = self.batch_dims + dim
+        if dim < 0 or dim >= self.ndim:
+            raise ValueError(
+                f"Cannot unbind along dimension {dim} with batch size {self.batch_size}."
+            )
+        if dim == self.stack_dim:
+            return tuple(self.tensordicts)
+        else:
+            # return a stack of unbound tensordicts
+            out = []
+            new_dim = dim if dim < self.stack_dim else dim - 1
+            new_stack_dim = (
+                self.stack_dim if dim > self.stack_dim else self.stack_dim - 1
+            )
+            for td in self.tensordicts:
+                out.append(td.unbind(new_dim))
+            return tuple(_stack(vals, new_stack_dim) for vals in zip(*out))
+
+    def _stack_onto_(
+        self,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> TensorDictBase:
+        if dim == self.stack_dim:
+            for source, tensordict_dest in zip(list_item, self.tensordicts):
+                tensordict_dest.update_(source)
+        else:
+            for i, td in enumerate(list_item):
+                idx = (slice(None),) * dim + (i,)
+                self.update_at_(td, idx)
+        return self
+
+    @cache  # noqa: B019
+    def _get_str(
+        self,
+        key: NestedKey,
+        default: str | CompatibleType = NO_DEFAULT,
+    ) -> CompatibleType:
+        # we can handle the case where the key is a tuple of length 1
+        tensors = []
+        for td in self.tensordicts:
+            tensors.append(td._get_str(key, default=default))
+            if (
+                tensors[-1] is default
+                and not isinstance(
+                    default, (MemmapTensor, KeyedJaggedTensor, torch.Tensor)
+                )
+                and not is_tensor_collection(default)
+            ):
+                # then we consider this default as non-stackable and return prematurly
+                return default
+        try:
+            out = torch.stack(tensors, self.stack_dim)
+            if _is_tensor_collection(out.__class__):
+                if self._td_dim_name is not None:
+                    out._td_dim_name = self._td_dim_name
+                if isinstance(out, LazyStackedTensorDict):
+                    # then it's a LazyStackedTD
+                    out.hook_out = self.hook_out
+                    out.hook_in = self.hook_in
+                else:
+                    # then it's a tensorclass
+                    out._tensordict.hook_out = self.hook_out
+                    out._tensordict.hook_in = self.hook_in
+            elif self.hook_out is not None:
+                out = self.hook_out(out)
+            return out
+        except RuntimeError as err:
+            if "stack expects each tensor to be equal size" in str(err):
+                shapes = {_shape(tensor) for tensor in tensors}
+                raise RuntimeError(
+                    f"Found more than one unique shape in the tensors to be "
+                    f"stacked ({shapes}). This is likely due to a modification "
+                    f"of one of the stacked TensorDicts, where a key has been "
+                    f"updated/created with an uncompatible shape. If the entries "
+                    f"are intended to have a different shape, use the get_nestedtensor "
+                    f"method instead."
+                )
+            else:
+                raise err
+
+    def _get_tuple(self, key, default):
+        first = self._get_str(key[0], None)
+        if first is None:
+            return self._default_get(first, default)
+        if len(key) == 1:
+            return first
+        try:
+            if isinstance(first, KeyedJaggedTensor):
+                if len(key) != 2:
+                    raise ValueError(f"Got too many keys for a KJT: {key}.")
+                return first[key[-1]]
+            else:
+                return first._get_tuple(key[1:], default=default)
+        except AttributeError as err:
+            if "has no attribute" in str(err):
+                raise ValueError(
+                    f"Expected a TensorDictBase instance but got {type(first)} instead"
+                    f" for key '{key[1:]}' in tensordict:\n{self}."
+                )
+
+    @cache  # noqa: B019
+    def _add_batch_dim(self, *, in_dim, vmap_level):
+        if self.is_memmap():
+            td = torch.stack([td.cpu().as_tensor() for td in self.tensordicts], 0)
+        else:
+            td = self
+        if in_dim < 0:
+            in_dim = self.ndim + in_dim
+        if in_dim == self.stack_dim:
+            return self._cached_add_batch_dims(td, in_dim=in_dim, vmap_level=vmap_level)
+        if in_dim < td.stack_dim:
+            # then we'll stack along a dim before
+            stack_dim = td.stack_dim - 1
+        else:
+            in_dim = in_dim - 1
+            stack_dim = td.stack_dim
+        tds = [
+            td.apply(
+                lambda _arg: _add_batch_dim(_arg, in_dim, vmap_level),
+                batch_size=[b for i, b in enumerate(td.batch_size) if i != in_dim],
+                names=[name for i, name in enumerate(td.names) if i != in_dim],
+            )
+            for td in td.tensordicts
+        ]
+        return LazyStackedTensorDict(*tds, stack_dim=stack_dim)
+
+    @classmethod
+    def _cached_add_batch_dims(cls, td, in_dim, vmap_level):
+        # we return a stack with hook_out, and hack the batch_size and names
+        # Per se it is still a LazyStack but the stacking dim is "hidden" from
+        # the outside
+        out = td.clone(False)
+
+        def hook_out(tensor, in_dim=in_dim, vmap_level=vmap_level):
+            return _add_batch_dim(tensor, in_dim, vmap_level)
+
+        n = len(td.tensordicts)
+
+        def hook_in(
+            tensor,
+            out_dim=in_dim,
+            batch_size=n,
+            vmap_level=vmap_level,
+        ):
+            return _remove_batch_dim(tensor, vmap_level, batch_size, out_dim)
+
+        out.hook_out = hook_out
+        out.hook_in = hook_in
+        out._batch_size = torch.Size(
+            [dim for i, dim in enumerate(out._batch_size) if i != out.stack_dim]
+        )
+        return out
+
+    @cache  # noqa: B019
+    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
+        if self.hook_out is not None:
+            # this is the hacked version. We just need to remove the hook_out and
+            # reset a proper batch size
+            return LazyStackedTensorDict(
+                *self.tensordicts,
+                stack_dim=out_dim,
+            )
+            # return self._cache_remove_batch_dim(vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim)
+        else:
+            # we must call _remove_batch_dim on all tensordicts
+            # batch_size: size of the batch when we unhide it.
+            # out_dim: dimension where the output will be found
+            new_batch_size = list(self.batch_size)
+            new_batch_size.insert(out_dim, batch_size)
+            new_names = list(self.names)
+            new_names.insert(out_dim, None)
+            # rebuild the lazy stack
+            # the stack dim is the same if the out_dim is past it, but it
+            # must be incremented by one otherwise.
+            # In the first case, the out_dim must be decremented by one
+            if out_dim > self.stack_dim:
+                stack_dim = self.stack_dim
+                out_dim = out_dim - 1
+            else:
+                stack_dim = self.stack_dim + 1
+            out = LazyStackedTensorDict(
+                *[
+                    td._remove_batch_dim(
+                        vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
+                    )
+                    for td in self.tensordicts
+                ],
+                stack_dim=stack_dim,
+            )
+        return out
+
+    def get_nestedtensor(
+        self,
+        key: NestedKey,
+        default: str | CompatibleType = NO_DEFAULT,
+    ) -> CompatibleType:
+        """Returns a nested tensor when stacking cannot be achieved.
+
+        Args:
+            key (NestedKey): the entry to nest.
+            default (Any, optiona): the default value to return in case the key
+                isn't in all sub-tensordicts.
+
+                .. note:: In case the default is a tensor, this method will attempt
+                  the construction of a nestedtensor with it. Otherwise, the default
+                  value will be returned.
+
+        Examples:
+            >>> td0 = TensorDict({"a": torch.zeros(4), "b": torch.zeros(4)}, [])
+            >>> td1 = TensorDict({"a": torch.ones(5)}, [])
+            >>> td = torch.stack([td0, td1], 0)
+            >>> a = td.get_nestedtensor("a")
+            >>> # using a tensor as default uses this default to build the nested tensor
+            >>> b = td.get_nestedtensor("b", default=torch.ones(4))
+            >>> assert (a == b).all()
+            >>> # using anything else as default returns the default
+            >>> b2 = td.get_nestedtensor("b", None)
+            >>> assert b2 is None
+
+        """
+        # disallow getting nested tensor if the stacking dimension is not 0
+        if self.stack_dim != 0:
+            raise RuntimeError(
+                "Because nested tensors can only be stacked along their first "
+                "dimension, LazyStackedTensorDict.get_nestedtensor can only be called "
+                "when the stack_dim is 0."
+            )
+
+        # we can handle the case where the key is a tuple of length 1
+        key = _unravel_key_to_tuple(key)
+        subkey = key[0]
+        if len(key) > 1:
+            tensordict = self.get(subkey, default)
+            if tensordict is default:
+                return default
+            return tensordict.get_nestedtensor(key[1:], default=default)
+        tensors = [td.get(subkey, default=default) for td in self.tensordicts]
+        if not isinstance(default, torch.Tensor) and any(
+            tensor is default for tensor in tensors
+        ):
+            # we don't stack but return the default
+            return default
+        return torch.nested.nested_tensor(tensors)
+
+    def is_contiguous(self) -> bool:
+        return False
+
+    def contiguous(self) -> TensorDictBase:
+        source = {key: value.contiguous() for key, value in self.items()}
+        batch_size = self.batch_size
+        device = self.device
+        out = TensorDict(
+            source=source,
+            batch_size=batch_size,
+            device=device,
+            names=self.names,
+            _run_checks=False,
+        )
+        return out
+
+    def clone(self, recurse: bool = True) -> TensorDictBase:
+        if recurse:
+            # This could be optimized using copy but we must be careful with
+            # metadata (_is_shared etc)
+            out = LazyStackedTensorDict(
+                *[td.clone() for td in self.tensordicts],
+                stack_dim=self.stack_dim,
+            )
+        else:
+            out = LazyStackedTensorDict(
+                *[td.clone(recurse=False) for td in self.tensordicts],
+                stack_dim=self.stack_dim,
+            )
+        if self._td_dim_name is not None:
+            out._td_dim_name = self._td_dim_name
+        return out
+
+    def pin_memory(self) -> TensorDictBase:
+        for td in self.tensordicts:
+            td.pin_memory()
+        return self
+
+    def to(self, dest: DeviceType | type, **kwargs) -> TensorDictBase:
+        if isinstance(dest, type) and issubclass(dest, TensorDictBase):
+            if isinstance(self, dest):
+                return self
+            kwargs.update({"batch_size": self.batch_size})
+            out = dest(source=self, **kwargs)
+            # if self._td_dim_name is not None:
+            # TODO: define a _has_names util to quickly check and avoid this
+            out.names = self.names
+            return out
+        elif isinstance(dest, (torch.device, str, int)):
+            dest = torch.device(dest)
+            if self.device is not None and dest == self.device:
+                return self
+            td = self.to_tensordict().to(dest, **kwargs)
+            return td
+
+        elif isinstance(dest, torch.Size):
+            self.batch_size = dest
+        elif dest is None:
+            return self
+        else:
+            raise NotImplementedError(
+                f"dest must be a string, torch.device or a TensorDict "
+                f"instance, {dest} not allowed"
+            )
+
+    def _check_new_batch_size(self, new_size: torch.Size) -> None:
+        if len(new_size) <= self.stack_dim:
+            raise RuntimeError(
+                "Changing the batch_size of a LazyStackedTensorDicts can only "
+                "be done with sizes that are at least as long as the "
+                "stacking dimension."
+            )
+        super()._check_new_batch_size(new_size)
+
+    def _change_batch_size(self, new_size: torch.Size) -> None:
+        if not hasattr(self, "_orig_batch_size"):
+            self._orig_batch_size = self.batch_size
+        elif self._orig_batch_size == new_size:
+            del self._orig_batch_size
+        self._batch_size = new_size
+
+    def keys(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> _LazyStackedTensorDictKeysView:
+        keys = _LazyStackedTensorDictKeysView(
+            self, include_nested=include_nested, leaves_only=leaves_only
+        )
+        return keys
+
+    valid_keys = keys
+
+    # def _iterate_over_keys(self) -> None:
+    #     for key in self.tensordicts[0].keys():
+    #         if all(key in td.keys() for td in self.tensordicts):
+    #             yield key
+    def _iterate_over_keys(self) -> None:
+        # this is about 20x faster than the version above
+        yield from self._key_list()
+
+    @cache  # noqa: B019
+    def _key_list(self):
+        keys = set(self.tensordicts[0].keys())
+        for td in self.tensordicts[1:]:
+            keys = keys.intersection(td.keys())
+        return sorted(keys, key=str)
+
+    def entry_class(self, key: NestedKey) -> type:
+        data_type = type(self.tensordicts[0].get(key))
+        if _is_tensor_collection(data_type):
+            return LazyStackedTensorDict
+        return data_type
+
+    def apply_(self, fn: Callable, *others):
+        for i, td in enumerate(self.tensordicts):
+            idx = (slice(None),) * self.stack_dim + (i,)
+            td.apply_(fn, *[other[idx] for other in others])
+        return self
+
+    def apply(
+        self,
+        fn: Callable,
+        *others: TensorDictBase,
+        batch_size: Sequence[int] | None = None,
+        device: torch.device | None = None,
+        names: Sequence[str] | None = None,
+        inplace: bool = False,
+        **constructor_kwargs,
+    ) -> TensorDictBase:
+        if inplace:
+            if any(arg for arg in (batch_size, device, names, constructor_kwargs)):
+                raise ValueError(
+                    "Cannot pass other arguments to LazyStackedTensorDict.apply when inplace=True."
+                )
+            return self.apply_(fn, *others)
+        else:
+            if batch_size is not None:
+                return super().apply(
+                    fn,
+                    *others,
+                    batch_size=batch_size,
+                    device=device,
+                    names=names,
+                    **constructor_kwargs,
+                )
+            others = (other.unbind(self.stack_dim) for other in others)
+            out = LazyStackedTensorDict(
+                *(
+                    td.apply(fn, *oth, device=device)
+                    for td, *oth in zip(self.tensordicts, *others)
+                ),
+                stack_dim=self.stack_dim,
+            )
+            if names is not None:
+                out.names = names
+            return out
+
+    def select(
+        self, *keys: str, inplace: bool = False, strict: bool = False
+    ) -> LazyStackedTensorDict:
+        # the following implementation keeps the hidden keys in the tensordicts
+        tensordicts = [
+            td.select(*keys, inplace=inplace, strict=strict) for td in self.tensordicts
+        ]
+        if inplace:
+            return self
+        return LazyStackedTensorDict(*tensordicts, stack_dim=self.stack_dim)
+
+    def exclude(self, *keys: str, inplace: bool = False) -> LazyStackedTensorDict:
+        tensordicts = [
+            tensordict.exclude(*keys, inplace=inplace)
+            for tensordict in self.tensordicts
+        ]
+        if inplace:
+            self.tensordicts = tensordicts
+            return self
+        return torch.stack(tensordicts, dim=self.stack_dim)
+
+    def __setitem__(self, index: IndexType, value: TensorDictBase) -> TensorDictBase:
+        if isinstance(index, (tuple, str)):
+            # try:
+            index_unravel = _unravel_key_to_tuple(index)
+            if index_unravel:
+                self._set_tuple(
+                    index_unravel,
+                    value,
+                    inplace=BEST_ATTEMPT_INPLACE
+                    if isinstance(self, SubTensorDict)
+                    else False,
+                    validated=False,
+                )
+                return
+
+            if any(isinstance(sub_index, (list, range)) for sub_index in index):
+                index = tuple(
+                    torch.tensor(sub_index, device=self.device)
+                    if isinstance(sub_index, (list, range))
+                    else sub_index
+                    for sub_index in index
+                )
+
+        if index is Ellipsis or (isinstance(index, tuple) and Ellipsis in index):
+            index = convert_ellipsis_to_idx(index, self.batch_size)
+        elif isinstance(index, (list, range)):
+            index = torch.tensor(index, device=self.device)
+
+        if isinstance(value, (TensorDictBase, dict)):
+            indexed_bs = _getitem_batch_size(self.batch_size, index)
+            if isinstance(value, dict):
+                value = TensorDict(
+                    value, batch_size=indexed_bs, device=self.device, _run_checks=False
+                )
+            if value.batch_size != indexed_bs:
+                # try to expand
+                try:
+                    value = value.expand(indexed_bs)
+                except RuntimeError as err:
+                    raise RuntimeError(
+                        f"indexed destination TensorDict batch size is {indexed_bs} "
+                        f"(batch_size = {self.batch_size}, index={index}), "
+                        f"which differs from the source batch size {value.batch_size}"
+                    ) from err
+            split_index = self._split_index(index)
+            converted_idx = split_index["index_dict"]
+            num_single = split_index["num_single"]
+            isinteger = split_index["isinteger"]
+            has_bool = split_index["has_bool"]
+            num_squash = split_index.get("num_squash", 0)
+            num_none = split_index.get("num_none", 0)
+            is_nd_tensor = split_index.get("is_nd_tensor", False)
+            if isinteger:
+                # this will break if the index along the stack dim is [0] or :1 or smth
+                for i, _idx in converted_idx.items():
+                    self.tensordicts[i][_idx] = value
+                return self
+            if is_nd_tensor:
+                raise RuntimeError(
+                    "Indexing along stack dim with a non-boolean tensor is not supported yet. "
+                    "Use SubTensorDict instead."
+                )
+            if not has_bool:
+                unbind_dim = self.stack_dim - num_single + num_none - num_squash
+                value_unbind = value.unbind(unbind_dim)
+                for (i, _idx), _value in zip(
+                    converted_idx.items(),
+                    value_unbind,
+                ):
+                    self.tensordicts[i][_idx] = _value
+            else:
+                # we must split, not unbind
+                mask_unbind = split_index["individual_masks"]
+                split_dim = split_index["split_dim"]
+                splits = [_mask_unbind.sum().item() for _mask_unbind in mask_unbind]
+                value_unbind = value.split(splits, split_dim)
+                if mask_unbind[0].ndim == 0:
+                    # we can return a stack
+                    for (i, _idx), mask, _value in zip(
+                        converted_idx.items(),
+                        mask_unbind,
+                        value_unbind,
+                    ):
+                        if mask.any():
+                            self.tensordicts[i][_idx] = _value
+                else:
+                    for (i, _idx), _value in zip(converted_idx.items(), value_unbind):
+                        self_idx = (slice(None),) * split_index["mask_loc"] + (i,)
+                        self[self_idx][_idx] = _value
+        else:
+            for key in self.keys():
+                self.set_at_(key, value, index)
+
+    def __contains__(self, item: IndexType) -> bool:
+        if isinstance(item, TensorDictBase):
+            return any(item is td for td in self.tensordicts)
+        return super().__contains__(item)
+
+    def __getitem__(self, index: IndexType) -> TensorDictBase:
+        if isinstance(index, (tuple, str)):
+            index_key = _unravel_key_to_tuple(index)
+            if index_key:
+                return self._get_tuple(index_key, NO_DEFAULT)
+        split_index = self._split_index(index)
+        converted_idx = split_index["index_dict"]
+        isinteger = split_index["isinteger"]
+        has_bool = split_index["has_bool"]
+        is_nd_tensor = split_index["is_nd_tensor"]
+        num_single = split_index.get("num_single", 0)
+        num_none = split_index.get("num_none", 0)
+        num_squash = split_index.get("num_squash", 0)
+        if has_bool:
+            mask_unbind = split_index["individual_masks"]
+            cat_dim = split_index["mask_loc"] - num_single
+            out = []
+            if mask_unbind[0].ndim == 0:
+                # we can return a stack
+                for (i, _idx), mask in zip(converted_idx.items(), mask_unbind):
+                    if mask.any():
+                        if mask.all() and self.tensordicts[i].ndim == 0:
+                            out.append(self.tensordicts[i])
+                        else:
+                            out.append(self.tensordicts[i][_idx])
+                            out[-1] = out[-1].squeeze(cat_dim)
+                return torch.stack(out, cat_dim)
+            else:
+                for i, _idx in converted_idx.items():
+                    self_idx = (slice(None),) * split_index["mask_loc"] + (i,)
+                    out.append(self[self_idx][_idx])
+                return torch.cat(out, cat_dim)
+        elif is_nd_tensor:
+            new_stack_dim = self.stack_dim - num_single + num_none
+            return torch.stack(
+                [self[idx] for idx in converted_idx.values()], new_stack_dim
+            )
+        else:
+            if isinteger:
+                for (
+                    i,
+                    _idx,
+                ) in (
+                    converted_idx.items()
+                ):  # for convenience but there's only one element
+                    out = self.tensordicts[i]
+                    if _idx is not None and _idx != ():
+                        out = out[_idx]
+                    return out
+            else:
+                out = []
+                new_stack_dim = self.stack_dim - num_single + num_none - num_squash
+                for i, _idx in converted_idx.items():
+                    out.append(self.tensordicts[i][_idx])
+                out = torch.stack(out, new_stack_dim)
+                out._td_dim_name = self._td_dim_name
+                return out
+
+        # index_dict = _convert_index_lazystack(index, self.stack_dim, self.batch_size)
+        # if index_dict is None:
+        #     # then we use a sub-tensordict
+        #     return self.get_sub_tensordict(index)
+        # td_index = index_dict["remaining_index"]
+        # stack_index = index_dict["stack_index"]
+        # new_stack_dim = index_dict["new_stack_dim"]
+        # if new_stack_dim is not None:
+        #     if isinstance(stack_index, slice):
+        #         # we can't iterate but we can index the list directly
+        #         out = LazyStackedTensorDict(
+        #             *[td[td_index] for td in self.tensordicts[stack_index]],
+        #             stack_dim=new_stack_dim,
+        #         )
+        #     elif isinstance(stack_index, (list, range)):
+        #         # then we can iterate
+        #         out = LazyStackedTensorDict(
+        #             *[self.tensordicts[idx][td_index] for idx in stack_index],
+        #             stack_dim=new_stack_dim,
+        #         )
+        #     elif isinstance(stack_index, Tensor):
+        #         # td_index is a nested tuple that mimics the shape of stack_index
+        #         def _nested_stack(t: list, stack_idx: Tensor, td_index):
+        #             if stack_idx.ndim:
+        #                 out = LazyStackedTensorDict(
+        #                     *[
+        #                         _nested_stack(t, _idx, td_index[i])
+        #                         for i, _idx in enumerate(stack_idx.unbind(0))
+        #                     ],
+        #                     stack_dim=new_stack_dim,
+        #                 )
+        #                 return out
+        #             return t[stack_idx][td_index]
+        #
+        #         # print(index, td_index, stack_index)
+        #         out = _nested_stack(self.tensordicts, stack_index, td_index)
+        #     else:
+        #         raise TypeError("Invalid index used for stack dimension.")
+        #     out._td_dim_name = self._td_dim_name
+        #     return out
+        # out = self.tensordicts[stack_index]
+        # if td_index:
+        #     return out[td_index]
+        # return out
+
+    # def __hash__(self):
+    #     return hash(self.tensordicts)
+
+    def __eq__(self, other):
+        if is_tensorclass(other):
+            return other == self
+        if isinstance(other, (dict,)):
+            other = TensorDict.from_dict(other)
+        if _is_tensor_collection(other.__class__):
+            out = []
+            for i, td in enumerate(self.tensordicts):
+                idx = (slice(None),) * self.stack_dim + (i,)
+                out.append(other[idx] == td)
+            return torch.stack(out, self.stack_dim)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return torch.stack(
+                [td == other for td in self.tensordicts],
+                self.stack_dim,
+            )
+        return False
+
+    def __ne__(self, other):
+        if is_tensorclass(other):
+            return other != self
+        if isinstance(other, (dict,)):
+            other = TensorDict.from_dict(other)
+        if _is_tensor_collection(other.__class__):
+            out = []
+            for i, td in enumerate(self.tensordicts):
+                idx = (slice(None),) * self.stack_dim + (i,)
+                out.append(other[idx] != td)
+            return torch.stack(out, self.stack_dim)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return torch.stack(
+                [td != other for td in self.tensordicts],
+                self.stack_dim,
+            )
+        return True
+
+    def all(self, dim: int = None) -> bool | TensorDictBase:
+        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
+            raise RuntimeError(
+                "dim must be greater than or equal to -tensordict.batch_dims and "
+                "smaller than tensordict.batch_dims"
+            )
+        if dim is not None:
+            # TODO: we need to adapt this to LazyStackedTensorDict too
+            if dim < 0:
+                dim = self.batch_dims + dim
+            return TensorDict(
+                source={key: value.all(dim=dim) for key, value in self.items()},
+                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
+                device=self.device,
+            )
+        return all(value.all() for value in self.tensordicts)
+
+    def any(self, dim: int = None) -> bool | TensorDictBase:
+        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
+            raise RuntimeError(
+                "dim must be greater than or equal to -tensordict.batch_dims and "
+                "smaller than tensordict.batch_dims"
+            )
+        if dim is not None:
+            # TODO: we need to adapt this to LazyStackedTensorDict too
+            if dim < 0:
+                dim = self.batch_dims + dim
+            return TensorDict(
+                source={key: value.any(dim=dim) for key, value in self.items()},
+                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
+                device=self.device,
+            )
+        return any(value.any() for value in self.tensordicts)
+
+    def _send(self, dst: int, _tag: int = -1, pseudo_rand: bool = False) -> int:
+        for td in self.tensordicts:
+            _tag = td._send(dst, _tag=_tag, pseudo_rand=pseudo_rand)
+        return _tag
+
+    def _isend(
+        self,
+        dst: int,
+        _tag: int = -1,
+        _futures: list[torch.Future] | None = None,
+        pseudo_rand: bool = False,
+    ) -> int:
+        if _futures is None:
+            is_root = True
+            _futures = []
+        else:
+            is_root = False
+        for td in self.tensordicts:
+            _tag = td._isend(dst, _tag=_tag, pseudo_rand=pseudo_rand, _futures=_futures)
+        if is_root:
+            for future in _futures:
+                future.wait()
+        return _tag
+
+    def _recv(self, src: int, _tag: int = -1, pseudo_rand: bool = False) -> int:
+        for td in self.tensordicts:
+            _tag = td._recv(src, _tag=_tag, pseudo_rand=pseudo_rand)
+        return _tag
+
+    def _irecv(
+        self,
+        src: int,
+        return_premature: bool = False,
+        _tag: int = -1,
+        _future_list: list[torch.Future] = None,
+        pseudo_rand: bool = False,
+    ) -> tuple[int, list[torch.Future]] | list[torch.Future] | None:
+        root = False
+        if _future_list is None:
+            _future_list = []
+            root = True
+        for td in self.tensordicts:
+            _tag, _future_list = td._irecv(
+                src=src,
+                return_premature=return_premature,
+                _tag=_tag,
+                _future_list=_future_list,
+                pseudo_rand=pseudo_rand,
+            )
+
+        if not root:
+            return _tag, _future_list
+        elif return_premature:
+            return _future_list
+        else:
+            for future in _future_list:
+                future.wait()
+            return
+
+    @lock_blocked
+    def del_(self, key: NestedKey, **kwargs: Any) -> TensorDictBase:
+        ids = set()
+        cur_len = len(ids)
+        is_deleted = False
+        error = None
+        for td in self.tensordicts:
+            # checking that the td has not been processed yet.
+            # It could be that not all sub-tensordicts have the appropriate
+            # entry but one must have it (or an error is thrown).
+            tdid = id(td)
+            ids.add(tdid)
+            new_cur_len = len(ids)
+            if new_cur_len == cur_len:
+                continue
+            cur_len = new_cur_len
+            try:
+                td.del_(key, **kwargs)
+                is_deleted = True
+            except KeyError as err:
+                error = err
+                continue
+        if not is_deleted:
+            # we know err is defined because LazyStackedTensorDict cannot be empty
+            raise error
+        return self
+
+    def pop(
+        self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
+    ) -> CompatibleType:
+        # using try/except for get/del is suboptimal, but
+        # this is faster that checkink if key in self keys
+        key = _unravel_key_to_tuple(key)
+        if len(key) == 1:
+            key = key[0]
+        present = False
+        if isinstance(key, tuple):
+            if key in self.keys(True):
+                present = True
+                value = self._get_tuple(key, NO_DEFAULT)
+        elif key in self.keys():
+            present = True
+            value = self._get_str(key, NO_DEFAULT)
+        if present:
+            self.del_(key)
+        elif default is not NO_DEFAULT:
+            value = default
+        else:
+            raise KeyError(
+                f"You are trying to pop key `{key}` which is not in dict "
+                f"without providing default value."
+            )
+        return value
+
+    def share_memory_(self) -> TensorDictBase:
+        for td in self.tensordicts:
+            td.share_memory_()
+        self._is_shared = True
+        self.lock_()
+        return self
+
+    def detach_(self) -> TensorDictBase:
+        for td in self.tensordicts:
+            td.detach_()
+        return self
+
+    def memmap_(
+        self, prefix: str | None = None, copy_existing: bool = False
+    ) -> TensorDictBase:
+        if prefix is not None:
+            prefix = Path(prefix)
+            if not prefix.exists():
+                os.makedirs(prefix, exist_ok=True)
+            torch.save({"stack_dim": self.stack_dim}, prefix / "meta.pt")
+        for i, td in enumerate(self.tensordicts):
+            td.memmap_(
+                prefix=(prefix / str(i)) if prefix is not None else None,
+                copy_existing=copy_existing,
+            )
+        self._is_memmap = True
+        self.lock_()
+        return self
+
+    def memmap_like(
+        self,
+        prefix: str | None = None,
+    ) -> TensorDictBase:
+        tds = []
+        if prefix is not None:
+            prefix = Path(prefix)
+            if not prefix.exists():
+                os.makedirs(prefix, exist_ok=True)
+            torch.save({"stack_dim": self.stack_dim}, prefix / "meta.pt")
+        for i, td in enumerate(self.tensordicts):
+            td_like = td.memmap_like(
+                prefix=(prefix / str(i)) if prefix is not None else None,
+            )
+            tds.append(td_like)
+        td_out = torch.stack(tds, self.stack_dim)
+        td_out._is_memmap = True
+        td_out.lock_()
+        return td_out
+
+    @classmethod
+    def load_memmap(cls, prefix: str) -> LazyStackedTensorDict:
+        prefix = Path(prefix)
+        tensordicts = []
+        i = 0
+        while (prefix / str(i)).exists():
+            tensordicts.append(TensorDict.load_memmap(prefix / str(i)))
+            i += 1
+
+        metadata = torch.load(prefix / "meta.pt")
+        return cls(*tensordicts, stack_dim=metadata["stack_dim"])
+
+    def expand(self, *shape: int, inplace: bool = False) -> TensorDictBase:
+        if len(shape) == 1 and isinstance(shape[0], Sequence):
+            shape = tuple(shape[0])
+        stack_dim = len(shape) + self.stack_dim - self.ndimension()
+        new_shape_tensordicts = [v for i, v in enumerate(shape) if i != stack_dim]
+        tensordicts = [td.expand(*new_shape_tensordicts) for td in self.tensordicts]
+        if inplace:
+            self.tensordicts = tensordicts
+            self.stack_dim = stack_dim
+            return self
+        return torch.stack(tensordicts, stack_dim)
+
+    def update(
+        self, input_dict_or_td: TensorDictBase, clone: bool = False, **kwargs: Any
+    ) -> TensorDictBase:
+        if input_dict_or_td is self:
+            # no op
+            return self
+
+        if (
+            isinstance(input_dict_or_td, LazyStackedTensorDict)
+            and input_dict_or_td.stack_dim == self.stack_dim
+        ):
+            if not input_dict_or_td.shape[self.stack_dim] == len(self.tensordicts):
+                raise ValueError(
+                    "cannot update stacked tensordicts with different shapes."
+                )
+            for td_dest, td_source in zip(
+                self.tensordicts, input_dict_or_td.tensordicts
+            ):
+                td_dest.update(td_source, clone=clone, **kwargs)
+            return self
+
+        keys = self.keys(False)
+        for key, value in input_dict_or_td.items():
+            if clone and hasattr(value, "clone"):
+                value = value.clone()
+            else:
+                value = tree_map(torch.clone, value)
+            if isinstance(key, tuple):
+                key, subkey = key[0], key[1:]
+            else:
+                subkey = ()
+            # the key must be a string by now. Let's check if it is present
+            if key in keys:
+                target_class = self.entry_class(key)
+                if _is_tensor_collection(target_class):
+                    if isinstance(value, dict):
+                        value_unbind = TensorDict(
+                            value, self.batch_size, _run_checks=False
+                        ).unbind(self.stack_dim)
+                    else:
+                        value_unbind = value.unbind(self.stack_dim)
+                    for t, _value in zip(self.tensordicts, value_unbind):
+                        if len(subkey):
+                            t.update({key: {subkey: _value}}, clone=clone, **kwargs)
+                        else:
+                            t.update({key: _value}, clone=clone, **kwargs)
+                    continue
+            if len(subkey):
+                self.set((key, *subkey), value, **kwargs)
+            else:
+                self.set(key, value, **kwargs)
+        return self
+
+    def update_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        clone: bool = False,
+        **kwargs: Any,
+    ) -> TensorDictBase:
+        if input_dict_or_td is self:
+            # no op
+            return self
+        if isinstance(input_dict_or_td, LazyStackedTensorDict):
+            if input_dict_or_td.stack_dim == self.stack_dim:
+                if not input_dict_or_td.shape[self.stack_dim] == len(self.tensordicts):
+                    raise ValueError(
+                        "cannot update stacked tensordicts with different shapes."
+                    )
+                for td_dest, td_source in zip(
+                    self.tensordicts, input_dict_or_td.tensordicts
+                ):
+                    td_dest.update_(td_source)
+                return self
+            else:
+                for i, td in enumerate(input_dict_or_td.tensordicts):
+                    idx = (slice(None),) * input_dict_or_td.stack_dim + (i,)
+                    self.update_at_(td, idx)
+        for key, value in input_dict_or_td.items():
+            if not isinstance(value, tuple(_ACCEPTED_CLASSES)):
+                raise TypeError(
+                    f"Expected value to be one of types {_ACCEPTED_CLASSES} "
+                    f"but got {type(value)}"
+                )
+            if clone:
+                value = value.clone()
+            self.set_(key, value, **kwargs)
+        return self
+
+    def update_at_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        index: IndexType,
+        clone: bool = False,
+    ) -> TensorDictBase:
+        if isinstance(input_dict_or_td, TensorDictBase):
+            split_index = self._split_index(index)
+            converted_idx = split_index["index_dict"]
+            num_single = split_index["num_single"]
+            isinteger = split_index["isinteger"]
+            if isinteger:
+                # this will break if the index along the stack dim is [0] or :1 or smth
+                for i, _idx in converted_idx.items():
+                    self.tensordicts[i].update_at_(
+                        input_dict_or_td,
+                        _idx,
+                    )
+                return self
+            unbind_dim = self.stack_dim - num_single
+            for (i, _idx), _value in zip(
+                converted_idx.items(),
+                input_dict_or_td.unbind(unbind_dim),
+            ):
+                self.tensordicts[i].update_at_(
+                    _value,
+                    _idx,
+                )
+            return self
+        for key, value in input_dict_or_td.items():
+            if not isinstance(value, _ACCEPTED_CLASSES):
+                raise TypeError(
+                    f"Expected value to be one of types {_ACCEPTED_CLASSES} "
+                    f"but got {type(value)}"
+                )
+            if clone:
+                value = value.clone()
+            self.set_at_(key, value, index)
+        return self
+
+    def rename_key_(
+        self, old_key: str, new_key: str, safe: bool = False
+    ) -> TensorDictBase:
+        def sort_keys(element):
+            if isinstance(element, tuple):
+                return "_-|-_".join(element)
+            return element
+
+        for td in self.tensordicts:
+            td.rename_key_(old_key, new_key, safe=safe)
+        return self
+
+    rename_key = _renamed_inplace_method(rename_key_)
+
+    def where(self, condition, other, *, out=None):
+        condition = condition.unbind(self.stack_dim)
+        if _is_tensor_collection(other.__class__) or (
+            isinstance(other, Tensor)
+            and other.shape[: self.stack_dim] == self.shape[: self.stack_dim]
+        ):
+            other = other.unbind(self.stack_dim)
+            return torch.stack(
+                [
+                    td.where(cond, _other)
+                    for td, cond, _other in zip(self.tensordicts, condition, other)
+                ],
+                self.stack_dim,
+                out=out,
+            )
+        return torch.stack(
+            [td.where(cond, other) for td, cond in zip(self.tensordicts, condition)],
+            self.stack_dim,
+            out=out,
+        )
+
+    def masked_fill_(self, mask: Tensor, value: float | bool) -> TensorDictBase:
+        mask_unbind = mask.unbind(dim=self.stack_dim)
+        for _mask, td in zip(mask_unbind, self.tensordicts):
+            td.masked_fill_(_mask, value)
+        return self
+
+    def masked_fill(self, mask: Tensor, value: float | bool) -> TensorDictBase:
+        td_copy = self.clone()
+        return td_copy.masked_fill_(mask, value)
+
+    @lock_blocked
+    def insert(self, index: int, tensordict: TensorDictBase) -> None:
+        """Insert a TensorDict into the stack at the specified index.
+
+        Analogous to list.insert. The inserted TensorDict must have compatible
+        batch_size and device. Insertion is in-place, nothing is returned.
+
+        Args:
+            index (int): The index at which the new TensorDict should be inserted.
+            tensordict (TensorDictBase): The TensorDict to be inserted into the stack.
+
+        """
+        if not isinstance(tensordict, TensorDictBase):
+            raise TypeError(
+                "Expected new value to be TensorDictBase instance but got "
+                f"{type(tensordict)} instead."
+            )
+
+        batch_size = self.tensordicts[0].batch_size
+        device = self.tensordicts[0].device
+
+        _batch_size = tensordict.batch_size
+        _device = tensordict.device
+
+        if device != _device:
+            raise ValueError(
+                f"Devices differ: stack has device={device}, new value has "
+                f"device={_device}."
+            )
+        if _batch_size != batch_size:
+            raise ValueError(
+                f"Batch sizes in tensordicts differs: stack has "
+                f"batch_size={batch_size}, new_value has batch_size={_batch_size}."
+            )
+
+        self.tensordicts.insert(index, tensordict)
+
+        N = len(self.tensordicts)
+        self._batch_size = self._compute_batch_size(batch_size, self.stack_dim, N)
+
+    @lock_blocked
+    def append(self, tensordict: TensorDictBase) -> None:
+        """Append a TensorDict onto the stack.
+
+        Analogous to list.append. The appended TensorDict must have compatible
+        batch_size and device. The append operation is in-place, nothing is returned.
+
+        Args:
+            tensordict (TensorDictBase): The TensorDict to be appended onto the stack.
+
+        """
+        self.insert(len(self.tensordicts), tensordict)
+
+    @property
+    def is_locked(self) -> bool:
+        if self._is_locked is not None:
+            # if tensordicts have been locked through this Lazy stack, then we can
+            # trust this lazy stack to contain the info.
+            # In all other cases we must check
+            return self._is_locked
+        # If any of the tensordicts is not locked, we assume that the lazy stack
+        # is not locked either. Caching is then disabled and
+        for td in self.tensordicts:
+            if not td.is_locked:
+                return False
+        else:
+            # In this case, all tensordicts were locked before the lazy stack
+            # was created and they were not locked through the lazy stack.
+            # This means we cannot cache the value because this lazy stack
+            # if not part of the graph. We don't want it to be part of the graph
+            # because this object being locked is only a side-effect.
+            # Calling self.lock_() here could however speed things up.
+            return True
+
+    @is_locked.setter
+    def is_locked(self, value: bool) -> None:
+        if value:
+            self.lock_()
+        else:
+            self.unlock_()
+
+    @property
+    def _lock_id(self):
+        """Ids of all tensordicts that need to be unlocked for this to be unlocked."""
+        _lock_id = set()
+        for tensordict in self.tensordicts:
+            _lock_id = _lock_id.union(tensordict._lock_id)
+        _lock_id = _lock_id - {id(self)}
+        return _lock_id
+
+    def _lock_propagate(self, lock_ids=None):
+        """Registers the parent tensordict that handles the lock."""
+        self._is_locked = True
+        _locked_tensordicts = []
+        is_root = lock_ids is None
+        if is_root:
+            lock_ids = set()
+        lock_ids = lock_ids.union({id(self)})
+        for dest in self.tensordicts:
+            dest._lock_propagate(lock_ids)
+            _locked_tensordicts.append(dest)
+
+    def _remove_lock(self, lock_id):
+        for td in self.tensordicts:
+            td._remove_lock(lock_id)
+
+    @erase_cache
+    def _propagate_unlock(self, lock_ids=None):
+        # we can't set _is_locked to False because after it's unlocked, anything
+        # can happen to a child tensordict.
+        self._is_locked = None
+        if lock_ids is None:
+            lock_ids = set()
+
+        unlocked_tds = [self]
+        lock_ids.add(id(self))
+        for dest in self.tensordicts:
+            unlocked_tds.extend(dest._propagate_unlock(lock_ids))
+
+        self._is_shared = False
+        self._is_memmap = False
+        return unlocked_tds
+
+    def __del__(self):
+        if self._is_locked is None:
+            # then we can reliably say that this lazy stack is not part of
+            # the tensordicts graphs
+            return
+        # this can be a perf bottleneck
+        for td in self.tensordicts:
+            td._remove_lock(id(self))
+
+    def __repr__(self):
+        fields = _td_fields(self)
+        field_str = indent(f"fields={{{fields}}}", 4 * " ")
+        exclusive_fields_str = indent(
+            f"exclusive_fields={{{self._repr_exclusive_fields()}}}", 4 * " "
+        )
+        batch_size_str = indent(f"batch_size={self.batch_size}", 4 * " ")
+        device_str = indent(f"device={self.device}", 4 * " ")
+        is_shared_str = indent(f"is_shared={self.is_shared()}", 4 * " ")
+        stack_dim = indent(f"stack_dim={self.stack_dim}", 4 * " ")
+        string = ",\n".join(
+            [
+                field_str,
+                exclusive_fields_str,
+                batch_size_str,
+                device_str,
+                is_shared_str,
+                stack_dim,
+            ]
+        )
+        return f"{type(self).__name__}(\n{string})"
+
+    def _repr_exclusive_fields(self):
+        keys = set(self.keys())
+        exclusive_keys = [
+            _td_fields(td, [k for k in td.keys() if k not in keys])
+            for td in self.tensordicts
+        ]
+        exclusive_key_str = ",\n".join(
+            [
+                indent(f"{i} ->{line}", 4 * " ")
+                for i, line in enumerate(exclusive_keys)
+                if line != "\n"
+            ]
+        )
+
+        return "\n" + exclusive_key_str
+
+    lock_ = TensorDictBase.lock_
+    lock = _renamed_inplace_method(lock_)
+
+    unlock_ = TensorDictBase.unlock_
+    unlock = _renamed_inplace_method(unlock_)
+
+
+class _CustomOpTensorDict(TensorDictBase):
+    """Encodes lazy operations on tensors contained in a TensorDict."""
+
+    def __new__(cls, *args: Any, **kwargs: Any) -> _CustomOpTensorDict:
+        return super().__new__(cls, *args, _safe=False, _lazy=True, **kwargs)
+
+    def __init__(
+        self,
+        source: TensorDictBase,
+        custom_op: str,
+        inv_op: str | None = None,
+        custom_op_kwargs: dict | None = None,
+        inv_op_kwargs: dict | None = None,
+        batch_size: Sequence[int] | None = None,
+    ) -> None:
+        self._is_shared = source.is_shared()
+        self._is_memmap = source.is_memmap()
+
+        if not isinstance(source, TensorDictBase):
+            raise TypeError(
+                f"Expected source to be a TensorDictBase isntance, "
+                f"but got {type(source)} instead."
+            )
+        self._source = source
+        self.custom_op = custom_op
+        self.inv_op = inv_op
+        self.custom_op_kwargs = custom_op_kwargs if custom_op_kwargs is not None else {}
+        self.inv_op_kwargs = inv_op_kwargs if inv_op_kwargs is not None else {}
+        self._batch_size = None
+        if batch_size is not None and batch_size != self.batch_size:
+            raise RuntimeError("batch_size does not match self.batch_size.")
+
+    # def __hash__(self):
+    #     return hash((self._source, self.custom_op, self.inv_op, self.custom_op_kwargs, self.inv_op_kwargs))
+
+    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
+        """Allows for a transformation to be customized for a certain shape, device or dtype.
+
+        By default, this is a no-op on self.custom_op_kwargs
+
+        Args:
+            source_tensor: corresponding Tensor
+
+        Returns:
+            a dictionary with the kwargs of the operation to execute
+            for the tensor
+
+        """
+        return self.custom_op_kwargs
+
+    def _update_inv_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
+        """Allows for an inverse transformation to be customized for a certain shape, device or dtype.
+
+        By default, this is a no-op on self.inv_op_kwargs
+
+        Args:
+            source_tensor: corresponding tensor
+
+        Returns:
+            a dictionary with the kwargs of the operation to execute for
+            the tensor
+
+        """
+        return self.inv_op_kwargs
+
+    def entry_class(self, key: NestedKey) -> type:
+        return type(self._source.get(key))
+
+    @property
+    def device(self) -> torch.device | None:
+        return self._source.device
+
+    @device.setter
+    def device(self, value: DeviceType) -> None:
+        self._source.device = value
+
+    @property
+    def batch_size(self) -> torch.Size:
+        if self._batch_size is None:
+            self._batch_size = getattr(
+                torch.zeros(self._source.batch_size, device="meta"), self.custom_op
+            )(**self.custom_op_kwargs).shape
+        return self._batch_size
+
+    @batch_size.setter
+    def batch_size(self, new_size: torch.Size) -> None:
+        self._batch_size_setter(new_size)
+
+    def _has_names(self):
+        return self._source._has_names()
+
+    def _erase_names(self):
+        raise RuntimeError(
+            f"Cannot erase names of a {type(self)}. "
+            f"Erase source TensorDict's names instead."
+        )
+
+    def _rename_subtds(self, names):
+        for key in self.keys():
+            if _is_tensor_collection(self.entry_class(key)):
+                raise RuntimeError(
+                    "Cannot rename dimensions of a lazy TensorDict with "
+                    "nested collections. Convert the instance to a regular "
+                    "tensordict by using the `to_tensordict()` method first."
+                )
+
+    def _change_batch_size(self, new_size: torch.Size) -> None:
+        if not hasattr(self, "_orig_batch_size"):
+            self._orig_batch_size = self.batch_size
+        elif self._orig_batch_size == new_size:
+            del self._orig_batch_size
+        self._batch_size = new_size
+
+    def _get_str(self, key, default):
+        tensor = self._source._get_str(key, default)
+        if tensor is default:
+            return tensor
+        return self._transform_value(tensor)
+
+    def _get_tuple(self, key, default):
+        tensor = self._source._get_tuple(key, default)
+        if tensor is default:
+            return tensor
+        return self._transform_value(tensor)
+
+    def _transform_value(self, item):
+        return getattr(item, self.custom_op)(**self._update_custom_op_kwargs(item))
+
+    def _set_str(self, key, value, *, inplace: bool, validated: bool):
+        if not validated:
+            value = self._validate_value(value, check_shape=True)
+            validated = True
+        value = getattr(value, self.inv_op)(**self._update_inv_op_kwargs(value))
+        self._source._set_str(key, value, inplace=inplace, validated=validated)
+        return self
+
+    def _set_tuple(self, key, value, *, inplace: bool, validated: bool):
+        if len(key) == 1:
+            return self._set_str(key[0], value, inplace=inplace, validated=validated)
+        source = self._source._get_str(key[0], None)
+        if source is None:
+            self._source._create_nested_str(key[0])
+            source = self._source._get_str(key[0], NO_DEFAULT)
+        nested = type(self)(
+            source,
+            custom_op=self.custom_op,
+            inv_op=self.inv_op,
+            custom_op_kwargs=self._update_custom_op_kwargs(source),
+            inv_op_kwargs=self._update_inv_op_kwargs(source),
+        )
+        nested._set_tuple(key[1:], value, inplace=inplace, validated=validated)
+        return self
+
+    def _set_at_str(self, key, value, idx, *, validated):
+        transformed_tensor, original_tensor = self._get_str(
+            key, NO_DEFAULT
+        ), self._source._get_str(key, NO_DEFAULT)
+        if transformed_tensor.data_ptr() != original_tensor.data_ptr():
+            raise RuntimeError(
+                f"{self} original tensor and transformed_in do not point to the "
+                f"same storage. Setting values in place is not currently "
+                f"supported in this setting, consider calling "
+                f"`td.clone()` before `td.set_at_(...)`"
+            )
+        transformed_tensor[idx] = value
+        return self
+
+    def _set_at_tuple(self, key, value, idx, *, validated):
+        transformed_tensor, original_tensor = self._get_tuple(
+            key, NO_DEFAULT
+        ), self._source._get_tuple(key, NO_DEFAULT)
+        if transformed_tensor.data_ptr() != original_tensor.data_ptr():
+            raise RuntimeError(
+                f"{self} original tensor and transformed_in do not point to the "
+                f"same storage. Setting values in place is not currently "
+                f"supported in this setting, consider calling "
+                f"`td.clone()` before `td.set_at_(...)`"
+            )
+        if not validated:
+            value = self._validate_value(value, check_shape=False)
+
+        transformed_tensor[idx] = value
+        return self
+
+    def _stack_onto_(
+        self,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> TensorDictBase:
+        raise RuntimeError(
+            f"stacking tensordicts is not allowed for type {type(self)}"
+            f"consider calling 'to_tensordict()` first"
+        )
+
+    def __repr__(self) -> str:
+        custom_op_kwargs_str = ", ".join(
+            [f"{key}={value}" for key, value in self.custom_op_kwargs.items()]
+        )
+        indented_source = textwrap.indent(f"source={self._source}", "\t")
+        return (
+            f"{self.__class__.__name__}(\n{indented_source}, "
+            f"\n\top={self.custom_op}({custom_op_kwargs_str}))"
+        )
+
+    # @cache  # noqa: B019
+    def keys(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> _TensorDictKeysView:
+        return self._source.keys(include_nested=include_nested, leaves_only=leaves_only)
+
+    def select(
+        self, *keys: str, inplace: bool = False, strict: bool = True
+    ) -> _CustomOpTensorDict:
+        if inplace:
+            self._source.select(*keys, inplace=inplace, strict=strict)
+            return self
+        self_copy = copy(self)
+        self_copy._source = self_copy._source.select(*keys, strict=strict)
+        return self_copy
+
+    def exclude(self, *keys: str, inplace: bool = False) -> TensorDictBase:
+        if inplace:
+            return super().exclude(*keys, inplace=True)
+        return TensorDict(
+            {key: value.clone() for key, value in self.items()},
+            batch_size=self.batch_size,
+            device=self.device,
+            _run_checks=False,
+            _is_memmap=self.is_memmap(),
+            _is_shared=self.is_shared(),
+        ).exclude(*keys, inplace=True)
+
+    def clone(self, recurse: bool = True) -> TensorDictBase:
+        """Clones the Lazy TensorDict.
+
+        Args:
+            recurse (bool, optional): if ``True`` (default), a regular
+                :class:`TensorDict` instance will be returned.
+                Otherwise, another :class:`SubTensorDict` with identical content
+                will be returned.
+        """
+        if not recurse:
+            return type(self)(
+                source=self._source.clone(False),
+                custom_op=self.custom_op,
+                inv_op=self.inv_op,
+                custom_op_kwargs=self.custom_op_kwargs,
+                inv_op_kwargs=self.inv_op_kwargs,
+                batch_size=self.batch_size,
+            )
+        return self.to_tensordict()
+
+    def is_contiguous(self) -> bool:
+        return all([value.is_contiguous() for _, value in self.items()])
+
+    def contiguous(self) -> TensorDictBase:
+        if self.is_contiguous():
+            return self
+        return self.to(TensorDict)
+
+    def rename_key_(
+        self, old_key: str, new_key: str, safe: bool = False
+    ) -> _CustomOpTensorDict:
+        self._source.rename_key_(old_key, new_key, safe=safe)
+        return self
+
+    rename_key = _renamed_inplace_method(rename_key_)
+
+    @lock_blocked
+    def del_(self, key: NestedKey) -> _CustomOpTensorDict:
+        self._source = self._source.del_(key)
+        return self
+
+    def to(self, dest: DeviceType | type, **kwargs) -> TensorDictBase:
+        if isinstance(dest, type) and issubclass(dest, TensorDictBase):
+            if isinstance(self, dest):
+                return self
+            out = dest(source=self)
+            if self._td_dim_names is not None:
+                out.names = self._td_dim_names
+            return out
+        elif isinstance(dest, (torch.device, str, int)):
+            if self.device is not None and torch.device(dest) == self.device:
+                return self
+            td = self._source.to(dest, **kwargs)
+            self_copy = copy(self)
+            self_copy._source = td
+            return self_copy
+        elif dest is None:
+            return self
+        else:
+            raise NotImplementedError(
+                f"dest must be a string, torch.device or a TensorDict "
+                f"instance, {dest} not allowed"
+            )
+
+    def pin_memory(self) -> _CustomOpTensorDict:
+        self._source.pin_memory()
+        return self
+
+    def detach_(self) -> _CustomOpTensorDict:
+        self._source.detach_()
+        return self
+
+    def where(self, condition, other, *, out=None):
+        return self.to_tensordict().where(condition=condition, other=other, out=out)
+
+    def masked_fill_(self, mask: Tensor, value: float | bool) -> _CustomOpTensorDict:
+        for key, item in self.items():
+            val = self._source.get(key)
+            mask_exp = expand_right(
+                mask, list(mask.shape) + list(val.shape[self._source.batch_dims :])
+            )
+            mask_proc_inv = getattr(mask_exp, self.inv_op)(
+                **self._update_inv_op_kwargs(item)
+            )
+            val[mask_proc_inv] = value
+            self._source.set(key, val)
+        return self
+
+    def masked_fill(self, mask: Tensor, value: float | bool) -> TensorDictBase:
+        td_copy = self.clone()
+        return td_copy.masked_fill_(mask, value)
+
+    def memmap_(
+        self, prefix: str | None = None, copy_existing: bool = False
+    ) -> _CustomOpTensorDict:
+        self._source.memmap_(prefix=prefix, copy_existing=copy_existing)
+        if prefix is not None:
+            prefix = Path(prefix)
+            metadata = torch.load(prefix / "meta.pt")
+            metadata["custom_op"] = self.custom_op
+            metadata["inv_op"] = self.inv_op
+            metadata["custom_op_kwargs"] = self.custom_op_kwargs
+            metadata["inv_op_kwargs"] = self.inv_op_kwargs
+            torch.save(metadata, prefix / "meta.pt")
+
+        self._is_memmap = True
+        self.lock_()
+        return self
+
+    @classmethod
+    def load_memmap(cls, prefix: str) -> _CustomOpTensorDict:
+        prefix = Path(prefix)
+        source = TensorDict.load_memmap(prefix)
+        metadata = torch.load(prefix / "meta.pt")
+        return cls(
+            source,
+            custom_op=metadata["custom_op"],
+            inv_op=metadata["inv_op"],
+            custom_op_kwargs=metadata["custom_op_kwargs"],
+            inv_op_kwargs=metadata["inv_op_kwargs"],
+        )
+
+    def share_memory_(self) -> _CustomOpTensorDict:
+        self._source.share_memory_()
+        self._is_shared = True
+        self.lock_()
+        return self
+
+    @property
+    def _td_dim_names(self):
+        # we also want for _td_dim_names to be accurate
+        if self._source._td_dim_names is None:
+            return None
+        return self.names
+
+    @property
+    def is_locked(self) -> bool:
+        return self._source.is_locked
+
+    @is_locked.setter
+    def is_locked(self, value) -> bool:
+        if value:
+            self.lock_()
+        else:
+            self.unlock_()
+
+    @as_decorator("is_locked")
+    def lock_(self) -> TensorDictBase:
+        self._source.lock_()
+        return self
+
+    @erase_cache
+    @as_decorator("is_locked")
+    def unlock_(self) -> TensorDictBase:
+        self._source.unlock_()
+        return self
+
+    def _remove_lock(self, lock_id):
+        return self._source._remove_lock(lock_id)
+
+    @erase_cache
+    def _lock_propagate(self, lock_ids):
+        return self._source._lock_propagate(lock_ids)
+
+    lock = _renamed_inplace_method(lock_)
+    unlock = _renamed_inplace_method(unlock_)
+
+    def __del__(self):
+        pass
+
+    @property
+    def sorted_keys(self):
+        return self._source.sorted_keys
+
+
+class _UnsqueezedTensorDict(_CustomOpTensorDict):
+    """A lazy view on an unsqueezed TensorDict.
+
+    When calling `tensordict.unsqueeze(dim)`, a lazy view of this operation is
+    returned such that the following code snippet works without raising an
+    exception:
+
+        >>> assert tensordict.unsqueeze(dim).squeeze(dim) is tensordict
+
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> import torch
+        >>> td = TensorDict({'a': torch.randn(3, 4)}, batch_size=[3])
+        >>> td_unsqueeze = td.unsqueeze(-1)
+        >>> print(td_unsqueeze.shape)
+        torch.Size([3, 1])
+        >>> print(td_unsqueeze.squeeze(-1) is td)
+        True
+    """
+
+    def squeeze(self, dim: int | None) -> TensorDictBase:
+        if dim is not None and dim < 0:
+            dim = self.batch_dims + dim
+        if dim == self.custom_op_kwargs.get("dim"):
+            return self._source
+        return super().squeeze(dim)
+
+    def _stack_onto_(
+        self,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> TensorDictBase:
+        unsqueezed_dim = self.custom_op_kwargs["dim"]
+        diff_to_apply = 1 if dim < unsqueezed_dim else 0
+        list_item_unsqueeze = [
+            item.squeeze(unsqueezed_dim - diff_to_apply) for item in list_item
+        ]
+        return self._source._stack_onto_(list_item_unsqueeze, dim)
+
+    @property
+    def names(self):
+        names = copy(self._source.names)
+        dim = self.custom_op_kwargs.get("dim")
+        names.insert(dim, None)
+        return names
+
+    @names.setter
+    def names(self, value):
+        if value[: self.batch_dims] == self.names:
+            return
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
+
+class _SqueezedTensorDict(_CustomOpTensorDict):
+    """A lazy view on a squeezed TensorDict.
+
+    See the `UnsqueezedTensorDict` class documentation for more information.
+
+    """
+
+    def unsqueeze(self, dim: int) -> TensorDictBase:
+        if dim < 0:
+            dim = self.batch_dims + dim + 1
+        inv_op_dim = self.inv_op_kwargs.get("dim")
+        if inv_op_dim < 0:
+            inv_op_dim = self.batch_dims + inv_op_dim + 1
+        if dim == inv_op_dim:
+            return self._source
+        return super().unsqueeze(dim)
+
+    def _stack_onto_(
+        self,
+        # key: str,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> TensorDictBase:
+        squeezed_dim = self.custom_op_kwargs["dim"]
+        # dim=0, squeezed_dim=2, [3, 4, 5] [3, 4, 1, 5] [[4, 5], [4, 5], [4, 5]] => unsq 1
+        # dim=1, squeezed_dim=2, [3, 4, 5] [3, 4, 1, 5] [[3, 5], [3, 5], [3, 5], [3, 4]] => unsq 1
+        # dim=2, squeezed_dim=2, [3, 4, 5] [3, 4, 1, 5] [[3, 4], [3, 4], ...] => unsq 2
+        diff_to_apply = 1 if dim < squeezed_dim else 0
+        list_item_unsqueeze = [
+            item.unsqueeze(squeezed_dim - diff_to_apply) for item in list_item
+        ]
+        return self._source._stack_onto_(list_item_unsqueeze, dim)
+
+    @property
+    def names(self):
+        names = copy(self._source.names)
+        dim = self.custom_op_kwargs["dim"]
+        if self._source.batch_size[dim] == 1:
+            del names[dim]
+        return names
+
+    @names.setter
+    def names(self, value):
+        if value[: self.batch_dims] == self.names:
+            return
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
+
+class _ViewedTensorDict(_CustomOpTensorDict):
+    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
+        new_dim_list = list(self.custom_op_kwargs.get("size"))
+        new_dim_list += list(source_tensor.shape[self._source.batch_dims :])
+        new_dim = torch.Size(new_dim_list)
+        new_dict = deepcopy(self.custom_op_kwargs)
+        new_dict.update({"size": new_dim})
+        return new_dict
+
+    def _update_inv_op_kwargs(self, tensor: Tensor) -> dict:
+        size = list(self.inv_op_kwargs.get("size"))
+        size += list(_shape(tensor)[self.batch_dims :])
+        new_dim = torch.Size(size)
+        new_dict = deepcopy(self.inv_op_kwargs)
+        new_dict.update({"size": new_dim})
+        return new_dict
+
+    def view(
+        self, *shape: int, size: list | tuple | torch.Size | None = None
+    ) -> TensorDictBase:
+        if len(shape) == 0 and size is not None:
+            return self.view(*size)
+        elif len(shape) == 1 and isinstance(shape[0], (list, tuple, torch.Size)):
+            return self.view(*shape[0])
+        elif not isinstance(shape, torch.Size):
+            shape = infer_size_impl(shape, self.numel())
+            shape = torch.Size(shape)
+        if shape == self._source.batch_size:
+            return self._source
+        return super().view(*shape)
+
+    @property
+    def names(self):
+        return [None] * self.ndim
+
+    @names.setter
+    def names(self, value):
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
+
+class _TransposedTensorDict(_CustomOpTensorDict):
+    """A lazy view on a TensorDict with two batch dimensions transposed.
+
+    When calling `tensordict.permute(dims_list, dim)`, a lazy view of this operation is
+    returned such that the following code snippet works without raising an
+    exception:
+
+        >>> assert tensordict.transpose(dims_list, dim).transpose(dims_list, dim) is tensordict
+
+    """
+
+    def transpose(self, dim0, dim1) -> TensorDictBase:
+        if dim0 < 0:
+            dim0 = self.ndim + dim0
+        if dim1 < 0:
+            dim1 = self.ndim + dim1
+        if any((dim0 < 0, dim1 < 0)):
+            raise ValueError(
+                "The provided dimensions are incompatible with the tensordict batch-size."
+            )
+        if dim0 == dim1:
+            return self
+        dims = (self.inv_op_kwargs.get("dim0"), self.inv_op_kwargs.get("dim1"))
+        if dim0 in dims and dim1 in dims:
+            return self._source
+        return super().permute(dim0, dim1)
+
+    def add_missing_dims(
+        self, num_dims: int, batch_dims: tuple[int, ...]
+    ) -> tuple[int, ...]:
+        dim_diff = num_dims - len(batch_dims)
+        all_dims = list(range(num_dims))
+        for i, x in enumerate(batch_dims):
+            if x < 0:
+                x = x - dim_diff
+            all_dims[i] = x
+        return tuple(all_dims)
+
+    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
+        return self.custom_op_kwargs
+
+    def _update_inv_op_kwargs(self, tensor: Tensor) -> dict[str, Any]:
+        return self.custom_op_kwargs
+
+    def _stack_onto_(
+        self,
+        # key: str,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> TensorDictBase:
+        trsp = self.custom_op_kwargs["dim0"], self.custom_op_kwargs["dim1"]
+        if dim == trsp[0]:
+            dim = trsp[1]
+        elif dim == trsp[1]:
+            dim = trsp[0]
+
+        list_permuted_items = []
+        for item in list_item:
+            list_permuted_items.append(item.transpose(*trsp))
+        self._source._stack_onto_(list_permuted_items, dim)
+        return self
+
+    @property
+    def names(self):
+        names = copy(self._source.names)
+        dim0 = self.custom_op_kwargs["dim0"]
+        dim1 = self.custom_op_kwargs["dim1"]
+        names = [
+            names[dim0] if i == dim1 else names[dim1] if i == dim0 else name
+            for i, name in enumerate(names)
+        ]
+        return names
+
+    @names.setter
+    def names(self, value):
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
+
+class _PermutedTensorDict(_CustomOpTensorDict):
+    """A lazy view on a TensorDict with the batch dimensions permuted.
+
+    When calling `tensordict.permute(dims_list, dim)`, a lazy view of this operation is
+    returned such that the following code snippet works without raising an
+    exception:
+
+        >>> assert tensordict.permute(dims_list, dim).permute(dims_list, dim) is tensordict
+
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> import torch
+        >>> td = TensorDict({'a': torch.randn(4, 5, 6, 9)}, batch_size=[3])
+        >>> td_permute = td.permute(dims=(2, 1, 0))
+        >>> print(td_permute.shape)
+        torch.Size([6, 5, 4])
+        >>> print(td_permute.permute(dims=(2, 1, 0)) is td)
+        True
+
+    """
+
+    def permute(
+        self,
+        *dims_list: int,
+        dims: Sequence[int] | None = None,
+    ) -> TensorDictBase:
+        if len(dims_list) == 0:
+            dims_list = dims
+        elif len(dims_list) == 1 and not isinstance(dims_list[0], int):
+            dims_list = dims_list[0]
+        if len(dims_list) != len(self.shape):
+            raise RuntimeError(
+                f"number of dims don't match in permute (got {len(dims_list)}, expected {len(self.shape)}"
+            )
+        if not len(dims_list) and not self.batch_dims:
+            return self
+        if np.array_equal(dims_list, range(self.batch_dims)):
+            return self
+        if np.array_equal(np.argsort(dims_list), self.inv_op_kwargs.get("dims")):
+            return self._source
+        return super().permute(*dims_list)
+
+    def add_missing_dims(
+        self, num_dims: int, batch_dims: tuple[int, ...]
+    ) -> tuple[int, ...]:
+        # Adds the feature dimensions to the permute dims
+        dim_diff = num_dims - len(batch_dims)
+        all_dims = list(range(num_dims))
+        for i, x in enumerate(batch_dims):
+            if x < 0:
+                x = x - dim_diff
+            all_dims[i] = x
+        return tuple(all_dims)
+
+    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
+        new_dims = self.add_missing_dims(
+            len(source_tensor.shape), self.custom_op_kwargs["dims"]
+        )
+        kwargs = deepcopy(self.custom_op_kwargs)
+        kwargs.update({"dims": new_dims})
+        return kwargs
+
+    def _update_inv_op_kwargs(self, tensor: Tensor) -> dict[str, Any]:
+        new_dims = self.add_missing_dims(
+            self._source.batch_dims + len(_shape(tensor)[self.batch_dims :]),
+            self.custom_op_kwargs["dims"],
+        )
+        kwargs = deepcopy(self.custom_op_kwargs)
+        kwargs.update({"dims": tuple(np.argsort(new_dims))})
+        return kwargs
+
+    def _stack_onto_(
+        self,
+        # key: str,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> TensorDictBase:
+        permute_dims = self.custom_op_kwargs["dims"]
+        inv_permute_dims = np.argsort(permute_dims)
+        new_dim = [i for i, v in enumerate(inv_permute_dims) if v == dim][0]
+        inv_permute_dims = [p for p in inv_permute_dims if p != dim]
+        inv_permute_dims = np.argsort(np.argsort(inv_permute_dims))
+
+        list_permuted_items = []
+        for item in list_item:
+            perm = list(inv_permute_dims) + list(
+                range(self.batch_dims - 1, item.ndimension())
+            )
+            list_permuted_items.append(item.permute(*perm))
+        self._source._stack_onto_(list_permuted_items, new_dim)
+        return self
+
+    @property
+    def names(self):
+        names = copy(self._source.names)
+        return [names[i] for i in self.custom_op_kwargs["dims"]]
+
+    @names.setter
+    def names(self, value):
+        if value[: self.batch_dims] == self.names:
+            return
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
+
+def _get_repr(tensor: Tensor) -> str:
+    s = ", ".join(
+        [
+            f"shape={_shape(tensor)}",
+            f"device={_device(tensor)}",
+            f"dtype={_dtype(tensor)}",
+            f"is_shared={_is_shared(tensor)}",
+        ]
+    )
+    return f"{tensor.__class__.__name__}({s})"
+
+
+def _get_repr_custom(cls, shape, device, dtype, is_shared) -> str:
+    s = ", ".join(
+        [
+            f"shape={shape}",
+            f"device={device}",
+            f"dtype={dtype}",
+            f"is_shared={is_shared}",
+        ]
+    )
+    return f"{cls.__name__}({s})"
+
+
+def _make_repr(key: str, item: CompatibleType, tensordict: TensorDictBase) -> str:
+    if _is_tensor_collection(type(item)):
+        return f"{key}: {repr(tensordict.get(key))}"
+    return f"{key}: {_get_repr(item)}"
+
+
+def _td_fields(td: TensorDictBase, keys=None) -> str:
+    strs = []
+    if keys is None:
+        keys = td.keys()
+    for key in keys:
+        shape = td.get_item_shape(key)
+        if -1 not in shape:
+            item = td.get(key)
+            strs.append(_make_repr(key, item, td))
+        else:
+            # we know td is lazy stacked and the key is a leaf
+            # so we can get the shape and escape the error
+            temp_td = td
+            while isinstance(
+                temp_td, LazyStackedTensorDict
+            ):  # we need to grab the het tensor from the inner nesting level
+                temp_td = temp_td.tensordicts[0]
+            tensor = temp_td.get(key)
+            if isinstance(tensor, TensorDictBase):
+                substr = _td_fields(tensor)
+            else:
+                substr = _get_repr_custom(
+                    tensor.__class__,
+                    shape=shape,
+                    device=tensor.device,
+                    dtype=tensor.dtype,
+                    is_shared=tensor.is_shared(),
+                )
+            strs.append(f"{key}: {substr}")
+
+    return indent(
+        "\n" + ",\n".join(sorted(strs)),
+        4 * " ",
+    )
+
+
+def _check_keys(
+    list_of_tensordicts: Sequence[TensorDictBase],
+    strict: bool = False,
+    include_nested: bool = False,
+    leaves_only: bool = False,
+) -> set[str]:
+    if not len(list_of_tensordicts):
+        return set()
+    keys: set[str] = set(
+        list_of_tensordicts[0].keys(
+            include_nested=include_nested, leaves_only=leaves_only
+        )
+    )
+    for td in list_of_tensordicts[1:]:
+        k = td.keys(include_nested=include_nested, leaves_only=leaves_only)
+        if not strict:
+            keys = keys.intersection(k)
+        else:
+            if set(k) != keys:
+                raise KeyError(
+                    f"got keys {keys} and {set(td.keys())} which are " f"incompatible"
+                )
+    return keys
+
+
+def _expand_to_match_shape(
+    parent_batch_size: torch.Size,
+    tensor: Tensor,
+    self_batch_dims: int,
+    self_device: DeviceType,
+) -> Tensor | TensorDictBase:
+    if hasattr(tensor, "dtype"):
+        return torch.zeros(
+            (
+                *parent_batch_size,
+                *_shape(tensor)[self_batch_dims:],
+            ),
+            dtype=tensor.dtype,
+            device=self_device,
+        )
+    else:
+        # tensordict
+        out = TensorDict(
+            {},
+            [*parent_batch_size, *_shape(tensor)[self_batch_dims:]],
+            device=self_device,
+        )
+        return out
+
+
+def make_tensordict(
+    input_dict: dict[str, CompatibleType] | None = None,
+    batch_size: Sequence[int] | torch.Size | int | None = None,
+    device: DeviceType | None = None,
+    **kwargs: CompatibleType,  # source
+) -> TensorDict:
+    """Returns a TensorDict created from the keyword arguments or an input dictionary.
+
+    If ``batch_size`` is not specified, returns the maximum batch size possible.
+
+    This function works on nested dictionaries too, or can be used to determine the
+    batch-size of a nested tensordict.
+
+    Args:
+        input_dict (dictionary, optional): a dictionary to use as a data source
+            (nested keys compatible).
+        **kwargs (TensorDict or torch.Tensor): keyword arguments as data source
+            (incompatible with nested keys).
+        batch_size (iterable of int, optional): a batch size for the tensordict.
+        device (torch.device or compatible type, optional): a device for the TensorDict.
+
+    Examples:
+        >>> input_dict = {"a": torch.randn(3, 4), "b": torch.randn(3)}
+        >>> print(make_tensordict(input_dict))
+        TensorDict(
+            fields={
+                a: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                b: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+        >>> # alternatively
+        >>> td = make_tensordict(**input_dict)
+        >>> # nested dict: the nested TensorDict can have a different batch-size
+        >>> # as long as its leading dims match.
+        >>> input_dict = {"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}
+        >>> print(make_tensordict(input_dict))
+        TensorDict(
+            fields={
+                a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                b: TensorDict(
+                    fields={
+                        c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([3, 4]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+        >>> # we can also use this to work out the batch sie of a tensordict
+        >>> input_td = TensorDict({"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}, [])
+        >>> print(make_tensordict(input_td))
+        TensorDict(
+            fields={
+                a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                b: TensorDict(
+                    fields={
+                        c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([3, 4]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+    """
+    if input_dict is not None:
+        kwargs.update(input_dict)
+    return TensorDict.from_dict(kwargs, batch_size=batch_size, device=device)
+
+
+def dense_stack_tds(
+    td_list: Sequence[TensorDictBase] | LazyStackedTensorDict,
+    dim: int = None,
+) -> TensorDictBase:
+    """Densely stack a list of :class:`tensordict.TensorDictBase` objects (or a :class:`tensordict.LazyStackedTensorDict`) given that they have the same structure.
+
+    This function is called with a list of :class:`tensordict.TensorDictBase` (either passed directly or obtrained from
+    a :class:`tensordict.LazyStackedTensorDict`).
+    Instead of calling ``torch.stack(td_list)``, which would return a :class:`tensordict.LazyStackedTensorDict`,
+    this function expands the first element of the input list and stacks the input list onto that element.
+    This works only when all the elements of the input list have the same structure.
+    The :class:`tensordict.TensorDictBase` returned will have the same type of the elements of the input list.
+
+    This function is useful when some of the :class:`tensordict.TensorDictBase` objects that need to be stacked
+    are :class:`tensordict.LazyStackedTensorDict` or have :class:`tensordict.LazyStackedTensorDict`
+    among entries (or nested entries).
+    In those cases, calling ``torch.stack(td_list).to_tensordict()`` is infeasible.
+    Thus, this function provides an alternative for densely stacking the list provided.
+
+    Args:
+        td_list (List of TensorDictBase or LazyStackedTensorDict): the tds to stack.
+        dim (int, optional): the dimension to stack them.
+            If td_list is a LazyStackedTensorDict, it will be retrieved automatically.
+
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+        >>> from tensordict import dense_stack_tds
+        >>> from tensordict.tensordict import assert_allclose_td
+        >>> td0 = TensorDict({"a": torch.zeros(3)},[])
+        >>> td1 = TensorDict({"a": torch.zeros(4), "b": torch.zeros(2)},[])
+        >>> td_lazy = torch.stack([td0, td1], dim=0)
+        >>> td_container = TensorDict({"lazy": td_lazy}, [])
+        >>> td_container_clone = td_container.clone()
+        >>> td_stack = torch.stack([td_container, td_container_clone], dim=0)
+        >>> td_stack
+        LazyStackedTensorDict(
+            fields={
+                lazy: LazyStackedTensorDict(
+                    fields={
+                        a: Tensor(shape=torch.Size([2, 2, -1]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    exclusive_fields={
+                    },
+                    batch_size=torch.Size([2, 2]),
+                    device=None,
+                    is_shared=False,
+                    stack_dim=0)},
+            exclusive_fields={
+            },
+            batch_size=torch.Size([2]),
+            device=None,
+            is_shared=False,
+            stack_dim=0)
+        >>> td_stack = dense_stack_tds(td_stack) # Automatically use the LazyStackedTensorDict stack_dim
+        TensorDict(
+            fields={
+                lazy: LazyStackedTensorDict(
+                    fields={
+                        a: Tensor(shape=torch.Size([2, 2, -1]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    exclusive_fields={
+                        1 ->
+                            b: Tensor(shape=torch.Size([2, 2]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([2, 2]),
+                    device=None,
+                    is_shared=False,
+                    stack_dim=1)},
+            batch_size=torch.Size([2]),
+            device=None,
+            is_shared=False)
+        # Note that
+        # (1) td_stack is now a TensorDict
+        # (2) this has pushed the stack_dim of "lazy" (0 -> 1)
+        # (3) this has revealed the exclusive keys.
+        >>> assert_allclose_td(td_stack, dense_stack_tds([td_container, td_container_clone], dim=0))
+        # This shows it is the same to pass a list or a LazyStackedTensorDict
+
+    """
+    if isinstance(td_list, LazyStackedTensorDict):
+        dim = td_list.stack_dim
+        td_list = td_list.tensordicts
+    elif dim is None:
+        raise ValueError(
+            "If a list of tensordicts is provided, stack_dim must not be None"
+        )
+    shape = list(td_list[0].shape)
+    shape.insert(dim, len(td_list))
+
+    out = td_list[0].unsqueeze(dim).expand(shape).clone()
+    return torch.stack(td_list, dim=dim, out=out)
+
+
+def _set_max_batch_size(source: TensorDictBase, batch_dims=None):
+    """Updates a tensordict with its maximium batch size."""
+    tensor_data = list(source.values())
+
+    for val in tensor_data:
+        if _is_tensor_collection(val.__class__):
+            _set_max_batch_size(val, batch_dims=batch_dims)
+    batch_size = []
+    if not tensor_data:  # when source is empty
+        source.batch_size = batch_size
+        return
+    curr_dim = 0
+    while True:
+        if tensor_data[0].dim() > curr_dim:
+            curr_dim_size = tensor_data[0].size(curr_dim)
+        else:
+            source.batch_size = batch_size
+            return
+        for tensor in tensor_data[1:]:
+            if tensor.dim() <= curr_dim or tensor.size(curr_dim) != curr_dim_size:
+                source.batch_size = batch_size
+                return
+        if batch_dims is None or len(batch_size) < batch_dims:
+            batch_size.append(curr_dim_size)
+        curr_dim += 1
+
+
+def _iter_items_lazystack(
+    tensordict: LazyStackedTensorDict,
+) -> Iterator[tuple[str, CompatibleType]]:
+    return tensordict.items()
+
+
+def _clone_value(value: CompatibleType, recurse: bool) -> CompatibleType:
+    if recurse:
+        return value.clone()
+    elif _is_tensor_collection(value.__class__):
+        return value.clone(recurse=False)
+    else:
+        return value
+
+
+def _is_number(item):
+    if isinstance(item, Number):
+        return True
+    if isinstance(item, Tensor) and item.ndim == 0:
+        return True
+    if isinstance(item, np.ndarray) and item.ndim == 0:
+        return True
+    return False
+
+
+def _expand_index(index, batch_size):
+    len_index = sum(True for idx in index if idx is not None)
+    if len_index > len(batch_size):
+        raise ValueError
+    if len_index < len(batch_size):
+        index = index + (slice(None),) * (len(batch_size) - len_index)
+    return index
+
+
+def _broadcast_tensors(index):
+    # tensors and range need to be broadcast
+    tensors = {
+        i: tensor if isinstance(tensor, Tensor) else torch.tensor(tensor)
+        for i, tensor in enumerate(index)
+        if isinstance(tensor, (range, list, np.ndarray, torch.Tensor))
+    }
+    if tensors:
+        shape = torch.broadcast_shapes(*[tensor.shape for tensor in tensors.values()])
+        tensors = {i: tensor.expand(shape) for i, tensor in tensors.items()}
+        index = tuple(
+            idx if i not in tensors else tensors[i] for i, idx in enumerate(index)
+        )
+    return index
+
+
+def _reduce_index(index):
+    if all(
+        idx is Ellipsis or (isinstance(idx, slice) and idx == slice(None))
+        for idx in index
+    ):
+        index = ()
+    return index
+
+
+def _convert_index_lazystack(index, stack_dim, batch_size):
+    out = {
+        "remaining_index": None,
+        "stack_index": None,
+        "new_stack_dim": None,
+    }
+    index = convert_ellipsis_to_idx(index, batch_size)
+    if not isinstance(index, tuple):
+        index = (index,)
+    if any(
+        isinstance(idx, (MemmapTensor, Tensor)) and idx.dtype == torch.bool
+        for idx in index
+    ):
+        return None
+    index = _expand_index(index, batch_size)
+    index = _broadcast_tensors(index)
+    # find the index corresponding to the stack dim
+    stack_dim_index = 0
+    new_stack_dim = 0
+    count = -1
+    has_first_tensor = False
+    while True:
+        if index[stack_dim_index] is None:
+            stack_dim_index += 1
+            new_stack_dim += 1
+            continue
+
+        count += 1
+        if count == stack_dim:
+            remaining_index = tuple(
+                idx for i, idx in enumerate(index) if i != stack_dim_index
+            )
+            stack_index = index[stack_dim_index]
+            break
+        if isinstance(index[stack_dim_index], int) or (
+            isinstance(index[stack_dim_index], (Tensor, np.ndarray))
+            and (not index[stack_dim_index].ndim or has_first_tensor)
+        ):
+            new_stack_dim_incr = 0
+        else:
+            new_stack_dim_incr = 1
+            has_first_tensor = has_first_tensor or isinstance(
+                index[stack_dim_index], (Tensor, np.ndarray)
+            )
+        new_stack_dim += new_stack_dim_incr
+        stack_dim_index += 1
+
+    out["stack_index"] = stack_index
+    if not isinstance(stack_index, int):
+        out["new_stack_dim"] = new_stack_dim
+    if isinstance(stack_index, Tensor):
+        # we build one index for each resulting item
+        # all the tensors are indexed along the same integer
+        def _dispatch(remaining_index, stack_index, i=None):
+            if i is not None:
+                remaining_index = tuple(
+                    idx[i] if isinstance(idx, Tensor) else idx
+                    for idx in remaining_index
+                )
+            if isinstance(stack_index, list):
+                out = []
+                for j, _stack_index in enumerate(stack_index):
+                    out.append(_dispatch(remaining_index, _stack_index, j))
+                return tuple(out)
+            return remaining_index
+
+        remaining_index = _dispatch(remaining_index, stack_index.tolist())
+    out["remaining_index"] = _reduce_index(remaining_index)
+    return out
+
+
+@implements_for_td(torch.where)
+def where(condition, input, other, *, out=None):
+    """Return a ``TensorDict`` of elements selected from either input or other, depending on condition.
+
+    Args:
+        condition (BoolTensor): When ``True`` (nonzero), yield ``input``, otherwise yield ``other``.
+        input (TensorDictBase or Scalar): value (if ``input`` is a scalar) or values selected at indices where condition is ``True``.
+        other (TensorDictBase or Scalar): value (if ``other`` is a scalar) or values selected at indices where condition is ``False``.
+        out (Tensor, optional): the output ``TensorDictBase`` instance.
+
+    """
+    from tensordict.persistent import PersistentTensorDict
+
+    if isinstance(out, PersistentTensorDict):
+        raise RuntimeError(
+            "Cannot use a persistent tensordict as output of torch.where."
+        )
+    return input.where(condition, other, out=out)
```

## tensordict/utils.py

 * *Ordering differences only*

```diff
@@ -1,1245 +1,1245 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import dataclasses
-import inspect
-import math
-import time
-
-import warnings
-from collections import defaultdict
-from collections.abc import KeysView
-from copy import copy
-from functools import wraps
-from importlib import import_module
-from typing import Any, Callable, List, Sequence, Tuple, TYPE_CHECKING, Union
-
-import numpy as np
-import torch
-
-from packaging.version import parse
-from tensordict._tensordict import (  # noqa: F401
-    _unravel_key_to_tuple,  # noqa: F401
-    unravel_key,  # noqa: F401
-    unravel_key_list,  # noqa: F401
-    unravel_keys,  # noqa: F401
-)
-from torch import Tensor
-
-if TYPE_CHECKING:
-    from tensordict.memmap import MemmapTensor
-    from tensordict.tensordict import TensorDictBase
-
-try:
-    try:
-        from functorch._C import get_unwrapped, is_batchedtensor
-    except ImportError:
-        from torch._C._functorch import get_unwrapped, is_batchedtensor
-except ImportError:
-    pass
-
-try:
-    from torchrec import KeyedJaggedTensor
-
-    _has_torchrec = True
-except ImportError as err:
-    _has_torchrec = False
-
-    class KeyedJaggedTensor:  # noqa: D101, D102
-        pass
-
-    TORCHREC_ERR = str(err)
-
-
-IndexType = Union[None, int, slice, str, Tensor, List[Any], Tuple[Any, ...]]
-DeviceType = Union[torch.device, str, int]
-NestedKey = Union[str, Tuple[str, ...]]
-
-
-def _sub_index(tensor: torch.Tensor, idx: IndexType) -> torch.Tensor:
-    """Allows indexing of tensors with nested tuples.
-
-     >>> sub_tensor1 = tensor[tuple1][tuple2]
-     >>> sub_tensor2 = _sub_index(tensor, (tuple1, tuple2))
-     >>> assert torch.allclose(sub_tensor1, sub_tensor2)
-
-    Args:
-        tensor (torch.Tensor): tensor to be indexed.
-        idx (tuple of indices): indices sequence to be used.
-
-    """
-    if isinstance(idx, tuple) and len(idx) and isinstance(idx[0], tuple):
-        idx0 = idx[0]
-        idx1 = idx[1:]
-        return _sub_index(_sub_index(tensor, idx0), idx1)
-    return tensor[idx]
-
-
-def _getitem_batch_size(batch_size, index):
-    """Given an input shape and an index, returns the size of the resulting indexed tensor.
-
-    This function is aimed to be used when indexing is an
-    expensive operation.
-    Args:
-        shape (torch.Size): Input shape
-        items (index): Index of the hypothetical tensor
-
-    Returns:
-        Size of the resulting object (tensor or tensordict)
-
-    Examples:
-        >>> idx = (None, ..., None)
-        >>> torch.zeros(4, 3, 2, 1)[idx].shape
-        torch.Size([1, 4, 3, 2, 1, 1])
-        >>> _getitem_batch_size([4, 3, 2, 1], idx)
-        torch.Size([1, 4, 3, 2, 1, 1])
-    """
-    if not isinstance(index, tuple):
-        if isinstance(index, int):
-            return batch_size[1:]
-        if isinstance(index, slice) and index == slice(None):
-            return batch_size
-        index = (index,)
-    # index = convert_ellipsis_to_idx(index, batch_size)
-    # broadcast shapes
-    shapes_dict = {}
-    look_for_disjoint = False
-    disjoint = False
-    bools = []
-    for i, idx in enumerate(index):
-        boolean = False
-        if isinstance(idx, (range, list)):
-            shape = len(idx)
-        elif isinstance(idx, (torch.Tensor, np.ndarray)):
-            if idx.dtype == torch.bool or idx.dtype == np.dtype("bool"):
-                shape = torch.Size([idx.sum()])
-                boolean = True
-            else:
-                shape = idx.shape
-        elif isinstance(idx, slice):
-            look_for_disjoint = not disjoint and (len(shapes_dict) > 0)
-            shape = None
-        else:
-            shape = None
-        if shape is not None:
-            if look_for_disjoint:
-                disjoint = True
-            shapes_dict[i] = shape
-        bools.append(boolean)
-    bs_shape = None
-    if shapes_dict:
-        bs_shape = torch.broadcast_shapes(*shapes_dict.values())
-    out = []
-    count = -1
-    for i, idx in enumerate(index):
-        if idx is None:
-            out.append(1)
-            continue
-        count += 1 if not bools[i] else idx.ndim
-        if i in shapes_dict:
-            if bs_shape is not None:
-                if disjoint:
-                    # the indices will be put at the beginning
-                    out = list(bs_shape) + out
-                else:
-                    # if there is a single tensor or similar, we just extend
-                    out.extend(bs_shape)
-                bs_shape = None
-            continue
-        elif isinstance(idx, int):
-            # could be spared for efficiency
-            continue
-        elif isinstance(idx, slice):
-            batch = batch_size[count]
-            out.append(len(range(*idx.indices(batch))))
-    count += 1
-    if batch_size[count:]:
-        out.extend(batch_size[count:])
-    return torch.Size(out)
-
-
-# def _getitem_batch_size(shape: torch.Size, items: IndexType) -> torch.Size:
-#     """Given an input shape and an index, returns the size of the resulting indexed tensor.
-#
-#     This function is aimed to be used when indexing is an
-#     expensive operation.
-#     Args:
-#         shape (torch.Size): Input shape
-#         items (index): Index of the hypothetical tensor
-#
-#     Returns:
-#         Size of the resulting object (tensor or tensordict)
-#
-#     Examples:
-#         >>> idx = (None, ..., None)
-#         >>> torch.zeros(4, 3, 2, 1)[idx].shape
-#         torch.Size([1, 4, 3, 2, 1, 1])
-#         >>> _getitem_batch_size([4, 3, 2, 1], idx)
-#         torch.Size([1, 4, 3, 2, 1, 1])
-#     """
-#     # let's start with simple cases
-#     if isinstance(items, tuple) and len(items) == 1:
-#         items = items[0]
-#     if isinstance(items, int):
-#         return shape[1:]
-#     if isinstance(items, torch.Tensor) and items.dtype is torch.bool:
-#         return torch.Size([items.sum(), *shape[items.ndimension() :]])
-#     if (
-#         isinstance(items, (torch.Tensor, np.ndarray)) and len(items.shape) <= 1
-#     ) or isinstance(items, list):
-#         if isinstance(items, torch.Tensor) and not items.shape:
-#             return shape[1:]
-#         if _is_lis_of_list_of_bools(items):
-#             warnings.warn(
-#                 "Got a list of list of bools: this indexing behaviour will be deprecated soon.",
-#                 category=DeprecationWarning,
-#             )
-#             items = torch.tensor(items)
-#             return torch.Size([items.sum(), *shape[items.ndimension() :]])
-#         if len(items):
-#             return torch.Size([len(items), *shape[1:]])
-#         else:
-#             return shape[1:]
-#
-#     if not isinstance(items, tuple):
-#         items = (items,)
-#
-#     if any(item is Ellipsis for item in items):
-#         items = convert_ellipsis_to_idx(items, shape)
-#
-#     sanitized_items = []
-#     shapes = []
-#     for _item in items:
-#         if isinstance(_item, (list, range, )):
-#             # _item = torch.tensor(_item)
-#             shapes.append(torch.Size([len(_item)]))
-#         elif isinstance(_item, np.ndarray):
-#             shapes.append(torch.Size(np.shape))
-#         elif isinstance(_item, torch.Tensor):
-#             shapes.append(_item.shape)
-#         else:
-#             shapes.append(None)
-#         if isinstance(_item, torch.Tensor) and _item.dtype is torch.bool:
-#             # when using NumPy's advanced indexing patterns, any index containing a
-#             # boolean array can be equivalently replaced with index.nonzero()
-#             # note we add unbind(-1) since behaviour of numpy.ndarray.nonzero returns
-#             # tuples of arrays whereas torch.Tensor.nonzero returns a single tensor
-#             # https://numpy.org/doc/stable/user/basics.indexing.html#boolean-array-indexing
-#             sanitized_items.extend(_item.nonzero().unbind(-1))
-#         else:
-#             sanitized_items.append(_item)
-#
-#     # when multiple tensor-like indices are present, they must be broadcastable onto a
-#     # common shape. if this is satisfied then they are broadcast to that shape, and used
-#     # to extract diagonal entries of the array.
-#     # if the tensor indices are contiguous, or separated by scalars, they are replaced
-#     # in-place by the broadcast shape. if they are separated by non-scalar indices, the
-#     # broadcast shape is prepended to the new batch size
-#     # https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing
-#     tensor_indices = []
-#     contiguous, prev = True, None
-#     for i, (_item, _shape) in enumerate(zip(sanitized_items, shapes)):
-#         if isinstance(_item, (torch.Tensor, range, list, np.ndarray)):
-#             tensor_indices.append(_item)
-#             if prev is not None and i != prev + 1:
-#                 contiguous = False
-#             prev = i
-#         elif isinstance(_item, Number) and prev is not None and i == prev + 1:
-#             prev = i
-#
-#     bs = []
-#     if tensor_indices:
-#         try:
-#             b_shape = torch.broadcast_shapes(*[shape for shape in shapes if shape])
-#         except ValueError as err:
-#             raise ValueError(
-#                 "When indexing with tensor-like indices, each of those indices must be "
-#                 f"broadcastable to a common shape. Got indices: {tensor_indices}."
-#             ) from err
-#         if not contiguous:
-#             bs.extend(b_shape)
-#
-#     iter_bs = iter(shape)
-#
-#     cursor = -1
-#     for _item in sanitized_items:
-#         cursor += 1
-#         if isinstance(_item, slice):
-#             batch = next(iter_bs)
-#             bs.append(len(range(*_item.indices(batch))))
-#         elif isinstance(_item, range):
-#             batch = next(iter_bs)
-#             bs.append(min(batch, len(_item)))
-#         elif isinstance(_item, (list, torch.Tensor, np.ndarray)):
-#             batch = next(iter_bs)
-#             if b is not None:
-#                 # we haven't yet accounted for tensor indices, so we insert in-place
-#                 bs.extend(b.shape)
-#                 b = None
-#         elif _item is None:
-#             bs.append(1)
-#         elif isinstance(_item, Number):
-#             try:
-#                 batch = next(iter_bs)
-#             except StopIteration:
-#                 raise RuntimeError(
-#                     f"The shape {shape} is incompatible with " f"the index {items}."
-#                 )
-#             continue
-#         elif _item is Ellipsis:
-#             if cursor == len(sanitized_items) - 1:
-#                 # then we can just skip
-#                 continue
-#             n_upcoming = len(sanitized_items) - cursor - 1
-#             while cursor < len(shape) - n_upcoming:
-#                 batch = next(iter_bs)
-#                 bs.append(batch)
-#                 cursor += 1
-#         else:
-#             raise NotImplementedError(
-#                 f"batch dim cannot be computed for type {type(_item)}"
-#             )
-#
-#     list_iter_bs = list(iter_bs)
-#     bs += list_iter_bs
-#     return torch.Size(bs)
-
-
-def convert_ellipsis_to_idx(
-    idx: tuple[int | Ellipsis] | Ellipsis, batch_size: list[int]
-) -> tuple[int, ...]:
-    """Given an index containing an ellipsis or just an ellipsis, converts any ellipsis to slice(None).
-
-    Example:
-        >>> idx = (..., 0)
-        >>> batch_size = [1,2,3]
-        >>> new_index = convert_ellipsis_to_idx(idx, batch_size)
-        >>> print(new_index)
-        (slice(None, None, None), slice(None, None, None), 0)
-
-    Args:
-        idx (tuple, Ellipsis): Input index
-        batch_size (list): Shape of tensor to be indexed
-
-    Returns:
-        new_index (tuple): Output index
-    """
-    istuple = isinstance(idx, tuple)
-    if (not istuple and idx is not Ellipsis) or (
-        istuple and all(_idx is not Ellipsis for _idx in idx)
-    ):
-        return idx
-    new_index = ()
-    num_dims = len(batch_size)
-
-    if idx is Ellipsis:
-        idx = (...,)
-
-    num_ellipsis = sum(_idx is Ellipsis for _idx in idx)
-    if num_dims < (len(idx) - num_ellipsis - sum(item is None for item in idx)):
-        raise RuntimeError("Not enough dimensions in TensorDict for index provided.")
-
-    start_pos, after_ellipsis_length = None, 0
-    for i, item in enumerate(idx):
-        if item is Ellipsis:
-            if start_pos is not None:
-                raise RuntimeError("An index can only have one ellipsis at most.")
-            else:
-                start_pos = i
-        if item is not Ellipsis and start_pos is not None:
-            after_ellipsis_length += 1
-        if item is None:
-            # unsqueeze
-            num_dims += 1
-
-    before_ellipsis_length = start_pos
-    if start_pos is None:
-        return idx
-    else:
-        ellipsis_length = num_dims - after_ellipsis_length - before_ellipsis_length
-
-    new_index += idx[:start_pos]
-
-    ellipsis_start = start_pos
-    ellipsis_end = start_pos + ellipsis_length
-    new_index += (slice(None),) * (ellipsis_end - ellipsis_start)
-
-    new_index += idx[start_pos + 1 : start_pos + 1 + after_ellipsis_length]
-
-    if len(new_index) != num_dims:
-        raise RuntimeError(
-            f"The new index {new_index} is incompatible with the dimensions of the batch size {num_dims}."
-        )
-
-    return new_index
-
-
-def _copy(self: list[int]) -> list[int]:
-    return list(self)
-
-
-def infer_size_impl(shape: list[int], numel: int) -> list[int]:
-    """Infers the shape of an expanded tensor whose number of elements is indicated by :obj:`numel`.
-
-    Copied from pytorch for compatibility issues (See #386).
-    See https://github.com/pytorch/pytorch/blob/35d4fa444b67cbcbe34a862782ddf2d92f5b1ce7/torch/jit/_shape_functions.py
-    for the original copy.
-
-    """
-    newsize = 1
-    infer_dim: int | None = None
-    for dim in range(len(shape)):
-        if shape[dim] == -1:
-            if infer_dim is not None:
-                raise AssertionError("only one dimension can be inferred")
-            infer_dim = dim
-        elif shape[dim] >= 0:
-            newsize *= shape[dim]
-        else:
-            raise AssertionError("invalid shape dimensions")
-    if not (
-        numel == newsize
-        or (infer_dim is not None and newsize > 0 and numel % newsize == 0)
-    ):
-        raise AssertionError("invalid shape")
-    out = _copy(shape)
-    if infer_dim is not None:
-        out[infer_dim] = numel // newsize
-    return out
-
-
-def _unwrap_value(value: torch.Tensor) -> torch.Tensor:
-    # batch_dims = value.ndimension()
-    if not isinstance(value, torch.Tensor):
-        out = value
-    elif is_batchedtensor(value):
-        out = get_unwrapped(value)
-    else:
-        out = value
-    return out
-    # batch_dims = out.ndimension() - batch_dims
-    # batch_size = out.shape[:batch_dims]
-    # return out, batch_size
-
-
-if hasattr(math, "prod"):  # Python 3.8+
-
-    def prod(sequence):
-        """General prod function, that generalised usage across math and np.
-
-        Created for multiple python versions compatibility.
-
-        """
-        return math.prod(sequence)
-
-else:
-
-    def prod(sequence):
-        """General prod function, that generalised usage across math and np.
-
-        Created for multiple python versions compatibility.
-
-        """
-        return int(np.prod(sequence))
-
-
-def expand_as_right(
-    tensor: torch.Tensor | MemmapTensor | TensorDictBase,
-    dest: torch.Tensor | MemmapTensor | TensorDictBase,
-) -> torch.Tensor | MemmapTensor | TensorDictBase:
-    """Expand a tensor on the right to match another tensor shape.
-
-    Args:
-        tensor: tensor to be expanded
-        dest: tensor providing the target shape
-
-    Returns:
-         a tensor with shape matching the dest input tensor shape.
-
-    Examples:
-        >>> tensor = torch.zeros(3,4)
-        >>> dest = torch.zeros(3,4,5)
-        >>> print(expand_as_right(tensor, dest).shape)
-        torch.Size([3,4,5])
-
-    """
-    if dest.ndimension() < tensor.ndimension():
-        raise RuntimeError(
-            "expand_as_right requires the destination tensor to have less "
-            f"dimensions than the input tensor, got"
-            f" tensor.ndimension()={tensor.ndimension()} and "
-            f"dest.ndimension()={dest.ndimension()}"
-        )
-    if not (tensor.shape == dest.shape[: tensor.ndimension()]):
-        raise RuntimeError(
-            f"tensor shape is incompatible with dest shape, "
-            f"got: tensor.shape={tensor.shape}, dest={dest.shape}"
-        )
-    for _ in range(dest.ndimension() - tensor.ndimension()):
-        tensor = tensor.unsqueeze(-1)
-    return tensor.expand(dest.shape)
-
-
-def expand_right(
-    tensor: torch.Tensor | MemmapTensor, shape: Sequence[int]
-) -> torch.Tensor:
-    """Expand a tensor on the right to match a desired shape.
-
-    Args:
-        tensor: tensor to be expanded
-        shape: target shape
-
-    Returns:
-         a tensor with shape matching the target shape.
-
-    Examples:
-        >>> tensor = torch.zeros(3,4)
-        >>> shape = (3,4,5)
-        >>> print(expand_right(tensor, shape).shape)
-        torch.Size([3,4,5])
-
-    """
-    tensor_expand = tensor
-    while tensor_expand.ndimension() < len(shape):
-        tensor_expand = tensor_expand.unsqueeze(-1)
-    tensor_expand = tensor_expand.expand(*shape)
-    return tensor_expand
-
-
-NUMPY_TO_TORCH_DTYPE_DICT = {
-    np.dtype("bool"): torch.bool,
-    np.dtype("uint8"): torch.uint8,
-    np.dtype("int8"): torch.int8,
-    np.dtype("int16"): torch.int16,
-    np.dtype("int32"): torch.int32,
-    np.dtype("int64"): torch.int64,
-    np.dtype("float16"): torch.float16,
-    np.dtype("float32"): torch.float32,
-    np.dtype("float64"): torch.float64,
-    np.dtype("complex64"): torch.complex64,
-    np.dtype("complex128"): torch.complex128,
-}
-TORCH_TO_NUMPY_DTYPE_DICT = {
-    value: key for key, value in NUMPY_TO_TORCH_DTYPE_DICT.items()
-}
-
-
-def is_nested_key(key: NestedKey) -> bool:
-    """Returns True if key is a NestedKey."""
-    if isinstance(key, str):
-        return True
-    if key and isinstance(key, (list, tuple)):
-        return all(isinstance(subkey, str) for subkey in key)
-    return False
-
-
-def is_seq_of_nested_key(seq: Sequence[NestedKey]) -> bool:
-    """Returns True if seq is a Sequence[NestedKey]."""
-    if seq and isinstance(seq, Sequence):
-        return all(is_nested_key(k) for k in seq)
-    elif isinstance(seq, Sequence):
-        # we allow empty inputs
-        return True
-    return False
-
-
-def index_keyedjaggedtensor(
-    kjt: KeyedJaggedTensor, index: slice | range | list | torch.Tensor | np.ndarray
-) -> KeyedJaggedTensor:
-    """Indexes a KeyedJaggedTensor along the batch dimension.
-
-    Args:
-        kjt (KeyedJaggedTensor): a KeyedJaggedTensor to index
-        index (torch.Tensor or other indexing type): batch index to use.
-            Indexing with an integer will result in an error.
-
-    Examples:
-        >>> values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0])
-        >>> weights = torch.Tensor([1.0, 0.5, 1.5, 1.0, 0.5, 1.0, 1.0, 1.5, 1.0, 1.0, 1.0])
-        >>> keys = ["index_0", "index_1", "index_2"]
-        >>> offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8, 9, 10, 11])
-        >>>
-        >>> jag_tensor = KeyedJaggedTensor(
-        ...     values=values,
-        ...     keys=keys,
-        ...     offsets=offsets,
-        ...     weights=weights,
-        ... )
-        >>> ikjt = index_keyedjaggedtensor(jag_tensor, [0, 2])
-        >>> print(ikjt["index_0"].to_padded_dense(), j0.to_padded_dense())
-
-    """
-    if not _has_torchrec:
-        raise ImportError(TORCHREC_ERR)
-    if isinstance(index, (int,)):
-        raise ValueError(
-            "Indexing KeyedJaggedTensor instances with an integer is prohibited, "
-            "as this would result in a KeyedJaggedTensor without batch size. "
-            "If you want to get a single element from a KeyedJaggedTensor, "
-            "call `index_keyedjaggedtensor(kjt, torch.tensor([index]))` instead."
-        )
-    lengths = kjt.lengths()
-    keys = kjt.keys()
-    numel = len(lengths) // len(keys)
-    offsets = kjt.offsets()
-
-    _offsets1 = offsets[:-1].view(len(keys), numel)[:, index]
-    _offsets2 = offsets[1:].view(len(keys), numel)[:, index]
-    lengths = lengths.view(len(keys), numel)[:, index].reshape(-1)
-
-    full_index = torch.arange(offsets[-1]).view(1, 1, -1)
-    sel = (full_index >= _offsets1.unsqueeze(-1)) & (
-        full_index < _offsets2.unsqueeze(-1)
-    )
-    sel = sel.any(0).any(0)
-    full_index = full_index.squeeze()[sel]
-    values = kjt._values[full_index]
-    weights = kjt._weights[full_index]
-    return KeyedJaggedTensor(
-        values=values, keys=kjt.keys(), weights=weights, lengths=lengths
-    )
-
-
-def setitem_keyedjaggedtensor(
-    orig_tensor: KeyedJaggedTensor,
-    index: slice | range | list | torch.Tensor | np.ndarray,
-    other: KeyedJaggedTensor,
-) -> KeyedJaggedTensor:
-    """Equivalent of `tensor[index] = other` for KeyedJaggedTensors indexed along the batch dimension.
-
-    Args:
-        orig_tensor (torchrec.KeyedJaggedTensor): KeyedJaggedTensor to be updated.
-        index (list or equivalent index): batch index to be written.
-        other (torchrec.KeyedJaggedTensor): KeyedJaggedTensor to be written at
-            the batch locations.
-
-    Examples:
-        >>> values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0])
-        >>> weights = torch.Tensor([1.0, 0.5, 1.5, 1.0, 0.5, 1.0, 1.0, 1.5, 1.0, 1.0, 1.0])
-        >>> keys = ["index_0", "index_1", "index_2"]
-        >>> offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8, 9, 10, 11])
-        >>> jag_tensor = KeyedJaggedTensor(
-        ...    values=values,
-        ...    keys=keys,
-        ...    offsets=offsets,
-        ...    weights=weights,
-        ... )
-        >>> keys = ["index_0", "index_1", "index_2"]
-        >>> lengths2 = torch.IntTensor([2, 4, 6, 4, 2, 1])
-        >>> values2 = torch.zeros(
-        ...     lengths2.sum(),
-        ... )
-        >>> weights2 = -torch.ones(
-        ...     lengths2.sum(),
-        ... )
-        >>> sub_jag_tensor = KeyedJaggedTensor(
-        ...     values=values2,
-        ...     keys=keys,
-        ...     lengths=lengths2,
-        ...     weights=weights2,
-        ... )
-        >>> setitem_keyedjaggedtensor(jag_tensor, [0, 2], sub_jag_tensor)
-    """
-    #     if not _has_torchrec:
-    #         raise ImportError(TORCHREC_ERR)
-
-    orig_tensor_lengths = orig_tensor.lengths()
-    orig_tensor_keys = orig_tensor.keys()
-    orig_tensor_numel = len(orig_tensor_lengths) // len(orig_tensor_keys)
-    orig_tensor_offsets = orig_tensor.offsets()
-
-    other_lengths = other.lengths()
-    other_keys = other.keys()
-    other_numel = len(other_lengths) // len(other_keys)
-    # other_offsets = other.offsets()
-
-    if not other_keys == orig_tensor_keys:
-        raise KeyError("Mismatch in orig_tensor and other keys.")
-    #     if other_numel - len(index) != orig_tensor_numel:
-    #         raise RuntimeError("orig_tensor and otherination batch differ.")
-
-    _offsets1 = orig_tensor_offsets[:-1]
-    _offsets2 = orig_tensor_offsets[1:]
-    _orig_tensor_shape = len(orig_tensor_keys), orig_tensor_numel
-
-    _lengths_out = orig_tensor_lengths.view(_orig_tensor_shape).clone()
-    _lengths_out[:, index] = other_lengths.view(len(orig_tensor_keys), other_numel)
-    _lengths_out = _lengths_out.view(-1)
-
-    # get the values of orig_tensor that we'll be keeping
-    full_index = torch.arange(orig_tensor_offsets[-1]).view(1, 1, -1)
-    sel = (full_index >= _offsets1.view(_orig_tensor_shape)[:, index].unsqueeze(-1)) & (
-        full_index < _offsets2.view(_orig_tensor_shape)[:, index].unsqueeze(-1)
-    )
-    sel = (~sel).all(0).all(0)
-    index_to_keep = full_index.squeeze()[sel]
-    values_to_keep = orig_tensor._values[index_to_keep]
-    new_values = other._values
-    weights_to_keep = orig_tensor._weights[index_to_keep]
-    new_weights = other._weights
-
-    # compute new offsets
-    _offsets = torch.cat([_lengths_out[:1] * 0, _lengths_out], 0)
-    _offsets = _offsets.cumsum(0)
-
-    # get indices of offsets for new elts
-    _offsets1 = _offsets[:-1]
-    _offsets2 = _offsets[1:]
-    full_index = torch.arange(_offsets[-1]).view(1, 1, -1)
-    sel = (full_index >= _offsets1.view(_orig_tensor_shape)[:, index].unsqueeze(-1)) & (
-        full_index < _offsets2.view(_orig_tensor_shape)[:, index].unsqueeze(-1)
-    )
-    sel = sel.any(0).any(0)
-    new_index_new_elts = full_index.squeeze()[sel]
-    sel = (full_index >= _offsets1.view(_orig_tensor_shape)[:, index].unsqueeze(-1)) & (
-        full_index < _offsets2.view(_orig_tensor_shape)[:, index].unsqueeze(-1)
-    )
-    sel = (~sel).all(0).all(0)
-    new_index_to_keep = full_index.squeeze()[sel]
-
-    # create an empty values tensor
-    values_numel = values_to_keep.shape[0] + other._values.shape[0]
-    tensor = torch.empty(
-        [values_numel, *values_to_keep.shape[1:]],
-        dtype=values_to_keep.dtype,
-        device=values_to_keep.device,
-    )
-    tensor_weights = torch.empty(
-        [values_numel, *values_to_keep.shape[1:]],
-        dtype=weights_to_keep.dtype,
-        device=weights_to_keep.device,
-    )
-    tensor[new_index_to_keep] = values_to_keep
-    tensor[new_index_new_elts] = new_values
-    tensor_weights[new_index_to_keep] = weights_to_keep
-    tensor_weights[new_index_new_elts] = new_weights
-
-    kjt = KeyedJaggedTensor(
-        values=tensor,
-        keys=orig_tensor_keys,
-        weights=tensor_weights,
-        lengths=_lengths_out,
-    )
-    for k, item in kjt.__dict__.items():
-        orig_tensor.__dict__[k] = item
-    return orig_tensor
-
-
-def _ndimension(tensor: torch.Tensor) -> int:
-    if isinstance(tensor, torch.Tensor):
-        return tensor.ndimension()
-    elif isinstance(tensor, KeyedJaggedTensor):
-        return 1
-    else:
-        return tensor.ndimension()
-
-
-def _shape(tensor: torch.Tensor) -> torch.Size:
-    try:
-        return tensor.shape
-    except AttributeError as err:
-        if type(tensor) is KeyedJaggedTensor:
-            return torch.Size([len(tensor.lengths()) // len(tensor.keys())])
-        raise err
-
-
-def _device(tensor: torch.Tensor) -> torch.device:
-    if isinstance(tensor, torch.Tensor):
-        return tensor.device
-    elif isinstance(tensor, KeyedJaggedTensor):
-        return tensor.device()
-    else:
-        return tensor.device
-
-
-def _is_shared(tensor: torch.Tensor) -> bool:
-    if isinstance(tensor, torch.Tensor):
-        if torch._C._functorch.is_batchedtensor(tensor):
-            return None
-        return tensor.is_shared()
-    elif isinstance(tensor, KeyedJaggedTensor):
-        return False
-    else:
-        return tensor.is_shared()
-
-
-def _is_meta(tensor: torch.Tensor) -> bool:
-    if isinstance(tensor, torch.Tensor):
-        return tensor.is_meta
-    elif isinstance(tensor, KeyedJaggedTensor):
-        return False
-    else:
-        return tensor.is_meta
-
-
-def _dtype(tensor: torch.Tensor) -> torch.dtype:
-    if isinstance(tensor, torch.Tensor):
-        return tensor.dtype
-    elif isinstance(tensor, KeyedJaggedTensor):
-        return tensor._values.dtype
-    else:
-        return tensor.dtype
-
-
-def _get_item(tensor: torch.Tensor, index: IndexType) -> torch.Tensor:
-    if isinstance(tensor, torch.Tensor):
-        try:
-            return tensor[index]
-        except IndexError as err:
-            # try to map list index to tensor, and assess type. If bool, we
-            # likely have a nested list of booleans which is not supported by pytorch
-            if _is_lis_of_list_of_bools(index):
-                index = torch.tensor(index, device=tensor.device)
-                if index.dtype is torch.bool:
-                    warnings.warn(
-                        "Indexing a tensor with a nested list of boolean values is "
-                        "going to be deprecated as this functionality is not supported "
-                        f"by PyTorch. (follows error: {err})",
-                        category=DeprecationWarning,
-                    )
-                return tensor[index]
-            raise err
-    elif isinstance(tensor, KeyedJaggedTensor):
-        return index_keyedjaggedtensor(tensor, index)
-    else:
-        return tensor[index]
-
-
-def _set_item(
-    tensor: torch.Tensor, index: IndexType, value: torch.Tensor, *, validated
-) -> torch.Tensor:
-    # the tensor must be validated
-    if not validated:
-        raise RuntimeError
-    if isinstance(tensor, torch.Tensor):
-        tensor[index] = value
-        return tensor
-    elif isinstance(tensor, KeyedJaggedTensor):
-        tensor = setitem_keyedjaggedtensor(tensor, index, value)
-        return tensor
-    else:
-        tensor[index] = value
-        return tensor
-
-
-def _requires_grad(tensor: torch.Tensor) -> bool:
-    if isinstance(tensor, torch.Tensor):
-        return tensor.requires_grad
-    elif isinstance(tensor, KeyedJaggedTensor):
-        return tensor._values.requires_grad
-    else:
-        return tensor.requires_grad
-
-
-class timeit:
-    """A dirty but easy to use decorator for profiling code."""
-
-    _REG = {}
-
-    def __init__(self, name) -> None:
-        self.name = name
-
-    def __call__(self, fn):
-        @wraps(fn)
-        def decorated_fn(*args, **kwargs):
-            with self:
-                out = fn(*args, **kwargs)
-                return out
-
-        return decorated_fn
-
-    def __enter__(self):
-        self.t0 = time.time()
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        t = time.time() - self.t0
-        val = self._REG.setdefault(self.name, [0.0, 0.0, 0])
-
-        count = val[2]
-        N = count + 1
-        val[0] = val[0] * (count / N) + t / N
-        val[1] += t
-        val[2] = N
-
-    @staticmethod
-    def print(prefix=None):
-        keys = list(timeit._REG)
-        keys.sort()
-        for name in keys:
-            strings = []
-            if prefix:
-                strings.append(prefix)
-            strings.append(
-                f"{name} took {timeit._REG[name][0] * 1000:4.4} msec (total = {timeit._REG[name][1]} sec)"
-            )
-            print(" -- ".join(strings))
-
-    @staticmethod
-    def erase():
-        for k in timeit._REG:
-            timeit._REG[k] = [0.0, 0.0, 0]
-
-
-def int_generator(seed):
-    """A pseudo-random chaing generator.
-
-    To be used to produce deterministic integer sequences
-
-    Examples:
-        >>> for _ in range(2):
-        ...     init_int = 10
-        ...     for _ in range(10):
-        ...        init_int = int_generator(init_int)
-        ...        print(init_int, end=", ")
-        ...     print("")
-        6756, 1717, 4410, 9740, 9611, 9716, 5397, 7745, 4521, 7523,
-        6756, 1717, 4410, 9740, 9611, 9716, 5397, 7745, 4521, 7523,
-    """
-    max_seed_val = 10_000
-    rng = np.random.default_rng(seed)
-    seed = int.from_bytes(rng.bytes(8), "big")
-    return seed % max_seed_val
-
-
-def _is_lis_of_list_of_bools(index, first_level=True):
-    # determines if an index is a list of list of bools.
-    # this is aimed at catching a deprecation feature where list of list
-    # of bools are valid indices
-    if first_level:
-        if not isinstance(index, list):
-            return False
-        if not len(index):
-            return False
-        if isinstance(index[0], list):
-            return _is_lis_of_list_of_bools(index[0], False)
-        return False
-    # then we know it is a list of lists
-    if isinstance(index[0], bool):
-        return True
-    if isinstance(index[0], list):
-        return _is_lis_of_list_of_bools(index[0], False)
-    return False
-
-
-def is_tensorclass(obj: type | Any) -> bool:
-    """Returns True if obj is either a tensorclass or an instance of a tensorclass."""
-    cls = obj if isinstance(obj, type) else type(obj)
-    return _is_tensorclass(cls)
-
-
-def _is_tensorclass(cls) -> bool:
-    return (
-        dataclasses.is_dataclass(cls)
-        and "to_tensordict" in cls.__dict__
-        and "_from_tensordict" in cls.__dict__
-    )
-
-
-class implement_for:
-    """A version decorator that checks the version in the environment and implements a function with the fitting one.
-
-    If specified module is missing or there is no fitting implementation, call of the decorated function
-    will lead to the explicit error.
-    In case of intersected ranges, last fitting implementation is used.
-
-    Args:
-        module_name (str or callable): version is checked for the module with this
-            name (e.g. "gym"). If a callable is provided, it should return the
-            module.
-        from_version: version from which implementation is compatible. Can be open (None).
-        to_version: version from which implementation is no longer compatible. Can be open (None).
-
-    Examples:
-        >>> @implement_for("torch", None, "1.13")
-        >>> def fun(self, x):
-        ...     # Older torch versions will return x + 1
-        ...     return x + 1
-        ...
-        >>> @implement_for("torch", "0.13", "2.0")
-        >>> def fun(self, x):
-        ...     # More recent torch versions will return x + 2
-        ...     return x + 2
-        ...
-        >>> @implement_for(lambda: import_module("torch"), "0.", None)
-        >>> def fun(self, x):
-        ...     # More recent gym versions will return x + 2
-        ...     return x + 2
-        ...
-        >>> @implement_for("gymnasium", "0.27", None)
-        >>> def fun(self, x):
-        ...     # If gymnasium is to be used instead of gym, x+3 will be returned
-        ...     return x + 3
-        ...
-
-        This indicates that the function is compatible with gym 0.13+, but doesn't with gym 0.14+.
-    """
-
-    # Stores pointers to fitting implementations: dict[func_name] = func_pointer
-    _implementations = {}
-    _setters = []
-
-    def __init__(
-        self,
-        module_name: Union[str, Callable],
-        from_version: str = None,
-        to_version: str = None,
-    ):
-        self.module_name = module_name
-        self.from_version = from_version
-        self.to_version = to_version
-        implement_for._setters.append(self)
-
-    @staticmethod
-    def check_version(version, from_version, to_version):
-        return (from_version is None or parse(version) >= parse(from_version)) and (
-            to_version is None or parse(version) < parse(to_version)
-        )
-
-    @staticmethod
-    def get_class_that_defined_method(f):
-        """Returns the class of a method, if it is defined, and None otherwise."""
-        return f.__globals__.get(f.__qualname__.split(".")[0], None)
-
-    @property
-    def func_name(self):
-        return self.fn.__name__
-
-    def module_set(self):
-        """Sets the function in its module, if it exists already."""
-        cls = self.get_class_that_defined_method(self.fn)
-        if cls is None:
-            # class not yet defined
-            return
-        if cls.__class__.__name__ == "function":
-            cls = inspect.getmodule(self.fn)
-        setattr(cls, self.fn.__name__, self.fn)
-
-    @staticmethod
-    def import_module(module_name: Union[Callable, str]) -> str:
-        """Imports module and returns its version."""
-        if not callable(module_name):
-            module = import_module(module_name)
-        else:
-            module = module_name()
-        return module.__version__
-
-    def __call__(self, fn):
-        self.fn = fn
-
-        # If the module is missing replace the function with the mock.
-        func_name = self.func_name
-        implementations = implement_for._implementations
-
-        @wraps(fn)
-        def unsupported(*args, **kwargs):
-            raise ModuleNotFoundError(
-                f"Supported version of '{func_name}' has not been found."
-            )
-
-        do_set = False
-        # Return fitting implementation if it was encountered before.
-        if func_name in implementations:
-            try:
-                # check that backends don't conflict
-                version = self.import_module(self.module_name)
-                if self.check_version(version, self.from_version, self.to_version):
-                    do_set = True
-                if not do_set:
-                    return implementations[func_name]
-            except ModuleNotFoundError:
-                # then it's ok, there is no conflict
-                return implementations[func_name]
-        else:
-            try:
-                version = self.import_module(self.module_name)
-                if self.check_version(version, self.from_version, self.to_version):
-                    do_set = True
-            except ModuleNotFoundError:
-                return unsupported
-        if do_set:
-            implementations[func_name] = fn
-            self.module_set()
-            return fn
-        return unsupported
-
-    @classmethod
-    def reset(cls, setters=None):
-        if setters is None:
-            setters = copy(cls._setters)
-        cls._setters = []
-        cls._implementations = {}
-        for setter in setters:
-            setter(setter.fn)
-            cls._setters.append(setter)
-
-
-def _unfold_sequence(seq):
-    for item in seq:
-        if isinstance(item, (list, tuple)):
-            yield tuple(_unfold_sequence(item))
-        else:
-            if isinstance(item, (str, int, slice)) or item is Ellipsis:
-                yield item
-            else:
-                yield id(item)
-
-
-def _make_cache_key(args, kwargs):
-    """Creats a key for the cache such that memory footprint is minimized."""
-    return (
-        tuple(_unfold_sequence(args)),
-        tuple(_unfold_sequence(sorted(kwargs.items()))),
-    )
-
-
-def cache(fun):
-    """A cache for TensorDictBase subclasses.
-
-    This decorator will cache the values returned by a method as long as the
-    input arguments match.
-    Leaves (tensors and such) are not cached.
-    The cache is stored within the tensordict such that it can be erased at any
-    point in time.
-
-    Examples:
-        >>> import timeit
-        >>> from tensordict import TensorDict
-        >>> class SomeOtherTd(TensorDict):
-        ...     @cache
-        ...     def all_keys(self):
-        ...         return set(self.keys(include_nested=True))
-        >>> td = SomeOtherTd({("a", "b", "c", "d", "e", "f", "g"): 1.0}, [])
-        >>> td.lock_()
-        >>> print(timeit.timeit("set(td.keys(True))", globals={'td': td}))
-        11.057
-        >>> print(timeit.timeit("set(td.all_keys())", globals={'td': td}))
-        0.88
-    """
-    from tensordict.memmap import MemmapTensor
-
-    @wraps(fun)
-    def newfun(_self: "TensorDictBase", *args, **kwargs):
-        if not _self.is_locked:
-            return fun(_self, *args, **kwargs)
-        cache = _self._cache
-        if cache is None:
-            cache = _self._cache = defaultdict(dict)
-        cache = cache[fun.__name__]
-        key = _make_cache_key(args, kwargs)
-        if key not in cache:
-            out = fun(_self, *args, **kwargs)
-            if not isinstance(out, (Tensor, MemmapTensor, KeyedJaggedTensor)):
-                # we don't cache tensors to avoid filling the mem and / or
-                # stacking them from their origin
-                cache[key] = out
-        else:
-            out = cache[key]
-        return out
-
-    return newfun
-
-
-def erase_cache(fun):
-    """A decorator to erase the cache at each call."""
-
-    @wraps(fun)
-    def new_fun(self, *args, **kwargs):
-        self._erase_cache()
-        return fun(self, *args, **kwargs)
-
-    return new_fun
-
-
-_NON_STR_KEY_TUPLE_ERR = "Nested membership checks with tuples of strings is only supported when setting `include_nested=True`."
-_NON_STR_KEY_ERR = "TensorDict keys are always strings. Membership checks are only supported for strings or non-empty tuples of strings (for nested TensorDicts)"
-_GENERIC_NESTED_ERR = "Only NestedKeys are supported."
-
-
-class _StringKeys(KeysView):
-    """A key view where contains is restricted to strings."""
-
-    def __contains__(self, item):
-        if not isinstance(item, str):
-            try:
-                unravel_item = _unravel_key_to_tuple(item)
-                if not unravel_item:  # catch errors during unravel
-                    raise TypeError
-            except Exception:
-                raise TypeError(_NON_STR_KEY_ERR)
-            if len(unravel_item) > 1:
-                raise TypeError(_NON_STR_KEY_TUPLE_ERR)
-            else:
-                item = unravel_item[0]
-        return super().__contains__(item)
-
-
-class _StringOnlyDict(dict):
-    """A dict class where contains is restricted to strings."""
-
-    # kept here for debugging
-    # def __setitem__(self, key, value):
-    #     if not isinstance(key, str):
-    #         raise RuntimeError
-    #     return super().__setitem__(key, value)
-
-    def __contains__(self, item):
-        if not isinstance(item, str):
-            try:
-                unravel_item = _unravel_key_to_tuple(item)
-                if not unravel_item:  # catch errors during unravel
-                    raise TypeError
-            except Exception:
-                raise TypeError(_NON_STR_KEY_ERR)
-            if len(unravel_item) > 1:
-                raise TypeError(_NON_STR_KEY_TUPLE_ERR)
-            else:
-                item = unravel_item[0]
-        return super().__contains__(item)
-
-    def keys(self):
-        return _StringKeys(self)
-
-
-def lock_blocked(func):
-    """Checks that the tensordict is unlocked before executing a function."""
-
-    @wraps(func)
-    def new_func(self, *args, **kwargs):
-        if self.is_locked:
-            raise RuntimeError(self.LOCK_ERROR)
-        return func(self, *args, **kwargs)
-
-    return new_func
-
-
-class as_decorator:
-    """Converts a method to a decorator.
-
-    Examples:
-        >>> from tensordict import TensorDict
-        >>> data = TensorDict({}, [])
-        >>> with data.lock_(): # lock_ is decorated
-        ...     assert data.is_locked
-        >>> assert not data.is_locked
-    """
-
-    def __init__(self, attr):
-        self.attr = attr
-
-    def __call__(self, func):
-        @wraps(func)
-        def new_func(_self, *args, **kwargs):
-            _attr_pre = getattr(_self, self.attr)
-            out = func(_self, *args, **kwargs)
-            _attr_post = getattr(_self, self.attr)
-            if _attr_post is not _attr_pre:
-                _self._last_op = (new_func.__name__, (args, kwargs))
-            else:
-                _self._last_op = None
-            return out
-
-        return new_func
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import dataclasses
+import inspect
+import math
+import time
+
+import warnings
+from collections import defaultdict
+from collections.abc import KeysView
+from copy import copy
+from functools import wraps
+from importlib import import_module
+from typing import Any, Callable, List, Sequence, Tuple, TYPE_CHECKING, Union
+
+import numpy as np
+import torch
+
+from packaging.version import parse
+from tensordict._tensordict import (  # noqa: F401
+    _unravel_key_to_tuple,  # noqa: F401
+    unravel_key,  # noqa: F401
+    unravel_key_list,  # noqa: F401
+    unravel_keys,  # noqa: F401
+)
+from torch import Tensor
+
+if TYPE_CHECKING:
+    from tensordict.memmap import MemmapTensor
+    from tensordict.tensordict import TensorDictBase
+
+try:
+    try:
+        from functorch._C import get_unwrapped, is_batchedtensor
+    except ImportError:
+        from torch._C._functorch import get_unwrapped, is_batchedtensor
+except ImportError:
+    pass
+
+try:
+    from torchrec import KeyedJaggedTensor
+
+    _has_torchrec = True
+except ImportError as err:
+    _has_torchrec = False
+
+    class KeyedJaggedTensor:  # noqa: D101, D102
+        pass
+
+    TORCHREC_ERR = str(err)
+
+
+IndexType = Union[None, int, slice, str, Tensor, List[Any], Tuple[Any, ...]]
+DeviceType = Union[torch.device, str, int]
+NestedKey = Union[str, Tuple[str, ...]]
+
+
+def _sub_index(tensor: torch.Tensor, idx: IndexType) -> torch.Tensor:
+    """Allows indexing of tensors with nested tuples.
+
+     >>> sub_tensor1 = tensor[tuple1][tuple2]
+     >>> sub_tensor2 = _sub_index(tensor, (tuple1, tuple2))
+     >>> assert torch.allclose(sub_tensor1, sub_tensor2)
+
+    Args:
+        tensor (torch.Tensor): tensor to be indexed.
+        idx (tuple of indices): indices sequence to be used.
+
+    """
+    if isinstance(idx, tuple) and len(idx) and isinstance(idx[0], tuple):
+        idx0 = idx[0]
+        idx1 = idx[1:]
+        return _sub_index(_sub_index(tensor, idx0), idx1)
+    return tensor[idx]
+
+
+def _getitem_batch_size(batch_size, index):
+    """Given an input shape and an index, returns the size of the resulting indexed tensor.
+
+    This function is aimed to be used when indexing is an
+    expensive operation.
+    Args:
+        shape (torch.Size): Input shape
+        items (index): Index of the hypothetical tensor
+
+    Returns:
+        Size of the resulting object (tensor or tensordict)
+
+    Examples:
+        >>> idx = (None, ..., None)
+        >>> torch.zeros(4, 3, 2, 1)[idx].shape
+        torch.Size([1, 4, 3, 2, 1, 1])
+        >>> _getitem_batch_size([4, 3, 2, 1], idx)
+        torch.Size([1, 4, 3, 2, 1, 1])
+    """
+    if not isinstance(index, tuple):
+        if isinstance(index, int):
+            return batch_size[1:]
+        if isinstance(index, slice) and index == slice(None):
+            return batch_size
+        index = (index,)
+    # index = convert_ellipsis_to_idx(index, batch_size)
+    # broadcast shapes
+    shapes_dict = {}
+    look_for_disjoint = False
+    disjoint = False
+    bools = []
+    for i, idx in enumerate(index):
+        boolean = False
+        if isinstance(idx, (range, list)):
+            shape = len(idx)
+        elif isinstance(idx, (torch.Tensor, np.ndarray)):
+            if idx.dtype == torch.bool or idx.dtype == np.dtype("bool"):
+                shape = torch.Size([idx.sum()])
+                boolean = True
+            else:
+                shape = idx.shape
+        elif isinstance(idx, slice):
+            look_for_disjoint = not disjoint and (len(shapes_dict) > 0)
+            shape = None
+        else:
+            shape = None
+        if shape is not None:
+            if look_for_disjoint:
+                disjoint = True
+            shapes_dict[i] = shape
+        bools.append(boolean)
+    bs_shape = None
+    if shapes_dict:
+        bs_shape = torch.broadcast_shapes(*shapes_dict.values())
+    out = []
+    count = -1
+    for i, idx in enumerate(index):
+        if idx is None:
+            out.append(1)
+            continue
+        count += 1 if not bools[i] else idx.ndim
+        if i in shapes_dict:
+            if bs_shape is not None:
+                if disjoint:
+                    # the indices will be put at the beginning
+                    out = list(bs_shape) + out
+                else:
+                    # if there is a single tensor or similar, we just extend
+                    out.extend(bs_shape)
+                bs_shape = None
+            continue
+        elif isinstance(idx, int):
+            # could be spared for efficiency
+            continue
+        elif isinstance(idx, slice):
+            batch = batch_size[count]
+            out.append(len(range(*idx.indices(batch))))
+    count += 1
+    if batch_size[count:]:
+        out.extend(batch_size[count:])
+    return torch.Size(out)
+
+
+# def _getitem_batch_size(shape: torch.Size, items: IndexType) -> torch.Size:
+#     """Given an input shape and an index, returns the size of the resulting indexed tensor.
+#
+#     This function is aimed to be used when indexing is an
+#     expensive operation.
+#     Args:
+#         shape (torch.Size): Input shape
+#         items (index): Index of the hypothetical tensor
+#
+#     Returns:
+#         Size of the resulting object (tensor or tensordict)
+#
+#     Examples:
+#         >>> idx = (None, ..., None)
+#         >>> torch.zeros(4, 3, 2, 1)[idx].shape
+#         torch.Size([1, 4, 3, 2, 1, 1])
+#         >>> _getitem_batch_size([4, 3, 2, 1], idx)
+#         torch.Size([1, 4, 3, 2, 1, 1])
+#     """
+#     # let's start with simple cases
+#     if isinstance(items, tuple) and len(items) == 1:
+#         items = items[0]
+#     if isinstance(items, int):
+#         return shape[1:]
+#     if isinstance(items, torch.Tensor) and items.dtype is torch.bool:
+#         return torch.Size([items.sum(), *shape[items.ndimension() :]])
+#     if (
+#         isinstance(items, (torch.Tensor, np.ndarray)) and len(items.shape) <= 1
+#     ) or isinstance(items, list):
+#         if isinstance(items, torch.Tensor) and not items.shape:
+#             return shape[1:]
+#         if _is_lis_of_list_of_bools(items):
+#             warnings.warn(
+#                 "Got a list of list of bools: this indexing behaviour will be deprecated soon.",
+#                 category=DeprecationWarning,
+#             )
+#             items = torch.tensor(items)
+#             return torch.Size([items.sum(), *shape[items.ndimension() :]])
+#         if len(items):
+#             return torch.Size([len(items), *shape[1:]])
+#         else:
+#             return shape[1:]
+#
+#     if not isinstance(items, tuple):
+#         items = (items,)
+#
+#     if any(item is Ellipsis for item in items):
+#         items = convert_ellipsis_to_idx(items, shape)
+#
+#     sanitized_items = []
+#     shapes = []
+#     for _item in items:
+#         if isinstance(_item, (list, range, )):
+#             # _item = torch.tensor(_item)
+#             shapes.append(torch.Size([len(_item)]))
+#         elif isinstance(_item, np.ndarray):
+#             shapes.append(torch.Size(np.shape))
+#         elif isinstance(_item, torch.Tensor):
+#             shapes.append(_item.shape)
+#         else:
+#             shapes.append(None)
+#         if isinstance(_item, torch.Tensor) and _item.dtype is torch.bool:
+#             # when using NumPy's advanced indexing patterns, any index containing a
+#             # boolean array can be equivalently replaced with index.nonzero()
+#             # note we add unbind(-1) since behaviour of numpy.ndarray.nonzero returns
+#             # tuples of arrays whereas torch.Tensor.nonzero returns a single tensor
+#             # https://numpy.org/doc/stable/user/basics.indexing.html#boolean-array-indexing
+#             sanitized_items.extend(_item.nonzero().unbind(-1))
+#         else:
+#             sanitized_items.append(_item)
+#
+#     # when multiple tensor-like indices are present, they must be broadcastable onto a
+#     # common shape. if this is satisfied then they are broadcast to that shape, and used
+#     # to extract diagonal entries of the array.
+#     # if the tensor indices are contiguous, or separated by scalars, they are replaced
+#     # in-place by the broadcast shape. if they are separated by non-scalar indices, the
+#     # broadcast shape is prepended to the new batch size
+#     # https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing
+#     tensor_indices = []
+#     contiguous, prev = True, None
+#     for i, (_item, _shape) in enumerate(zip(sanitized_items, shapes)):
+#         if isinstance(_item, (torch.Tensor, range, list, np.ndarray)):
+#             tensor_indices.append(_item)
+#             if prev is not None and i != prev + 1:
+#                 contiguous = False
+#             prev = i
+#         elif isinstance(_item, Number) and prev is not None and i == prev + 1:
+#             prev = i
+#
+#     bs = []
+#     if tensor_indices:
+#         try:
+#             b_shape = torch.broadcast_shapes(*[shape for shape in shapes if shape])
+#         except ValueError as err:
+#             raise ValueError(
+#                 "When indexing with tensor-like indices, each of those indices must be "
+#                 f"broadcastable to a common shape. Got indices: {tensor_indices}."
+#             ) from err
+#         if not contiguous:
+#             bs.extend(b_shape)
+#
+#     iter_bs = iter(shape)
+#
+#     cursor = -1
+#     for _item in sanitized_items:
+#         cursor += 1
+#         if isinstance(_item, slice):
+#             batch = next(iter_bs)
+#             bs.append(len(range(*_item.indices(batch))))
+#         elif isinstance(_item, range):
+#             batch = next(iter_bs)
+#             bs.append(min(batch, len(_item)))
+#         elif isinstance(_item, (list, torch.Tensor, np.ndarray)):
+#             batch = next(iter_bs)
+#             if b is not None:
+#                 # we haven't yet accounted for tensor indices, so we insert in-place
+#                 bs.extend(b.shape)
+#                 b = None
+#         elif _item is None:
+#             bs.append(1)
+#         elif isinstance(_item, Number):
+#             try:
+#                 batch = next(iter_bs)
+#             except StopIteration:
+#                 raise RuntimeError(
+#                     f"The shape {shape} is incompatible with " f"the index {items}."
+#                 )
+#             continue
+#         elif _item is Ellipsis:
+#             if cursor == len(sanitized_items) - 1:
+#                 # then we can just skip
+#                 continue
+#             n_upcoming = len(sanitized_items) - cursor - 1
+#             while cursor < len(shape) - n_upcoming:
+#                 batch = next(iter_bs)
+#                 bs.append(batch)
+#                 cursor += 1
+#         else:
+#             raise NotImplementedError(
+#                 f"batch dim cannot be computed for type {type(_item)}"
+#             )
+#
+#     list_iter_bs = list(iter_bs)
+#     bs += list_iter_bs
+#     return torch.Size(bs)
+
+
+def convert_ellipsis_to_idx(
+    idx: tuple[int | Ellipsis] | Ellipsis, batch_size: list[int]
+) -> tuple[int, ...]:
+    """Given an index containing an ellipsis or just an ellipsis, converts any ellipsis to slice(None).
+
+    Example:
+        >>> idx = (..., 0)
+        >>> batch_size = [1,2,3]
+        >>> new_index = convert_ellipsis_to_idx(idx, batch_size)
+        >>> print(new_index)
+        (slice(None, None, None), slice(None, None, None), 0)
+
+    Args:
+        idx (tuple, Ellipsis): Input index
+        batch_size (list): Shape of tensor to be indexed
+
+    Returns:
+        new_index (tuple): Output index
+    """
+    istuple = isinstance(idx, tuple)
+    if (not istuple and idx is not Ellipsis) or (
+        istuple and all(_idx is not Ellipsis for _idx in idx)
+    ):
+        return idx
+    new_index = ()
+    num_dims = len(batch_size)
+
+    if idx is Ellipsis:
+        idx = (...,)
+
+    num_ellipsis = sum(_idx is Ellipsis for _idx in idx)
+    if num_dims < (len(idx) - num_ellipsis - sum(item is None for item in idx)):
+        raise RuntimeError("Not enough dimensions in TensorDict for index provided.")
+
+    start_pos, after_ellipsis_length = None, 0
+    for i, item in enumerate(idx):
+        if item is Ellipsis:
+            if start_pos is not None:
+                raise RuntimeError("An index can only have one ellipsis at most.")
+            else:
+                start_pos = i
+        if item is not Ellipsis and start_pos is not None:
+            after_ellipsis_length += 1
+        if item is None:
+            # unsqueeze
+            num_dims += 1
+
+    before_ellipsis_length = start_pos
+    if start_pos is None:
+        return idx
+    else:
+        ellipsis_length = num_dims - after_ellipsis_length - before_ellipsis_length
+
+    new_index += idx[:start_pos]
+
+    ellipsis_start = start_pos
+    ellipsis_end = start_pos + ellipsis_length
+    new_index += (slice(None),) * (ellipsis_end - ellipsis_start)
+
+    new_index += idx[start_pos + 1 : start_pos + 1 + after_ellipsis_length]
+
+    if len(new_index) != num_dims:
+        raise RuntimeError(
+            f"The new index {new_index} is incompatible with the dimensions of the batch size {num_dims}."
+        )
+
+    return new_index
+
+
+def _copy(self: list[int]) -> list[int]:
+    return list(self)
+
+
+def infer_size_impl(shape: list[int], numel: int) -> list[int]:
+    """Infers the shape of an expanded tensor whose number of elements is indicated by :obj:`numel`.
+
+    Copied from pytorch for compatibility issues (See #386).
+    See https://github.com/pytorch/pytorch/blob/35d4fa444b67cbcbe34a862782ddf2d92f5b1ce7/torch/jit/_shape_functions.py
+    for the original copy.
+
+    """
+    newsize = 1
+    infer_dim: int | None = None
+    for dim in range(len(shape)):
+        if shape[dim] == -1:
+            if infer_dim is not None:
+                raise AssertionError("only one dimension can be inferred")
+            infer_dim = dim
+        elif shape[dim] >= 0:
+            newsize *= shape[dim]
+        else:
+            raise AssertionError("invalid shape dimensions")
+    if not (
+        numel == newsize
+        or (infer_dim is not None and newsize > 0 and numel % newsize == 0)
+    ):
+        raise AssertionError("invalid shape")
+    out = _copy(shape)
+    if infer_dim is not None:
+        out[infer_dim] = numel // newsize
+    return out
+
+
+def _unwrap_value(value: torch.Tensor) -> torch.Tensor:
+    # batch_dims = value.ndimension()
+    if not isinstance(value, torch.Tensor):
+        out = value
+    elif is_batchedtensor(value):
+        out = get_unwrapped(value)
+    else:
+        out = value
+    return out
+    # batch_dims = out.ndimension() - batch_dims
+    # batch_size = out.shape[:batch_dims]
+    # return out, batch_size
+
+
+if hasattr(math, "prod"):  # Python 3.8+
+
+    def prod(sequence):
+        """General prod function, that generalised usage across math and np.
+
+        Created for multiple python versions compatibility.
+
+        """
+        return math.prod(sequence)
+
+else:
+
+    def prod(sequence):
+        """General prod function, that generalised usage across math and np.
+
+        Created for multiple python versions compatibility.
+
+        """
+        return int(np.prod(sequence))
+
+
+def expand_as_right(
+    tensor: torch.Tensor | MemmapTensor | TensorDictBase,
+    dest: torch.Tensor | MemmapTensor | TensorDictBase,
+) -> torch.Tensor | MemmapTensor | TensorDictBase:
+    """Expand a tensor on the right to match another tensor shape.
+
+    Args:
+        tensor: tensor to be expanded
+        dest: tensor providing the target shape
+
+    Returns:
+         a tensor with shape matching the dest input tensor shape.
+
+    Examples:
+        >>> tensor = torch.zeros(3,4)
+        >>> dest = torch.zeros(3,4,5)
+        >>> print(expand_as_right(tensor, dest).shape)
+        torch.Size([3,4,5])
+
+    """
+    if dest.ndimension() < tensor.ndimension():
+        raise RuntimeError(
+            "expand_as_right requires the destination tensor to have less "
+            f"dimensions than the input tensor, got"
+            f" tensor.ndimension()={tensor.ndimension()} and "
+            f"dest.ndimension()={dest.ndimension()}"
+        )
+    if not (tensor.shape == dest.shape[: tensor.ndimension()]):
+        raise RuntimeError(
+            f"tensor shape is incompatible with dest shape, "
+            f"got: tensor.shape={tensor.shape}, dest={dest.shape}"
+        )
+    for _ in range(dest.ndimension() - tensor.ndimension()):
+        tensor = tensor.unsqueeze(-1)
+    return tensor.expand(dest.shape)
+
+
+def expand_right(
+    tensor: torch.Tensor | MemmapTensor, shape: Sequence[int]
+) -> torch.Tensor:
+    """Expand a tensor on the right to match a desired shape.
+
+    Args:
+        tensor: tensor to be expanded
+        shape: target shape
+
+    Returns:
+         a tensor with shape matching the target shape.
+
+    Examples:
+        >>> tensor = torch.zeros(3,4)
+        >>> shape = (3,4,5)
+        >>> print(expand_right(tensor, shape).shape)
+        torch.Size([3,4,5])
+
+    """
+    tensor_expand = tensor
+    while tensor_expand.ndimension() < len(shape):
+        tensor_expand = tensor_expand.unsqueeze(-1)
+    tensor_expand = tensor_expand.expand(*shape)
+    return tensor_expand
+
+
+NUMPY_TO_TORCH_DTYPE_DICT = {
+    np.dtype("bool"): torch.bool,
+    np.dtype("uint8"): torch.uint8,
+    np.dtype("int8"): torch.int8,
+    np.dtype("int16"): torch.int16,
+    np.dtype("int32"): torch.int32,
+    np.dtype("int64"): torch.int64,
+    np.dtype("float16"): torch.float16,
+    np.dtype("float32"): torch.float32,
+    np.dtype("float64"): torch.float64,
+    np.dtype("complex64"): torch.complex64,
+    np.dtype("complex128"): torch.complex128,
+}
+TORCH_TO_NUMPY_DTYPE_DICT = {
+    value: key for key, value in NUMPY_TO_TORCH_DTYPE_DICT.items()
+}
+
+
+def is_nested_key(key: NestedKey) -> bool:
+    """Returns True if key is a NestedKey."""
+    if isinstance(key, str):
+        return True
+    if key and isinstance(key, (list, tuple)):
+        return all(isinstance(subkey, str) for subkey in key)
+    return False
+
+
+def is_seq_of_nested_key(seq: Sequence[NestedKey]) -> bool:
+    """Returns True if seq is a Sequence[NestedKey]."""
+    if seq and isinstance(seq, Sequence):
+        return all(is_nested_key(k) for k in seq)
+    elif isinstance(seq, Sequence):
+        # we allow empty inputs
+        return True
+    return False
+
+
+def index_keyedjaggedtensor(
+    kjt: KeyedJaggedTensor, index: slice | range | list | torch.Tensor | np.ndarray
+) -> KeyedJaggedTensor:
+    """Indexes a KeyedJaggedTensor along the batch dimension.
+
+    Args:
+        kjt (KeyedJaggedTensor): a KeyedJaggedTensor to index
+        index (torch.Tensor or other indexing type): batch index to use.
+            Indexing with an integer will result in an error.
+
+    Examples:
+        >>> values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0])
+        >>> weights = torch.Tensor([1.0, 0.5, 1.5, 1.0, 0.5, 1.0, 1.0, 1.5, 1.0, 1.0, 1.0])
+        >>> keys = ["index_0", "index_1", "index_2"]
+        >>> offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8, 9, 10, 11])
+        >>>
+        >>> jag_tensor = KeyedJaggedTensor(
+        ...     values=values,
+        ...     keys=keys,
+        ...     offsets=offsets,
+        ...     weights=weights,
+        ... )
+        >>> ikjt = index_keyedjaggedtensor(jag_tensor, [0, 2])
+        >>> print(ikjt["index_0"].to_padded_dense(), j0.to_padded_dense())
+
+    """
+    if not _has_torchrec:
+        raise ImportError(TORCHREC_ERR)
+    if isinstance(index, (int,)):
+        raise ValueError(
+            "Indexing KeyedJaggedTensor instances with an integer is prohibited, "
+            "as this would result in a KeyedJaggedTensor without batch size. "
+            "If you want to get a single element from a KeyedJaggedTensor, "
+            "call `index_keyedjaggedtensor(kjt, torch.tensor([index]))` instead."
+        )
+    lengths = kjt.lengths()
+    keys = kjt.keys()
+    numel = len(lengths) // len(keys)
+    offsets = kjt.offsets()
+
+    _offsets1 = offsets[:-1].view(len(keys), numel)[:, index]
+    _offsets2 = offsets[1:].view(len(keys), numel)[:, index]
+    lengths = lengths.view(len(keys), numel)[:, index].reshape(-1)
+
+    full_index = torch.arange(offsets[-1]).view(1, 1, -1)
+    sel = (full_index >= _offsets1.unsqueeze(-1)) & (
+        full_index < _offsets2.unsqueeze(-1)
+    )
+    sel = sel.any(0).any(0)
+    full_index = full_index.squeeze()[sel]
+    values = kjt._values[full_index]
+    weights = kjt._weights[full_index]
+    return KeyedJaggedTensor(
+        values=values, keys=kjt.keys(), weights=weights, lengths=lengths
+    )
+
+
+def setitem_keyedjaggedtensor(
+    orig_tensor: KeyedJaggedTensor,
+    index: slice | range | list | torch.Tensor | np.ndarray,
+    other: KeyedJaggedTensor,
+) -> KeyedJaggedTensor:
+    """Equivalent of `tensor[index] = other` for KeyedJaggedTensors indexed along the batch dimension.
+
+    Args:
+        orig_tensor (torchrec.KeyedJaggedTensor): KeyedJaggedTensor to be updated.
+        index (list or equivalent index): batch index to be written.
+        other (torchrec.KeyedJaggedTensor): KeyedJaggedTensor to be written at
+            the batch locations.
+
+    Examples:
+        >>> values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0])
+        >>> weights = torch.Tensor([1.0, 0.5, 1.5, 1.0, 0.5, 1.0, 1.0, 1.5, 1.0, 1.0, 1.0])
+        >>> keys = ["index_0", "index_1", "index_2"]
+        >>> offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8, 9, 10, 11])
+        >>> jag_tensor = KeyedJaggedTensor(
+        ...    values=values,
+        ...    keys=keys,
+        ...    offsets=offsets,
+        ...    weights=weights,
+        ... )
+        >>> keys = ["index_0", "index_1", "index_2"]
+        >>> lengths2 = torch.IntTensor([2, 4, 6, 4, 2, 1])
+        >>> values2 = torch.zeros(
+        ...     lengths2.sum(),
+        ... )
+        >>> weights2 = -torch.ones(
+        ...     lengths2.sum(),
+        ... )
+        >>> sub_jag_tensor = KeyedJaggedTensor(
+        ...     values=values2,
+        ...     keys=keys,
+        ...     lengths=lengths2,
+        ...     weights=weights2,
+        ... )
+        >>> setitem_keyedjaggedtensor(jag_tensor, [0, 2], sub_jag_tensor)
+    """
+    #     if not _has_torchrec:
+    #         raise ImportError(TORCHREC_ERR)
+
+    orig_tensor_lengths = orig_tensor.lengths()
+    orig_tensor_keys = orig_tensor.keys()
+    orig_tensor_numel = len(orig_tensor_lengths) // len(orig_tensor_keys)
+    orig_tensor_offsets = orig_tensor.offsets()
+
+    other_lengths = other.lengths()
+    other_keys = other.keys()
+    other_numel = len(other_lengths) // len(other_keys)
+    # other_offsets = other.offsets()
+
+    if not other_keys == orig_tensor_keys:
+        raise KeyError("Mismatch in orig_tensor and other keys.")
+    #     if other_numel - len(index) != orig_tensor_numel:
+    #         raise RuntimeError("orig_tensor and otherination batch differ.")
+
+    _offsets1 = orig_tensor_offsets[:-1]
+    _offsets2 = orig_tensor_offsets[1:]
+    _orig_tensor_shape = len(orig_tensor_keys), orig_tensor_numel
+
+    _lengths_out = orig_tensor_lengths.view(_orig_tensor_shape).clone()
+    _lengths_out[:, index] = other_lengths.view(len(orig_tensor_keys), other_numel)
+    _lengths_out = _lengths_out.view(-1)
+
+    # get the values of orig_tensor that we'll be keeping
+    full_index = torch.arange(orig_tensor_offsets[-1]).view(1, 1, -1)
+    sel = (full_index >= _offsets1.view(_orig_tensor_shape)[:, index].unsqueeze(-1)) & (
+        full_index < _offsets2.view(_orig_tensor_shape)[:, index].unsqueeze(-1)
+    )
+    sel = (~sel).all(0).all(0)
+    index_to_keep = full_index.squeeze()[sel]
+    values_to_keep = orig_tensor._values[index_to_keep]
+    new_values = other._values
+    weights_to_keep = orig_tensor._weights[index_to_keep]
+    new_weights = other._weights
+
+    # compute new offsets
+    _offsets = torch.cat([_lengths_out[:1] * 0, _lengths_out], 0)
+    _offsets = _offsets.cumsum(0)
+
+    # get indices of offsets for new elts
+    _offsets1 = _offsets[:-1]
+    _offsets2 = _offsets[1:]
+    full_index = torch.arange(_offsets[-1]).view(1, 1, -1)
+    sel = (full_index >= _offsets1.view(_orig_tensor_shape)[:, index].unsqueeze(-1)) & (
+        full_index < _offsets2.view(_orig_tensor_shape)[:, index].unsqueeze(-1)
+    )
+    sel = sel.any(0).any(0)
+    new_index_new_elts = full_index.squeeze()[sel]
+    sel = (full_index >= _offsets1.view(_orig_tensor_shape)[:, index].unsqueeze(-1)) & (
+        full_index < _offsets2.view(_orig_tensor_shape)[:, index].unsqueeze(-1)
+    )
+    sel = (~sel).all(0).all(0)
+    new_index_to_keep = full_index.squeeze()[sel]
+
+    # create an empty values tensor
+    values_numel = values_to_keep.shape[0] + other._values.shape[0]
+    tensor = torch.empty(
+        [values_numel, *values_to_keep.shape[1:]],
+        dtype=values_to_keep.dtype,
+        device=values_to_keep.device,
+    )
+    tensor_weights = torch.empty(
+        [values_numel, *values_to_keep.shape[1:]],
+        dtype=weights_to_keep.dtype,
+        device=weights_to_keep.device,
+    )
+    tensor[new_index_to_keep] = values_to_keep
+    tensor[new_index_new_elts] = new_values
+    tensor_weights[new_index_to_keep] = weights_to_keep
+    tensor_weights[new_index_new_elts] = new_weights
+
+    kjt = KeyedJaggedTensor(
+        values=tensor,
+        keys=orig_tensor_keys,
+        weights=tensor_weights,
+        lengths=_lengths_out,
+    )
+    for k, item in kjt.__dict__.items():
+        orig_tensor.__dict__[k] = item
+    return orig_tensor
+
+
+def _ndimension(tensor: torch.Tensor) -> int:
+    if isinstance(tensor, torch.Tensor):
+        return tensor.ndimension()
+    elif isinstance(tensor, KeyedJaggedTensor):
+        return 1
+    else:
+        return tensor.ndimension()
+
+
+def _shape(tensor: torch.Tensor) -> torch.Size:
+    try:
+        return tensor.shape
+    except AttributeError as err:
+        if type(tensor) is KeyedJaggedTensor:
+            return torch.Size([len(tensor.lengths()) // len(tensor.keys())])
+        raise err
+
+
+def _device(tensor: torch.Tensor) -> torch.device:
+    if isinstance(tensor, torch.Tensor):
+        return tensor.device
+    elif isinstance(tensor, KeyedJaggedTensor):
+        return tensor.device()
+    else:
+        return tensor.device
+
+
+def _is_shared(tensor: torch.Tensor) -> bool:
+    if isinstance(tensor, torch.Tensor):
+        if torch._C._functorch.is_batchedtensor(tensor):
+            return None
+        return tensor.is_shared()
+    elif isinstance(tensor, KeyedJaggedTensor):
+        return False
+    else:
+        return tensor.is_shared()
+
+
+def _is_meta(tensor: torch.Tensor) -> bool:
+    if isinstance(tensor, torch.Tensor):
+        return tensor.is_meta
+    elif isinstance(tensor, KeyedJaggedTensor):
+        return False
+    else:
+        return tensor.is_meta
+
+
+def _dtype(tensor: torch.Tensor) -> torch.dtype:
+    if isinstance(tensor, torch.Tensor):
+        return tensor.dtype
+    elif isinstance(tensor, KeyedJaggedTensor):
+        return tensor._values.dtype
+    else:
+        return tensor.dtype
+
+
+def _get_item(tensor: torch.Tensor, index: IndexType) -> torch.Tensor:
+    if isinstance(tensor, torch.Tensor):
+        try:
+            return tensor[index]
+        except IndexError as err:
+            # try to map list index to tensor, and assess type. If bool, we
+            # likely have a nested list of booleans which is not supported by pytorch
+            if _is_lis_of_list_of_bools(index):
+                index = torch.tensor(index, device=tensor.device)
+                if index.dtype is torch.bool:
+                    warnings.warn(
+                        "Indexing a tensor with a nested list of boolean values is "
+                        "going to be deprecated as this functionality is not supported "
+                        f"by PyTorch. (follows error: {err})",
+                        category=DeprecationWarning,
+                    )
+                return tensor[index]
+            raise err
+    elif isinstance(tensor, KeyedJaggedTensor):
+        return index_keyedjaggedtensor(tensor, index)
+    else:
+        return tensor[index]
+
+
+def _set_item(
+    tensor: torch.Tensor, index: IndexType, value: torch.Tensor, *, validated
+) -> torch.Tensor:
+    # the tensor must be validated
+    if not validated:
+        raise RuntimeError
+    if isinstance(tensor, torch.Tensor):
+        tensor[index] = value
+        return tensor
+    elif isinstance(tensor, KeyedJaggedTensor):
+        tensor = setitem_keyedjaggedtensor(tensor, index, value)
+        return tensor
+    else:
+        tensor[index] = value
+        return tensor
+
+
+def _requires_grad(tensor: torch.Tensor) -> bool:
+    if isinstance(tensor, torch.Tensor):
+        return tensor.requires_grad
+    elif isinstance(tensor, KeyedJaggedTensor):
+        return tensor._values.requires_grad
+    else:
+        return tensor.requires_grad
+
+
+class timeit:
+    """A dirty but easy to use decorator for profiling code."""
+
+    _REG = {}
+
+    def __init__(self, name) -> None:
+        self.name = name
+
+    def __call__(self, fn):
+        @wraps(fn)
+        def decorated_fn(*args, **kwargs):
+            with self:
+                out = fn(*args, **kwargs)
+                return out
+
+        return decorated_fn
+
+    def __enter__(self):
+        self.t0 = time.time()
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        t = time.time() - self.t0
+        val = self._REG.setdefault(self.name, [0.0, 0.0, 0])
+
+        count = val[2]
+        N = count + 1
+        val[0] = val[0] * (count / N) + t / N
+        val[1] += t
+        val[2] = N
+
+    @staticmethod
+    def print(prefix=None):
+        keys = list(timeit._REG)
+        keys.sort()
+        for name in keys:
+            strings = []
+            if prefix:
+                strings.append(prefix)
+            strings.append(
+                f"{name} took {timeit._REG[name][0] * 1000:4.4} msec (total = {timeit._REG[name][1]} sec)"
+            )
+            print(" -- ".join(strings))
+
+    @staticmethod
+    def erase():
+        for k in timeit._REG:
+            timeit._REG[k] = [0.0, 0.0, 0]
+
+
+def int_generator(seed):
+    """A pseudo-random chaing generator.
+
+    To be used to produce deterministic integer sequences
+
+    Examples:
+        >>> for _ in range(2):
+        ...     init_int = 10
+        ...     for _ in range(10):
+        ...        init_int = int_generator(init_int)
+        ...        print(init_int, end=", ")
+        ...     print("")
+        6756, 1717, 4410, 9740, 9611, 9716, 5397, 7745, 4521, 7523,
+        6756, 1717, 4410, 9740, 9611, 9716, 5397, 7745, 4521, 7523,
+    """
+    max_seed_val = 10_000
+    rng = np.random.default_rng(seed)
+    seed = int.from_bytes(rng.bytes(8), "big")
+    return seed % max_seed_val
+
+
+def _is_lis_of_list_of_bools(index, first_level=True):
+    # determines if an index is a list of list of bools.
+    # this is aimed at catching a deprecation feature where list of list
+    # of bools are valid indices
+    if first_level:
+        if not isinstance(index, list):
+            return False
+        if not len(index):
+            return False
+        if isinstance(index[0], list):
+            return _is_lis_of_list_of_bools(index[0], False)
+        return False
+    # then we know it is a list of lists
+    if isinstance(index[0], bool):
+        return True
+    if isinstance(index[0], list):
+        return _is_lis_of_list_of_bools(index[0], False)
+    return False
+
+
+def is_tensorclass(obj: type | Any) -> bool:
+    """Returns True if obj is either a tensorclass or an instance of a tensorclass."""
+    cls = obj if isinstance(obj, type) else type(obj)
+    return _is_tensorclass(cls)
+
+
+def _is_tensorclass(cls) -> bool:
+    return (
+        dataclasses.is_dataclass(cls)
+        and "to_tensordict" in cls.__dict__
+        and "_from_tensordict" in cls.__dict__
+    )
+
+
+class implement_for:
+    """A version decorator that checks the version in the environment and implements a function with the fitting one.
+
+    If specified module is missing or there is no fitting implementation, call of the decorated function
+    will lead to the explicit error.
+    In case of intersected ranges, last fitting implementation is used.
+
+    Args:
+        module_name (str or callable): version is checked for the module with this
+            name (e.g. "gym"). If a callable is provided, it should return the
+            module.
+        from_version: version from which implementation is compatible. Can be open (None).
+        to_version: version from which implementation is no longer compatible. Can be open (None).
+
+    Examples:
+        >>> @implement_for("torch", None, "1.13")
+        >>> def fun(self, x):
+        ...     # Older torch versions will return x + 1
+        ...     return x + 1
+        ...
+        >>> @implement_for("torch", "0.13", "2.0")
+        >>> def fun(self, x):
+        ...     # More recent torch versions will return x + 2
+        ...     return x + 2
+        ...
+        >>> @implement_for(lambda: import_module("torch"), "0.", None)
+        >>> def fun(self, x):
+        ...     # More recent gym versions will return x + 2
+        ...     return x + 2
+        ...
+        >>> @implement_for("gymnasium", "0.27", None)
+        >>> def fun(self, x):
+        ...     # If gymnasium is to be used instead of gym, x+3 will be returned
+        ...     return x + 3
+        ...
+
+        This indicates that the function is compatible with gym 0.13+, but doesn't with gym 0.14+.
+    """
+
+    # Stores pointers to fitting implementations: dict[func_name] = func_pointer
+    _implementations = {}
+    _setters = []
+
+    def __init__(
+        self,
+        module_name: Union[str, Callable],
+        from_version: str = None,
+        to_version: str = None,
+    ):
+        self.module_name = module_name
+        self.from_version = from_version
+        self.to_version = to_version
+        implement_for._setters.append(self)
+
+    @staticmethod
+    def check_version(version, from_version, to_version):
+        return (from_version is None or parse(version) >= parse(from_version)) and (
+            to_version is None or parse(version) < parse(to_version)
+        )
+
+    @staticmethod
+    def get_class_that_defined_method(f):
+        """Returns the class of a method, if it is defined, and None otherwise."""
+        return f.__globals__.get(f.__qualname__.split(".")[0], None)
+
+    @property
+    def func_name(self):
+        return self.fn.__name__
+
+    def module_set(self):
+        """Sets the function in its module, if it exists already."""
+        cls = self.get_class_that_defined_method(self.fn)
+        if cls is None:
+            # class not yet defined
+            return
+        if cls.__class__.__name__ == "function":
+            cls = inspect.getmodule(self.fn)
+        setattr(cls, self.fn.__name__, self.fn)
+
+    @staticmethod
+    def import_module(module_name: Union[Callable, str]) -> str:
+        """Imports module and returns its version."""
+        if not callable(module_name):
+            module = import_module(module_name)
+        else:
+            module = module_name()
+        return module.__version__
+
+    def __call__(self, fn):
+        self.fn = fn
+
+        # If the module is missing replace the function with the mock.
+        func_name = self.func_name
+        implementations = implement_for._implementations
+
+        @wraps(fn)
+        def unsupported(*args, **kwargs):
+            raise ModuleNotFoundError(
+                f"Supported version of '{func_name}' has not been found."
+            )
+
+        do_set = False
+        # Return fitting implementation if it was encountered before.
+        if func_name in implementations:
+            try:
+                # check that backends don't conflict
+                version = self.import_module(self.module_name)
+                if self.check_version(version, self.from_version, self.to_version):
+                    do_set = True
+                if not do_set:
+                    return implementations[func_name]
+            except ModuleNotFoundError:
+                # then it's ok, there is no conflict
+                return implementations[func_name]
+        else:
+            try:
+                version = self.import_module(self.module_name)
+                if self.check_version(version, self.from_version, self.to_version):
+                    do_set = True
+            except ModuleNotFoundError:
+                return unsupported
+        if do_set:
+            implementations[func_name] = fn
+            self.module_set()
+            return fn
+        return unsupported
+
+    @classmethod
+    def reset(cls, setters=None):
+        if setters is None:
+            setters = copy(cls._setters)
+        cls._setters = []
+        cls._implementations = {}
+        for setter in setters:
+            setter(setter.fn)
+            cls._setters.append(setter)
+
+
+def _unfold_sequence(seq):
+    for item in seq:
+        if isinstance(item, (list, tuple)):
+            yield tuple(_unfold_sequence(item))
+        else:
+            if isinstance(item, (str, int, slice)) or item is Ellipsis:
+                yield item
+            else:
+                yield id(item)
+
+
+def _make_cache_key(args, kwargs):
+    """Creats a key for the cache such that memory footprint is minimized."""
+    return (
+        tuple(_unfold_sequence(args)),
+        tuple(_unfold_sequence(sorted(kwargs.items()))),
+    )
+
+
+def cache(fun):
+    """A cache for TensorDictBase subclasses.
+
+    This decorator will cache the values returned by a method as long as the
+    input arguments match.
+    Leaves (tensors and such) are not cached.
+    The cache is stored within the tensordict such that it can be erased at any
+    point in time.
+
+    Examples:
+        >>> import timeit
+        >>> from tensordict import TensorDict
+        >>> class SomeOtherTd(TensorDict):
+        ...     @cache
+        ...     def all_keys(self):
+        ...         return set(self.keys(include_nested=True))
+        >>> td = SomeOtherTd({("a", "b", "c", "d", "e", "f", "g"): 1.0}, [])
+        >>> td.lock_()
+        >>> print(timeit.timeit("set(td.keys(True))", globals={'td': td}))
+        11.057
+        >>> print(timeit.timeit("set(td.all_keys())", globals={'td': td}))
+        0.88
+    """
+    from tensordict.memmap import MemmapTensor
+
+    @wraps(fun)
+    def newfun(_self: "TensorDictBase", *args, **kwargs):
+        if not _self.is_locked:
+            return fun(_self, *args, **kwargs)
+        cache = _self._cache
+        if cache is None:
+            cache = _self._cache = defaultdict(dict)
+        cache = cache[fun.__name__]
+        key = _make_cache_key(args, kwargs)
+        if key not in cache:
+            out = fun(_self, *args, **kwargs)
+            if not isinstance(out, (Tensor, MemmapTensor, KeyedJaggedTensor)):
+                # we don't cache tensors to avoid filling the mem and / or
+                # stacking them from their origin
+                cache[key] = out
+        else:
+            out = cache[key]
+        return out
+
+    return newfun
+
+
+def erase_cache(fun):
+    """A decorator to erase the cache at each call."""
+
+    @wraps(fun)
+    def new_fun(self, *args, **kwargs):
+        self._erase_cache()
+        return fun(self, *args, **kwargs)
+
+    return new_fun
+
+
+_NON_STR_KEY_TUPLE_ERR = "Nested membership checks with tuples of strings is only supported when setting `include_nested=True`."
+_NON_STR_KEY_ERR = "TensorDict keys are always strings. Membership checks are only supported for strings or non-empty tuples of strings (for nested TensorDicts)"
+_GENERIC_NESTED_ERR = "Only NestedKeys are supported."
+
+
+class _StringKeys(KeysView):
+    """A key view where contains is restricted to strings."""
+
+    def __contains__(self, item):
+        if not isinstance(item, str):
+            try:
+                unravel_item = _unravel_key_to_tuple(item)
+                if not unravel_item:  # catch errors during unravel
+                    raise TypeError
+            except Exception:
+                raise TypeError(_NON_STR_KEY_ERR)
+            if len(unravel_item) > 1:
+                raise TypeError(_NON_STR_KEY_TUPLE_ERR)
+            else:
+                item = unravel_item[0]
+        return super().__contains__(item)
+
+
+class _StringOnlyDict(dict):
+    """A dict class where contains is restricted to strings."""
+
+    # kept here for debugging
+    # def __setitem__(self, key, value):
+    #     if not isinstance(key, str):
+    #         raise RuntimeError
+    #     return super().__setitem__(key, value)
+
+    def __contains__(self, item):
+        if not isinstance(item, str):
+            try:
+                unravel_item = _unravel_key_to_tuple(item)
+                if not unravel_item:  # catch errors during unravel
+                    raise TypeError
+            except Exception:
+                raise TypeError(_NON_STR_KEY_ERR)
+            if len(unravel_item) > 1:
+                raise TypeError(_NON_STR_KEY_TUPLE_ERR)
+            else:
+                item = unravel_item[0]
+        return super().__contains__(item)
+
+    def keys(self):
+        return _StringKeys(self)
+
+
+def lock_blocked(func):
+    """Checks that the tensordict is unlocked before executing a function."""
+
+    @wraps(func)
+    def new_func(self, *args, **kwargs):
+        if self.is_locked:
+            raise RuntimeError(self.LOCK_ERROR)
+        return func(self, *args, **kwargs)
+
+    return new_func
+
+
+class as_decorator:
+    """Converts a method to a decorator.
+
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> data = TensorDict({}, [])
+        >>> with data.lock_(): # lock_ is decorated
+        ...     assert data.is_locked
+        >>> assert not data.is_locked
+    """
+
+    def __init__(self, attr):
+        self.attr = attr
+
+    def __call__(self, func):
+        @wraps(func)
+        def new_func(_self, *args, **kwargs):
+            _attr_pre = getattr(_self, self.attr)
+            out = func(_self, *args, **kwargs)
+            _attr_post = getattr(_self, self.attr)
+            if _attr_post is not _attr_pre:
+                _self._last_op = (new_func.__name__, (args, kwargs))
+            else:
+                _self._last_op = None
+            return out
+
+        return new_func
```

## tensordict/version.py

```diff
@@ -1,2 +1,2 @@
-__version__ = '2023.08.07'
-git_version = '37e66d1331081df12d662f2bb02d52ecf363084d'
+__version__ = '2023.08.08'
+git_version = 'Unknown'
```

## tensordict/nn/__init__.py

 * *Ordering differences only*

```diff
@@ -1,55 +1,55 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from tensordict.nn.common import (
-    dispatch,
-    make_tensordict,
-    TensorDictModule,
-    TensorDictModuleBase,
-    TensorDictModuleWrapper,
-)
-from tensordict.nn.distributions import NormalParamExtractor
-from tensordict.nn.ensemble import EnsembleModule
-from tensordict.nn.functional_modules import (
-    get_functional,
-    is_functional,
-    make_functional,
-    repopulate_module,
-)
-from tensordict.nn.params import TensorDictParams
-from tensordict.nn.probabilistic import (
-    InteractionType,
-    ProbabilisticTensorDictModule,
-    ProbabilisticTensorDictSequential,
-    set_interaction_mode,
-    set_interaction_type,
-)
-from tensordict.nn.sequence import TensorDictSequential
-from tensordict.nn.utils import (
-    biased_softplus,
-    inv_softplus,
-    set_skip_existing,
-    skip_existing,
-)
-
-__all__ = [
-    "dispatch",
-    "TensorDictModule",
-    "TensorDictModuleWrapper",
-    "get_functional",
-    "make_functional",
-    "repopulate_module",
-    "InteractionType",
-    "ProbabilisticTensorDictModule",
-    "ProbabilisticTensorDictSequential",
-    "set_interaction_mode",
-    "set_interaction_type",
-    "TensorDictSequential",
-    "make_tensordict",
-    "biased_softplus",
-    "inv_softplus",
-    "TensorDictParams",
-    "is_functional",
-]
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from tensordict.nn.common import (
+    dispatch,
+    make_tensordict,
+    TensorDictModule,
+    TensorDictModuleBase,
+    TensorDictModuleWrapper,
+)
+from tensordict.nn.distributions import NormalParamExtractor
+from tensordict.nn.ensemble import EnsembleModule
+from tensordict.nn.functional_modules import (
+    get_functional,
+    is_functional,
+    make_functional,
+    repopulate_module,
+)
+from tensordict.nn.params import TensorDictParams
+from tensordict.nn.probabilistic import (
+    InteractionType,
+    ProbabilisticTensorDictModule,
+    ProbabilisticTensorDictSequential,
+    set_interaction_mode,
+    set_interaction_type,
+)
+from tensordict.nn.sequence import TensorDictSequential
+from tensordict.nn.utils import (
+    biased_softplus,
+    inv_softplus,
+    set_skip_existing,
+    skip_existing,
+)
+
+__all__ = [
+    "dispatch",
+    "TensorDictModule",
+    "TensorDictModuleWrapper",
+    "get_functional",
+    "make_functional",
+    "repopulate_module",
+    "InteractionType",
+    "ProbabilisticTensorDictModule",
+    "ProbabilisticTensorDictSequential",
+    "set_interaction_mode",
+    "set_interaction_type",
+    "TensorDictSequential",
+    "make_tensordict",
+    "biased_softplus",
+    "inv_softplus",
+    "TensorDictParams",
+    "is_functional",
+]
```

## tensordict/nn/common.py

 * *Ordering differences only*

```diff
@@ -1,1277 +1,1277 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import functools
-import inspect
-import warnings
-from textwrap import indent
-from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union
-
-import torch
-from cloudpickle import dumps as cloudpickle_dumps, loads as cloudpickle_loads
-from tensordict._tensordict import unravel_key_list
-
-from tensordict.nn.functional_modules import (
-    _swap_state,
-    extract_weights_and_buffers,
-    is_functional,
-    make_functional,
-    repopulate_module,
-)
-
-from tensordict.nn.utils import set_skip_existing
-from tensordict.tensordict import is_tensor_collection, make_tensordict, TensorDictBase
-from tensordict.utils import implement_for, NestedKey
-from torch import nn, Tensor
-
-try:
-    from functorch import FunctionalModule, FunctionalModuleWithBuffers
-
-    _has_functorch = True
-except ImportError:
-    _has_functorch = False
-
-    class FunctionalModule:
-        pass
-
-    class FunctionalModuleWithBuffers:
-        pass
-
-
-__all__ = [
-    "TensorDictModule",
-    "TensorDictModuleWrapper",
-]
-
-
-class dispatch:
-    """Allows for a function expecting a TensorDict to be called using kwargs.
-
-    :func:`dispatch` must be used within modules that have an ``in_keys`` (or
-    another source of keys indicated by the ``source`` keyword argument) and
-    ``out_keys`` (or another ``dest`` key list) attributes indicating what keys
-    to be read and written from the tensordict. The wrapped function should
-    also have a ``tensordict`` leading argument.
-
-    The resulting function will return a single tensor (if there is a single
-    element in out_keys), otherwise it will return a tuple sorted as the ``out_keys``
-    of the module.
-
-    :func:`dispatch` can be used either as a method or as a class when extra arguments
-    need to be passed.
-
-    Args:
-        separator (str, optional): separator that combines sub-keys together
-            for ``in_keys`` that are tuples of strings.
-            Defaults to ``"_"``.
-        source (str or list of keys, optional): if a string is provided,
-            it points to the module attribute that contains the
-            list of input keys to be used. If a list is provided instead, it
-            will contain the keys used as input to the module.
-            Defaults to ``"in_keys"`` which is the attribute name of
-            :class:`~.TensorDictModule` list of input keys.
-        dest (str or list of keys, optional): if a string is provided,
-            it points to the module attribute that contains the
-            list of output keys to be used. If a list is provided instead, it
-            will contain the keys used as output to the module.
-            Defaults to ``"out_keys"`` which is the attribute name of
-            :class:`~.TensorDictModule` list of output keys.
-        auto_batch_size (bool, optional): if ``True``, the batch-size of the
-            input tensordict is determined automatically as the maximum number
-            of common dimensions across all the input tensors.
-            Defaults to ``True``.
-
-    Examples:
-        >>> class MyModule(nn.Module):
-        ...     in_keys = ["a"]
-        ...     out_keys = ["b"]
-        ...
-        ...     @dispatch
-        ...     def forward(self, tensordict):
-        ...         tensordict['b'] = tensordict['a'] + 1
-        ...         return tensordict
-        ...
-        >>> module = MyModule()
-        >>> b = module(a=torch.zeros(1, 2))
-        >>> assert (b == 1).all()
-        >>> # equivalently
-        >>> class MyModule(nn.Module):
-        ...     keys_in = ["a"]
-        ...     keys_out = ["b"]
-        ...
-        ...     @dispatch(source="keys_in", dest="keys_out")
-        ...     def forward(self, tensordict):
-        ...         tensordict['b'] = tensordict['a'] + 1
-        ...         return tensordict
-        ...
-        >>> module = MyModule()
-        >>> b = module(a=torch.zeros(1, 2))
-        >>> assert (b == 1).all()
-        >>> # or this
-        >>> class MyModule(nn.Module):
-        ...     @dispatch(source=["a"], dest=["b"])
-        ...     def forward(self, tensordict):
-        ...         tensordict['b'] = tensordict['a'] + 1
-        ...         return tensordict
-        ...
-        >>> module = MyModule()
-        >>> b = module(a=torch.zeros(1, 2))
-        >>> assert (b == 1).all()
-
-    :func:`dispatch_kwargs` will also work with nested keys with the default
-    ``"_"`` separator.
-
-    Examples:
-        >>> class MyModuleNest(nn.Module):
-        ...     in_keys = [("a", "c")]
-        ...     out_keys = ["b"]
-        ...
-        ...     @dispatch
-        ...     def forward(self, tensordict):
-        ...         tensordict['b'] = tensordict['a', 'c'] + 1
-        ...         return tensordict
-        ...
-        >>> module = MyModuleNest()
-        >>> b, = module(a_c=torch.zeros(1, 2))
-        >>> assert (b == 1).all()
-
-    If another separator is wanted, it can be indicated with the ``separator``
-    argument in the constructor:
-
-    Examples:
-        >>> class MyModuleNest(nn.Module):
-        ...     in_keys = [("a", "c")]
-        ...     out_keys = ["b"]
-        ...
-        ...     @dispatch(separator="sep")
-        ...     def forward(self, tensordict):
-        ...         tensordict['b'] = tensordict['a', 'c'] + 1
-        ...         return tensordict
-        ...
-        >>> module = MyModuleNest()
-        >>> b, = module(asepc=torch.zeros(1, 2))
-        >>> assert (b == 1).all()
-
-
-    Since the input keys is a sorted sequence of strings,
-    :func:`dispatch` can also be used with unnamed arguments where the order
-    must match the order of the input keys.
-
-    .. note::
-
-        If the first argument is a :class:`~.TensorDictBase` instance, it is
-        assumed that dispatch is __not__ being used and that this tensordict
-        contains all the necessary information to be run through the module.
-        In other words, one cannot decompose a tensordict with the first key
-        of the module inputs pointing to a tensordict instance.
-        In general, it is preferred to use :func:`dispatch` with tensordict
-        leaves only.
-
-    Examples:
-        >>> class MyModuleNest(nn.Module):
-        ...     in_keys = [("a", "c"), "d"]
-        ...     out_keys = ["b"]
-        ...
-        ...     @dispatch
-        ...     def forward(self, tensordict):
-        ...         tensordict['b'] = tensordict['a', 'c'] + tensordict["d"]
-        ...         return tensordict
-        ...
-        >>> module = MyModuleNest()
-        >>> b, = module(torch.zeros(1, 2), d=torch.ones(1, 2))  # works
-        >>> assert (b == 1).all()
-        >>> b, = module(torch.zeros(1, 2), torch.ones(1, 2))  # works
-        >>> assert (b == 1).all()
-        >>> try:
-        ...     b, = module(torch.zeros(1, 2), a_c=torch.ones(1, 2))  # fails
-        ... except:
-        ...     print("oopsy!")
-        ...
-
-    """
-
-    DEFAULT_SEPARATOR = "_"
-    DEFAULT_SOURCE = "in_keys"
-    DEFAULT_DEST = "out_keys"
-
-    def __new__(
-        cls,
-        separator=DEFAULT_SEPARATOR,
-        source=DEFAULT_SOURCE,
-        dest=DEFAULT_DEST,
-        auto_batch_size: bool = True,
-    ):
-        if callable(separator):
-            func = separator
-            separator = dispatch.DEFAULT_SEPARATOR
-            self = super().__new__(cls)
-            self.__init__(separator, source, dest)
-            return self.__call__(func)
-        return super().__new__(cls)
-
-    def __init__(
-        self,
-        separator=DEFAULT_SEPARATOR,
-        source=DEFAULT_SOURCE,
-        dest=DEFAULT_DEST,
-        auto_batch_size: bool = True,
-    ):
-        self.separator = separator
-        self.source = source
-        self.dest = dest
-        self.auto_batch_size = auto_batch_size
-
-    def __call__(self, func: Callable) -> Callable:
-        # sanity check
-        for i, key in enumerate(inspect.signature(func).parameters):
-            if i == 0:
-                # skip self
-                continue
-            if key != "tensordict":
-                raise RuntimeError(
-                    "the first argument of the wrapped function must be "
-                    "named 'tensordict'."
-                )
-            break
-
-        @functools.wraps(func)
-        def wrapper(_self, *args: Any, **kwargs: Any) -> Any:
-
-            source = self.source
-            if isinstance(source, str):
-                source = getattr(_self, source)
-            tensordict = None
-            if len(args):
-                if is_tensor_collection(args[0]):
-                    tensordict, args = args[0], args[1:]
-            if tensordict is None:
-                tensordict_values = {}
-                dest = self.dest
-                if isinstance(dest, str):
-                    dest = getattr(_self, dest)
-                for key in source:
-                    expected_key = (
-                        self.separator.join(key) if isinstance(key, tuple) else key
-                    )
-                    if len(args):
-                        tensordict_values[key] = args[0]
-                        args = args[1:]
-                        if expected_key in kwargs:
-                            raise RuntimeError(
-                                "Duplicated argument in args and kwargs."
-                            )
-                    elif expected_key in kwargs:
-                        try:
-                            tensordict_values[key] = kwargs.pop(expected_key)
-                        except KeyError:
-                            raise KeyError(
-                                f"The key {expected_key} wasn't found in the keyword arguments "
-                                f"but is expected to execute that function."
-                            )
-                tensordict = make_tensordict(
-                    tensordict_values,
-                    batch_size=torch.Size([]) if not self.auto_batch_size else None,
-                )
-                out = func(_self, tensordict, *args, **kwargs)
-                out = tuple(out[key] for key in dest)
-                return out[0] if len(out) == 1 else out
-            return func(_self, tensordict, *args, **kwargs)
-
-        return wrapper
-
-
-class _OutKeysSelect:
-    def __init__(self, out_keys):
-        self.out_keys = out_keys
-        self._initialized = False
-
-    def _init(self, module):
-        if self._initialized:
-            return
-        self._initialized = True
-        self.module = module
-        module.out_keys = list(self.out_keys)
-
-    @implement_for("torch", None, "2.0")
-    def __call__(  # noqa: F811
-        self,
-        module: TensorDictModuleBase,
-        tensordict_in: TensorDictBase,
-        tensordict_out: TensorDictBase,
-    ):
-        if not isinstance(tensordict_out, TensorDictBase):
-            raise RuntimeError(
-                "You are likely using tensordict.nn.dispatch with keyword arguments with an older (< 2.0) version of pytorch. "
-                "This is currently not supported. Please use unnamed arguments or upgrade pytorch."
-            )
-        # detect dispatch calls
-        in_keys = module.in_keys
-        is_dispatched = self._detect_dispatch(tensordict_in, in_keys)
-        out_keys = self.out_keys
-        # if dispatch filtered the out keys as they should we're happy
-        if is_dispatched:
-            if (not isinstance(tensordict_out, tuple) and len(out_keys) == 1) or (
-                len(out_keys) == len(tensordict_out)
-            ):
-                return tensordict_out
-        self._init(module)
-        if is_dispatched:
-            # it might be the case that dispatch was not aware of what the out-keys were.
-            if isinstance(tensordict_out, tuple):
-                out = tuple(
-                    item
-                    for i, item in enumerate(tensordict_out)
-                    if module._out_keys[i] in module.out_keys
-                )
-                if len(out) == 1:
-                    return out[0]
-                return out
-            elif module._out_keys[0] in module.out_keys and len(module._out_keys) == 1:
-                return tensordict_out
-            elif (
-                module._out_keys[0] not in module.out_keys
-                and len(module._out_keys) == 1
-            ):
-                return ()
-            else:
-                raise RuntimeError(
-                    f"Selecting out-keys failed. Original out_keys: {module._out_keys}, selected: {module.out_keys}."
-                )
-        if tensordict_out is tensordict_in:
-            return tensordict_out.select(
-                *in_keys,
-                *out_keys,
-                inplace=True,
-            )
-        else:
-            return tensordict_out.select(
-                *in_keys,
-                *out_keys,
-                inplace=True,
-                strict=False,
-            )
-
-    @implement_for("torch", "2.0", None)
-    def __call__(  # noqa: F811
-        self,
-        module: TensorDictModuleBase,
-        tensordict_in: TensorDictBase,
-        kwargs: Dict,
-        tensordict_out: TensorDictBase,
-    ):
-        # detect dispatch calls
-        in_keys = module.in_keys
-        if not tensordict_in and kwargs.get("tensordict", None) is not None:
-            tensordict_in = kwargs.pop("tensordict")
-        is_dispatched = self._detect_dispatch(tensordict_in, kwargs, in_keys)
-        out_keys = self.out_keys
-        # if dispatch filtered the out keys as they should we're happy
-        if is_dispatched:
-            if (not isinstance(tensordict_out, tuple) and len(out_keys) == 1) or (
-                len(out_keys) == len(tensordict_out)
-            ):
-                return tensordict_out
-        self._init(module)
-        if is_dispatched:
-            # it might be the case that dispatch was not aware of what the out-keys were.
-            if isinstance(tensordict_out, tuple):
-                out = tuple(
-                    item
-                    for i, item in enumerate(tensordict_out)
-                    if module._out_keys[i] in module.out_keys
-                )
-                if len(out) == 1:
-                    return out[0]
-                return out
-            elif module._out_keys[0] in module.out_keys and len(module._out_keys) == 1:
-                return tensordict_out
-            elif (
-                module._out_keys[0] not in module.out_keys
-                and len(module._out_keys) == 1
-            ):
-                return ()
-            else:
-                raise RuntimeError(
-                    f"Selecting out-keys failed. Original out_keys: {module._out_keys}, selected: {module.out_keys}."
-                )
-        if tensordict_out is tensordict_in:
-            return tensordict_out.select(
-                *in_keys,
-                *out_keys,
-                inplace=True,
-            )
-        else:
-            return tensordict_out.select(
-                *in_keys,
-                *out_keys,
-                inplace=True,
-                strict=False,
-            )
-
-    @implement_for("torch", None, "2.0")
-    def _detect_dispatch(self, tensordict_in, in_keys):  # noqa: F811
-        if isinstance(tensordict_in, TensorDictBase) and all(
-            key in tensordict_in.keys() for key in in_keys
-        ):
-            return False
-        elif isinstance(tensordict_in, tuple):
-            if len(tensordict_in):
-                if isinstance(tensordict_in[0], TensorDictBase):
-                    return self._detect_dispatch(tensordict_in[0], in_keys)
-                return True
-            return not len(in_keys)
-        # not a TDBase: must be True
-        return True
-
-    @implement_for("torch", "2.0", None)
-    def _detect_dispatch(self, tensordict_in, kwargs, in_keys):  # noqa: F811
-        if isinstance(tensordict_in, TensorDictBase) and all(
-            key in tensordict_in.keys(include_nested=True) for key in in_keys
-        ):
-            return False
-        elif isinstance(tensordict_in, tuple):
-            if len(tensordict_in) or len(kwargs):
-                if len(tensordict_in) and isinstance(tensordict_in[0], TensorDictBase):
-                    return self._detect_dispatch(tensordict_in[0], kwargs, in_keys)
-                elif (
-                    not len(tensordict_in)
-                    and len(kwargs)
-                    and isinstance(kwargs.get("tensordict", None), TensorDictBase)
-                ):
-                    return self._detect_dispatch(kwargs["tensordict"], in_keys)
-                return True
-            return not len(in_keys)
-        # not a TDBase: must be True
-        return True
-
-    def remove(self):
-        # reset ground truth
-        if self.module._out_keys is not None:
-            self.module.out_keys = self.module._out_keys
-
-    def __del__(self):
-        self.remove()
-
-
-class TensorDictModuleBase(nn.Module):
-    """Base class to TensorDict modules.
-
-    TensorDictModule subclasses are characterized by ``in_keys`` and ``out_keys``
-    key-lists that indicate what input entries are to be read and what output
-    entries should be expected to be written.
-
-    The forward method input/output signature should always follow the
-    convention:
-
-        >>> tensordict_out = module.forward(tensordict_in)
-
-    """
-
-    def __new__(cls, *args, **kwargs):
-        # check the out_keys and in_keys in the dict
-        if "in_keys" in cls.__dict__ and not isinstance(
-            cls.__dict__.get("in_keys"), property
-        ):
-            in_keys = cls.__dict__.get("in_keys")
-            # now let's remove it
-            delattr(cls, "in_keys")
-            cls._in_keys = unravel_key_list(in_keys)
-            cls.in_keys = TensorDictModuleBase.in_keys
-        if "out_keys" in cls.__dict__ and not isinstance(
-            cls.__dict__.get("out_keys"), property
-        ):
-            out_keys = cls.__dict__.get("out_keys")
-            # now let's remove it
-            delattr(cls, "out_keys")
-            out_keys = unravel_key_list(out_keys)
-            cls._out_keys = out_keys
-            cls._out_keys_apparent = out_keys
-            cls.out_keys = TensorDictModuleBase.out_keys
-        out = super().__new__(cls)
-        return out
-
-    @property
-    def in_keys(self):
-        return self._in_keys
-
-    @in_keys.setter
-    def in_keys(self, value: List[Union[str, Tuple[str]]]):
-        self._in_keys = unravel_key_list(value)
-
-    @property
-    def out_keys(self):
-        return self._out_keys_apparent
-
-    @property
-    def out_keys_source(self):
-        return self._out_keys
-
-    @out_keys.setter
-    def out_keys(self, value: List[Union[str, Tuple[str]]]):
-        # the first time out_keys are set, they are marked as ground truth
-        value = unravel_key_list(list(value))
-        if not hasattr(self, "_out_keys"):
-            self._out_keys = value
-        self._out_keys_apparent = value
-
-    @implement_for("torch", None, "2.0")
-    def select_out_keys(self, *out_keys):  # noqa: F811
-        """Selects the keys that will be found in the output tensordict.
-
-        This is useful whenever one wants to get rid of intermediate keys in a
-        complicated graph, or when the presence of these keys may trigger unexpected
-        behaviours.
-
-        The original ``out_keys`` can still be accessed via ``module.out_keys_source``.
-
-        Args:
-            *out_keys (a sequence of strings or tuples of strings): the
-                out_keys that should be found in the output tensordict.
-
-        Returns: the same module, modified in-place with updated ``out_keys``.
-
-        The simplest usage is with :class:`~.TensorDictModule`:
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
-            >>> import torch
-            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
-            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
-            >>> mod(td)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> mod.select_out_keys("d")
-            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
-            >>> mod(td)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        This feature will also work with dispatched arguments:
-        Examples:
-            >>> mod(torch.zeros(()), torch.ones(()))
-            tensor(2.)
-
-        This change will occur in-place (ie the same module will be returned
-        with an updated list of out_keys). It can be reverted using the
-        :meth:`TensorDictModuleBase.reset_out_keys` method.
-
-        Examples:
-            >>> mod.reset_out_keys()
-            >>> mod(TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, []))
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        This will work with other classes too, such as Sequential:
-        Examples:
-            >>> from tensordict.nn import TensorDictSequential
-            >>> seq = TensorDictSequential(
-            ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["y"]),
-            ...     TensorDictModule(lambda x: x+1, in_keys=["y"], out_keys=["z"]),
-            ... )
-            >>> td = TensorDict({"x": torch.zeros(())}, [])
-            >>> seq(td)
-            TensorDict(
-                fields={
-                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> seq.select_out_keys("z")
-            >>> td = TensorDict({"x": torch.zeros(())}, [])
-            >>> seq(td)
-            TensorDict(
-                fields={
-                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        """
-        out_keys = unravel_key_list(list(out_keys))
-        if len(out_keys) == 1:
-            if out_keys[0] not in self.out_keys:
-                err_msg = f"Can't select non existent key: {out_keys[0]}. "
-                if (
-                    out_keys[0]
-                    and isinstance(out_keys[0], (tuple, list))
-                    and out_keys[0][0] in self.out_keys
-                ):
-                    err_msg += f"Are you passing the keys in a list? Try unpacking as: `{', '.join(out_keys[0])}`"
-                raise ValueError(err_msg)
-        self.register_forward_hook(_OutKeysSelect(out_keys))
-        for hook in self._forward_hooks.values():
-            hook._init(self)
-        return self
-
-    @implement_for("torch", "2.0", None)
-    def select_out_keys(self, *out_keys):  # noqa: F811
-        """Selects the keys that will be found in the output tensordict.
-
-        This is useful whenever one wants to get rid of intermediate keys in a
-        complicated graph, or when the presence of these keys may trigger unexpected
-        behaviours.
-
-        The original ``out_keys`` can still be accessed via ``module.out_keys_source``.
-
-        Args:
-            *out_keys (a sequence of strings or tuples of strings): the
-                out_keys that should be found in the output tensordict.
-
-        Returns: the same module, modified in-place with updated ``out_keys``.
-
-        The simplest usage is with :class:`~.TensorDictModule`:
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
-            >>> import torch
-            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
-            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
-            >>> mod(td)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> mod.select_out_keys("d")
-            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
-            >>> mod(td)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        This feature will also work with dispatched arguments:
-        Examples:
-            >>> mod(torch.zeros(()), torch.ones(()))
-            tensor(2.)
-
-        This change will occur in-place (ie the same module will be returned
-        with an updated list of out_keys). It can be reverted using the
-        :meth:`TensorDictModuleBase.reset_out_keys` method.
-
-        Examples:
-            >>> mod.reset_out_keys()
-            >>> mod(TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, []))
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        This will work with other classes too, such as Sequential:
-        Examples:
-            >>> from tensordict.nn import TensorDictSequential
-            >>> seq = TensorDictSequential(
-            ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["y"]),
-            ...     TensorDictModule(lambda x: x+1, in_keys=["y"], out_keys=["z"]),
-            ... )
-            >>> td = TensorDict({"x": torch.zeros(())}, [])
-            >>> seq(td)
-            TensorDict(
-                fields={
-                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> seq.select_out_keys("z")
-            >>> td = TensorDict({"x": torch.zeros(())}, [])
-            >>> seq(td)
-            TensorDict(
-                fields={
-                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        """
-        out_keys = unravel_key_list(list(out_keys))
-        if len(out_keys) == 1:
-            if out_keys[0] not in self.out_keys:
-                err_msg = f"Can't select non existent key: {out_keys[0]}. "
-                if (
-                    out_keys[0]
-                    and isinstance(out_keys[0], (tuple, list))
-                    and out_keys[0][0] in self.out_keys
-                ):
-                    err_msg += f"Are you passing the keys in a list? Try unpacking as: `{', '.join(out_keys[0])}`"
-                raise ValueError(err_msg)
-        self.register_forward_hook(_OutKeysSelect(out_keys), with_kwargs=True)
-        for hook in self._forward_hooks.values():
-            hook._init(self)
-        return self
-
-    def reset_out_keys(self):
-        """Resets the ``out_keys`` attribute to its orignal value.
-
-        Returns: the same module, with its original ``out_keys`` values.
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
-            >>> import torch
-            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
-            >>> mod.select_out_keys("d")
-            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
-            >>> mod(td)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> mod.reset_out_keys()
-            >>> mod(td)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-        """
-        for i, hook in list(self._forward_hooks.items()):
-            if isinstance(hook, _OutKeysSelect):
-                del self._forward_hooks[i]
-        return self
-
-    def reset_parameters_recursive(
-        self, parameters: Optional[TensorDictBase] = None
-    ) -> Optional[TensorDictBase]:
-        """Recursively reset the parameters of the module and its children.
-
-        Args:
-            parameters (TensorDict of parameters, optional): If set to None, the module will reset using self.parameters().
-                Otherwise, we will reset the parameters in the tensordict in-place. This is
-                useful for functional modules where the parameters are not stored in the module itself.
-
-        Returns:
-            A tensordict of the new parameters, only if parameters was not None.
-
-        Examples:
-            >>> from tensordict.nn import TensorDictModule
-            >>> from torch import nn
-            >>> net = nn.Sequential(nn.Linear(2,3), nn.ReLU())
-            >>> old_param = net[0].weight.clone()
-            >>> module = TensorDictModule(net, in_keys=['bork'], out_keys=['dork'])
-            >>> module.reset_parameters()
-            >>> (old_param == net[0].weight).any()
-            tensor(False)
-
-        This method also supports functional parameter sampling:
-
-            >>> from tensordict import TensorDict
-            >>> from tensordict.nn import TensorDictModule
-            >>> from torch import nn
-            >>> net = nn.Sequential(nn.Linear(2,3), nn.ReLU())
-            >>> module = TensorDictModule(net, in_keys=['bork'], out_keys=['dork'])
-            >>> params = TensorDict.from_module(module)
-            >>> old_params = params.clone(recurse=True)
-            >>> module.reset_parameters(params)
-            >>> (old_params == params).any()
-            False
-        """
-        if parameters is None:
-            self._reset_parameters(self)
-            return
-        elif parameters.ndim:
-            raise RuntimeError(
-                "reset_parameters_recursive does not support batched TensorDicts, ensure `batch_size` is empty and the parameters shape match their original shape."
-            )
-
-        sanitized_parameters = parameters.apply(
-            lambda x: x.detach().requires_grad_(), inplace=False
-        )
-
-        if not is_functional(self):
-            make_functional(self, keep_params=True)
-        is_stateless = self._is_stateless
-        if is_stateless:
-            repopulate_module(self, sanitized_parameters)
-        else:
-            old_params = _swap_state(
-                self,
-                sanitized_parameters,
-                is_stateless=False,
-                return_old_tensordict=True,
-            )
-
-        self._reset_parameters(self)
-
-        if is_stateless:
-            new_parameters = extract_weights_and_buffers(self)
-        else:
-            new_parameters = _swap_state(
-                self, old_params, is_stateless=False, return_old_tensordict=True
-            )
-
-        return new_parameters
-
-    def _reset_parameters(self, module: nn.Module) -> None:
-        for child in module.children():
-            if isinstance(child, nn.Module):
-                self._reset_parameters(child)
-
-            if hasattr(child, "reset_parameters"):
-                child.reset_parameters()
-
-
-class TensorDictModule(TensorDictModuleBase):
-    """A TensorDictModule, is a python wrapper around a :obj:`nn.Module` that reads and writes to a TensorDict.
-
-    By default, :class:`TensorDictModule` subclasses are always functional,
-    meaning that they support the ``td_module(input, params=params)`` function
-    call signature.
-
-    Args:
-        module (Callable): a callable, typically a :class:`torch.nn.Module`,
-            used to map the input to the output parameter space. Its forward method
-            can return a single tensor, a tuple of tensors or even a dictionary.
-            In the latter case, the output keys of the :class:`TensorDictModule`
-            will be used to populate the output tensordict (ie. the keys present
-            in ``out_keys`` should be present in the dictionary returned by the
-            ``module`` forward method).
-        in_keys (iterable of NestedKeys, Dict[NestedStr, str]): keys to be read
-            from input tensordict and passed to the module. If it
-            contains more than one element, the values will be passed in the
-            order given by the in_keys iterable.
-            If ``in_keys`` is a dictionary, its keys must correspond to the key
-            to be read in the tensordict and its values must match the name of
-            the keyword argument in the function signature.
-        out_keys (iterable of str): keys to be written to the input tensordict. The length of out_keys must match the
-            number of tensors returned by the embedded module. Using "_" as a key avoid writing tensor to output.
-
-    Embedding a neural network in a TensorDictModule only requires to specify the input
-    and output keys. TensorDictModule support functional and regular :obj:`nn.Module`
-    objects. In the functional case, the 'params' (and 'buffers') keyword argument must
-    be specified:
-
-    Examples:
-        >>> from tensordict import TensorDict
-        >>> # one can wrap regular nn.Module
-        >>> module = TensorDictModule(nn.Transformer(128), in_keys=["input", "tgt"], out_keys=["out"])
-        >>> input = torch.ones(2, 3, 128)
-        >>> tgt = torch.zeros(2, 3, 128)
-        >>> data = TensorDict({"input": input, "tgt": tgt}, batch_size=[2, 3])
-        >>> data = module(data)
-        >>> print(data)
-        TensorDict(
-            fields={
-                input: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False),
-                out: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False),
-                tgt: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([2, 3]),
-            device=None,
-            is_shared=False)
-
-    We can also pass directly the tensors
-
-    Examples:
-        >>> out = module(input, tgt)
-        >>> assert out.shape == input.shape
-        >>> # we can also wrap regular functions
-        >>> module = TensorDictModule(lambda x: (x-1, x+1), in_keys=[("input", "x")], out_keys=[("output", "x-1"), ("output", "x+1")])
-        >>> module(TensorDict({("input", "x"): torch.zeros(())}, batch_size=[]))
-        TensorDict(
-            fields={
-                input: TensorDict(
-                    fields={
-                        x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([]),
-                    device=None,
-                    is_shared=False),
-                output: TensorDict(
-                    fields={
-                        x+1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                        x-1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([]),
-                    device=None,
-                    is_shared=False)},
-            batch_size=torch.Size([]),
-            device=None,
-            is_shared=False)
-
-    We can use TensorDictModule to populate a tensordict:
-
-    Examples:
-        >>> module = TensorDictModule(lambda: torch.randn(3), in_keys=[], out_keys=["x"])
-        >>> print(module(TensorDict({}, batch_size=[])))
-        TensorDict(
-            fields={
-                x: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([]),
-            device=None,
-            is_shared=False)
-
-    Another feature is passing a dictionary as input keys, to control the
-    dispatching of values to specific keyword arguments.
-
-    Examples:
-        >>> module = TensorDictModule(lambda x, *, y: x+y,
-        ...     in_keys={'1': 'x', '2': 'y'}, out_keys=['z'],
-        ...     )
-        >>> td = module(TensorDict({'1': torch.ones(()), '2': torch.ones(())*2}, []))
-        >>> td['z']
-        tensor(3.)
-
-    Functional calls to a tensordict module is easy:
-
-    Examples:
-        >>> import torch
-        >>> from tensordict import TensorDict
-        >>> from tensordict.nn import TensorDictModule
-        >>> from tensordict.nn.functional_modules import make_functional
-        >>> td = TensorDict({"input": torch.randn(3, 4), "hidden": torch.randn(3, 8)}, [3,])
-        >>> module = torch.nn.GRUCell(4, 8)
-        >>> td_module = TensorDictModule(
-        ...    module=module, in_keys=["input", "hidden"], out_keys=["output"]
-        ... )
-        >>> params = make_functional(td_module)
-        >>> td_functional = td_module(td.clone(), params=params)
-        >>> print(td_functional)
-        TensorDict(
-            fields={
-                hidden: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                output: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-
-    In the stateful case:
-        >>> module = torch.nn.GRUCell(4, 8)
-        >>> td_module = TensorDictModule(
-        ...    module=module, in_keys=["input", "hidden"], out_keys=["output"]
-        ... )
-        >>> td_stateful = td_module(td.clone())
-        >>> print(td_stateful)
-        TensorDict(
-            fields={
-                hidden: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                output: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-
-    One can use a vmap operator to call the functional module.
-
-    Examples:
-        >>> from torch import vmap
-        >>> from tensordict.nn.functional_modules import extract_weights_and_buffers
-        >>> params = extract_weights_and_buffers(td_module)
-        >>> params_repeat = params.expand(4)
-        >>> print(params_repeat)
-        TensorDict(
-            fields={
-                module: TensorDict(
-                    fields={
-                        bias_hh: Tensor(shape=torch.Size([4, 24]), device=cpu, dtype=torch.float32, is_shared=False),
-                        bias_ih: Tensor(shape=torch.Size([4, 24]), device=cpu, dtype=torch.float32, is_shared=False),
-                        weight_hh: Tensor(shape=torch.Size([4, 24, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                        weight_ih: Tensor(shape=torch.Size([4, 24, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([4]),
-                    device=None,
-                    is_shared=False)},
-            batch_size=torch.Size([4]),
-            device=None,
-            is_shared=False)
-        >>> td_vmap = vmap(td_module, (None, 0))(td.clone(), params_repeat)
-        >>> print(td_vmap)
-        TensorDict(
-            fields={
-                hidden: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                input: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                output: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([4, 3]),
-            device=None,
-            is_shared=False)
-
-    """
-
-    _IN_KEY_ERR = "in_keys must be of type list, str or tuples of str, or dict."
-    _OUT_KEY_ERR = "out_keys must be of type list, str or tuples of str."
-
-    def __init__(
-        self,
-        module: Callable,
-        in_keys: NestedKey | List[NestedKey] | Dict[NestedKey:str],
-        out_keys: NestedKey | List[NestedKey],
-    ) -> None:
-        super().__init__()
-
-        if isinstance(in_keys, dict):
-            # write the kwargs and create a list instead
-            _in_keys = []
-            self._kwargs = []
-            for key, value in in_keys.items():
-                self._kwargs.append(value)
-                _in_keys.append(key)
-            in_keys = _in_keys
-        else:
-            if isinstance(in_keys, (str, tuple)):
-                in_keys = [in_keys]
-            elif not isinstance(in_keys, list):
-                raise ValueError(self._IN_KEY_ERR)
-            self._kwargs = None
-
-        if isinstance(out_keys, (str, tuple)):
-            out_keys = [out_keys]
-        elif not isinstance(out_keys, list):
-            raise ValueError(self._OUT_KEY_ERR)
-        try:
-            in_keys = unravel_key_list(in_keys)
-        except Exception:
-            raise ValueError(self._IN_KEY_ERR)
-        try:
-            out_keys = unravel_key_list(out_keys)
-        except Exception:
-            raise ValueError(self._OUT_KEY_ERR)
-
-        if type(module) is type or not callable(module):
-            raise ValueError(
-                f"Module {module} if type {type(module)} is not callable. "
-                f"Typical accepted types are nn.Module or TensorDictModule."
-            )
-        self.out_keys = out_keys
-        self.in_keys = in_keys
-
-        if "_" in self.in_keys:
-            warnings.warn(
-                'key "_" is for ignoring output, it should not be used in input keys',
-                stacklevel=2,
-            )
-
-        self.module = module
-        make_functional(self, keep_params=True, return_params=False)
-
-    @property
-    def is_functional(self) -> bool:
-        return _has_functorch and isinstance(
-            self.module,
-            (FunctionalModule, FunctionalModuleWithBuffers),
-        )
-
-    def _write_to_tensordict(
-        self,
-        tensordict: TensorDictBase,
-        tensors: list[Tensor],
-        tensordict_out: TensorDictBase | None = None,
-        out_keys: Iterable[NestedKey] | None = None,
-    ) -> TensorDictBase:
-        if out_keys is None:
-            out_keys = self.out_keys_source
-        if tensordict_out is None:
-            tensordict_out = tensordict
-        for _out_key, _tensor in zip(out_keys, tensors):
-            if _out_key != "_":
-                tensordict_out.set(_out_key, _tensor)
-        return tensordict_out
-
-    def _call_module(
-        self, tensors: Sequence[Tensor], **kwargs: Any
-    ) -> Tensor | Sequence[Tensor]:
-        out = self.module(*tensors, **kwargs)
-        return out
-
-    @dispatch(auto_batch_size=False)
-    @set_skip_existing(None)
-    def forward(
-        self,
-        tensordict: TensorDictBase,
-        *args,
-        tensordict_out: TensorDictBase | None = None,
-        **kwargs: Any,
-    ) -> TensorDictBase:
-        """When the tensordict parameter is not set, kwargs are used to create an instance of TensorDict."""
-        try:
-            if len(args):
-                tensordict_out = args[0]
-                args = args[1:]
-                # we will get rid of tensordict_out as a regular arg, because it
-                # blocks us when using vmap
-                # with stateful but functional modules: the functional module checks if
-                # it still contains parameters. If so it considers that only a "params" kwarg
-                # is indicative of what the params are, when we could potentially make a
-                # special rule for TensorDictModule that states that the second arg is
-                # likely to be the module params.
-                warnings.warn(
-                    "tensordict_out will be deprecated soon.",
-                    category=DeprecationWarning,
-                )
-            if len(args):
-                raise ValueError(
-                    "Got a non-empty list of extra agruments, when none was expected."
-                )
-            if self._kwargs is not None:
-                kwargs.update(
-                    {
-                        kwarg: tensordict.get(in_key, None)
-                        for kwarg, in_key in zip(self._kwargs, self.in_keys)
-                    }
-                )
-                tensors = ()
-            else:
-                tensors = tuple(tensordict.get(in_key, None) for in_key in self.in_keys)
-            try:
-                tensors = self._call_module(tensors, **kwargs)
-            except Exception as err:
-                if any(tensor is None for tensor in tensors) and "None" in str(err):
-                    none_set = {
-                        key
-                        for key, tensor in zip(self.in_keys, tensors)
-                        if tensor is None
-                    }
-                    raise KeyError(
-                        "Some tensors that are necessary for the module call may "
-                        "not have not been found in the input tensordict: "
-                        f"the following inputs are None: {none_set}."
-                    ) from err
-                else:
-                    raise err
-            if isinstance(tensors, (dict, TensorDictBase)):
-                if isinstance(tensors, dict):
-                    keys = unravel_key_list(list(tensors.keys()))
-                    values = tensors.values()
-                    tensors = dict(zip(keys, values))
-                tensors = tuple(tensors.get(key, None) for key in self.out_keys)
-            if not isinstance(tensors, tuple):
-                tensors = (tensors,)
-            tensordict_out = self._write_to_tensordict(
-                tensordict, tensors, tensordict_out
-            )
-            return tensordict_out
-        except Exception as err:
-            module = self.module
-            if not isinstance(module, nn.Module):
-                try:
-                    import inspect
-
-                    module = inspect.getsource(module)
-                except OSError:
-                    # then we can't print the source code
-                    pass
-            module = indent(str(module), 4 * " ")
-            in_keys = indent(f"in_keys={self.in_keys}", 4 * " ")
-            out_keys = indent(f"out_keys={self.out_keys}", 4 * " ")
-            raise RuntimeError(
-                f"TensorDictModule failed with operation\n{module}\n{in_keys}\n{out_keys}."
-            ) from err
-
-    @property
-    def device(self) -> torch.device:
-        for p in self.parameters():
-            return p.device
-        return torch.device("cpu")
-
-    def __repr__(self) -> str:
-        fields = indent(
-            f"module={self.module},\n"
-            f"device={self.device},\n"
-            f"in_keys={self.in_keys},\n"
-            f"out_keys={self.out_keys}",
-            4 * " ",
-        )
-
-        return f"{self.__class__.__name__}(\n{fields})"
-
-    def __getattr__(self, name: str) -> Any:
-        try:
-            return super().__getattr__(name)
-        except AttributeError as err1:
-            try:
-                return getattr(super().__getattr__("module"), name)
-            except Exception as err2:
-                raise err2 from err1
-
-    def __getstate__(self):
-        state = self.__dict__.copy()
-        if not isinstance(self.module, nn.Module):
-            state["module"] = cloudpickle_dumps(state["module"])
-        return state
-
-    def __setstate__(self, state):
-        if "module" in state:
-            state["module"] = cloudpickle_loads(state["module"])
-        self.__dict__ = state
-
-
-class TensorDictModuleWrapper(TensorDictModuleBase):
-    """Wrapper class for TensorDictModule objects.
-
-    Once created, a TensorDictModuleWrapper will behave exactly as the
-    TensorDictModule it contains except for the methods that are
-    overwritten.
-
-    Args:
-        td_module (TensorDictModule): operator to be wrapped.
-
-    """
-
-    def __init__(self, td_module: TensorDictModule) -> None:
-        super().__init__()
-        self.td_module = td_module
-        if len(self.td_module._forward_hooks):
-            for pre_hook in self.td_module._forward_hooks:
-                self.register_forward_hook(self.td_module._forward_hooks[pre_hook])
-
-    def __getattr__(self, name: str) -> Any:
-        try:
-            return super().__getattr__(name)
-        except AttributeError:
-            if name not in self.__dict__ and not name.startswith("__"):
-                return getattr(self._modules["td_module"], name)
-            else:
-                raise AttributeError(
-                    f"attribute {name} not recognised in {type(self).__name__}"
-                )
-
-    def forward(self, *args: Any, **kwargs: Any) -> TensorDictBase:
-        return self.td_module.forward(*args, **kwargs)
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import functools
+import inspect
+import warnings
+from textwrap import indent
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union
+
+import torch
+from cloudpickle import dumps as cloudpickle_dumps, loads as cloudpickle_loads
+from tensordict._tensordict import unravel_key_list
+
+from tensordict.nn.functional_modules import (
+    _swap_state,
+    extract_weights_and_buffers,
+    is_functional,
+    make_functional,
+    repopulate_module,
+)
+
+from tensordict.nn.utils import set_skip_existing
+from tensordict.tensordict import is_tensor_collection, make_tensordict, TensorDictBase
+from tensordict.utils import implement_for, NestedKey
+from torch import nn, Tensor
+
+try:
+    from functorch import FunctionalModule, FunctionalModuleWithBuffers
+
+    _has_functorch = True
+except ImportError:
+    _has_functorch = False
+
+    class FunctionalModule:
+        pass
+
+    class FunctionalModuleWithBuffers:
+        pass
+
+
+__all__ = [
+    "TensorDictModule",
+    "TensorDictModuleWrapper",
+]
+
+
+class dispatch:
+    """Allows for a function expecting a TensorDict to be called using kwargs.
+
+    :func:`dispatch` must be used within modules that have an ``in_keys`` (or
+    another source of keys indicated by the ``source`` keyword argument) and
+    ``out_keys`` (or another ``dest`` key list) attributes indicating what keys
+    to be read and written from the tensordict. The wrapped function should
+    also have a ``tensordict`` leading argument.
+
+    The resulting function will return a single tensor (if there is a single
+    element in out_keys), otherwise it will return a tuple sorted as the ``out_keys``
+    of the module.
+
+    :func:`dispatch` can be used either as a method or as a class when extra arguments
+    need to be passed.
+
+    Args:
+        separator (str, optional): separator that combines sub-keys together
+            for ``in_keys`` that are tuples of strings.
+            Defaults to ``"_"``.
+        source (str or list of keys, optional): if a string is provided,
+            it points to the module attribute that contains the
+            list of input keys to be used. If a list is provided instead, it
+            will contain the keys used as input to the module.
+            Defaults to ``"in_keys"`` which is the attribute name of
+            :class:`~.TensorDictModule` list of input keys.
+        dest (str or list of keys, optional): if a string is provided,
+            it points to the module attribute that contains the
+            list of output keys to be used. If a list is provided instead, it
+            will contain the keys used as output to the module.
+            Defaults to ``"out_keys"`` which is the attribute name of
+            :class:`~.TensorDictModule` list of output keys.
+        auto_batch_size (bool, optional): if ``True``, the batch-size of the
+            input tensordict is determined automatically as the maximum number
+            of common dimensions across all the input tensors.
+            Defaults to ``True``.
+
+    Examples:
+        >>> class MyModule(nn.Module):
+        ...     in_keys = ["a"]
+        ...     out_keys = ["b"]
+        ...
+        ...     @dispatch
+        ...     def forward(self, tensordict):
+        ...         tensordict['b'] = tensordict['a'] + 1
+        ...         return tensordict
+        ...
+        >>> module = MyModule()
+        >>> b = module(a=torch.zeros(1, 2))
+        >>> assert (b == 1).all()
+        >>> # equivalently
+        >>> class MyModule(nn.Module):
+        ...     keys_in = ["a"]
+        ...     keys_out = ["b"]
+        ...
+        ...     @dispatch(source="keys_in", dest="keys_out")
+        ...     def forward(self, tensordict):
+        ...         tensordict['b'] = tensordict['a'] + 1
+        ...         return tensordict
+        ...
+        >>> module = MyModule()
+        >>> b = module(a=torch.zeros(1, 2))
+        >>> assert (b == 1).all()
+        >>> # or this
+        >>> class MyModule(nn.Module):
+        ...     @dispatch(source=["a"], dest=["b"])
+        ...     def forward(self, tensordict):
+        ...         tensordict['b'] = tensordict['a'] + 1
+        ...         return tensordict
+        ...
+        >>> module = MyModule()
+        >>> b = module(a=torch.zeros(1, 2))
+        >>> assert (b == 1).all()
+
+    :func:`dispatch_kwargs` will also work with nested keys with the default
+    ``"_"`` separator.
+
+    Examples:
+        >>> class MyModuleNest(nn.Module):
+        ...     in_keys = [("a", "c")]
+        ...     out_keys = ["b"]
+        ...
+        ...     @dispatch
+        ...     def forward(self, tensordict):
+        ...         tensordict['b'] = tensordict['a', 'c'] + 1
+        ...         return tensordict
+        ...
+        >>> module = MyModuleNest()
+        >>> b, = module(a_c=torch.zeros(1, 2))
+        >>> assert (b == 1).all()
+
+    If another separator is wanted, it can be indicated with the ``separator``
+    argument in the constructor:
+
+    Examples:
+        >>> class MyModuleNest(nn.Module):
+        ...     in_keys = [("a", "c")]
+        ...     out_keys = ["b"]
+        ...
+        ...     @dispatch(separator="sep")
+        ...     def forward(self, tensordict):
+        ...         tensordict['b'] = tensordict['a', 'c'] + 1
+        ...         return tensordict
+        ...
+        >>> module = MyModuleNest()
+        >>> b, = module(asepc=torch.zeros(1, 2))
+        >>> assert (b == 1).all()
+
+
+    Since the input keys is a sorted sequence of strings,
+    :func:`dispatch` can also be used with unnamed arguments where the order
+    must match the order of the input keys.
+
+    .. note::
+
+        If the first argument is a :class:`~.TensorDictBase` instance, it is
+        assumed that dispatch is __not__ being used and that this tensordict
+        contains all the necessary information to be run through the module.
+        In other words, one cannot decompose a tensordict with the first key
+        of the module inputs pointing to a tensordict instance.
+        In general, it is preferred to use :func:`dispatch` with tensordict
+        leaves only.
+
+    Examples:
+        >>> class MyModuleNest(nn.Module):
+        ...     in_keys = [("a", "c"), "d"]
+        ...     out_keys = ["b"]
+        ...
+        ...     @dispatch
+        ...     def forward(self, tensordict):
+        ...         tensordict['b'] = tensordict['a', 'c'] + tensordict["d"]
+        ...         return tensordict
+        ...
+        >>> module = MyModuleNest()
+        >>> b, = module(torch.zeros(1, 2), d=torch.ones(1, 2))  # works
+        >>> assert (b == 1).all()
+        >>> b, = module(torch.zeros(1, 2), torch.ones(1, 2))  # works
+        >>> assert (b == 1).all()
+        >>> try:
+        ...     b, = module(torch.zeros(1, 2), a_c=torch.ones(1, 2))  # fails
+        ... except:
+        ...     print("oopsy!")
+        ...
+
+    """
+
+    DEFAULT_SEPARATOR = "_"
+    DEFAULT_SOURCE = "in_keys"
+    DEFAULT_DEST = "out_keys"
+
+    def __new__(
+        cls,
+        separator=DEFAULT_SEPARATOR,
+        source=DEFAULT_SOURCE,
+        dest=DEFAULT_DEST,
+        auto_batch_size: bool = True,
+    ):
+        if callable(separator):
+            func = separator
+            separator = dispatch.DEFAULT_SEPARATOR
+            self = super().__new__(cls)
+            self.__init__(separator, source, dest)
+            return self.__call__(func)
+        return super().__new__(cls)
+
+    def __init__(
+        self,
+        separator=DEFAULT_SEPARATOR,
+        source=DEFAULT_SOURCE,
+        dest=DEFAULT_DEST,
+        auto_batch_size: bool = True,
+    ):
+        self.separator = separator
+        self.source = source
+        self.dest = dest
+        self.auto_batch_size = auto_batch_size
+
+    def __call__(self, func: Callable) -> Callable:
+        # sanity check
+        for i, key in enumerate(inspect.signature(func).parameters):
+            if i == 0:
+                # skip self
+                continue
+            if key != "tensordict":
+                raise RuntimeError(
+                    "the first argument of the wrapped function must be "
+                    "named 'tensordict'."
+                )
+            break
+
+        @functools.wraps(func)
+        def wrapper(_self, *args: Any, **kwargs: Any) -> Any:
+
+            source = self.source
+            if isinstance(source, str):
+                source = getattr(_self, source)
+            tensordict = None
+            if len(args):
+                if is_tensor_collection(args[0]):
+                    tensordict, args = args[0], args[1:]
+            if tensordict is None:
+                tensordict_values = {}
+                dest = self.dest
+                if isinstance(dest, str):
+                    dest = getattr(_self, dest)
+                for key in source:
+                    expected_key = (
+                        self.separator.join(key) if isinstance(key, tuple) else key
+                    )
+                    if len(args):
+                        tensordict_values[key] = args[0]
+                        args = args[1:]
+                        if expected_key in kwargs:
+                            raise RuntimeError(
+                                "Duplicated argument in args and kwargs."
+                            )
+                    elif expected_key in kwargs:
+                        try:
+                            tensordict_values[key] = kwargs.pop(expected_key)
+                        except KeyError:
+                            raise KeyError(
+                                f"The key {expected_key} wasn't found in the keyword arguments "
+                                f"but is expected to execute that function."
+                            )
+                tensordict = make_tensordict(
+                    tensordict_values,
+                    batch_size=torch.Size([]) if not self.auto_batch_size else None,
+                )
+                out = func(_self, tensordict, *args, **kwargs)
+                out = tuple(out[key] for key in dest)
+                return out[0] if len(out) == 1 else out
+            return func(_self, tensordict, *args, **kwargs)
+
+        return wrapper
+
+
+class _OutKeysSelect:
+    def __init__(self, out_keys):
+        self.out_keys = out_keys
+        self._initialized = False
+
+    def _init(self, module):
+        if self._initialized:
+            return
+        self._initialized = True
+        self.module = module
+        module.out_keys = list(self.out_keys)
+
+    @implement_for("torch", None, "2.0")
+    def __call__(  # noqa: F811
+        self,
+        module: TensorDictModuleBase,
+        tensordict_in: TensorDictBase,
+        tensordict_out: TensorDictBase,
+    ):
+        if not isinstance(tensordict_out, TensorDictBase):
+            raise RuntimeError(
+                "You are likely using tensordict.nn.dispatch with keyword arguments with an older (< 2.0) version of pytorch. "
+                "This is currently not supported. Please use unnamed arguments or upgrade pytorch."
+            )
+        # detect dispatch calls
+        in_keys = module.in_keys
+        is_dispatched = self._detect_dispatch(tensordict_in, in_keys)
+        out_keys = self.out_keys
+        # if dispatch filtered the out keys as they should we're happy
+        if is_dispatched:
+            if (not isinstance(tensordict_out, tuple) and len(out_keys) == 1) or (
+                len(out_keys) == len(tensordict_out)
+            ):
+                return tensordict_out
+        self._init(module)
+        if is_dispatched:
+            # it might be the case that dispatch was not aware of what the out-keys were.
+            if isinstance(tensordict_out, tuple):
+                out = tuple(
+                    item
+                    for i, item in enumerate(tensordict_out)
+                    if module._out_keys[i] in module.out_keys
+                )
+                if len(out) == 1:
+                    return out[0]
+                return out
+            elif module._out_keys[0] in module.out_keys and len(module._out_keys) == 1:
+                return tensordict_out
+            elif (
+                module._out_keys[0] not in module.out_keys
+                and len(module._out_keys) == 1
+            ):
+                return ()
+            else:
+                raise RuntimeError(
+                    f"Selecting out-keys failed. Original out_keys: {module._out_keys}, selected: {module.out_keys}."
+                )
+        if tensordict_out is tensordict_in:
+            return tensordict_out.select(
+                *in_keys,
+                *out_keys,
+                inplace=True,
+            )
+        else:
+            return tensordict_out.select(
+                *in_keys,
+                *out_keys,
+                inplace=True,
+                strict=False,
+            )
+
+    @implement_for("torch", "2.0", None)
+    def __call__(  # noqa: F811
+        self,
+        module: TensorDictModuleBase,
+        tensordict_in: TensorDictBase,
+        kwargs: Dict,
+        tensordict_out: TensorDictBase,
+    ):
+        # detect dispatch calls
+        in_keys = module.in_keys
+        if not tensordict_in and kwargs.get("tensordict", None) is not None:
+            tensordict_in = kwargs.pop("tensordict")
+        is_dispatched = self._detect_dispatch(tensordict_in, kwargs, in_keys)
+        out_keys = self.out_keys
+        # if dispatch filtered the out keys as they should we're happy
+        if is_dispatched:
+            if (not isinstance(tensordict_out, tuple) and len(out_keys) == 1) or (
+                len(out_keys) == len(tensordict_out)
+            ):
+                return tensordict_out
+        self._init(module)
+        if is_dispatched:
+            # it might be the case that dispatch was not aware of what the out-keys were.
+            if isinstance(tensordict_out, tuple):
+                out = tuple(
+                    item
+                    for i, item in enumerate(tensordict_out)
+                    if module._out_keys[i] in module.out_keys
+                )
+                if len(out) == 1:
+                    return out[0]
+                return out
+            elif module._out_keys[0] in module.out_keys and len(module._out_keys) == 1:
+                return tensordict_out
+            elif (
+                module._out_keys[0] not in module.out_keys
+                and len(module._out_keys) == 1
+            ):
+                return ()
+            else:
+                raise RuntimeError(
+                    f"Selecting out-keys failed. Original out_keys: {module._out_keys}, selected: {module.out_keys}."
+                )
+        if tensordict_out is tensordict_in:
+            return tensordict_out.select(
+                *in_keys,
+                *out_keys,
+                inplace=True,
+            )
+        else:
+            return tensordict_out.select(
+                *in_keys,
+                *out_keys,
+                inplace=True,
+                strict=False,
+            )
+
+    @implement_for("torch", None, "2.0")
+    def _detect_dispatch(self, tensordict_in, in_keys):  # noqa: F811
+        if isinstance(tensordict_in, TensorDictBase) and all(
+            key in tensordict_in.keys() for key in in_keys
+        ):
+            return False
+        elif isinstance(tensordict_in, tuple):
+            if len(tensordict_in):
+                if isinstance(tensordict_in[0], TensorDictBase):
+                    return self._detect_dispatch(tensordict_in[0], in_keys)
+                return True
+            return not len(in_keys)
+        # not a TDBase: must be True
+        return True
+
+    @implement_for("torch", "2.0", None)
+    def _detect_dispatch(self, tensordict_in, kwargs, in_keys):  # noqa: F811
+        if isinstance(tensordict_in, TensorDictBase) and all(
+            key in tensordict_in.keys(include_nested=True) for key in in_keys
+        ):
+            return False
+        elif isinstance(tensordict_in, tuple):
+            if len(tensordict_in) or len(kwargs):
+                if len(tensordict_in) and isinstance(tensordict_in[0], TensorDictBase):
+                    return self._detect_dispatch(tensordict_in[0], kwargs, in_keys)
+                elif (
+                    not len(tensordict_in)
+                    and len(kwargs)
+                    and isinstance(kwargs.get("tensordict", None), TensorDictBase)
+                ):
+                    return self._detect_dispatch(kwargs["tensordict"], in_keys)
+                return True
+            return not len(in_keys)
+        # not a TDBase: must be True
+        return True
+
+    def remove(self):
+        # reset ground truth
+        if self.module._out_keys is not None:
+            self.module.out_keys = self.module._out_keys
+
+    def __del__(self):
+        self.remove()
+
+
+class TensorDictModuleBase(nn.Module):
+    """Base class to TensorDict modules.
+
+    TensorDictModule subclasses are characterized by ``in_keys`` and ``out_keys``
+    key-lists that indicate what input entries are to be read and what output
+    entries should be expected to be written.
+
+    The forward method input/output signature should always follow the
+    convention:
+
+        >>> tensordict_out = module.forward(tensordict_in)
+
+    """
+
+    def __new__(cls, *args, **kwargs):
+        # check the out_keys and in_keys in the dict
+        if "in_keys" in cls.__dict__ and not isinstance(
+            cls.__dict__.get("in_keys"), property
+        ):
+            in_keys = cls.__dict__.get("in_keys")
+            # now let's remove it
+            delattr(cls, "in_keys")
+            cls._in_keys = unravel_key_list(in_keys)
+            cls.in_keys = TensorDictModuleBase.in_keys
+        if "out_keys" in cls.__dict__ and not isinstance(
+            cls.__dict__.get("out_keys"), property
+        ):
+            out_keys = cls.__dict__.get("out_keys")
+            # now let's remove it
+            delattr(cls, "out_keys")
+            out_keys = unravel_key_list(out_keys)
+            cls._out_keys = out_keys
+            cls._out_keys_apparent = out_keys
+            cls.out_keys = TensorDictModuleBase.out_keys
+        out = super().__new__(cls)
+        return out
+
+    @property
+    def in_keys(self):
+        return self._in_keys
+
+    @in_keys.setter
+    def in_keys(self, value: List[Union[str, Tuple[str]]]):
+        self._in_keys = unravel_key_list(value)
+
+    @property
+    def out_keys(self):
+        return self._out_keys_apparent
+
+    @property
+    def out_keys_source(self):
+        return self._out_keys
+
+    @out_keys.setter
+    def out_keys(self, value: List[Union[str, Tuple[str]]]):
+        # the first time out_keys are set, they are marked as ground truth
+        value = unravel_key_list(list(value))
+        if not hasattr(self, "_out_keys"):
+            self._out_keys = value
+        self._out_keys_apparent = value
+
+    @implement_for("torch", None, "2.0")
+    def select_out_keys(self, *out_keys):  # noqa: F811
+        """Selects the keys that will be found in the output tensordict.
+
+        This is useful whenever one wants to get rid of intermediate keys in a
+        complicated graph, or when the presence of these keys may trigger unexpected
+        behaviours.
+
+        The original ``out_keys`` can still be accessed via ``module.out_keys_source``.
+
+        Args:
+            *out_keys (a sequence of strings or tuples of strings): the
+                out_keys that should be found in the output tensordict.
+
+        Returns: the same module, modified in-place with updated ``out_keys``.
+
+        The simplest usage is with :class:`~.TensorDictModule`:
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
+            >>> import torch
+            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> mod.select_out_keys("d")
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        This feature will also work with dispatched arguments:
+        Examples:
+            >>> mod(torch.zeros(()), torch.ones(()))
+            tensor(2.)
+
+        This change will occur in-place (ie the same module will be returned
+        with an updated list of out_keys). It can be reverted using the
+        :meth:`TensorDictModuleBase.reset_out_keys` method.
+
+        Examples:
+            >>> mod.reset_out_keys()
+            >>> mod(TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, []))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        This will work with other classes too, such as Sequential:
+        Examples:
+            >>> from tensordict.nn import TensorDictSequential
+            >>> seq = TensorDictSequential(
+            ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["y"]),
+            ...     TensorDictModule(lambda x: x+1, in_keys=["y"], out_keys=["z"]),
+            ... )
+            >>> td = TensorDict({"x": torch.zeros(())}, [])
+            >>> seq(td)
+            TensorDict(
+                fields={
+                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> seq.select_out_keys("z")
+            >>> td = TensorDict({"x": torch.zeros(())}, [])
+            >>> seq(td)
+            TensorDict(
+                fields={
+                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        """
+        out_keys = unravel_key_list(list(out_keys))
+        if len(out_keys) == 1:
+            if out_keys[0] not in self.out_keys:
+                err_msg = f"Can't select non existent key: {out_keys[0]}. "
+                if (
+                    out_keys[0]
+                    and isinstance(out_keys[0], (tuple, list))
+                    and out_keys[0][0] in self.out_keys
+                ):
+                    err_msg += f"Are you passing the keys in a list? Try unpacking as: `{', '.join(out_keys[0])}`"
+                raise ValueError(err_msg)
+        self.register_forward_hook(_OutKeysSelect(out_keys))
+        for hook in self._forward_hooks.values():
+            hook._init(self)
+        return self
+
+    @implement_for("torch", "2.0", None)
+    def select_out_keys(self, *out_keys):  # noqa: F811
+        """Selects the keys that will be found in the output tensordict.
+
+        This is useful whenever one wants to get rid of intermediate keys in a
+        complicated graph, or when the presence of these keys may trigger unexpected
+        behaviours.
+
+        The original ``out_keys`` can still be accessed via ``module.out_keys_source``.
+
+        Args:
+            *out_keys (a sequence of strings or tuples of strings): the
+                out_keys that should be found in the output tensordict.
+
+        Returns: the same module, modified in-place with updated ``out_keys``.
+
+        The simplest usage is with :class:`~.TensorDictModule`:
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
+            >>> import torch
+            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> mod.select_out_keys("d")
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        This feature will also work with dispatched arguments:
+        Examples:
+            >>> mod(torch.zeros(()), torch.ones(()))
+            tensor(2.)
+
+        This change will occur in-place (ie the same module will be returned
+        with an updated list of out_keys). It can be reverted using the
+        :meth:`TensorDictModuleBase.reset_out_keys` method.
+
+        Examples:
+            >>> mod.reset_out_keys()
+            >>> mod(TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, []))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        This will work with other classes too, such as Sequential:
+        Examples:
+            >>> from tensordict.nn import TensorDictSequential
+            >>> seq = TensorDictSequential(
+            ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["y"]),
+            ...     TensorDictModule(lambda x: x+1, in_keys=["y"], out_keys=["z"]),
+            ... )
+            >>> td = TensorDict({"x": torch.zeros(())}, [])
+            >>> seq(td)
+            TensorDict(
+                fields={
+                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> seq.select_out_keys("z")
+            >>> td = TensorDict({"x": torch.zeros(())}, [])
+            >>> seq(td)
+            TensorDict(
+                fields={
+                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        """
+        out_keys = unravel_key_list(list(out_keys))
+        if len(out_keys) == 1:
+            if out_keys[0] not in self.out_keys:
+                err_msg = f"Can't select non existent key: {out_keys[0]}. "
+                if (
+                    out_keys[0]
+                    and isinstance(out_keys[0], (tuple, list))
+                    and out_keys[0][0] in self.out_keys
+                ):
+                    err_msg += f"Are you passing the keys in a list? Try unpacking as: `{', '.join(out_keys[0])}`"
+                raise ValueError(err_msg)
+        self.register_forward_hook(_OutKeysSelect(out_keys), with_kwargs=True)
+        for hook in self._forward_hooks.values():
+            hook._init(self)
+        return self
+
+    def reset_out_keys(self):
+        """Resets the ``out_keys`` attribute to its orignal value.
+
+        Returns: the same module, with its original ``out_keys`` values.
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
+            >>> import torch
+            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
+            >>> mod.select_out_keys("d")
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> mod.reset_out_keys()
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+        """
+        for i, hook in list(self._forward_hooks.items()):
+            if isinstance(hook, _OutKeysSelect):
+                del self._forward_hooks[i]
+        return self
+
+    def reset_parameters_recursive(
+        self, parameters: Optional[TensorDictBase] = None
+    ) -> Optional[TensorDictBase]:
+        """Recursively reset the parameters of the module and its children.
+
+        Args:
+            parameters (TensorDict of parameters, optional): If set to None, the module will reset using self.parameters().
+                Otherwise, we will reset the parameters in the tensordict in-place. This is
+                useful for functional modules where the parameters are not stored in the module itself.
+
+        Returns:
+            A tensordict of the new parameters, only if parameters was not None.
+
+        Examples:
+            >>> from tensordict.nn import TensorDictModule
+            >>> from torch import nn
+            >>> net = nn.Sequential(nn.Linear(2,3), nn.ReLU())
+            >>> old_param = net[0].weight.clone()
+            >>> module = TensorDictModule(net, in_keys=['bork'], out_keys=['dork'])
+            >>> module.reset_parameters()
+            >>> (old_param == net[0].weight).any()
+            tensor(False)
+
+        This method also supports functional parameter sampling:
+
+            >>> from tensordict import TensorDict
+            >>> from tensordict.nn import TensorDictModule
+            >>> from torch import nn
+            >>> net = nn.Sequential(nn.Linear(2,3), nn.ReLU())
+            >>> module = TensorDictModule(net, in_keys=['bork'], out_keys=['dork'])
+            >>> params = TensorDict.from_module(module)
+            >>> old_params = params.clone(recurse=True)
+            >>> module.reset_parameters(params)
+            >>> (old_params == params).any()
+            False
+        """
+        if parameters is None:
+            self._reset_parameters(self)
+            return
+        elif parameters.ndim:
+            raise RuntimeError(
+                "reset_parameters_recursive does not support batched TensorDicts, ensure `batch_size` is empty and the parameters shape match their original shape."
+            )
+
+        sanitized_parameters = parameters.apply(
+            lambda x: x.detach().requires_grad_(), inplace=False
+        )
+
+        if not is_functional(self):
+            make_functional(self, keep_params=True)
+        is_stateless = self._is_stateless
+        if is_stateless:
+            repopulate_module(self, sanitized_parameters)
+        else:
+            old_params = _swap_state(
+                self,
+                sanitized_parameters,
+                is_stateless=False,
+                return_old_tensordict=True,
+            )
+
+        self._reset_parameters(self)
+
+        if is_stateless:
+            new_parameters = extract_weights_and_buffers(self)
+        else:
+            new_parameters = _swap_state(
+                self, old_params, is_stateless=False, return_old_tensordict=True
+            )
+
+        return new_parameters
+
+    def _reset_parameters(self, module: nn.Module) -> None:
+        for child in module.children():
+            if isinstance(child, nn.Module):
+                self._reset_parameters(child)
+
+            if hasattr(child, "reset_parameters"):
+                child.reset_parameters()
+
+
+class TensorDictModule(TensorDictModuleBase):
+    """A TensorDictModule, is a python wrapper around a :obj:`nn.Module` that reads and writes to a TensorDict.
+
+    By default, :class:`TensorDictModule` subclasses are always functional,
+    meaning that they support the ``td_module(input, params=params)`` function
+    call signature.
+
+    Args:
+        module (Callable): a callable, typically a :class:`torch.nn.Module`,
+            used to map the input to the output parameter space. Its forward method
+            can return a single tensor, a tuple of tensors or even a dictionary.
+            In the latter case, the output keys of the :class:`TensorDictModule`
+            will be used to populate the output tensordict (ie. the keys present
+            in ``out_keys`` should be present in the dictionary returned by the
+            ``module`` forward method).
+        in_keys (iterable of NestedKeys, Dict[NestedStr, str]): keys to be read
+            from input tensordict and passed to the module. If it
+            contains more than one element, the values will be passed in the
+            order given by the in_keys iterable.
+            If ``in_keys`` is a dictionary, its keys must correspond to the key
+            to be read in the tensordict and its values must match the name of
+            the keyword argument in the function signature.
+        out_keys (iterable of str): keys to be written to the input tensordict. The length of out_keys must match the
+            number of tensors returned by the embedded module. Using "_" as a key avoid writing tensor to output.
+
+    Embedding a neural network in a TensorDictModule only requires to specify the input
+    and output keys. TensorDictModule support functional and regular :obj:`nn.Module`
+    objects. In the functional case, the 'params' (and 'buffers') keyword argument must
+    be specified:
+
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> # one can wrap regular nn.Module
+        >>> module = TensorDictModule(nn.Transformer(128), in_keys=["input", "tgt"], out_keys=["out"])
+        >>> input = torch.ones(2, 3, 128)
+        >>> tgt = torch.zeros(2, 3, 128)
+        >>> data = TensorDict({"input": input, "tgt": tgt}, batch_size=[2, 3])
+        >>> data = module(data)
+        >>> print(data)
+        TensorDict(
+            fields={
+                input: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False),
+                out: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False),
+                tgt: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([2, 3]),
+            device=None,
+            is_shared=False)
+
+    We can also pass directly the tensors
+
+    Examples:
+        >>> out = module(input, tgt)
+        >>> assert out.shape == input.shape
+        >>> # we can also wrap regular functions
+        >>> module = TensorDictModule(lambda x: (x-1, x+1), in_keys=[("input", "x")], out_keys=[("output", "x-1"), ("output", "x+1")])
+        >>> module(TensorDict({("input", "x"): torch.zeros(())}, batch_size=[]))
+        TensorDict(
+            fields={
+                input: TensorDict(
+                    fields={
+                        x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([]),
+                    device=None,
+                    is_shared=False),
+                output: TensorDict(
+                    fields={
+                        x+1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                        x-1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+
+    We can use TensorDictModule to populate a tensordict:
+
+    Examples:
+        >>> module = TensorDictModule(lambda: torch.randn(3), in_keys=[], out_keys=["x"])
+        >>> print(module(TensorDict({}, batch_size=[])))
+        TensorDict(
+            fields={
+                x: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+
+    Another feature is passing a dictionary as input keys, to control the
+    dispatching of values to specific keyword arguments.
+
+    Examples:
+        >>> module = TensorDictModule(lambda x, *, y: x+y,
+        ...     in_keys={'1': 'x', '2': 'y'}, out_keys=['z'],
+        ...     )
+        >>> td = module(TensorDict({'1': torch.ones(()), '2': torch.ones(())*2}, []))
+        >>> td['z']
+        tensor(3.)
+
+    Functional calls to a tensordict module is easy:
+
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import TensorDictModule
+        >>> from tensordict.nn.functional_modules import make_functional
+        >>> td = TensorDict({"input": torch.randn(3, 4), "hidden": torch.randn(3, 8)}, [3,])
+        >>> module = torch.nn.GRUCell(4, 8)
+        >>> td_module = TensorDictModule(
+        ...    module=module, in_keys=["input", "hidden"], out_keys=["output"]
+        ... )
+        >>> params = make_functional(td_module)
+        >>> td_functional = td_module(td.clone(), params=params)
+        >>> print(td_functional)
+        TensorDict(
+            fields={
+                hidden: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                output: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+
+    In the stateful case:
+        >>> module = torch.nn.GRUCell(4, 8)
+        >>> td_module = TensorDictModule(
+        ...    module=module, in_keys=["input", "hidden"], out_keys=["output"]
+        ... )
+        >>> td_stateful = td_module(td.clone())
+        >>> print(td_stateful)
+        TensorDict(
+            fields={
+                hidden: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                output: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+
+    One can use a vmap operator to call the functional module.
+
+    Examples:
+        >>> from torch import vmap
+        >>> from tensordict.nn.functional_modules import extract_weights_and_buffers
+        >>> params = extract_weights_and_buffers(td_module)
+        >>> params_repeat = params.expand(4)
+        >>> print(params_repeat)
+        TensorDict(
+            fields={
+                module: TensorDict(
+                    fields={
+                        bias_hh: Tensor(shape=torch.Size([4, 24]), device=cpu, dtype=torch.float32, is_shared=False),
+                        bias_ih: Tensor(shape=torch.Size([4, 24]), device=cpu, dtype=torch.float32, is_shared=False),
+                        weight_hh: Tensor(shape=torch.Size([4, 24, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                        weight_ih: Tensor(shape=torch.Size([4, 24, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([4]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([4]),
+            device=None,
+            is_shared=False)
+        >>> td_vmap = vmap(td_module, (None, 0))(td.clone(), params_repeat)
+        >>> print(td_vmap)
+        TensorDict(
+            fields={
+                hidden: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                output: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([4, 3]),
+            device=None,
+            is_shared=False)
+
+    """
+
+    _IN_KEY_ERR = "in_keys must be of type list, str or tuples of str, or dict."
+    _OUT_KEY_ERR = "out_keys must be of type list, str or tuples of str."
+
+    def __init__(
+        self,
+        module: Callable,
+        in_keys: NestedKey | List[NestedKey] | Dict[NestedKey:str],
+        out_keys: NestedKey | List[NestedKey],
+    ) -> None:
+        super().__init__()
+
+        if isinstance(in_keys, dict):
+            # write the kwargs and create a list instead
+            _in_keys = []
+            self._kwargs = []
+            for key, value in in_keys.items():
+                self._kwargs.append(value)
+                _in_keys.append(key)
+            in_keys = _in_keys
+        else:
+            if isinstance(in_keys, (str, tuple)):
+                in_keys = [in_keys]
+            elif not isinstance(in_keys, list):
+                raise ValueError(self._IN_KEY_ERR)
+            self._kwargs = None
+
+        if isinstance(out_keys, (str, tuple)):
+            out_keys = [out_keys]
+        elif not isinstance(out_keys, list):
+            raise ValueError(self._OUT_KEY_ERR)
+        try:
+            in_keys = unravel_key_list(in_keys)
+        except Exception:
+            raise ValueError(self._IN_KEY_ERR)
+        try:
+            out_keys = unravel_key_list(out_keys)
+        except Exception:
+            raise ValueError(self._OUT_KEY_ERR)
+
+        if type(module) is type or not callable(module):
+            raise ValueError(
+                f"Module {module} if type {type(module)} is not callable. "
+                f"Typical accepted types are nn.Module or TensorDictModule."
+            )
+        self.out_keys = out_keys
+        self.in_keys = in_keys
+
+        if "_" in self.in_keys:
+            warnings.warn(
+                'key "_" is for ignoring output, it should not be used in input keys',
+                stacklevel=2,
+            )
+
+        self.module = module
+        make_functional(self, keep_params=True, return_params=False)
+
+    @property
+    def is_functional(self) -> bool:
+        return _has_functorch and isinstance(
+            self.module,
+            (FunctionalModule, FunctionalModuleWithBuffers),
+        )
+
+    def _write_to_tensordict(
+        self,
+        tensordict: TensorDictBase,
+        tensors: list[Tensor],
+        tensordict_out: TensorDictBase | None = None,
+        out_keys: Iterable[NestedKey] | None = None,
+    ) -> TensorDictBase:
+        if out_keys is None:
+            out_keys = self.out_keys_source
+        if tensordict_out is None:
+            tensordict_out = tensordict
+        for _out_key, _tensor in zip(out_keys, tensors):
+            if _out_key != "_":
+                tensordict_out.set(_out_key, _tensor)
+        return tensordict_out
+
+    def _call_module(
+        self, tensors: Sequence[Tensor], **kwargs: Any
+    ) -> Tensor | Sequence[Tensor]:
+        out = self.module(*tensors, **kwargs)
+        return out
+
+    @dispatch(auto_batch_size=False)
+    @set_skip_existing(None)
+    def forward(
+        self,
+        tensordict: TensorDictBase,
+        *args,
+        tensordict_out: TensorDictBase | None = None,
+        **kwargs: Any,
+    ) -> TensorDictBase:
+        """When the tensordict parameter is not set, kwargs are used to create an instance of TensorDict."""
+        try:
+            if len(args):
+                tensordict_out = args[0]
+                args = args[1:]
+                # we will get rid of tensordict_out as a regular arg, because it
+                # blocks us when using vmap
+                # with stateful but functional modules: the functional module checks if
+                # it still contains parameters. If so it considers that only a "params" kwarg
+                # is indicative of what the params are, when we could potentially make a
+                # special rule for TensorDictModule that states that the second arg is
+                # likely to be the module params.
+                warnings.warn(
+                    "tensordict_out will be deprecated soon.",
+                    category=DeprecationWarning,
+                )
+            if len(args):
+                raise ValueError(
+                    "Got a non-empty list of extra agruments, when none was expected."
+                )
+            if self._kwargs is not None:
+                kwargs.update(
+                    {
+                        kwarg: tensordict.get(in_key, None)
+                        for kwarg, in_key in zip(self._kwargs, self.in_keys)
+                    }
+                )
+                tensors = ()
+            else:
+                tensors = tuple(tensordict.get(in_key, None) for in_key in self.in_keys)
+            try:
+                tensors = self._call_module(tensors, **kwargs)
+            except Exception as err:
+                if any(tensor is None for tensor in tensors) and "None" in str(err):
+                    none_set = {
+                        key
+                        for key, tensor in zip(self.in_keys, tensors)
+                        if tensor is None
+                    }
+                    raise KeyError(
+                        "Some tensors that are necessary for the module call may "
+                        "not have not been found in the input tensordict: "
+                        f"the following inputs are None: {none_set}."
+                    ) from err
+                else:
+                    raise err
+            if isinstance(tensors, (dict, TensorDictBase)):
+                if isinstance(tensors, dict):
+                    keys = unravel_key_list(list(tensors.keys()))
+                    values = tensors.values()
+                    tensors = dict(zip(keys, values))
+                tensors = tuple(tensors.get(key, None) for key in self.out_keys)
+            if not isinstance(tensors, tuple):
+                tensors = (tensors,)
+            tensordict_out = self._write_to_tensordict(
+                tensordict, tensors, tensordict_out
+            )
+            return tensordict_out
+        except Exception as err:
+            module = self.module
+            if not isinstance(module, nn.Module):
+                try:
+                    import inspect
+
+                    module = inspect.getsource(module)
+                except OSError:
+                    # then we can't print the source code
+                    pass
+            module = indent(str(module), 4 * " ")
+            in_keys = indent(f"in_keys={self.in_keys}", 4 * " ")
+            out_keys = indent(f"out_keys={self.out_keys}", 4 * " ")
+            raise RuntimeError(
+                f"TensorDictModule failed with operation\n{module}\n{in_keys}\n{out_keys}."
+            ) from err
+
+    @property
+    def device(self) -> torch.device:
+        for p in self.parameters():
+            return p.device
+        return torch.device("cpu")
+
+    def __repr__(self) -> str:
+        fields = indent(
+            f"module={self.module},\n"
+            f"device={self.device},\n"
+            f"in_keys={self.in_keys},\n"
+            f"out_keys={self.out_keys}",
+            4 * " ",
+        )
+
+        return f"{self.__class__.__name__}(\n{fields})"
+
+    def __getattr__(self, name: str) -> Any:
+        try:
+            return super().__getattr__(name)
+        except AttributeError as err1:
+            try:
+                return getattr(super().__getattr__("module"), name)
+            except Exception as err2:
+                raise err2 from err1
+
+    def __getstate__(self):
+        state = self.__dict__.copy()
+        if not isinstance(self.module, nn.Module):
+            state["module"] = cloudpickle_dumps(state["module"])
+        return state
+
+    def __setstate__(self, state):
+        if "module" in state:
+            state["module"] = cloudpickle_loads(state["module"])
+        self.__dict__ = state
+
+
+class TensorDictModuleWrapper(TensorDictModuleBase):
+    """Wrapper class for TensorDictModule objects.
+
+    Once created, a TensorDictModuleWrapper will behave exactly as the
+    TensorDictModule it contains except for the methods that are
+    overwritten.
+
+    Args:
+        td_module (TensorDictModule): operator to be wrapped.
+
+    """
+
+    def __init__(self, td_module: TensorDictModule) -> None:
+        super().__init__()
+        self.td_module = td_module
+        if len(self.td_module._forward_hooks):
+            for pre_hook in self.td_module._forward_hooks:
+                self.register_forward_hook(self.td_module._forward_hooks[pre_hook])
+
+    def __getattr__(self, name: str) -> Any:
+        try:
+            return super().__getattr__(name)
+        except AttributeError:
+            if name not in self.__dict__ and not name.startswith("__"):
+                return getattr(self._modules["td_module"], name)
+            else:
+                raise AttributeError(
+                    f"attribute {name} not recognised in {type(self).__name__}"
+                )
+
+    def forward(self, *args: Any, **kwargs: Any) -> TensorDictBase:
+        return self.td_module.forward(*args, **kwargs)
```

## tensordict/nn/ensemble.py

 * *Ordering differences only*

```diff
@@ -1,131 +1,131 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-import warnings
-
-import torch
-from tensordict import TensorDict
-from torch import nn
-
-from .common import TensorDictBase, TensorDictModuleBase
-from .functional_modules import make_functional
-
-
-class EnsembleModule(TensorDictModuleBase):
-    """Module that wraps a module and repeats it to form an ensemble.
-
-    Args:
-        module (nn.Module): The nn.module to duplicate and wrap.
-        num_copies (int): The number of copies of module to make.
-        parameter_init_function (Callable): A function that takes a module copy and initializes its parameters.
-        expand_input (bool): Whether to expand the input TensorDict to match the number of copies. This should be
-            True unless you are chaining ensemble modules together, e.g. EnsembleModule(cnn) -> EnsembleModule(mlp).
-            If False, EnsembleModule(mlp) will expected the previous module(s) to have already expanded the input.
-
-    Examples:
-        >>> import torch
-        >>> from torch import nn
-        >>> from tensordict.nn import TensorDictModule
-        >>> from torchrl.modules import EnsembleModule
-        >>> from tensordict import TensorDict
-        >>> net = nn.Sequential(nn.Linear(4, 32), nn.ReLU(), nn.Linear(32, 2))
-        >>> mod = TensorDictModule(net, in_keys=['a'], out_keys=['b'])
-        >>> ensemble = EnsembleModule(mod, num_copies=3)
-        >>> data = TensorDict({'a': torch.randn(10, 4)}, batch_size=[10])
-        >>> ensemble(data)
-        TensorDict(
-            fields={
-                a: Tensor(shape=torch.Size([3, 10, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                b: Tensor(shape=torch.Size([3, 10, 2]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([3, 10]),
-            device=None,
-            is_shared=False)
-
-    To stack EnsembleModules together, we should be mindful of turning off `expand_input` from the second module and on.
-
-    Examples:
-        >>> import torch
-        >>> from tensordict.nn import TensorDictModule, TensorDictSequential
-        >>> from torchrl.modules import EnsembleModule
-        >>> from tensordict import TensorDict
-        >>> module = TensorDictModule(torch.nn.Linear(2,3), in_keys=['bork'], out_keys=['dork'])
-        >>> next_module = TensorDictModule(torch.nn.Linear(3,1), in_keys=['dork'], out_keys=['spork'])
-        >>> e0 = EnsembleModule(module, num_copies=4, expand_input=True)
-        >>> e1 = EnsembleModule(next_module, num_copies=4, expand_input=False)
-        >>> seq = TensorDictSequential(e0, e1)
-        >>> data = TensorDict({'bork': torch.randn(5,2)}, batch_size=[5])
-        >>> seq(data)
-        TensorDict(
-            fields={
-                bork: Tensor(shape=torch.Size([4, 5, 2]), device=cpu, dtype=torch.float32, is_shared=False),
-                dork: Tensor(shape=torch.Size([4, 5, 3]), device=cpu, dtype=torch.float32, is_shared=False),
-                spork: Tensor(shape=torch.Size([4, 5, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([4, 5]),
-            device=None,
-            is_shared=False)
-    """
-
-    def __init__(
-        self,
-        module: TensorDictModuleBase,
-        num_copies: int,
-        expand_input: bool = True,
-    ):
-        super().__init__()
-        self.in_keys = module.in_keys
-        self.out_keys = module.out_keys
-        params_td = make_functional(module).expand(num_copies).to_tensordict()
-
-        self.module = module
-        self.params_td = params_td
-        self.ensemble_parameters = nn.ParameterList(
-            list(self.params_td.values(True, True))
-        )
-        if expand_input:
-            self.vmapped_forward = torch.vmap(self.module, (None, 0))
-        else:
-            self.vmapped_forward = torch.vmap(self.module, 0)
-
-        self.reset_parameters_recursive(self.params_td)
-
-    def forward(self, tensordict: TensorDict) -> TensorDict:
-        return self.vmapped_forward(tensordict, self.params_td)
-
-    def reset_parameters_recursive(
-        self, parameters: TensorDictBase = None
-    ) -> TensorDictBase:
-        """Resets the parameters of all the copies of the module.
-
-        Args:
-            parameters (TensorDict): A TensorDict of parameters for self.module. The batch dimension(s) of the tensordict
-                denote the number of module copies to reset.
-
-        Returns:
-            A TensorDict of pointers to the reset parameters.
-        """
-        if parameters is None:
-            raise ValueError(
-                "Ensembles are functional and require passing a TensorDict of parameters to reset_parameters_recursive"
-            )
-        if parameters.ndim:
-            params_pointers = []
-            for params_copy in parameters.unbind(0):
-                self.reset_parameters_recursive(params_copy)
-                params_pointers.append(params_copy)
-            return torch.stack(params_pointers, -1)
-        else:
-            # In case the user has added other neural networks to the EnsembleModule
-            # besides those in self.module
-            child_mods = [
-                mod
-                for name, mod in self.named_children()
-                if name != "module" and name != "ensemble_parameters"
-            ]
-            if child_mods:
-                warnings.warn(
-                    "EnsembleModule.reset_parameters_recursive() only resets parameters of self.module, but other parameters were detected. These parameters will not be reset."
-                )
-            # Reset all self.module descendant parameters
-            return self.module.reset_parameters_recursive(parameters)
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+import warnings
+
+import torch
+from tensordict import TensorDict
+from torch import nn
+
+from .common import TensorDictBase, TensorDictModuleBase
+from .functional_modules import make_functional
+
+
+class EnsembleModule(TensorDictModuleBase):
+    """Module that wraps a module and repeats it to form an ensemble.
+
+    Args:
+        module (nn.Module): The nn.module to duplicate and wrap.
+        num_copies (int): The number of copies of module to make.
+        parameter_init_function (Callable): A function that takes a module copy and initializes its parameters.
+        expand_input (bool): Whether to expand the input TensorDict to match the number of copies. This should be
+            True unless you are chaining ensemble modules together, e.g. EnsembleModule(cnn) -> EnsembleModule(mlp).
+            If False, EnsembleModule(mlp) will expected the previous module(s) to have already expanded the input.
+
+    Examples:
+        >>> import torch
+        >>> from torch import nn
+        >>> from tensordict.nn import TensorDictModule
+        >>> from torchrl.modules import EnsembleModule
+        >>> from tensordict import TensorDict
+        >>> net = nn.Sequential(nn.Linear(4, 32), nn.ReLU(), nn.Linear(32, 2))
+        >>> mod = TensorDictModule(net, in_keys=['a'], out_keys=['b'])
+        >>> ensemble = EnsembleModule(mod, num_copies=3)
+        >>> data = TensorDict({'a': torch.randn(10, 4)}, batch_size=[10])
+        >>> ensemble(data)
+        TensorDict(
+            fields={
+                a: Tensor(shape=torch.Size([3, 10, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                b: Tensor(shape=torch.Size([3, 10, 2]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([3, 10]),
+            device=None,
+            is_shared=False)
+
+    To stack EnsembleModules together, we should be mindful of turning off `expand_input` from the second module and on.
+
+    Examples:
+        >>> import torch
+        >>> from tensordict.nn import TensorDictModule, TensorDictSequential
+        >>> from torchrl.modules import EnsembleModule
+        >>> from tensordict import TensorDict
+        >>> module = TensorDictModule(torch.nn.Linear(2,3), in_keys=['bork'], out_keys=['dork'])
+        >>> next_module = TensorDictModule(torch.nn.Linear(3,1), in_keys=['dork'], out_keys=['spork'])
+        >>> e0 = EnsembleModule(module, num_copies=4, expand_input=True)
+        >>> e1 = EnsembleModule(next_module, num_copies=4, expand_input=False)
+        >>> seq = TensorDictSequential(e0, e1)
+        >>> data = TensorDict({'bork': torch.randn(5,2)}, batch_size=[5])
+        >>> seq(data)
+        TensorDict(
+            fields={
+                bork: Tensor(shape=torch.Size([4, 5, 2]), device=cpu, dtype=torch.float32, is_shared=False),
+                dork: Tensor(shape=torch.Size([4, 5, 3]), device=cpu, dtype=torch.float32, is_shared=False),
+                spork: Tensor(shape=torch.Size([4, 5, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([4, 5]),
+            device=None,
+            is_shared=False)
+    """
+
+    def __init__(
+        self,
+        module: TensorDictModuleBase,
+        num_copies: int,
+        expand_input: bool = True,
+    ):
+        super().__init__()
+        self.in_keys = module.in_keys
+        self.out_keys = module.out_keys
+        params_td = make_functional(module).expand(num_copies).to_tensordict()
+
+        self.module = module
+        self.params_td = params_td
+        self.ensemble_parameters = nn.ParameterList(
+            list(self.params_td.values(True, True))
+        )
+        if expand_input:
+            self.vmapped_forward = torch.vmap(self.module, (None, 0))
+        else:
+            self.vmapped_forward = torch.vmap(self.module, 0)
+
+        self.reset_parameters_recursive(self.params_td)
+
+    def forward(self, tensordict: TensorDict) -> TensorDict:
+        return self.vmapped_forward(tensordict, self.params_td)
+
+    def reset_parameters_recursive(
+        self, parameters: TensorDictBase = None
+    ) -> TensorDictBase:
+        """Resets the parameters of all the copies of the module.
+
+        Args:
+            parameters (TensorDict): A TensorDict of parameters for self.module. The batch dimension(s) of the tensordict
+                denote the number of module copies to reset.
+
+        Returns:
+            A TensorDict of pointers to the reset parameters.
+        """
+        if parameters is None:
+            raise ValueError(
+                "Ensembles are functional and require passing a TensorDict of parameters to reset_parameters_recursive"
+            )
+        if parameters.ndim:
+            params_pointers = []
+            for params_copy in parameters.unbind(0):
+                self.reset_parameters_recursive(params_copy)
+                params_pointers.append(params_copy)
+            return torch.stack(params_pointers, -1)
+        else:
+            # In case the user has added other neural networks to the EnsembleModule
+            # besides those in self.module
+            child_mods = [
+                mod
+                for name, mod in self.named_children()
+                if name != "module" and name != "ensemble_parameters"
+            ]
+            if child_mods:
+                warnings.warn(
+                    "EnsembleModule.reset_parameters_recursive() only resets parameters of self.module, but other parameters were detected. These parameters will not be reset."
+                )
+            # Reset all self.module descendant parameters
+            return self.module.reset_parameters_recursive(parameters)
```

## tensordict/nn/functional_modules.py

 * *Ordering differences only*

```diff
@@ -1,657 +1,657 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import inspect
-import re
-import types
-import warnings
-from copy import deepcopy
-from functools import wraps
-from typing import Any, Callable, Iterable
-
-import torch
-
-from tensordict import PYTREE_REGISTERED_TDS, TensorDict
-from tensordict.tensordict import _is_tensor_collection, TensorDictBase
-
-from tensordict.utils import implement_for
-from torch import nn
-from torch.utils._pytree import SUPPORTED_NODES
-
-try:
-    from torch.nn.modules.module import _global_parameter_registration_hooks
-except ImportError:
-    # old torch version, passing
-    pass
-
-__base__setattr__ = nn.Module.__setattr__
-
-
-@implement_for("torch", "2.0", None)
-def _register_params(self, name, param):
-    """A simplified version of register_param where checks are skipped."""
-    for hook in _global_parameter_registration_hooks.values():
-        output = hook(self, name, param)
-        if output is not None:
-            param = output
-    self._parameters[name] = param
-
-
-@implement_for("torch", None, "2.0")
-def _register_params(self, name, param):  # noqa: F811
-    self.register_parameter(name, param)
-
-
-def set_tensor(module: "torch.nn.Module", name: str, tensor: torch.Tensor) -> None:
-    """Simplified version of torch.nn.utils._named_member_accessor."""
-    if name in module._parameters:
-        del module._parameters[name]  # type: ignore[assignment]
-    was_buffer = name in module._buffers
-    if was_buffer:
-        del module._buffers[name]
-    if isinstance(tensor, nn.Parameter):
-        module.__dict__.pop(name, None)
-        # module.register_parameter(name, tensor)
-        _register_params(module, name, tensor)
-    elif was_buffer and isinstance(tensor, Tensor):
-        module._buffers[name] = tensor
-    else:
-        module.__dict__[name] = tensor
-
-
-@implement_for("torch", "2.0", None)
-def set_tensor_dict(  # noqa: F811
-    module_dict, module, name: str, tensor: torch.Tensor
-) -> None:
-    """Simplified version of torch.nn.utils._named_member_accessor."""
-    if name in module_dict["_parameters"]:
-        del module_dict["_parameters"][name]  # type: ignore[assignment]
-    was_buffer = name in module_dict["_buffers"]
-    if was_buffer:
-        del module_dict["_buffers"][name]
-    if isinstance(tensor, nn.Parameter):
-        module_dict.pop(name, None)
-        # module.register_parameter(name, tensor)
-        for hook in _global_parameter_registration_hooks.values():
-            output = hook(module, name, tensor)
-            if output is not None:
-                tensor = output
-        module_dict["_parameters"][name] = tensor
-    elif was_buffer and isinstance(tensor, Tensor):
-        module_dict["_buffers"][name] = tensor
-    else:
-        module_dict[name] = tensor
-
-
-@implement_for("torch", None, "2.0")
-def set_tensor_dict(  # noqa: F811
-    module_dict, module, name: str, tensor: torch.Tensor
-) -> None:
-    """Simplified version of torch.nn.utils._named_member_accessor."""
-    if name in module_dict["_parameters"]:
-        del module_dict["_parameters"][name]  # type: ignore[assignment]
-    was_buffer = name in module_dict["_buffers"]
-    if was_buffer:
-        del module_dict["_buffers"][name]
-    if isinstance(tensor, nn.Parameter):
-        module_dict.pop(name, None)
-        module.register_parameter(name, tensor)
-    elif was_buffer and isinstance(tensor, Tensor):
-        module_dict["_buffers"][name] = tensor
-    else:
-        module_dict[name] = tensor
-
-
-_RESET_OLD_TENSORDICT = True
-try:
-    import torch._functorch.vmap as vmap_src
-    from torch._functorch.vmap import (
-        _add_batch_dim,
-        _broadcast_to_and_flatten,
-        _get_name,
-        _remove_batch_dim,
-        _validate_and_get_batch_size,
-        Tensor,
-        tree_flatten,
-        tree_unflatten,
-    )
-
-    _has_functorch = True
-except ImportError:
-    try:
-        from functorch._src.vmap import (
-            _add_batch_dim,
-            _broadcast_to_and_flatten,
-            _get_name,
-            _remove_batch_dim,
-            _validate_and_get_batch_size,
-            Tensor,
-            tree_flatten,
-            tree_unflatten,
-        )
-
-        _has_functorch = True
-        import functorch._src.vmap as vmap_src
-    except ImportError:
-        _has_functorch = False
-
-
-class _exclude_td_from_pytree:
-    def __init__(self):
-        self.tdnodes = {}
-
-    def __enter__(self):
-        for tdtype in PYTREE_REGISTERED_TDS:
-            self.tdnodes[tdtype] = SUPPORTED_NODES.pop(tdtype)
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        for tdtype in PYTREE_REGISTERED_TDS:
-            SUPPORTED_NODES[tdtype] = self.tdnodes[tdtype]
-
-
-# Monkey-patch functorch, mainly for cases where a "isinstance(obj, Tensor) is invoked
-if _has_functorch:
-    # Monkey-patches
-
-    def _process_batched_inputs(
-        in_dims: int | tuple[int, ...], args: Any, func: Callable
-    ) -> tuple[Any, ...]:
-        if not isinstance(in_dims, int) and not isinstance(in_dims, tuple):
-            raise ValueError(
-                f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
-expected `in_dims` to be int or a (potentially nested) tuple
-matching the structure of inputs, got: {type(in_dims)}."""
-            )
-        if len(args) == 0:
-            raise ValueError(
-                f"""vmap({_get_name(func)})(<inputs>): got no inputs. Maybe you forgot to add
-inputs, or you are trying to vmap over a function with no inputs.
-The latter is unsupported."""
-            )
-
-        # we want to escape TensorDicts as they take care of adding the batch dimension
-        with _exclude_td_from_pytree():
-            flat_args, args_spec = tree_flatten(args)
-            flat_in_dims = _broadcast_to_and_flatten(in_dims, args_spec)
-            if flat_in_dims is None:
-                raise ValueError(
-                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
-    in_dims is not compatible with the structure of `inputs`.
-    in_dims has structure {tree_flatten(in_dims)[1]} but inputs
-    has structure {args_spec}."""
-                )
-
-        for i, (arg, in_dim) in enumerate(zip(flat_args, flat_in_dims)):
-            if not isinstance(in_dim, int) and in_dim is not None:
-                raise ValueError(
-                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
-Got in_dim={in_dim} for an input but in_dim must be either
-an integer dimension or None."""
-                )
-            if isinstance(in_dim, int) and not isinstance(
-                arg, (Tensor, TensorDictBase)
-            ):
-                raise ValueError(
-                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
-Got in_dim={in_dim} for an input but the input is of type
-{type(arg)}. We cannot vmap over non-Tensor arguments,
-please use None as the respective in_dim"""
-                )
-            if in_dim is not None and (in_dim < -arg.dim() or in_dim >= arg.dim()):
-                raise ValueError(
-                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
-Got in_dim={in_dim} for some input, but that input is a Tensor
-of dimensionality {arg.dim()} so expected in_dim to satisfy
--{arg.dim()} <= in_dim < {arg.dim()}."""
-                )
-            if in_dim is not None and in_dim < 0:
-                flat_in_dims[i] = in_dim % arg.dim()
-
-        return (
-            _validate_and_get_batch_size(flat_in_dims, flat_args),
-            flat_in_dims,
-            flat_args,
-            args_spec,
-        )
-
-    vmap_src._process_batched_inputs = _process_batched_inputs
-
-    def _create_batched_inputs(
-        flat_in_dims: list[int], flat_args: list[Any], vmap_level: int, args_spec
-    ) -> Any:
-        # See NOTE [Ignored _remove_batch_dim, _add_batch_dim]
-        # If tensordict, we remove the dim at batch_size[in_dim] such that the TensorDict can accept
-        # the batched tensors. This will be added in _unwrap_batched
-
-        batched_inputs = []
-        for in_dim, arg in zip(flat_in_dims, flat_args):
-            if in_dim is None:
-                if isinstance(arg, TensorDictBase):
-                    # this may be a perf bottleneck and could benefit from caching
-                    # arg = cache(arg.clone)(False)
-                    arg = arg.clone(False)
-
-                batched_input = arg
-            else:
-                if isinstance(arg, TensorDictBase):
-                    batched_input = arg._add_batch_dim(
-                        in_dim=in_dim, vmap_level=vmap_level
-                    )
-                else:
-                    batched_input = _add_batch_dim(arg, in_dim, vmap_level)
-            batched_inputs.append(batched_input)
-        with _exclude_td_from_pytree():
-            return tree_unflatten(batched_inputs, args_spec)
-
-    vmap_src._create_batched_inputs = _create_batched_inputs
-
-    def _unwrap_batched(
-        batched_outputs: Any,
-        out_dims: int | tuple[int, ...],
-        vmap_level: int,
-        batch_size: int,
-        func: Callable,
-    ) -> Any:
-        with _exclude_td_from_pytree():
-            flat_batched_outputs, output_spec = tree_flatten(batched_outputs)
-
-        for out in flat_batched_outputs:
-            # Change here:
-            if isinstance(out, (TensorDictBase, torch.Tensor)):
-                continue
-            raise ValueError(
-                f"vmap({_get_name(func)}, ...): `{_get_name(func)}` must only return "
-                f"Tensors, got type {type(out)} as a return."
-            )
-
-        def incompatible_error():
-            raise ValueError(
-                f"vmap({_get_name(func)}, ..., out_dims={out_dims})(<inputs>): "
-                f"out_dims is not compatible with the structure of `outputs`. "
-                f"out_dims has structure {tree_flatten(out_dims)[1]} but outputs "
-                f"has structure {output_spec}."
-            )
-
-        # Here:
-        if isinstance(batched_outputs, (TensorDictBase, torch.Tensor)):
-            # Some weird edge case requires us to spell out the following
-            # see test_out_dims_edge_case
-            if isinstance(out_dims, int):
-                flat_out_dims = [out_dims]
-            elif isinstance(out_dims, tuple) and len(out_dims) == 1:
-                flat_out_dims = out_dims
-                out_dims = out_dims[0]
-            else:
-                incompatible_error()
-        else:
-            flat_out_dims = _broadcast_to_and_flatten(out_dims, output_spec)
-            if flat_out_dims is None:
-                incompatible_error()
-        flat_outputs = []
-        for batched_output, out_dim in zip(flat_batched_outputs, flat_out_dims):
-            if not isinstance(batched_output, TensorDictBase):
-                out = _remove_batch_dim(batched_output, vmap_level, batch_size, out_dim)
-            else:
-                out = batched_output._remove_batch_dim(
-                    vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
-                )
-            flat_outputs.append(out)
-        with _exclude_td_from_pytree():
-            return tree_unflatten(flat_outputs, output_spec)
-
-    vmap_src._unwrap_batched = _unwrap_batched
-
-
-# Tensordict-compatible Functional modules
-
-
-def _decorate_funs(
-    model: nn.Module,
-    make_stateless: bool,
-    funs_to_decorate: Iterable[str] | None = None,
-) -> None:
-    if funs_to_decorate is None:
-        funs_to_decorate = ["forward"]
-    _is_functional = model.__dict__.get("_functionalized", False)
-    if not _is_functional:
-        model.__dict__["_functionalized"] = True
-        model.__dict__["_decorated_funs"] = set()
-
-    for fun_to_decorate in funs_to_decorate:
-        if fun_to_decorate in model.__dict__["_decorated_funs"]:
-            continue
-        try:
-            setattr(
-                model,
-                fun_to_decorate,
-                types.MethodType(_make_decorator(model, fun_to_decorate), model),
-            )
-            model.__dict__["_decorated_funs"].add(fun_to_decorate)
-        except AttributeError:
-            continue
-    if not model.__dict__.get("_is_stateless", False):
-        model.__dict__["_is_stateless"] = make_stateless
-
-    for module in model.children():
-        # we decorate forward for the sub-modules
-        _decorate_funs(module, make_stateless=make_stateless)
-
-
-def extract_weights_and_buffers(
-    model: nn.Module,
-) -> TensorDict:
-    """Extracts the weights and buffers of a model in a tensordict, and adapts the modules to read those inputs."""
-    tensordict = {}
-    for name, param in list(model.named_parameters(recurse=False)):
-        setattr(model, name, None)
-        tensordict[name] = param
-
-    for name, param in list(model.named_buffers(recurse=False)):
-        setattr(model, name, None)
-        tensordict[name] = param
-
-    for name, module in model.named_children():
-        module_tensordict = extract_weights_and_buffers(module)
-        if module_tensordict is not None:
-            tensordict[name] = module_tensordict
-    model.__dict__["_is_stateless"] = True
-    return TensorDict(tensordict, batch_size=torch.Size([]), _run_checks=False)
-
-
-# For bookkeeping: this function seems to have the same runtime but will not access
-# modules that don't have parameters if they're not registered as empty tensordicts
-# in the input. Hence they won't be turned as stateful, which could cause some bugs.
-def _swap_state(
-    model: nn.Module,
-    tensordict: TensorDict,
-    is_stateless: bool,
-    return_old_tensordict: bool = False,
-    old_tensordict: dict[str, torch.Tensor] | TensorDict | None = None,
-) -> dict[str, torch.Tensor] | TensorDict | None:
-    __dict__ = model.__dict__
-    was_stateless = __dict__.get("_is_stateless", None)
-    if was_stateless is None:
-        raise Exception(f"{model}\nhas no stateless attribute.")
-    __dict__["_is_stateless"] = is_stateless
-    # return_old_tensordict = return_old_tensordict and not was_stateless
-    if old_tensordict is None:
-        old_tensordict_dict = old_tensordict = {}
-    else:
-        old_tensordict_dict = {}
-    for key, value in tensordict.items():
-        cls = value.__class__
-        if _is_tensor_collection(cls) or issubclass(cls, dict):
-            _old_value = old_tensordict.get(key, None)
-            _old_value = _swap_state(
-                __dict__["_modules"][key],
-                value,
-                is_stateless=is_stateless,
-                old_tensordict=_old_value,
-                return_old_tensordict=return_old_tensordict,
-            )
-            old_tensordict_dict[key] = _old_value
-        else:
-            _old_value = None
-            if return_old_tensordict:
-                _old_value = __dict__["_parameters"].get(key, None)
-                if _old_value is None:
-                    _old_value = __dict__["_buffers"].get(key, None)
-                if _old_value is None:
-                    _old_value = __dict__.get(key, None)
-                if _old_value is None:
-                    pass
-                    # _old_value = torch.zeros(*value.shape, 0)
-                old_tensordict_dict[key] = _old_value
-                # old_tensordict_dict[key] = _old_value
-            if model.__class__.__setattr__ is __base__setattr__:
-                set_tensor_dict(__dict__, model, key, value)
-            else:
-                setattr(model, key, value)
-    old_tensordict.update(old_tensordict_dict)
-    if was_stateless or not return_old_tensordict:
-        return old_tensordict
-    else:
-        return TensorDict(old_tensordict, [], _run_checks=False)
-
-
-# def _swap_state(
-#     model: nn.Module,
-#     tensordict: TensorDict,
-#     is_stateless: bool,
-#     return_old_tensordict: bool = False,
-#     old_tensordict: dict[str, torch.Tensor] | TensorDict | None = None,
-# ) -> dict[str, torch.Tensor] | TensorDict | None:
-#     __dict__ = model.__dict__
-#     was_stateless = __dict__.get("_is_stateless", None)
-#     if was_stateless is None:
-#         raise Exception(f"{model}\nhas no stateless attribute.")
-#     __dict__["_is_stateless"] = is_stateless
-#     # return_old_tensordict = return_old_tensordict and not was_stateless
-#     if old_tensordict is None:
-#         old_tensordict_dict = old_tensordict = {}
-#     else:
-#         old_tensordict_dict = {}
-#     # keys = set(tensordict.keys())
-#     children = set()
-#     # this loop ignores the memo from named children
-#     for key, child in __dict__["_modules"].items():  # model.named_children():
-#         children.add(key)
-#         value = tensordict.get(key, None)
-#         if value is None:
-#             # faster than get(key, Tensordict(...))
-#             value = {}
-#
-#         _old_value = old_tensordict.get(key, None)
-#         _old_value = _swap_state(
-#             child,
-#             value,
-#             return_old_tensordict=return_old_tensordict,
-#             old_tensordict=_old_value,
-#             is_stateless=is_stateless,
-#         )
-#         old_tensordict_dict[key] = _old_value
-#     for key in tensordict.keys():
-#         if key in children:
-#             continue
-#         value = tensordict.get(key)
-#         if return_old_tensordict:
-#             old_attr = __dict__["_parameters"].get(key, None)
-#             if old_attr is None:
-#                 old_attr = __dict__["_buffers"].get(key, None)
-#             if old_attr is None:
-#                 old_attr = __dict__.get(key, None)
-#             if old_attr is None:
-#                 old_attr = torch.zeros(*value.shape, 0)
-#             old_tensordict_dict[key] = old_attr
-#         # is_param = key in model.__dict__.get("_parameters")
-#         # if is_param:
-#         #     delattr(model, key)
-#         #     print(value)
-#         set_tensor_dict(__dict__, model, key, value)
-#     old_tensordict.update(old_tensordict_dict)
-#     if was_stateless or not return_old_tensordict:
-#         return old_tensordict
-#     else:
-#         return TensorDict(old_tensordict, [])
-
-
-def is_functional(module: nn.Module):
-    """Checks if :func:`make_functional` has been called on the module."""
-    return "_functionalized" in module.__dict__
-
-
-def make_functional(
-    module: nn.Module,
-    funs_to_decorate: Iterable[str] | None = None,
-    keep_params: bool = False,
-    return_params: bool = True,
-) -> TensorDict:
-    """Converts a nn.Module to a functional module in-place, and returns its params.
-
-    Args:
-        module (torch.nn.Module): module that is to be made functional.
-        funs_to_decorate (iterable of str, optional): each string must correspond
-            to a function belonging to module. For nested modules, the
-            :meth:`torch.nn.Module.forward` method will be decorated.
-            Defaults to ``"forward"``.
-        keep_params (bool, optional): if ``True``, the module will keep its
-            parameters. Defaults to ``False``.
-        return_params (bool, optional): if ``True``, the parameters will
-            be collected in a nested tensordict and returned. If ``False``,
-            the module will be made functional but still be stateful.
-
-    """
-    _is_stateless = module.__dict__.get("_is_stateless", False)
-    _decorate_funs(
-        module,
-        funs_to_decorate=funs_to_decorate,
-        make_stateless=not keep_params,
-    )
-    if return_params and not _is_stateless:
-        params = extract_weights_and_buffers(
-            module,
-        )
-        if keep_params:
-            repopulate_module(module, params)
-        return params.lock_()
-    elif return_params and _is_stateless:
-        raise RuntimeError(
-            "Calling make_functional with return_params=True on a functional, stateless module. "
-        )
-    elif not keep_params:
-        extract_weights_and_buffers(module)
-
-
-def get_functional(
-    module: nn.Module,
-    funs_to_decorate: Iterable[str] | None = None,
-) -> nn.Module:
-    """Converts a nn.Module to a functional module in-place, and returns a stateful version of this module that can be used in functional settings."""
-    params = make_functional(module, funs_to_decorate=funs_to_decorate)
-    out = deepcopy(module)
-    repopulate_module(module, params)
-    return out
-
-
-def _make_decorator(module: nn.Module, fun_name: str) -> Callable:
-    fun = getattr(module, fun_name)
-
-    from tensordict.nn.common import TensorDictModuleBase
-
-    @wraps(fun)
-    def new_fun(self, *args, **kwargs):
-        # 3 use cases: (1) params is the last arg, (2) params is in kwargs, (3) no params
-        _is_stateless = self.__dict__.get("_is_stateless", False)
-        params = kwargs.pop("params", None)
-
-        if isinstance(self, TensorDictModuleBase):
-            if (
-                params is None
-                and len(args) == 2
-                and all(_is_tensor_collection(item.__class__) for item in args)
-            ):
-                params = args[1]
-                args = args[:1]
-        elif (
-            len(args) and _is_tensor_collection(args[0].__class__)
-        ) or "tensordict" in kwargs:
-            warnings.warn(
-                "You are passing a tensordict/tensorclass instance to a module that "
-                "does not inherit from TensorDictModuleBase. This may lead to unexpected "
-                "behaviours with functional calls."
-            )
-        if _is_stateless or params is not None:
-            if params is None:
-                params = args[-1]
-                args = args[:-1]
-                # get the previous params, and tell the submodules not to look for params anymore
-            old_params = _assign_params(
-                self, params, make_stateless=False, return_old_tensordict=True
-            )
-            try:
-                out = getattr(type(self), fun_name)(self, *args, **kwargs)
-            finally:
-                # reset the previous params, and tell the submodules to look for params
-                _assign_params(
-                    self,
-                    old_params,
-                    make_stateless=_is_stateless,
-                    return_old_tensordict=False,
-                )
-            return out
-        else:
-            try:
-                return getattr(type(self), fun_name)(self, *args, **kwargs)
-            except TypeError as err:
-                pattern = r".*takes \d+ positional arguments but \d+ were given|got multiple values for argument"
-                pattern = re.compile(pattern)
-                if pattern.search(str(err)) and isinstance(args[-1], TensorDictBase):
-                    # this is raised whenever the module is an nn.Module (not a TensorDictModuleBase)
-                    raise TypeError(
-                        "It seems you tried to provide the parameters as an argument to the module when the module was not stateless. "
-                        "If this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` "
-                        "syntax."
-                    ) from err
-                else:
-                    raise err
-
-    # we need to update the signature so that params can be the last positional arg
-    oldsig = inspect.signature(fun)
-    if "_forward_unimplemented" in fun.__name__:
-        raise AttributeError("_forward_unimplemented not supported")
-    # search if a VAR_POSITIONAL or VAR_KEYWORD is present
-    # if yes insert step parameter before it, else insert it in last position
-    params = list(oldsig.parameters.values())
-    for i, param in enumerate(params):
-        if param.kind == inspect.Parameter.KEYWORD_ONLY:
-            out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
-            break
-        if param.kind == inspect.Parameter.VAR_POSITIONAL:
-            out_type = inspect.Parameter.KEYWORD_ONLY
-            i = i + 1
-            break
-        if param.kind == inspect.Parameter.VAR_KEYWORD:
-            out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
-            break
-        if (
-            param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD
-            and param.default is not inspect._empty
-        ):
-            out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
-            break
-    else:
-        out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
-        i = len(params)
-    # new parameter name is params or params_[_...] if params if already present
-    name = "params"
-    while name in oldsig.parameters:
-        name += "_"
-    newparam = inspect.Parameter(name, out_type, default=None)
-    params.insert(i, newparam)
-    # we can now build the signature for the wrapper function
-    sig = oldsig.replace(parameters=params)
-
-    new_fun.__signature__ = sig
-    return new_fun
-
-
-def _assign_params(
-    module: nn.Module,
-    params: TensorDict,
-    make_stateless: bool,
-    return_old_tensordict: bool,
-) -> TensorDict | None:
-    if params is not None:
-        return _swap_state(module, params, make_stateless, return_old_tensordict)
-
-    return None
-
-
-def repopulate_module(model: nn.Module, tensordict: TensorDict) -> nn.Module:
-    """Repopulates a module with its parameters, presented as a nested TensorDict."""
-    _swap_state(model, tensordict, is_stateless=False)
-    return model
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import inspect
+import re
+import types
+import warnings
+from copy import deepcopy
+from functools import wraps
+from typing import Any, Callable, Iterable
+
+import torch
+
+from tensordict import PYTREE_REGISTERED_TDS, TensorDict
+from tensordict.tensordict import _is_tensor_collection, TensorDictBase
+
+from tensordict.utils import implement_for
+from torch import nn
+from torch.utils._pytree import SUPPORTED_NODES
+
+try:
+    from torch.nn.modules.module import _global_parameter_registration_hooks
+except ImportError:
+    # old torch version, passing
+    pass
+
+__base__setattr__ = nn.Module.__setattr__
+
+
+@implement_for("torch", "2.0", None)
+def _register_params(self, name, param):
+    """A simplified version of register_param where checks are skipped."""
+    for hook in _global_parameter_registration_hooks.values():
+        output = hook(self, name, param)
+        if output is not None:
+            param = output
+    self._parameters[name] = param
+
+
+@implement_for("torch", None, "2.0")
+def _register_params(self, name, param):  # noqa: F811
+    self.register_parameter(name, param)
+
+
+def set_tensor(module: "torch.nn.Module", name: str, tensor: torch.Tensor) -> None:
+    """Simplified version of torch.nn.utils._named_member_accessor."""
+    if name in module._parameters:
+        del module._parameters[name]  # type: ignore[assignment]
+    was_buffer = name in module._buffers
+    if was_buffer:
+        del module._buffers[name]
+    if isinstance(tensor, nn.Parameter):
+        module.__dict__.pop(name, None)
+        # module.register_parameter(name, tensor)
+        _register_params(module, name, tensor)
+    elif was_buffer and isinstance(tensor, Tensor):
+        module._buffers[name] = tensor
+    else:
+        module.__dict__[name] = tensor
+
+
+@implement_for("torch", "2.0", None)
+def set_tensor_dict(  # noqa: F811
+    module_dict, module, name: str, tensor: torch.Tensor
+) -> None:
+    """Simplified version of torch.nn.utils._named_member_accessor."""
+    if name in module_dict["_parameters"]:
+        del module_dict["_parameters"][name]  # type: ignore[assignment]
+    was_buffer = name in module_dict["_buffers"]
+    if was_buffer:
+        del module_dict["_buffers"][name]
+    if isinstance(tensor, nn.Parameter):
+        module_dict.pop(name, None)
+        # module.register_parameter(name, tensor)
+        for hook in _global_parameter_registration_hooks.values():
+            output = hook(module, name, tensor)
+            if output is not None:
+                tensor = output
+        module_dict["_parameters"][name] = tensor
+    elif was_buffer and isinstance(tensor, Tensor):
+        module_dict["_buffers"][name] = tensor
+    else:
+        module_dict[name] = tensor
+
+
+@implement_for("torch", None, "2.0")
+def set_tensor_dict(  # noqa: F811
+    module_dict, module, name: str, tensor: torch.Tensor
+) -> None:
+    """Simplified version of torch.nn.utils._named_member_accessor."""
+    if name in module_dict["_parameters"]:
+        del module_dict["_parameters"][name]  # type: ignore[assignment]
+    was_buffer = name in module_dict["_buffers"]
+    if was_buffer:
+        del module_dict["_buffers"][name]
+    if isinstance(tensor, nn.Parameter):
+        module_dict.pop(name, None)
+        module.register_parameter(name, tensor)
+    elif was_buffer and isinstance(tensor, Tensor):
+        module_dict["_buffers"][name] = tensor
+    else:
+        module_dict[name] = tensor
+
+
+_RESET_OLD_TENSORDICT = True
+try:
+    import torch._functorch.vmap as vmap_src
+    from torch._functorch.vmap import (
+        _add_batch_dim,
+        _broadcast_to_and_flatten,
+        _get_name,
+        _remove_batch_dim,
+        _validate_and_get_batch_size,
+        Tensor,
+        tree_flatten,
+        tree_unflatten,
+    )
+
+    _has_functorch = True
+except ImportError:
+    try:
+        from functorch._src.vmap import (
+            _add_batch_dim,
+            _broadcast_to_and_flatten,
+            _get_name,
+            _remove_batch_dim,
+            _validate_and_get_batch_size,
+            Tensor,
+            tree_flatten,
+            tree_unflatten,
+        )
+
+        _has_functorch = True
+        import functorch._src.vmap as vmap_src
+    except ImportError:
+        _has_functorch = False
+
+
+class _exclude_td_from_pytree:
+    def __init__(self):
+        self.tdnodes = {}
+
+    def __enter__(self):
+        for tdtype in PYTREE_REGISTERED_TDS:
+            self.tdnodes[tdtype] = SUPPORTED_NODES.pop(tdtype)
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        for tdtype in PYTREE_REGISTERED_TDS:
+            SUPPORTED_NODES[tdtype] = self.tdnodes[tdtype]
+
+
+# Monkey-patch functorch, mainly for cases where a "isinstance(obj, Tensor) is invoked
+if _has_functorch:
+    # Monkey-patches
+
+    def _process_batched_inputs(
+        in_dims: int | tuple[int, ...], args: Any, func: Callable
+    ) -> tuple[Any, ...]:
+        if not isinstance(in_dims, int) and not isinstance(in_dims, tuple):
+            raise ValueError(
+                f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
+expected `in_dims` to be int or a (potentially nested) tuple
+matching the structure of inputs, got: {type(in_dims)}."""
+            )
+        if len(args) == 0:
+            raise ValueError(
+                f"""vmap({_get_name(func)})(<inputs>): got no inputs. Maybe you forgot to add
+inputs, or you are trying to vmap over a function with no inputs.
+The latter is unsupported."""
+            )
+
+        # we want to escape TensorDicts as they take care of adding the batch dimension
+        with _exclude_td_from_pytree():
+            flat_args, args_spec = tree_flatten(args)
+            flat_in_dims = _broadcast_to_and_flatten(in_dims, args_spec)
+            if flat_in_dims is None:
+                raise ValueError(
+                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
+    in_dims is not compatible with the structure of `inputs`.
+    in_dims has structure {tree_flatten(in_dims)[1]} but inputs
+    has structure {args_spec}."""
+                )
+
+        for i, (arg, in_dim) in enumerate(zip(flat_args, flat_in_dims)):
+            if not isinstance(in_dim, int) and in_dim is not None:
+                raise ValueError(
+                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
+Got in_dim={in_dim} for an input but in_dim must be either
+an integer dimension or None."""
+                )
+            if isinstance(in_dim, int) and not isinstance(
+                arg, (Tensor, TensorDictBase)
+            ):
+                raise ValueError(
+                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
+Got in_dim={in_dim} for an input but the input is of type
+{type(arg)}. We cannot vmap over non-Tensor arguments,
+please use None as the respective in_dim"""
+                )
+            if in_dim is not None and (in_dim < -arg.dim() or in_dim >= arg.dim()):
+                raise ValueError(
+                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
+Got in_dim={in_dim} for some input, but that input is a Tensor
+of dimensionality {arg.dim()} so expected in_dim to satisfy
+-{arg.dim()} <= in_dim < {arg.dim()}."""
+                )
+            if in_dim is not None and in_dim < 0:
+                flat_in_dims[i] = in_dim % arg.dim()
+
+        return (
+            _validate_and_get_batch_size(flat_in_dims, flat_args),
+            flat_in_dims,
+            flat_args,
+            args_spec,
+        )
+
+    vmap_src._process_batched_inputs = _process_batched_inputs
+
+    def _create_batched_inputs(
+        flat_in_dims: list[int], flat_args: list[Any], vmap_level: int, args_spec
+    ) -> Any:
+        # See NOTE [Ignored _remove_batch_dim, _add_batch_dim]
+        # If tensordict, we remove the dim at batch_size[in_dim] such that the TensorDict can accept
+        # the batched tensors. This will be added in _unwrap_batched
+
+        batched_inputs = []
+        for in_dim, arg in zip(flat_in_dims, flat_args):
+            if in_dim is None:
+                if isinstance(arg, TensorDictBase):
+                    # this may be a perf bottleneck and could benefit from caching
+                    # arg = cache(arg.clone)(False)
+                    arg = arg.clone(False)
+
+                batched_input = arg
+            else:
+                if isinstance(arg, TensorDictBase):
+                    batched_input = arg._add_batch_dim(
+                        in_dim=in_dim, vmap_level=vmap_level
+                    )
+                else:
+                    batched_input = _add_batch_dim(arg, in_dim, vmap_level)
+            batched_inputs.append(batched_input)
+        with _exclude_td_from_pytree():
+            return tree_unflatten(batched_inputs, args_spec)
+
+    vmap_src._create_batched_inputs = _create_batched_inputs
+
+    def _unwrap_batched(
+        batched_outputs: Any,
+        out_dims: int | tuple[int, ...],
+        vmap_level: int,
+        batch_size: int,
+        func: Callable,
+    ) -> Any:
+        with _exclude_td_from_pytree():
+            flat_batched_outputs, output_spec = tree_flatten(batched_outputs)
+
+        for out in flat_batched_outputs:
+            # Change here:
+            if isinstance(out, (TensorDictBase, torch.Tensor)):
+                continue
+            raise ValueError(
+                f"vmap({_get_name(func)}, ...): `{_get_name(func)}` must only return "
+                f"Tensors, got type {type(out)} as a return."
+            )
+
+        def incompatible_error():
+            raise ValueError(
+                f"vmap({_get_name(func)}, ..., out_dims={out_dims})(<inputs>): "
+                f"out_dims is not compatible with the structure of `outputs`. "
+                f"out_dims has structure {tree_flatten(out_dims)[1]} but outputs "
+                f"has structure {output_spec}."
+            )
+
+        # Here:
+        if isinstance(batched_outputs, (TensorDictBase, torch.Tensor)):
+            # Some weird edge case requires us to spell out the following
+            # see test_out_dims_edge_case
+            if isinstance(out_dims, int):
+                flat_out_dims = [out_dims]
+            elif isinstance(out_dims, tuple) and len(out_dims) == 1:
+                flat_out_dims = out_dims
+                out_dims = out_dims[0]
+            else:
+                incompatible_error()
+        else:
+            flat_out_dims = _broadcast_to_and_flatten(out_dims, output_spec)
+            if flat_out_dims is None:
+                incompatible_error()
+        flat_outputs = []
+        for batched_output, out_dim in zip(flat_batched_outputs, flat_out_dims):
+            if not isinstance(batched_output, TensorDictBase):
+                out = _remove_batch_dim(batched_output, vmap_level, batch_size, out_dim)
+            else:
+                out = batched_output._remove_batch_dim(
+                    vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
+                )
+            flat_outputs.append(out)
+        with _exclude_td_from_pytree():
+            return tree_unflatten(flat_outputs, output_spec)
+
+    vmap_src._unwrap_batched = _unwrap_batched
+
+
+# Tensordict-compatible Functional modules
+
+
+def _decorate_funs(
+    model: nn.Module,
+    make_stateless: bool,
+    funs_to_decorate: Iterable[str] | None = None,
+) -> None:
+    if funs_to_decorate is None:
+        funs_to_decorate = ["forward"]
+    _is_functional = model.__dict__.get("_functionalized", False)
+    if not _is_functional:
+        model.__dict__["_functionalized"] = True
+        model.__dict__["_decorated_funs"] = set()
+
+    for fun_to_decorate in funs_to_decorate:
+        if fun_to_decorate in model.__dict__["_decorated_funs"]:
+            continue
+        try:
+            setattr(
+                model,
+                fun_to_decorate,
+                types.MethodType(_make_decorator(model, fun_to_decorate), model),
+            )
+            model.__dict__["_decorated_funs"].add(fun_to_decorate)
+        except AttributeError:
+            continue
+    if not model.__dict__.get("_is_stateless", False):
+        model.__dict__["_is_stateless"] = make_stateless
+
+    for module in model.children():
+        # we decorate forward for the sub-modules
+        _decorate_funs(module, make_stateless=make_stateless)
+
+
+def extract_weights_and_buffers(
+    model: nn.Module,
+) -> TensorDict:
+    """Extracts the weights and buffers of a model in a tensordict, and adapts the modules to read those inputs."""
+    tensordict = {}
+    for name, param in list(model.named_parameters(recurse=False)):
+        setattr(model, name, None)
+        tensordict[name] = param
+
+    for name, param in list(model.named_buffers(recurse=False)):
+        setattr(model, name, None)
+        tensordict[name] = param
+
+    for name, module in model.named_children():
+        module_tensordict = extract_weights_and_buffers(module)
+        if module_tensordict is not None:
+            tensordict[name] = module_tensordict
+    model.__dict__["_is_stateless"] = True
+    return TensorDict(tensordict, batch_size=torch.Size([]), _run_checks=False)
+
+
+# For bookkeeping: this function seems to have the same runtime but will not access
+# modules that don't have parameters if they're not registered as empty tensordicts
+# in the input. Hence they won't be turned as stateful, which could cause some bugs.
+def _swap_state(
+    model: nn.Module,
+    tensordict: TensorDict,
+    is_stateless: bool,
+    return_old_tensordict: bool = False,
+    old_tensordict: dict[str, torch.Tensor] | TensorDict | None = None,
+) -> dict[str, torch.Tensor] | TensorDict | None:
+    __dict__ = model.__dict__
+    was_stateless = __dict__.get("_is_stateless", None)
+    if was_stateless is None:
+        raise Exception(f"{model}\nhas no stateless attribute.")
+    __dict__["_is_stateless"] = is_stateless
+    # return_old_tensordict = return_old_tensordict and not was_stateless
+    if old_tensordict is None:
+        old_tensordict_dict = old_tensordict = {}
+    else:
+        old_tensordict_dict = {}
+    for key, value in tensordict.items():
+        cls = value.__class__
+        if _is_tensor_collection(cls) or issubclass(cls, dict):
+            _old_value = old_tensordict.get(key, None)
+            _old_value = _swap_state(
+                __dict__["_modules"][key],
+                value,
+                is_stateless=is_stateless,
+                old_tensordict=_old_value,
+                return_old_tensordict=return_old_tensordict,
+            )
+            old_tensordict_dict[key] = _old_value
+        else:
+            _old_value = None
+            if return_old_tensordict:
+                _old_value = __dict__["_parameters"].get(key, None)
+                if _old_value is None:
+                    _old_value = __dict__["_buffers"].get(key, None)
+                if _old_value is None:
+                    _old_value = __dict__.get(key, None)
+                if _old_value is None:
+                    pass
+                    # _old_value = torch.zeros(*value.shape, 0)
+                old_tensordict_dict[key] = _old_value
+                # old_tensordict_dict[key] = _old_value
+            if model.__class__.__setattr__ is __base__setattr__:
+                set_tensor_dict(__dict__, model, key, value)
+            else:
+                setattr(model, key, value)
+    old_tensordict.update(old_tensordict_dict)
+    if was_stateless or not return_old_tensordict:
+        return old_tensordict
+    else:
+        return TensorDict(old_tensordict, [], _run_checks=False)
+
+
+# def _swap_state(
+#     model: nn.Module,
+#     tensordict: TensorDict,
+#     is_stateless: bool,
+#     return_old_tensordict: bool = False,
+#     old_tensordict: dict[str, torch.Tensor] | TensorDict | None = None,
+# ) -> dict[str, torch.Tensor] | TensorDict | None:
+#     __dict__ = model.__dict__
+#     was_stateless = __dict__.get("_is_stateless", None)
+#     if was_stateless is None:
+#         raise Exception(f"{model}\nhas no stateless attribute.")
+#     __dict__["_is_stateless"] = is_stateless
+#     # return_old_tensordict = return_old_tensordict and not was_stateless
+#     if old_tensordict is None:
+#         old_tensordict_dict = old_tensordict = {}
+#     else:
+#         old_tensordict_dict = {}
+#     # keys = set(tensordict.keys())
+#     children = set()
+#     # this loop ignores the memo from named children
+#     for key, child in __dict__["_modules"].items():  # model.named_children():
+#         children.add(key)
+#         value = tensordict.get(key, None)
+#         if value is None:
+#             # faster than get(key, Tensordict(...))
+#             value = {}
+#
+#         _old_value = old_tensordict.get(key, None)
+#         _old_value = _swap_state(
+#             child,
+#             value,
+#             return_old_tensordict=return_old_tensordict,
+#             old_tensordict=_old_value,
+#             is_stateless=is_stateless,
+#         )
+#         old_tensordict_dict[key] = _old_value
+#     for key in tensordict.keys():
+#         if key in children:
+#             continue
+#         value = tensordict.get(key)
+#         if return_old_tensordict:
+#             old_attr = __dict__["_parameters"].get(key, None)
+#             if old_attr is None:
+#                 old_attr = __dict__["_buffers"].get(key, None)
+#             if old_attr is None:
+#                 old_attr = __dict__.get(key, None)
+#             if old_attr is None:
+#                 old_attr = torch.zeros(*value.shape, 0)
+#             old_tensordict_dict[key] = old_attr
+#         # is_param = key in model.__dict__.get("_parameters")
+#         # if is_param:
+#         #     delattr(model, key)
+#         #     print(value)
+#         set_tensor_dict(__dict__, model, key, value)
+#     old_tensordict.update(old_tensordict_dict)
+#     if was_stateless or not return_old_tensordict:
+#         return old_tensordict
+#     else:
+#         return TensorDict(old_tensordict, [])
+
+
+def is_functional(module: nn.Module):
+    """Checks if :func:`make_functional` has been called on the module."""
+    return "_functionalized" in module.__dict__
+
+
+def make_functional(
+    module: nn.Module,
+    funs_to_decorate: Iterable[str] | None = None,
+    keep_params: bool = False,
+    return_params: bool = True,
+) -> TensorDict:
+    """Converts a nn.Module to a functional module in-place, and returns its params.
+
+    Args:
+        module (torch.nn.Module): module that is to be made functional.
+        funs_to_decorate (iterable of str, optional): each string must correspond
+            to a function belonging to module. For nested modules, the
+            :meth:`torch.nn.Module.forward` method will be decorated.
+            Defaults to ``"forward"``.
+        keep_params (bool, optional): if ``True``, the module will keep its
+            parameters. Defaults to ``False``.
+        return_params (bool, optional): if ``True``, the parameters will
+            be collected in a nested tensordict and returned. If ``False``,
+            the module will be made functional but still be stateful.
+
+    """
+    _is_stateless = module.__dict__.get("_is_stateless", False)
+    _decorate_funs(
+        module,
+        funs_to_decorate=funs_to_decorate,
+        make_stateless=not keep_params,
+    )
+    if return_params and not _is_stateless:
+        params = extract_weights_and_buffers(
+            module,
+        )
+        if keep_params:
+            repopulate_module(module, params)
+        return params.lock_()
+    elif return_params and _is_stateless:
+        raise RuntimeError(
+            "Calling make_functional with return_params=True on a functional, stateless module. "
+        )
+    elif not keep_params:
+        extract_weights_and_buffers(module)
+
+
+def get_functional(
+    module: nn.Module,
+    funs_to_decorate: Iterable[str] | None = None,
+) -> nn.Module:
+    """Converts a nn.Module to a functional module in-place, and returns a stateful version of this module that can be used in functional settings."""
+    params = make_functional(module, funs_to_decorate=funs_to_decorate)
+    out = deepcopy(module)
+    repopulate_module(module, params)
+    return out
+
+
+def _make_decorator(module: nn.Module, fun_name: str) -> Callable:
+    fun = getattr(module, fun_name)
+
+    from tensordict.nn.common import TensorDictModuleBase
+
+    @wraps(fun)
+    def new_fun(self, *args, **kwargs):
+        # 3 use cases: (1) params is the last arg, (2) params is in kwargs, (3) no params
+        _is_stateless = self.__dict__.get("_is_stateless", False)
+        params = kwargs.pop("params", None)
+
+        if isinstance(self, TensorDictModuleBase):
+            if (
+                params is None
+                and len(args) == 2
+                and all(_is_tensor_collection(item.__class__) for item in args)
+            ):
+                params = args[1]
+                args = args[:1]
+        elif (
+            len(args) and _is_tensor_collection(args[0].__class__)
+        ) or "tensordict" in kwargs:
+            warnings.warn(
+                "You are passing a tensordict/tensorclass instance to a module that "
+                "does not inherit from TensorDictModuleBase. This may lead to unexpected "
+                "behaviours with functional calls."
+            )
+        if _is_stateless or params is not None:
+            if params is None:
+                params = args[-1]
+                args = args[:-1]
+                # get the previous params, and tell the submodules not to look for params anymore
+            old_params = _assign_params(
+                self, params, make_stateless=False, return_old_tensordict=True
+            )
+            try:
+                out = getattr(type(self), fun_name)(self, *args, **kwargs)
+            finally:
+                # reset the previous params, and tell the submodules to look for params
+                _assign_params(
+                    self,
+                    old_params,
+                    make_stateless=_is_stateless,
+                    return_old_tensordict=False,
+                )
+            return out
+        else:
+            try:
+                return getattr(type(self), fun_name)(self, *args, **kwargs)
+            except TypeError as err:
+                pattern = r".*takes \d+ positional arguments but \d+ were given|got multiple values for argument"
+                pattern = re.compile(pattern)
+                if pattern.search(str(err)) and isinstance(args[-1], TensorDictBase):
+                    # this is raised whenever the module is an nn.Module (not a TensorDictModuleBase)
+                    raise TypeError(
+                        "It seems you tried to provide the parameters as an argument to the module when the module was not stateless. "
+                        "If this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` "
+                        "syntax."
+                    ) from err
+                else:
+                    raise err
+
+    # we need to update the signature so that params can be the last positional arg
+    oldsig = inspect.signature(fun)
+    if "_forward_unimplemented" in fun.__name__:
+        raise AttributeError("_forward_unimplemented not supported")
+    # search if a VAR_POSITIONAL or VAR_KEYWORD is present
+    # if yes insert step parameter before it, else insert it in last position
+    params = list(oldsig.parameters.values())
+    for i, param in enumerate(params):
+        if param.kind == inspect.Parameter.KEYWORD_ONLY:
+            out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
+            break
+        if param.kind == inspect.Parameter.VAR_POSITIONAL:
+            out_type = inspect.Parameter.KEYWORD_ONLY
+            i = i + 1
+            break
+        if param.kind == inspect.Parameter.VAR_KEYWORD:
+            out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
+            break
+        if (
+            param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD
+            and param.default is not inspect._empty
+        ):
+            out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
+            break
+    else:
+        out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
+        i = len(params)
+    # new parameter name is params or params_[_...] if params if already present
+    name = "params"
+    while name in oldsig.parameters:
+        name += "_"
+    newparam = inspect.Parameter(name, out_type, default=None)
+    params.insert(i, newparam)
+    # we can now build the signature for the wrapper function
+    sig = oldsig.replace(parameters=params)
+
+    new_fun.__signature__ = sig
+    return new_fun
+
+
+def _assign_params(
+    module: nn.Module,
+    params: TensorDict,
+    make_stateless: bool,
+    return_old_tensordict: bool,
+) -> TensorDict | None:
+    if params is not None:
+        return _swap_state(module, params, make_stateless, return_old_tensordict)
+
+    return None
+
+
+def repopulate_module(model: nn.Module, tensordict: TensorDict) -> nn.Module:
+    """Repopulates a module with its parameters, presented as a nested TensorDict."""
+    _swap_state(model, tensordict, is_stateless=False)
+    return model
```

## tensordict/nn/params.py

```diff
@@ -1,824 +1,826 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-from __future__ import annotations
-
-import functools
-import inspect
-import numbers
-import re
-from copy import copy
-from functools import wraps
-from typing import Any, Callable, Iterator, Sequence
-
-import torch
-
-from tensordict import TensorDictBase
-from tensordict.nn.utils import Buffer
-from tensordict.tensordict import (
-    _CustomOpTensorDict,
-    _is_tensor_collection,
-    CompatibleType,
-    LazyStackedTensorDict,
-    lock_blocked,
-    NO_DEFAULT,
-    SubTensorDict,
-    TD_HANDLED_FUNCTIONS,
-    TensorDict,
-)
-from tensordict.utils import DeviceType, erase_cache, IndexType, NestedKey
-from torch import nn, Tensor
-from torch.utils._pytree import tree_map
-
-
-def _apply_leaves(data, fn):
-    if isinstance(data, TensorDict):
-        with data.unlock_():
-            for key, val in list(data.items()):
-                data._set_str(
-                    key, _apply_leaves(val, fn), validated=True, inplace=False
-                )
-        return data
-    elif isinstance(data, LazyStackedTensorDict):
-        # this is currently not implemented as the registration of params will only work
-        # with plain TensorDict. The solution will be using pytree to get each independent
-        # leaf
-        raise RuntimeError(
-            "Using a LazyStackedTensorDict within a TensorDictParams isn't permitted."
-        )
-        # for _data in data.tensordicts:
-        #     _apply_leaves(_data, fn)
-        # return data
-    elif isinstance(data, _CustomOpTensorDict):
-        _apply_leaves(data._source, fn)
-        return data
-    elif isinstance(data, SubTensorDict):
-        raise RuntimeError(
-            "Using a SubTensorDict within a TensorDictParams isn't permitted."
-        )
-    else:
-        return fn(data)
-
-
-def _get_args_dict(func, args, kwargs):
-    signature = inspect.signature(func)
-    bound_arguments = signature.bind(*args, **kwargs)
-    bound_arguments.apply_defaults()
-
-    args_dict = dict(bound_arguments.arguments)
-    return args_dict
-
-
-def _maybe_make_param(tensor):
-    if (
-        isinstance(tensor, Tensor)
-        and not isinstance(tensor, nn.Parameter)
-        and tensor.dtype in (torch.float, torch.double, torch.half)
-    ):
-        tensor = nn.Parameter(tensor)
-    return tensor
-
-
-def _maybe_make_param_or_buffer(tensor):
-    if (
-        isinstance(tensor, Tensor)
-        and not isinstance(tensor, nn.Parameter)
-        and tensor.dtype in (torch.float, torch.double, torch.half)
-    ):
-        # convert all non-parameters to buffers
-        tensor = Buffer(tensor)
-    return tensor
-
-
-class _unlock_and_set:
-    def __new__(cls, *args, **kwargs):
-        if len(args) and callable(args[0]):
-            return cls(**kwargs)(args[0])
-        return super().__new__(cls)
-
-    def __init__(self, **only_for_kwargs):
-        self.only_for_kwargs = only_for_kwargs
-
-    def __call__(self, func):
-        name = func.__name__
-
-        @wraps(func)
-        def new_func(_self, *args, **kwargs):
-            if self.only_for_kwargs:
-                arg_dict = _get_args_dict(func, (_self, *args), kwargs)
-                for kwarg, exp_value in self.only_for_kwargs.items():
-                    cur_val = arg_dict.get(kwarg, NO_DEFAULT)
-                    if cur_val != exp_value:
-                        # escape
-                        meth = getattr(_self._param_td, name)
-                        out = meth(*args, **kwargs)
-                        return out
-            if not _self.no_convert:
-                args = tree_map(_maybe_make_param, args)
-                kwargs = tree_map(_maybe_make_param, kwargs)
-            else:
-                args = tree_map(_maybe_make_param_or_buffer, args)
-                kwargs = tree_map(_maybe_make_param_or_buffer, kwargs)
-
-            with _self._param_td.unlock_():
-                meth = getattr(_self._param_td, name)
-                out = meth(*args, **kwargs)
-            _self._reset_params()
-            if out is _self._param_td:
-                return _self
-            return out
-
-        return new_func
-
-
-def _get_post_hook(func):
-    @wraps(func)
-    def new_func(self, *args, **kwargs):
-        out = func(self, *args, **kwargs)
-        return self._apply_get_post_hook(out)
-
-    return new_func
-
-
-def _fallback(func):
-    name = func.__name__
-
-    @wraps(func)
-    def new_func(self, *args, **kwargs):
-        out = getattr(self._param_td, name)(*args, **kwargs)
-        if out is self._param_td:
-            return self
-        return out
-
-    return new_func
-
-
-def _fallback_property(func):
-    name = func.__name__
-
-    @wraps(func)
-    def new_func(self):
-        out = getattr(self._param_td, name)
-        if out is self._param_td:
-            return self
-        return out
-
-    def setter(self, value):
-        return getattr(type(self._param_td), name).fset(self._param_td, value)
-
-    return property(new_func, setter)
-
-
-def _replace(func):
-    name = func.__name__
-
-    @wraps(func)
-    def new_func(self, *args, **kwargs):
-        out = getattr(self._param_td, name)(*args, **kwargs)
-        if out is self._param_td:
-            return self
-        self._param_td = out
-        return self
-
-    return new_func
-
-
-def _carry_over(func):
-    name = func.__name__
-
-    @wraps(func)
-    def new_func(self, *args, **kwargs):
-        out = getattr(self._param_td, name)(*args, **kwargs)
-        print("out is", out)
-        out = TensorDictParams(out, no_convert=True)
-        print("out is (2)", out)
-        out.no_convert = self.no_convert
-        return out
-
-    return new_func
-
-
-class TensorDictParams(TensorDictBase, nn.Module):
-    r"""Holds a TensorDictBase instance full of parameters.
-
-    This class exposes the contained parameters to a parent nn.Module
-    such that iterating over the parameters of the module also iterates over
-    the leaves of the tensordict.
-
-    Indexing works exactly as the indexing of the wrapped tensordict.
-    The parameter names will be registered within this module using :meth:`~.TensorDict.flatten_keys("_")`.
-    Therefore, the result of :meth:`~.named_parameters()` and the content of the
-    tensordict will differ slightly in term of key names.
-
-    Any operation that sets a tensor in the tensordict will be augmented by
-    a :class:`torch.nn.Parameter` conversion.
-
-    Args:
-        parameters (TensorDictBase): a tensordict to represent as parameters.
-            Values will be converted to parameters unless ``no_convert=True``.
-
-    Keyword Args:
-        no_convert (bool): if ``True``, no conversion to ``nn.Parameter`` will
-            occur at construction and after (unless the ``no_convert`` attribute is changed).
-            If ``no_convert`` is ``True`` and if non-parameters are present, they
-            will be registered as buffers.
-            Defaults to ``False``.
-
-    Examples:
-        >>> from torch import nn
-        >>> from tensordict import TensorDict
-        >>> module = nn.Sequential(nn.Linear(3, 4), nn.Linear(4, 4))
-        >>> params = TensorDict.from_module(module)
-        >>> params.lock_()
-        >>> p = TensorDictParams(params)
-        >>> print(p)
-        TensorDictParams(params=TensorDict(
-            fields={
-                0: TensorDict(
-                    fields={
-                        bias: Parameter(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
-                        weight: Parameter(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([]),
-                    device=None,
-                    is_shared=False),
-                1: TensorDict(
-                    fields={
-                        bias: Parameter(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
-                        weight: Parameter(shape=torch.Size([4, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([]),
-                    device=None,
-                    is_shared=False)},
-            batch_size=torch.Size([]),
-            device=None,
-            is_shared=False))
-        >>> class CustomModule(nn.Module):
-        ...     def __init__(self, params):
-        ...         super().__init__()
-        ...         self.params = params
-        >>> m = CustomModule(p)
-        >>> # the wrapper supports assignment and values are turned in Parameter
-        >>> m.params['other'] = torch.randn(3)
-        >>> assert isinstance(m.params['other'], nn.Parameter)
-
-    """
-
-    def __init__(self, parameters: TensorDictBase, *, no_convert=False):
-        super().__init__()
-        if isinstance(parameters, TensorDictParams):
-            parameters = parameters._param_td
-        self._param_td = parameters
-        self.no_convert = no_convert
-        if not no_convert:
-            func = _maybe_make_param
-        else:
-            func = _maybe_make_param_or_buffer
-        self._param_td = _apply_leaves(self._param_td, lambda x: func(x)).lock_()
-        self._reset_params()
-        self._is_locked = False
-        self._locked_tensordicts = []
-        self.__last_op_queue = None
-        self._get_post_hook = []
-
-    def register_get_post_hook(self, hook):
-        """Register a hook to be called after any get operation on leaf tensors."""
-        if not callable(hook):
-            raise ValueError("Hooks must be callables.")
-        self._get_post_hook.append(hook)
-
-    def _apply_get_post_hook(self, val):
-        if not _is_tensor_collection(type(val)):
-            for hook in self._get_post_hook:
-                new_val = hook(self, val)
-                if new_val is not None:
-                    val = new_val
-        return val
-
-    def _reset_params(self):
-        parameters = self._param_td
-        param_keys = []
-        buffer_keys = []
-        for key, value in parameters.items(True, True):
-            if isinstance(value, nn.Parameter):
-                param_keys.append(key)
-            else:
-                buffer_keys.append(key)
-        self.__dict__["_parameters"] = (
-            parameters.select(*param_keys).flatten_keys("_").to_dict()
-        )
-        self.__dict__["_buffers"] = (
-            parameters.select(*buffer_keys).flatten_keys("_").to_dict()
-        )
-
-    @classmethod
-    def __torch_function__(
-        cls,
-        func: Callable,
-        types: tuple[type, ...],
-        args: tuple[Any, ...] = (),
-        kwargs: dict[str, Any] | None = None,
-    ) -> Callable:
-        if kwargs is None:
-            kwargs = {}
-        if func not in TDPARAM_HANDLED_FUNCTIONS or not all(
-            issubclass(t, (Tensor, TensorDictBase)) for t in types
-        ):
-            return NotImplemented
-        return TDPARAM_HANDLED_FUNCTIONS[func](*args, **kwargs)
-
-    @classmethod
-    def _flatten_key(cls, key):
-        def make_valid_identifier(s):
-            # Replace invalid characters with underscores
-            s = re.sub(r"\W|^(?=\d)", "_", s)
-
-            # Ensure the string starts with a letter or underscore
-            if not s[0].isalpha() and s[0] != "_":
-                s = "_" + s
-
-            return s
-
-        key_flat = "_".join(key)
-        if not key_flat.isidentifier():
-            key_flat = make_valid_identifier(key_flat)
-        return key_flat
-
-    @lock_blocked
-    @_unlock_and_set
-    def __setitem__(
-        self,
-        index: IndexType,
-        value: TensorDictBase | dict | numbers.Number | CompatibleType,
-    ) -> None:
-        ...
-
-    @lock_blocked
-    @_unlock_and_set
-    def set(
-        self, key: NestedKey, item: CompatibleType, inplace: bool = False, **kwargs: Any
-    ) -> TensorDictBase:
-        ...
-
-    def update(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
-        clone: bool = False,
-        inplace: bool = False,
-    ) -> TensorDictBase:
-        if not self.no_convert:
-            func = _maybe_make_param
-        else:
-            func = _maybe_make_param_or_buffer
-        if isinstance(input_dict_or_td, TensorDictBase):
-            input_dict_or_td = input_dict_or_td.apply(func)
-        else:
-            input_dict_or_td = tree_map(func, input_dict_or_td)
-        with self._param_td.unlock_():
-            TensorDictBase.update(self, input_dict_or_td, clone=clone, inplace=inplace)
-            self._reset_params()
-        return self
-
-    @lock_blocked
-    @_unlock_and_set
-    def pop(
-        self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
-    ) -> CompatibleType:
-        ...
-
-    @lock_blocked
-    @_unlock_and_set
-    def rename_key_(
-        self, old_key: str, new_key: str, safe: bool = False
-    ) -> TensorDictBase:
-        ...
-
-    @_unlock_and_set
-    def apply_(self, fn: Callable, *others) -> TensorDictBase:
-        ...
-
-    @_unlock_and_set(inplace=True)
-    def apply(
-        self,
-        fn: Callable,
-        *others: TensorDictBase,
-        batch_size: Sequence[int] | None = None,
-        device: torch.device | None = None,
-        names: Sequence[str] | None = None,
-        inplace: bool = False,
-        **constructor_kwargs,
-    ) -> TensorDictBase:
-        ...
-
-    @_get_post_hook
-    @_fallback
-    def get(
-        self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
-    ) -> CompatibleType:
-        ...
-
-    @_get_post_hook
-    @_fallback
-    def __getitem__(self, index: IndexType) -> TensorDictBase:
-        ...
-
-    __getitems__ = __getitem__
-
-    def to(self, dest: DeviceType | type | torch.Size, **kwargs) -> TensorDictBase:
-        params = self._param_td.to(dest)
-        if params is self._param_td:
-            return self
-        return TensorDictParams(params)
-
-    def cpu(self):
-        params = self._param_td.cpu()
-        if params is self._param_td:
-            return self
-        return TensorDictParams(params)
-
-    def cuda(self, device=None):
-        params = self._param_td.cuda(device=device)
-        if params is self._param_td:
-            return self
-        return TensorDictParams(params)
-
-    def clone(self, recurse: bool = True) -> TensorDictBase:
-        return TensorDictParams(self._param_td.clone(recurse=recurse))
-
-    @_fallback
-    def chunk(self, chunks: int, dim: int = 0) -> tuple[TensorDictBase, ...]:
-        ...
-
-    @_fallback
-    def unbind(self, dim: int) -> tuple[TensorDictBase, ...]:
-        ...
-
-    @_fallback
-    def to_tensordict(self):
-        ...
-
-    @_fallback
-    def to_h5(
-        self,
-        filename,
-        **kwargs,
-    ):
-        ...
-
-    def __hash__(self):
-        return hash((id(self), id(self._param_td)))
-
-    @_fallback
-    def __eq__(self, other: object) -> TensorDictBase:
-        ...
-
-    @_fallback
-    def __ne__(self, other: object) -> TensorDictBase:
-        ...
-
-    def __getattr__(self, item: str) -> Any:
-        try:
-            return getattr(self._param_td, item)
-        except AttributeError:
-            return super().__getattr__(item)
-
-    @_fallback
-    def _change_batch_size(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def _erase_names(self, *args, **kwargs):
-        ...
-
-    @_get_post_hook
-    @_fallback
-    def _get_str(self, *args, **kwargs):
-        ...
-
-    @_get_post_hook
-    @_fallback
-    def _get_tuple(self, *args, **kwargs):
-        ...
-
-    @_get_post_hook
-    @_fallback
-    def _get_at_str(self, key, idx, default):
-        ...
-
-    @_get_post_hook
-    @_fallback
-    def _get_at_tuple(self, key, idx, default):
-        ...
-
-    @_fallback
-    def _has_names(self, *args, **kwargs):
-        ...
-
-    @_unlock_and_set
-    def _rename_subtds(self, *args, **kwargs):
-        ...
-
-    @_unlock_and_set
-    def _set_at_str(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def _set_at_tuple(self, *args, **kwargs):
-        ...
-
-    @_unlock_and_set
-    def _set_str(self, *args, **kwargs):
-        ...
-
-    @_unlock_and_set
-    def _set_tuple(self, *args, **kwargs):
-        ...
-
-    @_unlock_and_set
-    def _create_nested_str(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def _stack_onto_(self, *args, **kwargs):
-        ...
-
-    @_fallback_property
-    def batch_size(self) -> torch.Size:
-        ...
-
-    @_fallback
-    def contiguous(self, *args, **kwargs):
-        ...
-
-    @lock_blocked
-    @_unlock_and_set
-    def del_(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def detach_(self, *args, **kwargs):
-        ...
-
-    @_fallback_property
-    def device(self):
-        ...
-
-    @_fallback
-    def entry_class(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def is_contiguous(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def keys(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def masked_fill(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def masked_fill_(self, *args, **kwargs):
-        ...
-
-    def memmap_(
-        self, prefix: str | None = None, copy_existing: bool = False
-    ) -> TensorDictBase:
-        raise RuntimeError("Cannot build a memmap TensorDict in-place.")
-
-    @_fallback_property
-    def names(self):
-        ...
-
-    @_fallback
-    def pin_memory(self, *args, **kwargs):
-        ...
-
-    @_unlock_and_set
-    def select(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def share_memory_(self, *args, **kwargs):
-        ...
-
-    @property
-    def is_locked(self) -> bool:
-        # Cannot be locked
-        return self._is_locked
-
-    @is_locked.setter
-    def is_locked(self, value):
-        self._is_locked = bool(value)
-
-    @_fallback_property
-    def is_shared(self) -> bool:
-        ...
-
-    @_fallback_property
-    def is_memmap(self) -> bool:
-        ...
-
-    @_fallback_property
-    def shape(self) -> torch.Size:
-        ...
-
-    @erase_cache
-    def _propagate_unlock(self, lock_ids=None):
-        if lock_ids is not None:
-            self._lock_id.difference_update(lock_ids)
-        else:
-            lock_ids = set()
-        self._is_locked = False
-
-        unlocked_tds = [self]
-        lock_ids.add(id(self))
-        self._locked_tensordicts = []
-
-        self._is_shared = False
-        self._is_memmap = False
-        return unlocked_tds
-
-    unlock_ = TensorDict.unlock_
-    lock_ = TensorDict.lock_
-
-    @property
-    def data(self):
-        return self.apply(lambda x: x.data)
-
-    @_unlock_and_set(inplace=True)
-    def flatten_keys(
-        self, separator: str = ".", inplace: bool = False
-    ) -> TensorDictBase:
-        ...
-
-    @_unlock_and_set(inplace=True)
-    def unflatten_keys(
-        self, separator: str = ".", inplace: bool = False
-    ) -> TensorDictBase:
-        ...
-
-    @_unlock_and_set(inplace=True)
-    def exclude(self, *keys: str, inplace: bool = False) -> TensorDictBase:
-        ...
-
-    @_carry_over
-    def transpose(self, dim0, dim1):
-        ...
-
-    @_carry_over
-    def permute(
-        self,
-        *dims_list: int,
-        dims: list[int] | None = None,
-    ) -> TensorDictBase:
-        ...
-
-    @_carry_over
-    def squeeze(self, dim: int | None = None) -> TensorDictBase:
-        ...
-
-    @_carry_over
-    def unsqueeze(self, dim: int) -> TensorDictBase:
-        ...
-
-    @_unlock_and_set
-    def create_nested(self, key):
-        ...
-
-    def __repr__(self):
-        return f"TensorDictParams(params={self._param_td})"
-
-    def values(
-        self, include_nested: bool = False, leaves_only: bool = False
-    ) -> Iterator[CompatibleType]:
-        for v in self._param_td.values(include_nested, leaves_only):
-            if _is_tensor_collection(type(v)):
-                yield v
-                continue
-            yield self._apply_get_post_hook(v)
-
-    def items(
-        self, include_nested: bool = False, leaves_only: bool = False
-    ) -> Iterator[CompatibleType]:
-        for k, v in self._param_td.items(include_nested, leaves_only):
-            if _is_tensor_collection(type(v)):
-                yield k, v
-                continue
-            yield k, self._apply_get_post_hook(v)
-
-    def _apply(self, fn, recurse=True):
-        """Modifies torch.nn.Module._apply to work with Buffer class."""
-        if recurse:
-            for module in self.children():
-                module._apply(fn)
-
-        def compute_should_use_set_data(tensor, tensor_applied):
-            if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):
-                # If the new tensor has compatible tensor type as the existing tensor,
-                # the current behavior is to change the tensor in-place using `.data =`,
-                # and the future behavior is to overwrite the existing tensor. However,
-                # changing the current behavior is a BC-breaking change, and we want it
-                # to happen in future releases. So for now we introduce the
-                # `torch.__future__.get_overwrite_module_params_on_conversion()`
-                # global flag to let the user control whether they want the future
-                # behavior of overwriting the existing tensor or not.
-                return not torch.__future__.get_overwrite_module_params_on_conversion()
-            else:
-                return False
-
-        for key, param in self._parameters.items():
-            if param is None:
-                continue
-            # Tensors stored in modules are graph leaves, and we don't want to
-            # track autograd history of `param_applied`, so we have to use
-            # `with torch.no_grad():`
-            with torch.no_grad():
-                param_applied = fn(param)
-            should_use_set_data = compute_should_use_set_data(param, param_applied)
-            if should_use_set_data:
-                param.data = param_applied
-                out_param = param
-            else:
-                assert isinstance(param, nn.Parameter)
-                assert param.is_leaf
-                out_param = nn.Parameter(param_applied, param.requires_grad)
-                self._parameters[key] = out_param
-
-            if param.grad is not None:
-                with torch.no_grad():
-                    grad_applied = fn(param.grad)
-                should_use_set_data = compute_should_use_set_data(
-                    param.grad, grad_applied
-                )
-                if should_use_set_data:
-                    assert out_param.grad is not None
-                    out_param.grad.data = grad_applied
-                else:
-                    assert param.grad.is_leaf
-                    out_param.grad = grad_applied.requires_grad_(
-                        param.grad.requires_grad
-                    )
-
-        for key, buffer in self._buffers.items():
-            if buffer is None:
-                continue
-            # Tensors stored in modules are graph leaves, and we don't want to
-            # track autograd history of `buffer_applied`, so we have to use
-            # `with torch.no_grad():`
-            with torch.no_grad():
-                buffer_applied = fn(buffer)
-            should_use_set_data = compute_should_use_set_data(buffer, buffer_applied)
-            if should_use_set_data:
-                buffer.data = buffer_applied
-                out_buffer = buffer
-            else:
-                assert isinstance(buffer, Buffer)
-                assert buffer.is_leaf
-                out_buffer = Buffer(buffer_applied, buffer.requires_grad)
-                self._buffers[key] = out_buffer
-
-            if buffer.grad is not None:
-                with torch.no_grad():
-                    grad_applied = fn(buffer.grad)
-                should_use_set_data = compute_should_use_set_data(
-                    buffer.grad, grad_applied
-                )
-                if should_use_set_data:
-                    assert out_buffer.grad is not None
-                    out_buffer.grad.data = grad_applied
-                else:
-                    assert buffer.grad.is_leaf
-                    out_buffer.grad = grad_applied.requires_grad_(
-                        buffer.grad.requires_grad
-                    )
-
-        return self
-
-
-TDPARAM_HANDLED_FUNCTIONS = copy(TD_HANDLED_FUNCTIONS)
-
-
-def implements_for_tdparam(torch_function: Callable) -> Callable[[Callable], Callable]:
-    """Register a torch function override for TensorDictParams."""
-
-    @functools.wraps(torch_function)
-    def decorator(func: Callable) -> Callable:
-        TDPARAM_HANDLED_FUNCTIONS[torch_function] = func
-        return func
-
-    return decorator
-
-
-@implements_for_tdparam(torch.empty_like)
-def _empty_like(td: TensorDictBase, *args, **kwargs) -> TensorDictBase:
-    try:
-        tdclone = td.clone()
-    except Exception as err:
-        raise RuntimeError(
-            "The tensordict passed to torch.empty_like cannot be "
-            "cloned, preventing empty_like to be called. "
-            "Consider calling tensordict.to_tensordict() first."
-        ) from err
-    return tdclone.data.apply_(lambda x: torch.empty_like(x, *args, **kwargs))
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+from __future__ import annotations
+
+import functools
+import inspect
+import numbers
+import re
+from copy import copy
+from functools import wraps
+from typing import Any, Callable, Iterator, Sequence
+
+import torch
+
+from tensordict import TensorDictBase
+from tensordict.nn.utils import Buffer
+from tensordict.tensordict import (
+    _CustomOpTensorDict,
+    _is_tensor_collection,
+    CompatibleType,
+    LazyStackedTensorDict,
+    lock_blocked,
+    NO_DEFAULT,
+    SubTensorDict,
+    TD_HANDLED_FUNCTIONS,
+    TensorDict,
+)
+from tensordict.utils import DeviceType, erase_cache, IndexType, NestedKey
+from torch import nn, Tensor
+from torch.utils._pytree import tree_map
+
+
+def _apply_leaves(data, fn):
+    if isinstance(data, TensorDict):
+        with data.unlock_():
+            for key, val in list(data.items()):
+                data._set_str(
+                    key, _apply_leaves(val, fn), validated=True, inplace=False
+                )
+        return data
+    elif isinstance(data, LazyStackedTensorDict):
+        # this is currently not implemented as the registration of params will only work
+        # with plain TensorDict. The solution will be using pytree to get each independent
+        # leaf
+        raise RuntimeError(
+            "Using a LazyStackedTensorDict within a TensorDictParams isn't permitted."
+        )
+        # for _data in data.tensordicts:
+        #     _apply_leaves(_data, fn)
+        # return data
+    elif isinstance(data, _CustomOpTensorDict):
+        _apply_leaves(data._source, fn)
+        return data
+    elif isinstance(data, SubTensorDict):
+        raise RuntimeError(
+            "Using a SubTensorDict within a TensorDictParams isn't permitted."
+        )
+    else:
+        return fn(data)
+
+
+def _get_args_dict(func, args, kwargs):
+    signature = inspect.signature(func)
+    bound_arguments = signature.bind(*args, **kwargs)
+    bound_arguments.apply_defaults()
+
+    args_dict = dict(bound_arguments.arguments)
+    return args_dict
+
+
+def _maybe_make_param(tensor):
+    if (
+        isinstance(tensor, Tensor)
+        and not isinstance(tensor, nn.Parameter)
+        and tensor.dtype in (torch.float, torch.double, torch.half)
+    ):
+        tensor = nn.Parameter(tensor)
+    return tensor
+
+
+def _maybe_make_param_or_buffer(tensor):
+    if (
+        isinstance(tensor, Tensor)
+        and not isinstance(tensor, nn.Parameter)
+        and tensor.dtype in (torch.float, torch.double, torch.half)
+    ):
+        # convert all non-parameters to buffers
+        tensor = Buffer(tensor)
+    return tensor
+
+
+class _unlock_and_set:
+    def __new__(cls, *args, **kwargs):
+        if len(args) and callable(args[0]):
+            return cls(**kwargs)(args[0])
+        return super().__new__(cls)
+
+    def __init__(self, **only_for_kwargs):
+        self.only_for_kwargs = only_for_kwargs
+
+    def __call__(self, func):
+        name = func.__name__
+
+        @wraps(func)
+        def new_func(_self, *args, **kwargs):
+            if self.only_for_kwargs:
+                arg_dict = _get_args_dict(func, (_self, *args), kwargs)
+                for kwarg, exp_value in self.only_for_kwargs.items():
+                    cur_val = arg_dict.get(kwarg, NO_DEFAULT)
+                    if cur_val != exp_value:
+                        # escape
+                        meth = getattr(_self._param_td, name)
+                        out = meth(*args, **kwargs)
+                        return out
+            if not _self.no_convert:
+                args = tree_map(_maybe_make_param, args)
+                kwargs = tree_map(_maybe_make_param, kwargs)
+            else:
+                args = tree_map(_maybe_make_param_or_buffer, args)
+                kwargs = tree_map(_maybe_make_param_or_buffer, kwargs)
+
+            with _self._param_td.unlock_():
+                meth = getattr(_self._param_td, name)
+                out = meth(*args, **kwargs)
+            _self._reset_params()
+            if out is _self._param_td:
+                return _self
+            return out
+
+        return new_func
+
+
+def _get_post_hook(func):
+    @wraps(func)
+    def new_func(self, *args, **kwargs):
+        out = func(self, *args, **kwargs)
+        return self._apply_get_post_hook(out)
+
+    return new_func
+
+
+def _fallback(func):
+    name = func.__name__
+
+    @wraps(func)
+    def new_func(self, *args, **kwargs):
+        out = getattr(self._param_td, name)(*args, **kwargs)
+        if out is self._param_td:
+            return self
+        return out
+
+    return new_func
+
+
+def _fallback_property(func):
+    name = func.__name__
+
+    @wraps(func)
+    def new_func(self):
+        out = getattr(self._param_td, name)
+        if out is self._param_td:
+            return self
+        return out
+
+    def setter(self, value):
+        return getattr(type(self._param_td), name).fset(self._param_td, value)
+
+    return property(new_func, setter)
+
+
+def _replace(func):
+    name = func.__name__
+
+    @wraps(func)
+    def new_func(self, *args, **kwargs):
+        out = getattr(self._param_td, name)(*args, **kwargs)
+        if out is self._param_td:
+            return self
+        self._param_td = out
+        return self
+
+    return new_func
+
+
+def _carry_over(func):
+    name = func.__name__
+
+    @wraps(func)
+    def new_func(self, *args, **kwargs):
+        out = getattr(self._param_td, name)(*args, **kwargs)
+        out = TensorDictParams(out, no_convert=True)
+        out.no_convert = self.no_convert
+        return out
+
+    return new_func
+
+
+class TensorDictParams(TensorDictBase, nn.Module):
+    r"""Holds a TensorDictBase instance full of parameters.
+
+    This class exposes the contained parameters to a parent nn.Module
+    such that iterating over the parameters of the module also iterates over
+    the leaves of the tensordict.
+
+    Indexing works exactly as the indexing of the wrapped tensordict.
+    The parameter names will be registered within this module using :meth:`~.TensorDict.flatten_keys("_")`.
+    Therefore, the result of :meth:`~.named_parameters()` and the content of the
+    tensordict will differ slightly in term of key names.
+
+    Any operation that sets a tensor in the tensordict will be augmented by
+    a :class:`torch.nn.Parameter` conversion.
+
+    Args:
+        parameters (TensorDictBase): a tensordict to represent as parameters.
+            Values will be converted to parameters unless ``no_convert=True``.
+
+    Keyword Args:
+        no_convert (bool): if ``True``, no conversion to ``nn.Parameter`` will
+            occur at construction and after (unless the ``no_convert`` attribute is changed).
+            If ``no_convert`` is ``True`` and if non-parameters are present, they
+            will be registered as buffers.
+            Defaults to ``False``.
+
+    Examples:
+        >>> from torch import nn
+        >>> from tensordict import TensorDict
+        >>> module = nn.Sequential(nn.Linear(3, 4), nn.Linear(4, 4))
+        >>> params = TensorDict.from_module(module)
+        >>> params.lock_()
+        >>> p = TensorDictParams(params)
+        >>> print(p)
+        TensorDictParams(params=TensorDict(
+            fields={
+                0: TensorDict(
+                    fields={
+                        bias: Parameter(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
+                        weight: Parameter(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([]),
+                    device=None,
+                    is_shared=False),
+                1: TensorDict(
+                    fields={
+                        bias: Parameter(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
+                        weight: Parameter(shape=torch.Size([4, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False))
+        >>> class CustomModule(nn.Module):
+        ...     def __init__(self, params):
+        ...         super().__init__()
+        ...         self.params = params
+        >>> m = CustomModule(p)
+        >>> # the wrapper supports assignment and values are turned in Parameter
+        >>> m.params['other'] = torch.randn(3)
+        >>> assert isinstance(m.params['other'], nn.Parameter)
+
+    """
+
+    def __init__(self, parameters: TensorDictBase, *, no_convert=False):
+        super().__init__()
+        if isinstance(parameters, TensorDictParams):
+            parameters = parameters._param_td
+        self._param_td = parameters
+        self.no_convert = no_convert
+        if not no_convert:
+            func = _maybe_make_param
+        else:
+            func = _maybe_make_param_or_buffer
+        self._param_td = _apply_leaves(self._param_td, lambda x: func(x)).lock_()
+        self._reset_params()
+        self._is_locked = False
+        self._locked_tensordicts = []
+        self.__last_op_queue = None
+        self._get_post_hook = []
+
+    def register_get_post_hook(self, hook):
+        """Register a hook to be called after any get operation on leaf tensors."""
+        if not callable(hook):
+            raise ValueError("Hooks must be callables.")
+        self._get_post_hook.append(hook)
+
+    def _apply_get_post_hook(self, val):
+        if not _is_tensor_collection(type(val)):
+            for hook in self._get_post_hook:
+                new_val = hook(self, val)
+                if new_val is not None:
+                    val = new_val
+        return val
+
+    def _reset_params(self):
+        parameters = self._param_td
+        param_keys = []
+        buffer_keys = []
+        for key, value in parameters.items(True, True):
+            if isinstance(value, nn.Parameter):
+                param_keys.append(key)
+            else:
+                buffer_keys.append(key)
+        self.__dict__["_parameters"] = (
+            parameters.select(*param_keys).flatten_keys("_").to_dict()
+        )
+        self.__dict__["_buffers"] = (
+            parameters.select(*buffer_keys).flatten_keys("_").to_dict()
+        )
+
+    @classmethod
+    def __torch_function__(
+        cls,
+        func: Callable,
+        types: tuple[type, ...],
+        args: tuple[Any, ...] = (),
+        kwargs: dict[str, Any] | None = None,
+    ) -> Callable:
+        if kwargs is None:
+            kwargs = {}
+        if func not in TDPARAM_HANDLED_FUNCTIONS or not all(
+            issubclass(t, (Tensor, TensorDictBase)) for t in types
+        ):
+            return NotImplemented
+        return TDPARAM_HANDLED_FUNCTIONS[func](*args, **kwargs)
+
+    @classmethod
+    def _flatten_key(cls, key):
+        def make_valid_identifier(s):
+            # Replace invalid characters with underscores
+            s = re.sub(r"\W|^(?=\d)", "_", s)
+
+            # Ensure the string starts with a letter or underscore
+            if not s[0].isalpha() and s[0] != "_":
+                s = "_" + s
+
+            return s
+
+        key_flat = "_".join(key)
+        if not key_flat.isidentifier():
+            key_flat = make_valid_identifier(key_flat)
+        return key_flat
+
+    @lock_blocked
+    @_unlock_and_set
+    def __setitem__(
+        self,
+        index: IndexType,
+        value: TensorDictBase | dict | numbers.Number | CompatibleType,
+    ) -> None:
+        ...
+
+    @lock_blocked
+    @_unlock_and_set
+    def set(
+        self, key: NestedKey, item: CompatibleType, inplace: bool = False, **kwargs: Any
+    ) -> TensorDictBase:
+        ...
+
+    def update(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        clone: bool = False,
+        inplace: bool = False,
+    ) -> TensorDictBase:
+        if not self.no_convert:
+            func = _maybe_make_param
+        else:
+            func = _maybe_make_param_or_buffer
+        if isinstance(input_dict_or_td, TensorDictBase):
+            input_dict_or_td = input_dict_or_td.apply(func)
+        else:
+            input_dict_or_td = tree_map(func, input_dict_or_td)
+        with self._param_td.unlock_():
+            TensorDictBase.update(self, input_dict_or_td, clone=clone, inplace=inplace)
+            self._reset_params()
+        return self
+
+    @lock_blocked
+    @_unlock_and_set
+    def pop(
+        self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
+    ) -> CompatibleType:
+        ...
+
+    @lock_blocked
+    @_unlock_and_set
+    def rename_key_(
+        self, old_key: str, new_key: str, safe: bool = False
+    ) -> TensorDictBase:
+        ...
+
+    @_unlock_and_set
+    def apply_(self, fn: Callable, *others) -> TensorDictBase:
+        ...
+
+    @_unlock_and_set(inplace=True)
+    def apply(
+        self,
+        fn: Callable,
+        *others: TensorDictBase,
+        batch_size: Sequence[int] | None = None,
+        device: torch.device | None = None,
+        names: Sequence[str] | None = None,
+        inplace: bool = False,
+        **constructor_kwargs,
+    ) -> TensorDictBase:
+        ...
+
+    @_get_post_hook
+    @_fallback
+    def get(
+        self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
+    ) -> CompatibleType:
+        ...
+
+    @_get_post_hook
+    @_fallback
+    def __getitem__(self, index: IndexType) -> TensorDictBase:
+        ...
+
+    __getitems__ = __getitem__
+
+    def to(self, dest: DeviceType | type | torch.Size, **kwargs) -> TensorDictBase:
+        params = self._param_td.to(dest)
+        if params is self._param_td:
+            return self
+        return TensorDictParams(params)
+
+    def cpu(self):
+        params = self._param_td.cpu()
+        if params is self._param_td:
+            return self
+        return TensorDictParams(params)
+
+    def cuda(self, device=None):
+        params = self._param_td.cuda(device=device)
+        if params is self._param_td:
+            return self
+        return TensorDictParams(params)
+
+    def clone(self, recurse: bool = True) -> TensorDictBase:
+        return TensorDictParams(self._param_td.clone(recurse=recurse))
+
+    @_fallback
+    def chunk(self, chunks: int, dim: int = 0) -> tuple[TensorDictBase, ...]:
+        ...
+
+    @_fallback
+    def unbind(self, dim: int) -> tuple[TensorDictBase, ...]:
+        ...
+
+    @_fallback
+    def to_tensordict(self):
+        ...
+
+    @_fallback
+    def to_h5(
+        self,
+        filename,
+        **kwargs,
+    ):
+        ...
+
+    def __hash__(self):
+        return hash((id(self), id(self._param_td)))
+
+    @_fallback
+    def __eq__(self, other: object) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def __ne__(self, other: object) -> TensorDictBase:
+        ...
+
+    def __getattr__(self, item: str) -> Any:
+        try:
+            return getattr(self._param_td, item)
+        except AttributeError:
+            return super().__getattr__(item)
+
+    @_fallback
+    def _change_batch_size(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def _erase_names(self, *args, **kwargs):
+        ...
+
+    @_get_post_hook
+    @_fallback
+    def _get_str(self, *args, **kwargs):
+        ...
+
+    @_get_post_hook
+    @_fallback
+    def _get_tuple(self, *args, **kwargs):
+        ...
+
+    @_get_post_hook
+    @_fallback
+    def _get_at_str(self, key, idx, default):
+        ...
+
+    @_get_post_hook
+    @_fallback
+    def _get_at_tuple(self, key, idx, default):
+        ...
+
+    @_fallback
+    def _has_names(self, *args, **kwargs):
+        ...
+
+    @_unlock_and_set
+    def _rename_subtds(self, *args, **kwargs):
+        ...
+
+    @_unlock_and_set
+    def _set_at_str(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def _set_at_tuple(self, *args, **kwargs):
+        ...
+
+    @_unlock_and_set
+    def _set_str(self, *args, **kwargs):
+        ...
+
+    @_unlock_and_set
+    def _set_tuple(self, *args, **kwargs):
+        ...
+
+    @_unlock_and_set
+    def _create_nested_str(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def _stack_onto_(self, *args, **kwargs):
+        ...
+
+    @_fallback_property
+    def batch_size(self) -> torch.Size:
+        ...
+
+    @_fallback
+    def contiguous(self, *args, **kwargs):
+        ...
+
+    @lock_blocked
+    @_unlock_and_set
+    def del_(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def detach_(self, *args, **kwargs):
+        ...
+
+    @_fallback_property
+    def device(self):
+        ...
+
+    @_fallback
+    def entry_class(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def is_contiguous(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def keys(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def masked_fill(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def masked_fill_(self, *args, **kwargs):
+        ...
+
+    def memmap_(
+        self, prefix: str | None = None, copy_existing: bool = False
+    ) -> TensorDictBase:
+        raise RuntimeError("Cannot build a memmap TensorDict in-place.")
+
+    @_fallback_property
+    def names(self):
+        ...
+
+    @_fallback
+    def pin_memory(self, *args, **kwargs):
+        ...
+
+    @_unlock_and_set
+    def select(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def share_memory_(self, *args, **kwargs):
+        ...
+
+    @property
+    def is_locked(self) -> bool:
+        # Cannot be locked
+        return self._is_locked
+
+    @is_locked.setter
+    def is_locked(self, value):
+        self._is_locked = bool(value)
+
+    @_fallback_property
+    def is_shared(self) -> bool:
+        ...
+
+    @_fallback_property
+    def is_memmap(self) -> bool:
+        ...
+
+    @_fallback_property
+    def shape(self) -> torch.Size:
+        ...
+
+    @erase_cache
+    def _propagate_unlock(self, lock_ids=None):
+        if lock_ids is not None:
+            self._lock_id.difference_update(lock_ids)
+        else:
+            lock_ids = set()
+        self._is_locked = False
+
+        unlocked_tds = [self]
+        lock_ids.add(id(self))
+        self._locked_tensordicts = []
+
+        self._is_shared = False
+        self._is_memmap = False
+        return unlocked_tds
+
+    unlock_ = TensorDict.unlock_
+    lock_ = TensorDict.lock_
+
+    @property
+    def data(self):
+        return self.apply(lambda x: x.data)
+
+    @_unlock_and_set(inplace=True)
+    def flatten_keys(
+        self, separator: str = ".", inplace: bool = False
+    ) -> TensorDictBase:
+        ...
+
+    @_unlock_and_set(inplace=True)
+    def unflatten_keys(
+        self, separator: str = ".", inplace: bool = False
+    ) -> TensorDictBase:
+        ...
+
+    @_unlock_and_set(inplace=True)
+    def exclude(self, *keys: str, inplace: bool = False) -> TensorDictBase:
+        ...
+
+    @_carry_over
+    def transpose(self, dim0, dim1):
+        ...
+
+    @_carry_over
+    def where(self, condition, other, *, out=None):
+        ...
+
+    @_carry_over
+    def permute(
+        self,
+        *dims_list: int,
+        dims: list[int] | None = None,
+    ) -> TensorDictBase:
+        ...
+
+    @_carry_over
+    def squeeze(self, dim: int | None = None) -> TensorDictBase:
+        ...
+
+    @_carry_over
+    def unsqueeze(self, dim: int) -> TensorDictBase:
+        ...
+
+    @_unlock_and_set
+    def create_nested(self, key):
+        ...
+
+    def __repr__(self):
+        return f"TensorDictParams(params={self._param_td})"
+
+    def values(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> Iterator[CompatibleType]:
+        for v in self._param_td.values(include_nested, leaves_only):
+            if _is_tensor_collection(type(v)):
+                yield v
+                continue
+            yield self._apply_get_post_hook(v)
+
+    def items(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> Iterator[CompatibleType]:
+        for k, v in self._param_td.items(include_nested, leaves_only):
+            if _is_tensor_collection(type(v)):
+                yield k, v
+                continue
+            yield k, self._apply_get_post_hook(v)
+
+    def _apply(self, fn, recurse=True):
+        """Modifies torch.nn.Module._apply to work with Buffer class."""
+        if recurse:
+            for module in self.children():
+                module._apply(fn)
+
+        def compute_should_use_set_data(tensor, tensor_applied):
+            if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):
+                # If the new tensor has compatible tensor type as the existing tensor,
+                # the current behavior is to change the tensor in-place using `.data =`,
+                # and the future behavior is to overwrite the existing tensor. However,
+                # changing the current behavior is a BC-breaking change, and we want it
+                # to happen in future releases. So for now we introduce the
+                # `torch.__future__.get_overwrite_module_params_on_conversion()`
+                # global flag to let the user control whether they want the future
+                # behavior of overwriting the existing tensor or not.
+                return not torch.__future__.get_overwrite_module_params_on_conversion()
+            else:
+                return False
+
+        for key, param in self._parameters.items():
+            if param is None:
+                continue
+            # Tensors stored in modules are graph leaves, and we don't want to
+            # track autograd history of `param_applied`, so we have to use
+            # `with torch.no_grad():`
+            with torch.no_grad():
+                param_applied = fn(param)
+            should_use_set_data = compute_should_use_set_data(param, param_applied)
+            if should_use_set_data:
+                param.data = param_applied
+                out_param = param
+            else:
+                assert isinstance(param, nn.Parameter)
+                assert param.is_leaf
+                out_param = nn.Parameter(param_applied, param.requires_grad)
+                self._parameters[key] = out_param
+
+            if param.grad is not None:
+                with torch.no_grad():
+                    grad_applied = fn(param.grad)
+                should_use_set_data = compute_should_use_set_data(
+                    param.grad, grad_applied
+                )
+                if should_use_set_data:
+                    assert out_param.grad is not None
+                    out_param.grad.data = grad_applied
+                else:
+                    assert param.grad.is_leaf
+                    out_param.grad = grad_applied.requires_grad_(
+                        param.grad.requires_grad
+                    )
+
+        for key, buffer in self._buffers.items():
+            if buffer is None:
+                continue
+            # Tensors stored in modules are graph leaves, and we don't want to
+            # track autograd history of `buffer_applied`, so we have to use
+            # `with torch.no_grad():`
+            with torch.no_grad():
+                buffer_applied = fn(buffer)
+            should_use_set_data = compute_should_use_set_data(buffer, buffer_applied)
+            if should_use_set_data:
+                buffer.data = buffer_applied
+                out_buffer = buffer
+            else:
+                assert isinstance(buffer, Buffer)
+                assert buffer.is_leaf
+                out_buffer = Buffer(buffer_applied, buffer.requires_grad)
+                self._buffers[key] = out_buffer
+
+            if buffer.grad is not None:
+                with torch.no_grad():
+                    grad_applied = fn(buffer.grad)
+                should_use_set_data = compute_should_use_set_data(
+                    buffer.grad, grad_applied
+                )
+                if should_use_set_data:
+                    assert out_buffer.grad is not None
+                    out_buffer.grad.data = grad_applied
+                else:
+                    assert buffer.grad.is_leaf
+                    out_buffer.grad = grad_applied.requires_grad_(
+                        buffer.grad.requires_grad
+                    )
+
+        return self
+
+
+TDPARAM_HANDLED_FUNCTIONS = copy(TD_HANDLED_FUNCTIONS)
+
+
+def implements_for_tdparam(torch_function: Callable) -> Callable[[Callable], Callable]:
+    """Register a torch function override for TensorDictParams."""
+
+    @functools.wraps(torch_function)
+    def decorator(func: Callable) -> Callable:
+        TDPARAM_HANDLED_FUNCTIONS[torch_function] = func
+        return func
+
+    return decorator
+
+
+@implements_for_tdparam(torch.empty_like)
+def _empty_like(td: TensorDictBase, *args, **kwargs) -> TensorDictBase:
+    try:
+        tdclone = td.clone()
+    except Exception as err:
+        raise RuntimeError(
+            "The tensordict passed to torch.empty_like cannot be "
+            "cloned, preventing empty_like to be called. "
+            "Consider calling tensordict.to_tensordict() first."
+        ) from err
+    return tdclone.data.apply_(lambda x: torch.empty_like(x, *args, **kwargs))
```

## tensordict/nn/probabilistic.py

 * *Ordering differences only*

```diff
@@ -1,524 +1,524 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import re
-import warnings
-from enum import auto, Enum
-from textwrap import indent
-from typing import Any, Callable, Dict, List, Optional
-from warnings import warn
-
-from tensordict._contextlib import _DecoratorContextManager
-
-from tensordict.nn.common import dispatch, TensorDictModule, TensorDictModuleBase
-from tensordict.nn.distributions import Delta, distributions_maps
-from tensordict.nn.sequence import TensorDictSequential
-
-from tensordict.nn.utils import set_skip_existing
-from tensordict.tensordict import TensorDictBase
-from tensordict.utils import NestedKey
-from torch import distributions as D, Tensor
-
-__all__ = ["ProbabilisticTensorDictModule", "ProbabilisticTensorDictSequential"]
-
-
-class InteractionType(Enum):
-    MODE = auto()
-    MEDIAN = auto()
-    MEAN = auto()
-    RANDOM = auto()
-
-    @classmethod
-    def from_str(cls, type_str: str) -> InteractionType:
-        """Return the interaction_type with name matched to the provided string (case insensitive)."""
-        for member_type in cls:
-            if member_type.name == type_str.upper():
-                return member_type
-        raise ValueError(f"The provided interaction type {type_str} is unsupported!")
-
-
-_INTERACTION_TYPE: InteractionType | None = None
-
-
-def _insert_interaction_mode_deprecation_warning(
-    prefix: str = "",
-) -> Callable[[str, Warning, int], None]:
-    return warn(
-        (
-            f"{prefix}interaction_mode is deprecated for naming clarity. "
-            f"Please use {prefix}interaction_type with InteractionType enum instead."
-        ),
-        DeprecationWarning,
-        stacklevel=2,
-    )
-
-
-def interaction_type() -> InteractionType | None:
-    """Returns the current sampling type."""
-    return _INTERACTION_TYPE
-
-
-def interaction_mode() -> str | None:
-    """*Deprecated* Returns the current sampling mode."""
-    _insert_interaction_mode_deprecation_warning()
-    type = interaction_type()
-    return type.name.lower() if type else None
-
-
-class set_interaction_mode(_DecoratorContextManager):
-    """*Deprecated* Sets the sampling mode of all ProbabilisticTDModules to the desired mode.
-
-    Args:
-        mode (str): mode to use when the policy is being called.
-    """
-
-    def __init__(self, mode: str | None = "mode") -> None:
-        _insert_interaction_mode_deprecation_warning("set_")
-        super().__init__()
-        self.mode = InteractionType.from_str(mode) if mode else None
-
-    def clone(self) -> set_interaction_mode:
-        # override this method if your children class takes __init__ parameters
-        return self.__class__(self.mode)
-
-    def __enter__(self) -> None:
-        global _INTERACTION_TYPE
-        self.prev = _INTERACTION_TYPE
-        _INTERACTION_TYPE = self.mode
-
-    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
-        global _INTERACTION_TYPE
-        _INTERACTION_TYPE = self.prev
-
-
-class set_interaction_type(_DecoratorContextManager):
-    """Sets all ProbabilisticTDModules sampling to the desired type.
-
-    Args:
-        type (InteractionType): sampling type to use when the policy is being called.
-    """
-
-    def __init__(self, type: InteractionType | None = InteractionType.MODE) -> None:
-        super().__init__()
-        self.type = type
-
-    def clone(self) -> set_interaction_type:
-        # override this method if your children class takes __init__ parameters
-        return self.__class__(self.type)
-
-    def __enter__(self) -> None:
-        global _INTERACTION_TYPE
-        self.prev = _INTERACTION_TYPE
-        _INTERACTION_TYPE = self.type
-
-    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
-        global _INTERACTION_TYPE
-        _INTERACTION_TYPE = self.prev
-
-
-class ProbabilisticTensorDictModule(TensorDictModuleBase):
-    """A probabilistic TD Module.
-
-    `ProbabilisticTensorDictModule` is a non-parametric module representing a
-    probability distribution. It reads the distribution parameters from an input
-    TensorDict using the specified `in_keys`. The output is sampled given some rule,
-    specified by the input :obj:`default_interaction_type` argument and the
-    :obj:`interaction_type()` global function.
-
-    :obj:`ProbabilisticTensorDictModule` can be used to construct the distribution
-    (through the :obj:`get_dist()` method) and/or sampling from this distribution
-    (through a regular :obj:`__call__()` to the module).
-
-    A :obj:`ProbabilisticTensorDictModule` instance has two main features:
-    - It reads and writes TensorDict objects
-    - It uses a real mapping R^n -> R^m to create a distribution in R^d from
-    which values can be sampled or computed.
-
-    When the :obj:`__call__` / :obj:`forward` method is called, a distribution is
-    created, and a value computed (using the 'mean', 'mode', 'median' attribute or
-    the 'rsample', 'sample' method). The sampling step is skipped if the supplied
-    TensorDict has all of the desired key-value pairs already.
-
-    By default, ProbabilisticTensorDictModule distribution class is a Delta
-    distribution, making ProbabilisticTensorDictModule a simple wrapper around
-    a deterministic mapping function.
-
-    Args:
-        in_keys (NestedKey or list of NestedKey or dict): key(s) that will be read from the
-            input TensorDict and used to build the distribution. Importantly, if it's an
-            list of NestedKey or a NestedKey, the leaf (last element) of those keys must match the keywords used by
-            the distribution class of interest, e.g. :obj:`"loc"` and :obj:`"scale"` for
-            the Normal distribution and similar. If in_keys is a dictionary, the keys
-            are the keys of the distribution and the values are the keys in the
-            tensordict that will get match to the corresponding distribution keys.
-        out_keys (NestedKey or list of NestedKey): keys where the sampled values will be
-            written. Importantly, if these keys are found in the input TensorDict, the
-            sampling step will be skipped.
-        default_interaction_mode (str, optional): *Deprecated* keyword-only argument.
-            Please use default_interaction_type instead.
-        default_interaction_type (InteractionType, optional): keyword-only argument.
-            Default method to be used to retrieve
-            the output value. Should be one of InteractionType: MODE, MEDIAN, MEAN or RANDOM
-            (in which case the value is sampled randomly from the distribution). Default
-            is MODE.
-            Note: When a sample is drawn, the :obj:`ProbabilisticTDModule` instance will
-            first look for the interaction mode dictated by the `interaction_type()`
-            global function. If this returns `None` (its default value), then the
-            `default_interaction_type` of the `ProbabilisticTDModule` instance will be
-            used. Note that DataCollector instances will use `set_interaction_type` to
-            `RANDOM` by default.
-        distribution_class (Type, optional): keyword-only argument.
-            A :class:`torch.distributions.Distribution` class to
-            be used for sampling.
-            Default is :class:`tensordict.nn.distributions.Delta`.
-        distribution_kwargs (dict, optional): keyword-only argument.
-            Keyword-argument pairs to be passed to the distribution.
-        return_log_prob (bool, optional): keyword-only argument.
-            If ``True``, the log-probability of the
-            distribution sample will be written in the tensordict with the key
-            `log_prob_key`. Default is ``False``.
-        log_prob_key (NestedKey, optional): key where to write the log_prob if return_log_prob = True.
-            Defaults to `'sample_log_prob'`.
-        cache_dist (bool, optional): keyword-only argument.
-            EXPERIMENTAL: if ``True``, the parameters of the
-            distribution (i.e. the output of the module) will be written to the
-            tensordict along with the sample. Those parameters can be used to re-compute
-            the original distribution later on (e.g. to compute the divergence between
-            the distribution used to sample the action and the updated distribution in
-            PPO). Default is ``False``.
-        n_empirical_estimate (int, optional): keyword-only argument.
-            Number of samples to compute the empirical
-            mean when it is not available. Defaults to 1000.
-
-    Examples:
-        >>> import torch
-        >>> from tensordict import TensorDict
-        >>> from tensordict.nn import (
-        ...     ProbabilisticTensorDictModule,
-        ...     ProbabilisticTensorDictSequential,
-        ...     TensorDictModule,
-        ... )
-        >>> from tensordict.nn.distributions import NormalParamExtractor
-        >>> from tensordict.nn.functional_modules import make_functional
-        >>> from torch.distributions import Normal
-        >>> td = TensorDict(
-        ...     {"input": torch.randn(3, 4), "hidden": torch.randn(3, 8)}, [3]
-        ... )
-        >>> net = torch.nn.GRUCell(4, 8)
-        >>> module = TensorDictModule(
-        ...     net, in_keys=["input", "hidden"], out_keys=["params"]
-        ... )
-        >>> normal_params = TensorDictModule(
-        ...     NormalParamExtractor(), in_keys=["params"], out_keys=["loc", "scale"]
-        ... )
-        >>> prob_module = ProbabilisticTensorDictModule(
-        ...     in_keys=["loc", "scale"],
-        ...     out_keys=["action"],
-        ...     distribution_class=Normal,
-        ...     return_log_prob=True,
-        ... )
-        >>> td_module = ProbabilisticTensorDictSequential(
-        ...     module, normal_params, prob_module
-        ... )
-        >>> params = make_functional(td_module, funs_to_decorate=["forward", "get_dist"])
-        >>> _ = td_module(td, params=params)
-        >>> print(td)
-        TensorDict(
-            fields={
-                action: Tensor(torch.Size([3, 4]), dtype=torch.float32),
-                hidden: Tensor(torch.Size([3, 8]), dtype=torch.float32),
-                input: Tensor(torch.Size([3, 4]), dtype=torch.float32),
-                loc: Tensor(torch.Size([3, 4]), dtype=torch.float32),
-                params: Tensor(torch.Size([3, 8]), dtype=torch.float32),
-                sample_log_prob: Tensor(torch.Size([3, 4]), dtype=torch.float32),
-                scale: Tensor(torch.Size([3, 4]), dtype=torch.float32)},
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-        >>> dist = td_module.get_dist(td, params=params)
-        >>> print(dist)
-        Normal(loc: torch.Size([3, 4]), scale: torch.Size([3, 4]))
-        >>> # we can also apply the module to the TensorDict with vmap
-        >>> from torch import vmap
-        >>> params = params.expand(4)
-        >>> td_vmap = vmap(td_module, (None, 0))(td, params)
-        >>> print(td_vmap)
-        TensorDict(
-            fields={
-                action: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32),
-                hidden: Tensor(torch.Size([4, 3, 8]), dtype=torch.float32),
-                input: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32),
-                loc: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32),
-                params: Tensor(torch.Size([4, 3, 8]), dtype=torch.float32),
-                sample_log_prob: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32),
-                scale: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32)},
-            batch_size=torch.Size([4, 3]),
-            device=None,
-            is_shared=False)
-
-    """
-
-    def __init__(
-        self,
-        in_keys: NestedKey | List[NestedKey] | Dict[str, NestedKey],
-        out_keys: NestedKey | List[NestedKey] | None = None,
-        *,
-        default_interaction_mode: str | None = None,
-        default_interaction_type: InteractionType = InteractionType.MODE,
-        distribution_class: type = Delta,
-        distribution_kwargs: dict | None = None,
-        return_log_prob: bool = False,
-        log_prob_key: Optional[NestedKey] = "sample_log_prob",
-        cache_dist: bool = False,
-        n_empirical_estimate: int = 1000,
-    ) -> None:
-        super().__init__()
-        if isinstance(in_keys, (str, tuple)):
-            in_keys = [in_keys]
-        if isinstance(out_keys, (str, tuple)):
-            out_keys = [out_keys]
-        elif out_keys is None:
-            out_keys = ["_"]
-        if isinstance(in_keys, dict):
-            dist_keys, in_keys = zip(*in_keys.items())
-            if set(map(type, dist_keys)) != {str}:
-                raise ValueError(
-                    f"If in_keys is dict, its keys must be strings matching to the distribution kwargs."
-                    f"{self.__class__.__name__} got {dist_keys}"
-                )
-        else:
-            dist_keys = in_keys
-
-        self.out_keys = out_keys
-        self.in_keys = in_keys
-        self.dist_keys = dist_keys
-        if log_prob_key is None:
-            log_prob_key = "sample_log_prob"
-        self.log_prob_key = log_prob_key
-
-        if default_interaction_mode is not None:
-            _insert_interaction_mode_deprecation_warning("default_")
-            self.default_interaction_type = InteractionType.from_str(
-                default_interaction_mode
-            )
-        else:
-            self.default_interaction_type = default_interaction_type
-
-        if isinstance(distribution_class, str):
-            distribution_class = distributions_maps.get(distribution_class.lower())
-        self.distribution_class = distribution_class
-        self.distribution_kwargs = (
-            distribution_kwargs if distribution_kwargs is not None else {}
-        )
-        self.n_empirical_estimate = n_empirical_estimate
-        self._dist = None
-        self.cache_dist = cache_dist if hasattr(distribution_class, "update") else False
-        self.return_log_prob = return_log_prob
-        if self.return_log_prob:
-            self.out_keys.append(self.log_prob_key)
-
-    def get_dist(self, tensordict: TensorDictBase) -> D.Distribution:
-        try:
-            dist_kwargs = {}
-            for dist_key, td_key in zip(self.dist_keys, self.in_keys):
-                if isinstance(dist_key, tuple):
-                    dist_key = dist_key[-1]
-                dist_kwargs[dist_key] = tensordict.get(td_key)
-            dist = self.distribution_class(**dist_kwargs, **self.distribution_kwargs)
-        except TypeError as err:
-            if "an unexpected keyword argument" in str(err):
-                raise TypeError(
-                    "distribution keywords and tensordict keys indicated by ProbabilisticTensorDictModule.dist_keys must match."
-                    f"Got this error message: \n{indent(str(err), 4 * ' ')}\nwith dist_keys={self.dist_keys}"
-                )
-            elif re.search(r"missing.*required positional arguments", str(err)):
-                raise TypeError(
-                    f"TensorDict with keys {tensordict.keys()} does not match the distribution {self.distribution_class} keywords."
-                )
-            else:
-                raise err
-        return dist
-
-    @property
-    def SAMPLE_LOG_PROB_KEY(self):
-        warnings.warn(
-            "SAMPLE_LOG_PROB_KEY will be deprecated soon."
-            "Use 'obj.log_prob_key' instead",
-            category=DeprecationWarning,
-        )
-        return self.log_prob_key
-
-    @dispatch(auto_batch_size=False)
-    @set_skip_existing(None)
-    def forward(
-        self,
-        tensordict: TensorDictBase,
-        tensordict_out: TensorDictBase | None = None,
-        _requires_sample: bool = True,
-    ) -> TensorDictBase:
-        if tensordict_out is None:
-            tensordict_out = tensordict
-
-        dist = self.get_dist(tensordict)
-        if _requires_sample:
-            out_tensors = self._dist_sample(dist, interaction_type=interaction_type())
-            if isinstance(out_tensors, Tensor):
-                out_tensors = (out_tensors,)
-            tensordict_out.update(
-                {key: value for key, value in zip(self.out_keys, out_tensors)}
-            )
-            if self.return_log_prob:
-                log_prob = dist.log_prob(*out_tensors)
-                tensordict_out.set(self.log_prob_key, log_prob)
-        elif self.return_log_prob:
-            out_tensors = [
-                tensordict.get(key) for key in self.out_keys if key != self.log_prob_key
-            ]
-            log_prob = dist.log_prob(*out_tensors)
-            tensordict_out.set(self.log_prob_key, log_prob)
-            # raise RuntimeError(
-            #     "ProbabilisticTensorDictModule.return_log_prob = True is incompatible with settings in which "
-            #     "the submodule is responsible for sampling. To manually gather the log-probability, call first "
-            #     "\n>>> dist, tensordict = tensordict_module.get_dist(tensordict)"
-            #     "\n>>> tensordict.set('sample_log_prob', dist.log_prob(tensordict.get(sample_key))"
-            # )
-        return tensordict_out
-
-    def _dist_sample(
-        self,
-        dist: D.Distribution,
-        interaction_type: InteractionType | None = None,
-    ) -> tuple[Tensor, ...] | Tensor:
-        if interaction_type is None:
-            interaction_type = self.default_interaction_type
-
-        if interaction_type is InteractionType.MODE:
-            try:
-                return dist.mode
-            except AttributeError:
-                raise NotImplementedError(
-                    f"method {type(dist)}.mode is not implemented"
-                )
-
-        elif interaction_type is InteractionType.MEDIAN:
-            try:
-                return dist.median
-            except AttributeError:
-                raise NotImplementedError(
-                    f"method {type(dist)}.median is not implemented"
-                )
-
-        elif interaction_type is InteractionType.MEAN:
-            try:
-                return dist.mean
-            except (AttributeError, NotImplementedError):
-                if dist.has_rsample:
-                    return dist.rsample((self.n_empirical_estimate,)).mean(0)
-                else:
-                    return dist.sample((self.n_empirical_estimate,)).mean(0)
-
-        elif interaction_type is InteractionType.RANDOM:
-            if dist.has_rsample:
-                return dist.rsample()
-            else:
-                return dist.sample()
-        else:
-            raise NotImplementedError(f"unknown interaction_type {interaction_type}")
-
-
-class ProbabilisticTensorDictSequential(TensorDictSequential):
-    """A sequence of TensorDictModules ending in a ProbabilistictTensorDictModule.
-
-    Similarly to :obj:`TensorDictSequential`, but enforces that the final module in the
-    sequence is an :obj:`ProbabilisticTensorDictModule` and also exposes ``get_dist``
-    method to recover the distribution object from the ``ProbabilisticTensorDictModule``
-
-    Args:
-         modules (sequence of TensorDictModules): ordered sequence of TensorDictModule
-            instances, terminating in ProbabilisticTensorDictModule, to be run
-            sequentially.
-         partial_tolerant (bool, optional): if True, the input tensordict can miss some
-            of the input keys. If so, the only module that will be executed are those
-            who can be executed given the keys that are present. Also, if the input
-            tensordict is a lazy stack of tensordicts AND if partial_tolerant is
-            :obj:`True` AND if the stack does not have the required keys, then
-            TensorDictSequential will scan through the sub-tensordicts looking for those
-            that have the required keys, if any.
-
-    """
-
-    def __init__(
-        self,
-        *modules: TensorDictModule | ProbabilisticTensorDictModule,
-        partial_tolerant: bool = False,
-    ) -> None:
-        if len(modules) == 0:
-            raise ValueError(
-                "ProbabilisticTensorDictSequential must consist of zero or more "
-                "TensorDictModules followed by a ProbabilisticTensorDictModule"
-            )
-        if not isinstance(
-            modules[-1],
-            (ProbabilisticTensorDictModule, ProbabilisticTensorDictSequential),
-        ):
-            raise TypeError(
-                "The final module passed to ProbabilisticTensorDictSequential must be "
-                "an instance of ProbabilisticTensorDictModule or another "
-                "ProbabilisticTensorDictSequential"
-            )
-        # if the modules not including the final probabilistic module return the sampled
-        # key we wont be sampling it again, in that case
-        # ProbabilisticTensorDictSequential is presumably used to return the
-        # distribution using `get_dist` or to sample log_probabilities
-        _, out_keys = self._compute_in_and_out_keys(modules[:-1])
-        self._requires_sample = modules[-1].out_keys[0] not in set(out_keys)
-        super().__init__(*modules, partial_tolerant=partial_tolerant)
-
-    @property
-    def det_part(self):
-        if not hasattr(self, "_det_part"):
-            # we use a list to avoid having the submodules listed in module.modules()
-            self._det_part = [TensorDictSequential(*self.module[:-1])]
-        return self._det_part[0]
-
-    def get_dist_params(
-        self,
-        tensordict: TensorDictBase,
-        tensordict_out: TensorDictBase | None = None,
-        **kwargs,
-    ) -> tuple[D.Distribution, TensorDictBase]:
-        tds = self.det_part
-        type = interaction_type()
-        if type is None:
-            type = self.module[-1].default_interaction_type
-        with set_interaction_type(type):
-            return tds(tensordict, tensordict_out, **kwargs)
-
-    def get_dist(
-        self,
-        tensordict: TensorDictBase,
-        tensordict_out: TensorDictBase | None = None,
-        **kwargs,
-    ) -> D.Distribution:
-        """Get the distribution that results from passing the input tensordict through the sequence, and then using the resulting parameters."""
-        tensordict_out = self.get_dist_params(tensordict, tensordict_out, **kwargs)
-        return self.build_dist_from_params(tensordict_out)
-
-    def build_dist_from_params(self, tensordict: TensorDictBase) -> D.Distribution:
-        """Construct a distribution from the input parameters. Other modules in the sequence are not evaluated."""
-        return self.module[-1].get_dist(tensordict)
-
-    @dispatch(auto_batch_size=False)
-    @set_skip_existing(None)
-    def forward(
-        self,
-        tensordict: TensorDictBase,
-        tensordict_out: TensorDictBase | None = None,
-        **kwargs,
-    ) -> TensorDictBase:
-        tensordict_out = self.get_dist_params(tensordict, tensordict_out, **kwargs)
-        return self.module[-1](tensordict_out, _requires_sample=self._requires_sample)
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import re
+import warnings
+from enum import auto, Enum
+from textwrap import indent
+from typing import Any, Callable, Dict, List, Optional
+from warnings import warn
+
+from tensordict._contextlib import _DecoratorContextManager
+
+from tensordict.nn.common import dispatch, TensorDictModule, TensorDictModuleBase
+from tensordict.nn.distributions import Delta, distributions_maps
+from tensordict.nn.sequence import TensorDictSequential
+
+from tensordict.nn.utils import set_skip_existing
+from tensordict.tensordict import TensorDictBase
+from tensordict.utils import NestedKey
+from torch import distributions as D, Tensor
+
+__all__ = ["ProbabilisticTensorDictModule", "ProbabilisticTensorDictSequential"]
+
+
+class InteractionType(Enum):
+    MODE = auto()
+    MEDIAN = auto()
+    MEAN = auto()
+    RANDOM = auto()
+
+    @classmethod
+    def from_str(cls, type_str: str) -> InteractionType:
+        """Return the interaction_type with name matched to the provided string (case insensitive)."""
+        for member_type in cls:
+            if member_type.name == type_str.upper():
+                return member_type
+        raise ValueError(f"The provided interaction type {type_str} is unsupported!")
+
+
+_INTERACTION_TYPE: InteractionType | None = None
+
+
+def _insert_interaction_mode_deprecation_warning(
+    prefix: str = "",
+) -> Callable[[str, Warning, int], None]:
+    return warn(
+        (
+            f"{prefix}interaction_mode is deprecated for naming clarity. "
+            f"Please use {prefix}interaction_type with InteractionType enum instead."
+        ),
+        DeprecationWarning,
+        stacklevel=2,
+    )
+
+
+def interaction_type() -> InteractionType | None:
+    """Returns the current sampling type."""
+    return _INTERACTION_TYPE
+
+
+def interaction_mode() -> str | None:
+    """*Deprecated* Returns the current sampling mode."""
+    _insert_interaction_mode_deprecation_warning()
+    type = interaction_type()
+    return type.name.lower() if type else None
+
+
+class set_interaction_mode(_DecoratorContextManager):
+    """*Deprecated* Sets the sampling mode of all ProbabilisticTDModules to the desired mode.
+
+    Args:
+        mode (str): mode to use when the policy is being called.
+    """
+
+    def __init__(self, mode: str | None = "mode") -> None:
+        _insert_interaction_mode_deprecation_warning("set_")
+        super().__init__()
+        self.mode = InteractionType.from_str(mode) if mode else None
+
+    def clone(self) -> set_interaction_mode:
+        # override this method if your children class takes __init__ parameters
+        return self.__class__(self.mode)
+
+    def __enter__(self) -> None:
+        global _INTERACTION_TYPE
+        self.prev = _INTERACTION_TYPE
+        _INTERACTION_TYPE = self.mode
+
+    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
+        global _INTERACTION_TYPE
+        _INTERACTION_TYPE = self.prev
+
+
+class set_interaction_type(_DecoratorContextManager):
+    """Sets all ProbabilisticTDModules sampling to the desired type.
+
+    Args:
+        type (InteractionType): sampling type to use when the policy is being called.
+    """
+
+    def __init__(self, type: InteractionType | None = InteractionType.MODE) -> None:
+        super().__init__()
+        self.type = type
+
+    def clone(self) -> set_interaction_type:
+        # override this method if your children class takes __init__ parameters
+        return self.__class__(self.type)
+
+    def __enter__(self) -> None:
+        global _INTERACTION_TYPE
+        self.prev = _INTERACTION_TYPE
+        _INTERACTION_TYPE = self.type
+
+    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
+        global _INTERACTION_TYPE
+        _INTERACTION_TYPE = self.prev
+
+
+class ProbabilisticTensorDictModule(TensorDictModuleBase):
+    """A probabilistic TD Module.
+
+    `ProbabilisticTensorDictModule` is a non-parametric module representing a
+    probability distribution. It reads the distribution parameters from an input
+    TensorDict using the specified `in_keys`. The output is sampled given some rule,
+    specified by the input :obj:`default_interaction_type` argument and the
+    :obj:`interaction_type()` global function.
+
+    :obj:`ProbabilisticTensorDictModule` can be used to construct the distribution
+    (through the :obj:`get_dist()` method) and/or sampling from this distribution
+    (through a regular :obj:`__call__()` to the module).
+
+    A :obj:`ProbabilisticTensorDictModule` instance has two main features:
+    - It reads and writes TensorDict objects
+    - It uses a real mapping R^n -> R^m to create a distribution in R^d from
+    which values can be sampled or computed.
+
+    When the :obj:`__call__` / :obj:`forward` method is called, a distribution is
+    created, and a value computed (using the 'mean', 'mode', 'median' attribute or
+    the 'rsample', 'sample' method). The sampling step is skipped if the supplied
+    TensorDict has all of the desired key-value pairs already.
+
+    By default, ProbabilisticTensorDictModule distribution class is a Delta
+    distribution, making ProbabilisticTensorDictModule a simple wrapper around
+    a deterministic mapping function.
+
+    Args:
+        in_keys (NestedKey or list of NestedKey or dict): key(s) that will be read from the
+            input TensorDict and used to build the distribution. Importantly, if it's an
+            list of NestedKey or a NestedKey, the leaf (last element) of those keys must match the keywords used by
+            the distribution class of interest, e.g. :obj:`"loc"` and :obj:`"scale"` for
+            the Normal distribution and similar. If in_keys is a dictionary, the keys
+            are the keys of the distribution and the values are the keys in the
+            tensordict that will get match to the corresponding distribution keys.
+        out_keys (NestedKey or list of NestedKey): keys where the sampled values will be
+            written. Importantly, if these keys are found in the input TensorDict, the
+            sampling step will be skipped.
+        default_interaction_mode (str, optional): *Deprecated* keyword-only argument.
+            Please use default_interaction_type instead.
+        default_interaction_type (InteractionType, optional): keyword-only argument.
+            Default method to be used to retrieve
+            the output value. Should be one of InteractionType: MODE, MEDIAN, MEAN or RANDOM
+            (in which case the value is sampled randomly from the distribution). Default
+            is MODE.
+            Note: When a sample is drawn, the :obj:`ProbabilisticTDModule` instance will
+            first look for the interaction mode dictated by the `interaction_type()`
+            global function. If this returns `None` (its default value), then the
+            `default_interaction_type` of the `ProbabilisticTDModule` instance will be
+            used. Note that DataCollector instances will use `set_interaction_type` to
+            `RANDOM` by default.
+        distribution_class (Type, optional): keyword-only argument.
+            A :class:`torch.distributions.Distribution` class to
+            be used for sampling.
+            Default is :class:`tensordict.nn.distributions.Delta`.
+        distribution_kwargs (dict, optional): keyword-only argument.
+            Keyword-argument pairs to be passed to the distribution.
+        return_log_prob (bool, optional): keyword-only argument.
+            If ``True``, the log-probability of the
+            distribution sample will be written in the tensordict with the key
+            `log_prob_key`. Default is ``False``.
+        log_prob_key (NestedKey, optional): key where to write the log_prob if return_log_prob = True.
+            Defaults to `'sample_log_prob'`.
+        cache_dist (bool, optional): keyword-only argument.
+            EXPERIMENTAL: if ``True``, the parameters of the
+            distribution (i.e. the output of the module) will be written to the
+            tensordict along with the sample. Those parameters can be used to re-compute
+            the original distribution later on (e.g. to compute the divergence between
+            the distribution used to sample the action and the updated distribution in
+            PPO). Default is ``False``.
+        n_empirical_estimate (int, optional): keyword-only argument.
+            Number of samples to compute the empirical
+            mean when it is not available. Defaults to 1000.
+
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import (
+        ...     ProbabilisticTensorDictModule,
+        ...     ProbabilisticTensorDictSequential,
+        ...     TensorDictModule,
+        ... )
+        >>> from tensordict.nn.distributions import NormalParamExtractor
+        >>> from tensordict.nn.functional_modules import make_functional
+        >>> from torch.distributions import Normal
+        >>> td = TensorDict(
+        ...     {"input": torch.randn(3, 4), "hidden": torch.randn(3, 8)}, [3]
+        ... )
+        >>> net = torch.nn.GRUCell(4, 8)
+        >>> module = TensorDictModule(
+        ...     net, in_keys=["input", "hidden"], out_keys=["params"]
+        ... )
+        >>> normal_params = TensorDictModule(
+        ...     NormalParamExtractor(), in_keys=["params"], out_keys=["loc", "scale"]
+        ... )
+        >>> prob_module = ProbabilisticTensorDictModule(
+        ...     in_keys=["loc", "scale"],
+        ...     out_keys=["action"],
+        ...     distribution_class=Normal,
+        ...     return_log_prob=True,
+        ... )
+        >>> td_module = ProbabilisticTensorDictSequential(
+        ...     module, normal_params, prob_module
+        ... )
+        >>> params = make_functional(td_module, funs_to_decorate=["forward", "get_dist"])
+        >>> _ = td_module(td, params=params)
+        >>> print(td)
+        TensorDict(
+            fields={
+                action: Tensor(torch.Size([3, 4]), dtype=torch.float32),
+                hidden: Tensor(torch.Size([3, 8]), dtype=torch.float32),
+                input: Tensor(torch.Size([3, 4]), dtype=torch.float32),
+                loc: Tensor(torch.Size([3, 4]), dtype=torch.float32),
+                params: Tensor(torch.Size([3, 8]), dtype=torch.float32),
+                sample_log_prob: Tensor(torch.Size([3, 4]), dtype=torch.float32),
+                scale: Tensor(torch.Size([3, 4]), dtype=torch.float32)},
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+        >>> dist = td_module.get_dist(td, params=params)
+        >>> print(dist)
+        Normal(loc: torch.Size([3, 4]), scale: torch.Size([3, 4]))
+        >>> # we can also apply the module to the TensorDict with vmap
+        >>> from torch import vmap
+        >>> params = params.expand(4)
+        >>> td_vmap = vmap(td_module, (None, 0))(td, params)
+        >>> print(td_vmap)
+        TensorDict(
+            fields={
+                action: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32),
+                hidden: Tensor(torch.Size([4, 3, 8]), dtype=torch.float32),
+                input: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32),
+                loc: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32),
+                params: Tensor(torch.Size([4, 3, 8]), dtype=torch.float32),
+                sample_log_prob: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32),
+                scale: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32)},
+            batch_size=torch.Size([4, 3]),
+            device=None,
+            is_shared=False)
+
+    """
+
+    def __init__(
+        self,
+        in_keys: NestedKey | List[NestedKey] | Dict[str, NestedKey],
+        out_keys: NestedKey | List[NestedKey] | None = None,
+        *,
+        default_interaction_mode: str | None = None,
+        default_interaction_type: InteractionType = InteractionType.MODE,
+        distribution_class: type = Delta,
+        distribution_kwargs: dict | None = None,
+        return_log_prob: bool = False,
+        log_prob_key: Optional[NestedKey] = "sample_log_prob",
+        cache_dist: bool = False,
+        n_empirical_estimate: int = 1000,
+    ) -> None:
+        super().__init__()
+        if isinstance(in_keys, (str, tuple)):
+            in_keys = [in_keys]
+        if isinstance(out_keys, (str, tuple)):
+            out_keys = [out_keys]
+        elif out_keys is None:
+            out_keys = ["_"]
+        if isinstance(in_keys, dict):
+            dist_keys, in_keys = zip(*in_keys.items())
+            if set(map(type, dist_keys)) != {str}:
+                raise ValueError(
+                    f"If in_keys is dict, its keys must be strings matching to the distribution kwargs."
+                    f"{self.__class__.__name__} got {dist_keys}"
+                )
+        else:
+            dist_keys = in_keys
+
+        self.out_keys = out_keys
+        self.in_keys = in_keys
+        self.dist_keys = dist_keys
+        if log_prob_key is None:
+            log_prob_key = "sample_log_prob"
+        self.log_prob_key = log_prob_key
+
+        if default_interaction_mode is not None:
+            _insert_interaction_mode_deprecation_warning("default_")
+            self.default_interaction_type = InteractionType.from_str(
+                default_interaction_mode
+            )
+        else:
+            self.default_interaction_type = default_interaction_type
+
+        if isinstance(distribution_class, str):
+            distribution_class = distributions_maps.get(distribution_class.lower())
+        self.distribution_class = distribution_class
+        self.distribution_kwargs = (
+            distribution_kwargs if distribution_kwargs is not None else {}
+        )
+        self.n_empirical_estimate = n_empirical_estimate
+        self._dist = None
+        self.cache_dist = cache_dist if hasattr(distribution_class, "update") else False
+        self.return_log_prob = return_log_prob
+        if self.return_log_prob:
+            self.out_keys.append(self.log_prob_key)
+
+    def get_dist(self, tensordict: TensorDictBase) -> D.Distribution:
+        try:
+            dist_kwargs = {}
+            for dist_key, td_key in zip(self.dist_keys, self.in_keys):
+                if isinstance(dist_key, tuple):
+                    dist_key = dist_key[-1]
+                dist_kwargs[dist_key] = tensordict.get(td_key)
+            dist = self.distribution_class(**dist_kwargs, **self.distribution_kwargs)
+        except TypeError as err:
+            if "an unexpected keyword argument" in str(err):
+                raise TypeError(
+                    "distribution keywords and tensordict keys indicated by ProbabilisticTensorDictModule.dist_keys must match."
+                    f"Got this error message: \n{indent(str(err), 4 * ' ')}\nwith dist_keys={self.dist_keys}"
+                )
+            elif re.search(r"missing.*required positional arguments", str(err)):
+                raise TypeError(
+                    f"TensorDict with keys {tensordict.keys()} does not match the distribution {self.distribution_class} keywords."
+                )
+            else:
+                raise err
+        return dist
+
+    @property
+    def SAMPLE_LOG_PROB_KEY(self):
+        warnings.warn(
+            "SAMPLE_LOG_PROB_KEY will be deprecated soon."
+            "Use 'obj.log_prob_key' instead",
+            category=DeprecationWarning,
+        )
+        return self.log_prob_key
+
+    @dispatch(auto_batch_size=False)
+    @set_skip_existing(None)
+    def forward(
+        self,
+        tensordict: TensorDictBase,
+        tensordict_out: TensorDictBase | None = None,
+        _requires_sample: bool = True,
+    ) -> TensorDictBase:
+        if tensordict_out is None:
+            tensordict_out = tensordict
+
+        dist = self.get_dist(tensordict)
+        if _requires_sample:
+            out_tensors = self._dist_sample(dist, interaction_type=interaction_type())
+            if isinstance(out_tensors, Tensor):
+                out_tensors = (out_tensors,)
+            tensordict_out.update(
+                {key: value for key, value in zip(self.out_keys, out_tensors)}
+            )
+            if self.return_log_prob:
+                log_prob = dist.log_prob(*out_tensors)
+                tensordict_out.set(self.log_prob_key, log_prob)
+        elif self.return_log_prob:
+            out_tensors = [
+                tensordict.get(key) for key in self.out_keys if key != self.log_prob_key
+            ]
+            log_prob = dist.log_prob(*out_tensors)
+            tensordict_out.set(self.log_prob_key, log_prob)
+            # raise RuntimeError(
+            #     "ProbabilisticTensorDictModule.return_log_prob = True is incompatible with settings in which "
+            #     "the submodule is responsible for sampling. To manually gather the log-probability, call first "
+            #     "\n>>> dist, tensordict = tensordict_module.get_dist(tensordict)"
+            #     "\n>>> tensordict.set('sample_log_prob', dist.log_prob(tensordict.get(sample_key))"
+            # )
+        return tensordict_out
+
+    def _dist_sample(
+        self,
+        dist: D.Distribution,
+        interaction_type: InteractionType | None = None,
+    ) -> tuple[Tensor, ...] | Tensor:
+        if interaction_type is None:
+            interaction_type = self.default_interaction_type
+
+        if interaction_type is InteractionType.MODE:
+            try:
+                return dist.mode
+            except AttributeError:
+                raise NotImplementedError(
+                    f"method {type(dist)}.mode is not implemented"
+                )
+
+        elif interaction_type is InteractionType.MEDIAN:
+            try:
+                return dist.median
+            except AttributeError:
+                raise NotImplementedError(
+                    f"method {type(dist)}.median is not implemented"
+                )
+
+        elif interaction_type is InteractionType.MEAN:
+            try:
+                return dist.mean
+            except (AttributeError, NotImplementedError):
+                if dist.has_rsample:
+                    return dist.rsample((self.n_empirical_estimate,)).mean(0)
+                else:
+                    return dist.sample((self.n_empirical_estimate,)).mean(0)
+
+        elif interaction_type is InteractionType.RANDOM:
+            if dist.has_rsample:
+                return dist.rsample()
+            else:
+                return dist.sample()
+        else:
+            raise NotImplementedError(f"unknown interaction_type {interaction_type}")
+
+
+class ProbabilisticTensorDictSequential(TensorDictSequential):
+    """A sequence of TensorDictModules ending in a ProbabilistictTensorDictModule.
+
+    Similarly to :obj:`TensorDictSequential`, but enforces that the final module in the
+    sequence is an :obj:`ProbabilisticTensorDictModule` and also exposes ``get_dist``
+    method to recover the distribution object from the ``ProbabilisticTensorDictModule``
+
+    Args:
+         modules (sequence of TensorDictModules): ordered sequence of TensorDictModule
+            instances, terminating in ProbabilisticTensorDictModule, to be run
+            sequentially.
+         partial_tolerant (bool, optional): if True, the input tensordict can miss some
+            of the input keys. If so, the only module that will be executed are those
+            who can be executed given the keys that are present. Also, if the input
+            tensordict is a lazy stack of tensordicts AND if partial_tolerant is
+            :obj:`True` AND if the stack does not have the required keys, then
+            TensorDictSequential will scan through the sub-tensordicts looking for those
+            that have the required keys, if any.
+
+    """
+
+    def __init__(
+        self,
+        *modules: TensorDictModule | ProbabilisticTensorDictModule,
+        partial_tolerant: bool = False,
+    ) -> None:
+        if len(modules) == 0:
+            raise ValueError(
+                "ProbabilisticTensorDictSequential must consist of zero or more "
+                "TensorDictModules followed by a ProbabilisticTensorDictModule"
+            )
+        if not isinstance(
+            modules[-1],
+            (ProbabilisticTensorDictModule, ProbabilisticTensorDictSequential),
+        ):
+            raise TypeError(
+                "The final module passed to ProbabilisticTensorDictSequential must be "
+                "an instance of ProbabilisticTensorDictModule or another "
+                "ProbabilisticTensorDictSequential"
+            )
+        # if the modules not including the final probabilistic module return the sampled
+        # key we wont be sampling it again, in that case
+        # ProbabilisticTensorDictSequential is presumably used to return the
+        # distribution using `get_dist` or to sample log_probabilities
+        _, out_keys = self._compute_in_and_out_keys(modules[:-1])
+        self._requires_sample = modules[-1].out_keys[0] not in set(out_keys)
+        super().__init__(*modules, partial_tolerant=partial_tolerant)
+
+    @property
+    def det_part(self):
+        if not hasattr(self, "_det_part"):
+            # we use a list to avoid having the submodules listed in module.modules()
+            self._det_part = [TensorDictSequential(*self.module[:-1])]
+        return self._det_part[0]
+
+    def get_dist_params(
+        self,
+        tensordict: TensorDictBase,
+        tensordict_out: TensorDictBase | None = None,
+        **kwargs,
+    ) -> tuple[D.Distribution, TensorDictBase]:
+        tds = self.det_part
+        type = interaction_type()
+        if type is None:
+            type = self.module[-1].default_interaction_type
+        with set_interaction_type(type):
+            return tds(tensordict, tensordict_out, **kwargs)
+
+    def get_dist(
+        self,
+        tensordict: TensorDictBase,
+        tensordict_out: TensorDictBase | None = None,
+        **kwargs,
+    ) -> D.Distribution:
+        """Get the distribution that results from passing the input tensordict through the sequence, and then using the resulting parameters."""
+        tensordict_out = self.get_dist_params(tensordict, tensordict_out, **kwargs)
+        return self.build_dist_from_params(tensordict_out)
+
+    def build_dist_from_params(self, tensordict: TensorDictBase) -> D.Distribution:
+        """Construct a distribution from the input parameters. Other modules in the sequence are not evaluated."""
+        return self.module[-1].get_dist(tensordict)
+
+    @dispatch(auto_batch_size=False)
+    @set_skip_existing(None)
+    def forward(
+        self,
+        tensordict: TensorDictBase,
+        tensordict_out: TensorDictBase | None = None,
+        **kwargs,
+    ) -> TensorDictBase:
+        tensordict_out = self.get_dist_params(tensordict, tensordict_out, **kwargs)
+        return self.module[-1](tensordict_out, _requires_sample=self._requires_sample)
```

## tensordict/nn/sequence.py

 * *Ordering differences only*

```diff
@@ -1,449 +1,449 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-from copy import deepcopy
-from typing import Any, Iterable
-
-from tensordict.nn.utils import set_skip_existing
-
-_has_functorch = False
-try:
-    import functorch
-
-    _has_functorch = True
-except ImportError:
-    print(
-        "failed to import functorch. TensorDict's features that do not require "
-        "functional programming should work, but functionality and performance "
-        "may be affected. Consider installing functorch and/or upgrating pytorch."
-    )
-    FUNCTORCH_ERROR = "functorch not installed. Consider installing functorch to use this functionality."
-
-
-from tensordict._tensordict import unravel_key_list
-from tensordict.nn.common import dispatch, TensorDictModule
-from tensordict.tensordict import LazyStackedTensorDict, TensorDictBase
-from tensordict.utils import NestedKey
-from torch import nn
-
-__all__ = ["TensorDictSequential"]
-
-
-class TensorDictSequential(TensorDictModule):
-    """A sequence of TensorDictModules.
-
-    By default, :class:`TensorDictSequential` subclasses are always functional,
-    meaning that they support the ``td_module(input, params=params)`` function
-    call signature.
-
-    Similarly to :obj:`nn.Sequence` which passes a tensor through a chain of mappings that read and write a single tensor
-    each, this module will read and write over a tensordict by querying each of the input modules.
-    When calling a :obj:`TensorDictSequencial` instance with a functional module, it is expected that the parameter lists (and
-    buffers) will be concatenated in a single list.
-
-    Args:
-         modules (iterable of TensorDictModules): ordered sequence of TensorDictModule instances to be run sequentially.
-         partial_tolerant (bool, optional): if True, the input tensordict can miss some of the input keys.
-            If so, the only module that will be executed are those who can be executed given the keys that
-            are present.
-            Also, if the input tensordict is a lazy stack of tensordicts AND if partial_tolerant is :obj:`True` AND if the
-            stack does not have the required keys, then TensorDictSequential will scan through the sub-tensordicts
-            looking for those that have the required keys, if any.
-
-    Examples:
-        >>> import torch
-        >>> from tensordict import TensorDict
-        >>> from tensordict.nn import TensorDictModule, TensorDictSequential
-        >>> torch.manual_seed(0)
-        >>> module = TensorDictSequential(
-        ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["x+1"]),
-        ...     TensorDictModule(nn.Linear(3, 4), in_keys=["x+1"], out_keys=["w*(x+1)+b"]),
-        ... )
-        >>> # with tensordict input
-        >>> print(module(TensorDict({"x": torch.zeros(3)}, [])))
-        TensorDict(
-            fields={
-                w*(x+1)+b: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
-                x+1: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
-                x: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([]),
-            device=None,
-            is_shared=False)
-        >>> # with tensor input: returns all the output keys in the order of the modules, ie "x+1" and "w*(x+1)+b"
-        >>> module(x=torch.zeros(3))
-        (tensor([1., 1., 1.]), tensor([-0.7214, -0.8748,  0.1571, -0.1138], grad_fn=<AddBackward0>))
-        >>> module(torch.zeros(3))
-        (tensor([1., 1., 1.]), tensor([-0.7214, -0.8748,  0.1571, -0.1138], grad_fn=<AddBackward0>))
-
-    TensorDictSequence supports functional, modular and vmap coding:
-    Examples:
-        >>> import torch
-        >>> from tensordict import TensorDict
-        >>> from tensordict.nn import (
-        ...     ProbabilisticTensorDictModule,
-        ...     ProbabilisticTensorDictSequential,
-        ...     TensorDictModule,
-        ...     TensorDictSequential,
-        ... )
-        >>> from tensordict.nn.distributions import NormalParamExtractor
-        >>> from tensordict.nn.functional_modules import make_functional
-        >>> from torch.distributions import Normal
-        >>> td = TensorDict({"input": torch.randn(3, 4)}, [3,])
-        >>> net1 = torch.nn.Linear(4, 8)
-        >>> module1 = TensorDictModule(net1, in_keys=["input"], out_keys=["params"])
-        >>> normal_params = TensorDictModule(
-        ...      NormalParamExtractor(), in_keys=["params"], out_keys=["loc", "scale"]
-        ...  )
-        >>> td_module1 = ProbabilisticTensorDictSequential(
-        ...     module1,
-        ...     normal_params,
-        ...     ProbabilisticTensorDictModule(
-        ...         in_keys=["loc", "scale"],
-        ...         out_keys=["hidden"],
-        ...         distribution_class=Normal,
-        ...         return_log_prob=True,
-        ...     )
-        ... )
-        >>> module2 = torch.nn.Linear(4, 8)
-        >>> td_module2 = TensorDictModule(
-        ...    module=module2, in_keys=["hidden"], out_keys=["output"]
-        ... )
-        >>> td_module = TensorDictSequential(td_module1, td_module2)
-        >>> params = make_functional(td_module)
-        >>> _ = td_module(td, params=params)
-        >>> print(td)
-        TensorDict(
-            fields={
-                hidden: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                loc: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                output: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                params: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                sample_log_prob: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                scale: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-
-    In the vmap case:
-        >>> from torch import vmap
-        >>> params = params.expand(4)
-        >>> td_vmap = vmap(td_module, (None, 0))(td, params)
-        >>> print(td_vmap)
-        TensorDict(
-            fields={
-                hidden: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                input: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                loc: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                output: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                params: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                sample_log_prob: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                scale: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([4, 3]),
-            device=None,
-            is_shared=False)
-
-    """
-
-    module: nn.ModuleList
-
-    def __init__(
-        self,
-        *modules: TensorDictModule,
-        partial_tolerant: bool = False,
-    ) -> None:
-        in_keys, out_keys = self._compute_in_and_out_keys(modules)
-
-        super().__init__(
-            module=nn.ModuleList(list(modules)), in_keys=in_keys, out_keys=out_keys
-        )
-
-        self.partial_tolerant = partial_tolerant
-
-    def _compute_in_and_out_keys(
-        self, modules: list[TensorDictModule]
-    ) -> tuple[list[NestedKey], list[NestedKey]]:
-        in_keys = []
-        out_keys = []
-        for module in modules:
-            # we sometimes use in_keys to select keys of a tensordict that are
-            # necessary to run a TensorDictModule. If a key is an intermediary in
-            # the chain, there is no reason why it should belong to the input
-            # TensorDict.
-            for in_key in module.in_keys:
-                if in_key not in (out_keys + in_keys):
-                    in_keys.append(in_key)
-            out_keys += module.out_keys
-
-        out_keys = [
-            out_key
-            for i, out_key in enumerate(out_keys)
-            if out_key not in out_keys[i + 1 :]
-        ]
-        return in_keys, out_keys
-
-    @staticmethod
-    def _find_functional_module(module: TensorDictModule) -> nn.Module:
-        if not _has_functorch:
-            raise ImportError(FUNCTORCH_ERROR)
-        fmodule = module
-        while not isinstance(
-            fmodule, (functorch.FunctionalModule, functorch.FunctionalModuleWithBuffers)
-        ):
-            try:
-                fmodule = fmodule.module
-            except AttributeError:
-                raise AttributeError(
-                    f"couldn't find a functional module in module of type {type(module)}"
-                )
-        return fmodule
-
-    def select_subsequence(
-        self,
-        in_keys: Iterable[NestedKey] | None = None,
-        out_keys: Iterable[NestedKey] | None = None,
-    ) -> TensorDictSequential:
-        """Returns a new TensorDictSequential with only the modules that are necessary to compute the given output keys with the given input keys.
-
-        Args:
-            in_keys: input keys of the subsequence we want to select.
-                All the keys absent from ``in_keys`` will be considered as
-                non-relevant, and modules that *just* take these keys as inputs
-                will be discarded.
-                The resulting sequential module will follow the pattern "all
-                the modules which output will be affected by a different value
-                for any in <in_keys>".
-                If none is provided, the module's ``in_keys`` are assumed.
-            out_keys: output keys of the subsequence we want to select.
-                Only the modules that are necessary to get the ``out_keys``
-                will be found in the resulting sequence.
-                The resulting sequential module will follow the pattern "all
-                the modules that condition the value or <out_keys> entries."
-                If none is provided, the module's ``out_keys`` are assumed.
-
-        Returns:
-            A new TensorDictSequential with only the modules that are necessary acording to the given input and output keys.
-
-        Examples:
-            >>> from tensordict.nn import TensorDictSequential as Seq, TensorDictModule as Mod
-            >>> idn = lambda x: x
-            >>> module = Seq(
-            ...     Mod(idn, in_keys=["a"], out_keys=["b"]),
-            ...     Mod(idn, in_keys=["b"], out_keys=["c"]),
-            ...     Mod(idn, in_keys=["c"], out_keys=["d"]),
-            ...     Mod(idn, in_keys=["a"], out_keys=["e"]),
-            ... )
-            >>> # select all modules whose output depend on "a"
-            >>> module.select_subsequence(in_keys=["a"])
-            TensorDictSequential(
-                module=ModuleList(
-                  (0): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['a'],
-                      out_keys=['b'])
-                  (1): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['b'],
-                      out_keys=['c'])
-                  (2): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['c'],
-                      out_keys=['d'])
-                  (3): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['a'],
-                      out_keys=['e'])
-                ),
-                device=cpu,
-                in_keys=['a'],
-                out_keys=['b', 'c', 'd', 'e'])
-            >>> # select all modules whose output depend on "c"
-            >>> module.select_subsequence(in_keys=["c"])
-            TensorDictSequential(
-                module=ModuleList(
-                  (0): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['c'],
-                      out_keys=['d'])
-                ),
-                device=cpu,
-                in_keys=['c'],
-                out_keys=['d'])
-            >>> # select all modules that affect the value of "c"
-            >>> module.select_subsequence(out_keys=["c"])
-            TensorDictSequential(
-                module=ModuleList(
-                  (0): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['a'],
-                      out_keys=['b'])
-                  (1): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['b'],
-                      out_keys=['c'])
-                ),
-                device=cpu,
-                in_keys=['a'],
-                out_keys=['b', 'c'])
-            >>> # select all modules that affect the value of "e"
-            >>> module.select_subsequence(out_keys=["e"])
-            TensorDictSequential(
-                module=ModuleList(
-                  (0): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['a'],
-                      out_keys=['e'])
-                ),
-                device=cpu,
-                in_keys=['a'],
-                out_keys=['e'])
-
-        This method propagates to nested sequential:
-
-            >>> module = Seq(
-            ...     Seq(
-            ...         Mod(idn, in_keys=["a"], out_keys=["b"]),
-            ...         Mod(idn, in_keys=["b"], out_keys=["c"]),
-            ...     ),
-            ...     Seq(
-            ...         Mod(idn, in_keys=["b"], out_keys=["d"]),
-            ...         Mod(idn, in_keys=["d"], out_keys=["e"]),
-            ...     ),
-            ... )
-            >>> # select submodules whose output will be affected by a change in "b" or "d" AND which output is "e"
-            >>> module.select_subsequence(in_keys=["b", "d"], out_keys=["e"])
-            TensorDictSequential(
-                module=ModuleList(
-                  (0): TensorDictSequential(
-                      module=ModuleList(
-                        (0): TensorDictModule(
-                            module=<function <lambda> at 0x129efae50>,
-                            device=cpu,
-                            in_keys=['b'],
-                            out_keys=['d'])
-                        (1): TensorDictModule(
-                            module=<function <lambda> at 0x129efae50>,
-                            device=cpu,
-                            in_keys=['d'],
-                            out_keys=['e'])
-                      ),
-                      device=cpu,
-                      in_keys=['b'],
-                      out_keys=['d', 'e'])
-                ),
-                device=cpu,
-                in_keys=['b'],
-                out_keys=['d', 'e'])
-
-        """
-        if in_keys is None:
-            in_keys = deepcopy(self.in_keys)
-        in_keys = unravel_key_list(in_keys)
-        if out_keys is None:
-            out_keys = deepcopy(self.out_keys)
-        out_keys = unravel_key_list(out_keys)
-
-        module_list = list(self.module)
-        id_to_keep = set(range(len(module_list)))
-        for i, module in enumerate(module_list):
-            if (
-                type(module) is TensorDictSequential
-            ):  # no isinstance because we don't want to mess up subclasses
-                try:
-                    module = module_list[i] = module.select_subsequence(in_keys=in_keys)
-                except ValueError:
-                    # then the module can be removed
-                    id_to_keep.remove(i)
-                    continue
-
-            if all(key in in_keys for key in module.in_keys):
-                in_keys.extend(module.out_keys)
-            else:
-                id_to_keep.remove(i)
-        for i, module in reversed(list(enumerate(module_list))):
-            if i in id_to_keep:
-                if any(key in out_keys for key in module.out_keys):
-                    if (
-                        type(module) is TensorDictSequential
-                    ):  # no isinstance because we don't want to mess up subclasses
-                        module = module_list[i] = module.select_subsequence(
-                            out_keys=out_keys
-                        )
-                    out_keys.extend(module.in_keys)
-                else:
-                    id_to_keep.remove(i)
-        id_to_keep = sorted(id_to_keep)
-
-        modules = [module_list[i] for i in id_to_keep]
-
-        if modules == []:
-            raise ValueError(
-                "No modules left after selection. Make sure that in_keys and out_keys are coherent."
-            )
-
-        return self.__class__(*modules)
-
-    def _run_module(
-        self,
-        module: TensorDictModule,
-        tensordict: TensorDictBase,
-        **kwargs: Any,
-    ) -> Any:
-        if not self.partial_tolerant or all(
-            key in tensordict.keys(include_nested=True) for key in module.in_keys
-        ):
-            tensordict = module(tensordict, **kwargs)
-        elif self.partial_tolerant and isinstance(tensordict, LazyStackedTensorDict):
-            for sub_td in tensordict.tensordicts:
-                if all(
-                    key in sub_td.keys(include_nested=True) for key in module.in_keys
-                ):
-                    module(sub_td, **kwargs)
-        return tensordict
-
-    @dispatch(auto_batch_size=False)
-    @set_skip_existing(None)
-    def forward(
-        self,
-        tensordict: TensorDictBase,
-        tensordict_out: TensorDictBase | None = None,
-        **kwargs: Any,
-    ) -> TensorDictBase:
-        if not len(kwargs):
-            for module in self.module:
-                tensordict = self._run_module(module, tensordict, **kwargs)
-        else:
-            raise RuntimeError(
-                f"TensorDictSequential does not support keyword arguments other than 'tensordict_out' or in_keys: {self.in_keys}. Got {kwargs.keys()} instead."
-            )
-        if tensordict_out is not None:
-            tensordict_out.update(tensordict, inplace=True)
-            return tensordict_out
-        return tensordict
-
-    def __len__(self) -> int:
-        return len(self.module)
-
-    def __getitem__(self, index: int | slice) -> TensorDictModule:
-        if isinstance(index, int):
-            return self.module.__getitem__(index)
-        else:
-            return self.__class__(*self.module.__getitem__(index))
-
-    def __setitem__(self, index: int, tensordict_module: TensorDictModule) -> None:
-        return self.module.__setitem__(idx=index, module=tensordict_module)
-
-    def __delitem__(self, index: int | slice) -> None:
-        self.module.__delitem__(idx=index)
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+from copy import deepcopy
+from typing import Any, Iterable
+
+from tensordict.nn.utils import set_skip_existing
+
+_has_functorch = False
+try:
+    import functorch
+
+    _has_functorch = True
+except ImportError:
+    print(
+        "failed to import functorch. TensorDict's features that do not require "
+        "functional programming should work, but functionality and performance "
+        "may be affected. Consider installing functorch and/or upgrating pytorch."
+    )
+    FUNCTORCH_ERROR = "functorch not installed. Consider installing functorch to use this functionality."
+
+
+from tensordict._tensordict import unravel_key_list
+from tensordict.nn.common import dispatch, TensorDictModule
+from tensordict.tensordict import LazyStackedTensorDict, TensorDictBase
+from tensordict.utils import NestedKey
+from torch import nn
+
+__all__ = ["TensorDictSequential"]
+
+
+class TensorDictSequential(TensorDictModule):
+    """A sequence of TensorDictModules.
+
+    By default, :class:`TensorDictSequential` subclasses are always functional,
+    meaning that they support the ``td_module(input, params=params)`` function
+    call signature.
+
+    Similarly to :obj:`nn.Sequence` which passes a tensor through a chain of mappings that read and write a single tensor
+    each, this module will read and write over a tensordict by querying each of the input modules.
+    When calling a :obj:`TensorDictSequencial` instance with a functional module, it is expected that the parameter lists (and
+    buffers) will be concatenated in a single list.
+
+    Args:
+         modules (iterable of TensorDictModules): ordered sequence of TensorDictModule instances to be run sequentially.
+         partial_tolerant (bool, optional): if True, the input tensordict can miss some of the input keys.
+            If so, the only module that will be executed are those who can be executed given the keys that
+            are present.
+            Also, if the input tensordict is a lazy stack of tensordicts AND if partial_tolerant is :obj:`True` AND if the
+            stack does not have the required keys, then TensorDictSequential will scan through the sub-tensordicts
+            looking for those that have the required keys, if any.
+
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import TensorDictModule, TensorDictSequential
+        >>> torch.manual_seed(0)
+        >>> module = TensorDictSequential(
+        ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["x+1"]),
+        ...     TensorDictModule(nn.Linear(3, 4), in_keys=["x+1"], out_keys=["w*(x+1)+b"]),
+        ... )
+        >>> # with tensordict input
+        >>> print(module(TensorDict({"x": torch.zeros(3)}, [])))
+        TensorDict(
+            fields={
+                w*(x+1)+b: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
+                x+1: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                x: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+        >>> # with tensor input: returns all the output keys in the order of the modules, ie "x+1" and "w*(x+1)+b"
+        >>> module(x=torch.zeros(3))
+        (tensor([1., 1., 1.]), tensor([-0.7214, -0.8748,  0.1571, -0.1138], grad_fn=<AddBackward0>))
+        >>> module(torch.zeros(3))
+        (tensor([1., 1., 1.]), tensor([-0.7214, -0.8748,  0.1571, -0.1138], grad_fn=<AddBackward0>))
+
+    TensorDictSequence supports functional, modular and vmap coding:
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import (
+        ...     ProbabilisticTensorDictModule,
+        ...     ProbabilisticTensorDictSequential,
+        ...     TensorDictModule,
+        ...     TensorDictSequential,
+        ... )
+        >>> from tensordict.nn.distributions import NormalParamExtractor
+        >>> from tensordict.nn.functional_modules import make_functional
+        >>> from torch.distributions import Normal
+        >>> td = TensorDict({"input": torch.randn(3, 4)}, [3,])
+        >>> net1 = torch.nn.Linear(4, 8)
+        >>> module1 = TensorDictModule(net1, in_keys=["input"], out_keys=["params"])
+        >>> normal_params = TensorDictModule(
+        ...      NormalParamExtractor(), in_keys=["params"], out_keys=["loc", "scale"]
+        ...  )
+        >>> td_module1 = ProbabilisticTensorDictSequential(
+        ...     module1,
+        ...     normal_params,
+        ...     ProbabilisticTensorDictModule(
+        ...         in_keys=["loc", "scale"],
+        ...         out_keys=["hidden"],
+        ...         distribution_class=Normal,
+        ...         return_log_prob=True,
+        ...     )
+        ... )
+        >>> module2 = torch.nn.Linear(4, 8)
+        >>> td_module2 = TensorDictModule(
+        ...    module=module2, in_keys=["hidden"], out_keys=["output"]
+        ... )
+        >>> td_module = TensorDictSequential(td_module1, td_module2)
+        >>> params = make_functional(td_module)
+        >>> _ = td_module(td, params=params)
+        >>> print(td)
+        TensorDict(
+            fields={
+                hidden: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                loc: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                output: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                params: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                sample_log_prob: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                scale: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+
+    In the vmap case:
+        >>> from torch import vmap
+        >>> params = params.expand(4)
+        >>> td_vmap = vmap(td_module, (None, 0))(td, params)
+        >>> print(td_vmap)
+        TensorDict(
+            fields={
+                hidden: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                loc: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                output: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                params: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                sample_log_prob: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                scale: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([4, 3]),
+            device=None,
+            is_shared=False)
+
+    """
+
+    module: nn.ModuleList
+
+    def __init__(
+        self,
+        *modules: TensorDictModule,
+        partial_tolerant: bool = False,
+    ) -> None:
+        in_keys, out_keys = self._compute_in_and_out_keys(modules)
+
+        super().__init__(
+            module=nn.ModuleList(list(modules)), in_keys=in_keys, out_keys=out_keys
+        )
+
+        self.partial_tolerant = partial_tolerant
+
+    def _compute_in_and_out_keys(
+        self, modules: list[TensorDictModule]
+    ) -> tuple[list[NestedKey], list[NestedKey]]:
+        in_keys = []
+        out_keys = []
+        for module in modules:
+            # we sometimes use in_keys to select keys of a tensordict that are
+            # necessary to run a TensorDictModule. If a key is an intermediary in
+            # the chain, there is no reason why it should belong to the input
+            # TensorDict.
+            for in_key in module.in_keys:
+                if in_key not in (out_keys + in_keys):
+                    in_keys.append(in_key)
+            out_keys += module.out_keys
+
+        out_keys = [
+            out_key
+            for i, out_key in enumerate(out_keys)
+            if out_key not in out_keys[i + 1 :]
+        ]
+        return in_keys, out_keys
+
+    @staticmethod
+    def _find_functional_module(module: TensorDictModule) -> nn.Module:
+        if not _has_functorch:
+            raise ImportError(FUNCTORCH_ERROR)
+        fmodule = module
+        while not isinstance(
+            fmodule, (functorch.FunctionalModule, functorch.FunctionalModuleWithBuffers)
+        ):
+            try:
+                fmodule = fmodule.module
+            except AttributeError:
+                raise AttributeError(
+                    f"couldn't find a functional module in module of type {type(module)}"
+                )
+        return fmodule
+
+    def select_subsequence(
+        self,
+        in_keys: Iterable[NestedKey] | None = None,
+        out_keys: Iterable[NestedKey] | None = None,
+    ) -> TensorDictSequential:
+        """Returns a new TensorDictSequential with only the modules that are necessary to compute the given output keys with the given input keys.
+
+        Args:
+            in_keys: input keys of the subsequence we want to select.
+                All the keys absent from ``in_keys`` will be considered as
+                non-relevant, and modules that *just* take these keys as inputs
+                will be discarded.
+                The resulting sequential module will follow the pattern "all
+                the modules which output will be affected by a different value
+                for any in <in_keys>".
+                If none is provided, the module's ``in_keys`` are assumed.
+            out_keys: output keys of the subsequence we want to select.
+                Only the modules that are necessary to get the ``out_keys``
+                will be found in the resulting sequence.
+                The resulting sequential module will follow the pattern "all
+                the modules that condition the value or <out_keys> entries."
+                If none is provided, the module's ``out_keys`` are assumed.
+
+        Returns:
+            A new TensorDictSequential with only the modules that are necessary acording to the given input and output keys.
+
+        Examples:
+            >>> from tensordict.nn import TensorDictSequential as Seq, TensorDictModule as Mod
+            >>> idn = lambda x: x
+            >>> module = Seq(
+            ...     Mod(idn, in_keys=["a"], out_keys=["b"]),
+            ...     Mod(idn, in_keys=["b"], out_keys=["c"]),
+            ...     Mod(idn, in_keys=["c"], out_keys=["d"]),
+            ...     Mod(idn, in_keys=["a"], out_keys=["e"]),
+            ... )
+            >>> # select all modules whose output depend on "a"
+            >>> module.select_subsequence(in_keys=["a"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['a'],
+                      out_keys=['b'])
+                  (1): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['b'],
+                      out_keys=['c'])
+                  (2): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['c'],
+                      out_keys=['d'])
+                  (3): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['a'],
+                      out_keys=['e'])
+                ),
+                device=cpu,
+                in_keys=['a'],
+                out_keys=['b', 'c', 'd', 'e'])
+            >>> # select all modules whose output depend on "c"
+            >>> module.select_subsequence(in_keys=["c"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['c'],
+                      out_keys=['d'])
+                ),
+                device=cpu,
+                in_keys=['c'],
+                out_keys=['d'])
+            >>> # select all modules that affect the value of "c"
+            >>> module.select_subsequence(out_keys=["c"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['a'],
+                      out_keys=['b'])
+                  (1): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['b'],
+                      out_keys=['c'])
+                ),
+                device=cpu,
+                in_keys=['a'],
+                out_keys=['b', 'c'])
+            >>> # select all modules that affect the value of "e"
+            >>> module.select_subsequence(out_keys=["e"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['a'],
+                      out_keys=['e'])
+                ),
+                device=cpu,
+                in_keys=['a'],
+                out_keys=['e'])
+
+        This method propagates to nested sequential:
+
+            >>> module = Seq(
+            ...     Seq(
+            ...         Mod(idn, in_keys=["a"], out_keys=["b"]),
+            ...         Mod(idn, in_keys=["b"], out_keys=["c"]),
+            ...     ),
+            ...     Seq(
+            ...         Mod(idn, in_keys=["b"], out_keys=["d"]),
+            ...         Mod(idn, in_keys=["d"], out_keys=["e"]),
+            ...     ),
+            ... )
+            >>> # select submodules whose output will be affected by a change in "b" or "d" AND which output is "e"
+            >>> module.select_subsequence(in_keys=["b", "d"], out_keys=["e"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictSequential(
+                      module=ModuleList(
+                        (0): TensorDictModule(
+                            module=<function <lambda> at 0x129efae50>,
+                            device=cpu,
+                            in_keys=['b'],
+                            out_keys=['d'])
+                        (1): TensorDictModule(
+                            module=<function <lambda> at 0x129efae50>,
+                            device=cpu,
+                            in_keys=['d'],
+                            out_keys=['e'])
+                      ),
+                      device=cpu,
+                      in_keys=['b'],
+                      out_keys=['d', 'e'])
+                ),
+                device=cpu,
+                in_keys=['b'],
+                out_keys=['d', 'e'])
+
+        """
+        if in_keys is None:
+            in_keys = deepcopy(self.in_keys)
+        in_keys = unravel_key_list(in_keys)
+        if out_keys is None:
+            out_keys = deepcopy(self.out_keys)
+        out_keys = unravel_key_list(out_keys)
+
+        module_list = list(self.module)
+        id_to_keep = set(range(len(module_list)))
+        for i, module in enumerate(module_list):
+            if (
+                type(module) is TensorDictSequential
+            ):  # no isinstance because we don't want to mess up subclasses
+                try:
+                    module = module_list[i] = module.select_subsequence(in_keys=in_keys)
+                except ValueError:
+                    # then the module can be removed
+                    id_to_keep.remove(i)
+                    continue
+
+            if all(key in in_keys for key in module.in_keys):
+                in_keys.extend(module.out_keys)
+            else:
+                id_to_keep.remove(i)
+        for i, module in reversed(list(enumerate(module_list))):
+            if i in id_to_keep:
+                if any(key in out_keys for key in module.out_keys):
+                    if (
+                        type(module) is TensorDictSequential
+                    ):  # no isinstance because we don't want to mess up subclasses
+                        module = module_list[i] = module.select_subsequence(
+                            out_keys=out_keys
+                        )
+                    out_keys.extend(module.in_keys)
+                else:
+                    id_to_keep.remove(i)
+        id_to_keep = sorted(id_to_keep)
+
+        modules = [module_list[i] for i in id_to_keep]
+
+        if modules == []:
+            raise ValueError(
+                "No modules left after selection. Make sure that in_keys and out_keys are coherent."
+            )
+
+        return self.__class__(*modules)
+
+    def _run_module(
+        self,
+        module: TensorDictModule,
+        tensordict: TensorDictBase,
+        **kwargs: Any,
+    ) -> Any:
+        if not self.partial_tolerant or all(
+            key in tensordict.keys(include_nested=True) for key in module.in_keys
+        ):
+            tensordict = module(tensordict, **kwargs)
+        elif self.partial_tolerant and isinstance(tensordict, LazyStackedTensorDict):
+            for sub_td in tensordict.tensordicts:
+                if all(
+                    key in sub_td.keys(include_nested=True) for key in module.in_keys
+                ):
+                    module(sub_td, **kwargs)
+        return tensordict
+
+    @dispatch(auto_batch_size=False)
+    @set_skip_existing(None)
+    def forward(
+        self,
+        tensordict: TensorDictBase,
+        tensordict_out: TensorDictBase | None = None,
+        **kwargs: Any,
+    ) -> TensorDictBase:
+        if not len(kwargs):
+            for module in self.module:
+                tensordict = self._run_module(module, tensordict, **kwargs)
+        else:
+            raise RuntimeError(
+                f"TensorDictSequential does not support keyword arguments other than 'tensordict_out' or in_keys: {self.in_keys}. Got {kwargs.keys()} instead."
+            )
+        if tensordict_out is not None:
+            tensordict_out.update(tensordict, inplace=True)
+            return tensordict_out
+        return tensordict
+
+    def __len__(self) -> int:
+        return len(self.module)
+
+    def __getitem__(self, index: int | slice) -> TensorDictModule:
+        if isinstance(index, int):
+            return self.module.__getitem__(index)
+        else:
+            return self.__class__(*self.module.__getitem__(index))
+
+    def __setitem__(self, index: int, tensordict_module: TensorDictModule) -> None:
+        return self.module.__setitem__(idx=index, module=tensordict_module)
+
+    def __delitem__(self, index: int | slice) -> None:
+        self.module.__delitem__(idx=index)
```

## tensordict/nn/utils.py

 * *Ordering differences only*

```diff
@@ -1,331 +1,331 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import functools
-import inspect
-from typing import Any, Callable, OrderedDict
-
-import torch
-from torch import nn
-
-__all__ = ["mappings", "inv_softplus", "biased_softplus"]
-_SKIP_EXISTING = False
-
-from tensordict._contextlib import _DecoratorContextManager
-from torch.nn.parameter import _disabled_torch_function_impl, _ParameterMeta
-
-
-def inv_softplus(bias: float | torch.Tensor) -> float | torch.Tensor:
-    """Inverse softplus function.
-
-    Args:
-        bias (float or tensor): the value to be softplus-inverted.
-    """
-    is_tensor = True
-    if not isinstance(bias, torch.Tensor):
-        is_tensor = False
-        bias = torch.tensor(bias)
-    out = bias.expm1().clamp_min(1e-6).log()
-    if not is_tensor and out.numel() == 1:
-        return out.item()
-    return out
-
-
-class biased_softplus(nn.Module):
-    """A biased softplus module.
-
-    The bias indicates the value that is to be returned when a zero-tensor is
-    passed through the transform.
-
-    Args:
-        bias (scalar): 'bias' of the softplus transform. If bias=1.0, then a _bias shift will be computed such that
-            softplus(0.0 + _bias) = bias.
-        min_val (scalar): minimum value of the transform.
-            default: 0.1
-    """
-
-    def __init__(self, bias: float, min_val: float = 0.01) -> None:
-        super().__init__()
-        self.bias = inv_softplus(bias - min_val)
-        self.min_val = min_val
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        return torch.nn.functional.softplus(x + self.bias) + self.min_val
-
-
-def mappings(key: str) -> Callable:
-    """Given an input string, returns a surjective function f(x): R -> R^+.
-
-    Args:
-        key (str): one of "softplus", "exp", "relu", "expln",
-            or "biased_softplus". If the key beggins with "biased_softplus",
-            then it needs to take the following form:
-            ```"biased_softplus_{bias}"``` where ```bias``` can be converted to a floating point number that will be used to bias the softplus function.
-            Alternatively, the ```"biased_softplus_{bias}_{min_val}"``` syntax can be used. In that case, the additional ```min_val``` term is a floating point
-            number that will be used to encode the minimum value of the softplus transform.
-            In practice, the equation used is softplus(x + bias) + min_val, where bias and min_val are values computed such that the conditions above are met.
-
-    Returns:
-         a Callable
-
-    """
-    _mappings: dict[str, Callable] = {
-        "softplus": torch.nn.functional.softplus,
-        "exp": torch.exp,
-        "relu": torch.relu,
-        "biased_softplus": biased_softplus(1.0),
-    }
-    if key in _mappings:
-        return _mappings[key]
-    elif key.startswith("biased_softplus"):
-        stripped_key = key.split("_")
-        if len(stripped_key) == 3:
-            return biased_softplus(float(stripped_key[-1]))
-        elif len(stripped_key) == 4:
-            return biased_softplus(
-                float(stripped_key[-2]), min_val=float(stripped_key[-1])
-            )
-        else:
-            raise ValueError(f"Invalid number of args in  {key}")
-
-    else:
-        raise NotImplementedError(f"Unknown mapping {key}")
-
-
-class set_skip_existing(_DecoratorContextManager):
-    """A context manager for skipping existing nodes in a TensorDict graph.
-
-    When used as a context manager, it will set the `skip_existing()` value
-    to the ``mode`` indicated, leaving the user able to code up methods that
-    will check the global value and execute the code accordingly.
-
-    When used as a method decorator, it will check the tensordict input keys
-    and if the ``skip_existing()`` call returns ``True``, it will skip the method
-    if all the output keys are already present.
-    This not not expected to be used as a decorator for methods that do not
-    respect the following signature: ``def fun(self, tensordict, *args, **kwargs)``.
-
-    Args:
-        mode (bool, optional):
-            If ``True``, it indicates that existing entries in the graph
-            won't be overwritten, unless they are only partially present. :func:`~.skip_existing`
-            will return ``True``.
-            If ``False``, no check will be performed.
-            If ``None``, the value of :func:`~.skip_existing` will not be
-            changed. This is intended to be used exclusively for decorating
-            methods and allow their behaviour to depend on the same class
-            when used as a context manager (see example below).
-            Defaults to ``True``.
-        in_key_attr (str, optional): the name of the input key list attribute
-            in the module's method being decorated. Defaults to ``in_keys``.
-        out_key_attr (str, optional): the name of the output key list attribute
-            in the module's method being decorated. Defaults to ``out_keys``.
-
-    Examples:
-        >>> with set_skip_existing():
-        ...     if skip_existing():
-        ...         print("True")
-        ...     else:
-        ...         print("False")
-        ...
-        True
-        >>> print("calling from outside:", skip_existing())
-        calling from outside: False
-
-    This class can also be used as a decorator:
-    Examples:
-        >>> from tensordict import TensorDict
-        >>> from tensordict.nn import set_skip_existing, skip_existing, TensorDictModuleBase
-        >>> class MyModule(TensorDictModuleBase):
-        ...     in_keys = []
-        ...     out_keys = ["out"]
-        ...     @set_skip_existing()
-        ...     def forward(self, tensordict):
-        ...         print("hello")
-        ...         tensordict.set("out", torch.zeros(()))
-        ...         return tensordict
-        >>> module = MyModule()
-        >>> module(TensorDict({"out": torch.zeros(())}, []))  # does not print anything
-        TensorDict(
-            fields={
-                out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([]),
-            device=None,
-            is_shared=False)
-        >>> module(TensorDict({}, []))  # prints hello
-        hello
-        TensorDict(
-            fields={
-                out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([]),
-            device=None,
-            is_shared=False)
-
-    Decorating a method with the mode set to ``None`` is useful whenever one
-    wants ot let the context manager take care of skipping things from the outside:
-        >>> from tensordict import TensorDict
-        >>> from tensordict.nn import set_skip_existing, skip_existing, TensorDictModuleBase
-        >>> class MyModule(TensorDictModuleBase):
-        ...     in_keys = []
-        ...     out_keys = ["out"]
-        ...     @set_skip_existing(None)
-        ...     def forward(self, tensordict):
-        ...         print("hello")
-        ...         tensordict.set("out", torch.zeros(()))
-        ...         return tensordict
-        >>> module = MyModule()
-        >>> _ = module(TensorDict({"out": torch.zeros(())}, []))  # prints "hello"
-        hello
-        >>> with set_skip_existing(True):
-        ...     _ = module(TensorDict({"out": torch.zeros(())}, []))  # no print
-
-
-    .. note::
-        To allow for modules to have the same input and output keys and not
-        mistakenly ignoring subgraphs, ``@set_skip_existing(True)`` will be
-        deactivated whenever the output keys are also the input keys:
-
-            >>> class MyModule(TensorDictModuleBase):
-            ...     in_keys = ["out"]
-            ...     out_keys = ["out"]
-            ...     @set_skip_existing()
-            ...     def forward(self, tensordict):
-            ...         print("calling the method!")
-            ...         return tensordict
-            ...
-            >>> module = MyModule()
-            >>> module(TensorDict({"out": torch.zeros(())}, []))  # does not print anything
-            calling the method!
-            TensorDict(
-                fields={
-                    out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-
-    """
-
-    def __init__(
-        self, mode: bool | None = True, in_key_attr="in_keys", out_key_attr="out_keys"
-    ):
-        self.mode = mode
-        self.in_key_attr = in_key_attr
-        self.out_key_attr = out_key_attr
-        self._called = False
-
-    def clone(self) -> set_skip_existing:
-        # override this method if your children class takes __init__ parameters
-        out = self.__class__(self.mode)
-        out._called = self._called
-        return out
-
-    def __call__(self, func: Callable):
-
-        self._called = True
-
-        # sanity check
-        for i, key in enumerate(inspect.signature(func).parameters):
-            if i == 0:
-                # skip self
-                continue
-            if key != "tensordict":
-                raise RuntimeError(
-                    "the first argument of the wrapped function must be "
-                    "named 'tensordict'."
-                )
-            break
-
-        @functools.wraps(func)
-        def wrapper(_self, tensordict, *args: Any, **kwargs: Any) -> Any:
-            in_keys = getattr(_self, self.in_key_attr)
-            out_keys = getattr(_self, self.out_key_attr)
-            # we use skip_existing to allow users to override the mode internally
-            if (
-                skip_existing()
-                and all(key in tensordict.keys(True) for key in out_keys)
-                and not any(key in out_keys for key in in_keys)
-            ):
-                return tensordict
-            return func(_self, tensordict, *args, **kwargs)
-
-        return super().__call__(wrapper)
-
-    def __enter__(self) -> None:
-        global _SKIP_EXISTING
-        self.prev = _SKIP_EXISTING
-        if self.mode is not None:
-            _SKIP_EXISTING = self.mode
-        elif not self._called:
-            raise RuntimeError(
-                f"It seems you are using {self.__class__.__name__} as a decorator with ``None`` input. "
-                f"This behaviour is not allowed."
-            )
-
-    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
-        global _SKIP_EXISTING
-        _SKIP_EXISTING = self.prev
-
-
-def skip_existing():
-    """Returns whether or not existing entries in a tensordict should be re-computed by a module."""
-    return _SKIP_EXISTING
-
-
-def _rebuild_buffer(data, requires_grad, backward_hooks):
-    buffer = Buffer(data, requires_grad)
-    # NB: This line exists only for backwards compatibility; the
-    # general expectation is that backward_hooks is an empty
-    # OrderedDict.  See Note [Don't serialize hooks]
-    buffer._backward_hooks = backward_hooks
-
-    return buffer
-
-
-class Buffer(torch.Tensor, metaclass=_ParameterMeta):
-    r"""A kind of Tensor that is to be considered a module buffer.
-
-    Args:
-        data (Tensor): buffer tensor.
-        requires_grad (bool, optional): if the buffer requires gradient. See
-            :ref:`locally-disable-grad-doc` for more details. Default: `False`
-    """
-
-    def __new__(cls, data=None, requires_grad=False):
-        if data is None:
-            data = torch.empty(0)
-        if type(data) is torch.Tensor or type(data) is Buffer:
-            # For ease of BC maintenance, keep this path for standard Tensor.
-            # Eventually (tm), we should change the behavior for standard Tensor to match.
-            return torch.Tensor._make_subclass(cls, data, requires_grad)
-
-        # Path for custom tensors: set a flag on the instance to indicate parameter-ness.
-        t = data.detach().requires_grad_(requires_grad)
-        t._is_buffer = True
-        return t
-
-    def __deepcopy__(self, memo):
-        if id(self) in memo:
-            return memo[id(self)]
-        else:
-            result = type(self)(
-                self.data.clone(memory_format=torch.preserve_format), self.requires_grad
-            )
-            memo[id(self)] = result
-            return result
-
-    def __repr__(self):
-        return "Buffer containing:\n" + super(Buffer, self).__repr__()
-
-    def __reduce_ex__(self, proto):
-        # See Note [Don't serialize hooks]
-        return (
-            torch._utils._rebuild_parameter,
-            (self.data, self.requires_grad, OrderedDict()),
-        )
-
-    __torch_function__ = _disabled_torch_function_impl
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import functools
+import inspect
+from typing import Any, Callable, OrderedDict
+
+import torch
+from torch import nn
+
+__all__ = ["mappings", "inv_softplus", "biased_softplus"]
+_SKIP_EXISTING = False
+
+from tensordict._contextlib import _DecoratorContextManager
+from torch.nn.parameter import _disabled_torch_function_impl, _ParameterMeta
+
+
+def inv_softplus(bias: float | torch.Tensor) -> float | torch.Tensor:
+    """Inverse softplus function.
+
+    Args:
+        bias (float or tensor): the value to be softplus-inverted.
+    """
+    is_tensor = True
+    if not isinstance(bias, torch.Tensor):
+        is_tensor = False
+        bias = torch.tensor(bias)
+    out = bias.expm1().clamp_min(1e-6).log()
+    if not is_tensor and out.numel() == 1:
+        return out.item()
+    return out
+
+
+class biased_softplus(nn.Module):
+    """A biased softplus module.
+
+    The bias indicates the value that is to be returned when a zero-tensor is
+    passed through the transform.
+
+    Args:
+        bias (scalar): 'bias' of the softplus transform. If bias=1.0, then a _bias shift will be computed such that
+            softplus(0.0 + _bias) = bias.
+        min_val (scalar): minimum value of the transform.
+            default: 0.1
+    """
+
+    def __init__(self, bias: float, min_val: float = 0.01) -> None:
+        super().__init__()
+        self.bias = inv_softplus(bias - min_val)
+        self.min_val = min_val
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        return torch.nn.functional.softplus(x + self.bias) + self.min_val
+
+
+def mappings(key: str) -> Callable:
+    """Given an input string, returns a surjective function f(x): R -> R^+.
+
+    Args:
+        key (str): one of "softplus", "exp", "relu", "expln",
+            or "biased_softplus". If the key beggins with "biased_softplus",
+            then it needs to take the following form:
+            ```"biased_softplus_{bias}"``` where ```bias``` can be converted to a floating point number that will be used to bias the softplus function.
+            Alternatively, the ```"biased_softplus_{bias}_{min_val}"``` syntax can be used. In that case, the additional ```min_val``` term is a floating point
+            number that will be used to encode the minimum value of the softplus transform.
+            In practice, the equation used is softplus(x + bias) + min_val, where bias and min_val are values computed such that the conditions above are met.
+
+    Returns:
+         a Callable
+
+    """
+    _mappings: dict[str, Callable] = {
+        "softplus": torch.nn.functional.softplus,
+        "exp": torch.exp,
+        "relu": torch.relu,
+        "biased_softplus": biased_softplus(1.0),
+    }
+    if key in _mappings:
+        return _mappings[key]
+    elif key.startswith("biased_softplus"):
+        stripped_key = key.split("_")
+        if len(stripped_key) == 3:
+            return biased_softplus(float(stripped_key[-1]))
+        elif len(stripped_key) == 4:
+            return biased_softplus(
+                float(stripped_key[-2]), min_val=float(stripped_key[-1])
+            )
+        else:
+            raise ValueError(f"Invalid number of args in  {key}")
+
+    else:
+        raise NotImplementedError(f"Unknown mapping {key}")
+
+
+class set_skip_existing(_DecoratorContextManager):
+    """A context manager for skipping existing nodes in a TensorDict graph.
+
+    When used as a context manager, it will set the `skip_existing()` value
+    to the ``mode`` indicated, leaving the user able to code up methods that
+    will check the global value and execute the code accordingly.
+
+    When used as a method decorator, it will check the tensordict input keys
+    and if the ``skip_existing()`` call returns ``True``, it will skip the method
+    if all the output keys are already present.
+    This not not expected to be used as a decorator for methods that do not
+    respect the following signature: ``def fun(self, tensordict, *args, **kwargs)``.
+
+    Args:
+        mode (bool, optional):
+            If ``True``, it indicates that existing entries in the graph
+            won't be overwritten, unless they are only partially present. :func:`~.skip_existing`
+            will return ``True``.
+            If ``False``, no check will be performed.
+            If ``None``, the value of :func:`~.skip_existing` will not be
+            changed. This is intended to be used exclusively for decorating
+            methods and allow their behaviour to depend on the same class
+            when used as a context manager (see example below).
+            Defaults to ``True``.
+        in_key_attr (str, optional): the name of the input key list attribute
+            in the module's method being decorated. Defaults to ``in_keys``.
+        out_key_attr (str, optional): the name of the output key list attribute
+            in the module's method being decorated. Defaults to ``out_keys``.
+
+    Examples:
+        >>> with set_skip_existing():
+        ...     if skip_existing():
+        ...         print("True")
+        ...     else:
+        ...         print("False")
+        ...
+        True
+        >>> print("calling from outside:", skip_existing())
+        calling from outside: False
+
+    This class can also be used as a decorator:
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import set_skip_existing, skip_existing, TensorDictModuleBase
+        >>> class MyModule(TensorDictModuleBase):
+        ...     in_keys = []
+        ...     out_keys = ["out"]
+        ...     @set_skip_existing()
+        ...     def forward(self, tensordict):
+        ...         print("hello")
+        ...         tensordict.set("out", torch.zeros(()))
+        ...         return tensordict
+        >>> module = MyModule()
+        >>> module(TensorDict({"out": torch.zeros(())}, []))  # does not print anything
+        TensorDict(
+            fields={
+                out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+        >>> module(TensorDict({}, []))  # prints hello
+        hello
+        TensorDict(
+            fields={
+                out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+
+    Decorating a method with the mode set to ``None`` is useful whenever one
+    wants ot let the context manager take care of skipping things from the outside:
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import set_skip_existing, skip_existing, TensorDictModuleBase
+        >>> class MyModule(TensorDictModuleBase):
+        ...     in_keys = []
+        ...     out_keys = ["out"]
+        ...     @set_skip_existing(None)
+        ...     def forward(self, tensordict):
+        ...         print("hello")
+        ...         tensordict.set("out", torch.zeros(()))
+        ...         return tensordict
+        >>> module = MyModule()
+        >>> _ = module(TensorDict({"out": torch.zeros(())}, []))  # prints "hello"
+        hello
+        >>> with set_skip_existing(True):
+        ...     _ = module(TensorDict({"out": torch.zeros(())}, []))  # no print
+
+
+    .. note::
+        To allow for modules to have the same input and output keys and not
+        mistakenly ignoring subgraphs, ``@set_skip_existing(True)`` will be
+        deactivated whenever the output keys are also the input keys:
+
+            >>> class MyModule(TensorDictModuleBase):
+            ...     in_keys = ["out"]
+            ...     out_keys = ["out"]
+            ...     @set_skip_existing()
+            ...     def forward(self, tensordict):
+            ...         print("calling the method!")
+            ...         return tensordict
+            ...
+            >>> module = MyModule()
+            >>> module(TensorDict({"out": torch.zeros(())}, []))  # does not print anything
+            calling the method!
+            TensorDict(
+                fields={
+                    out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+
+    """
+
+    def __init__(
+        self, mode: bool | None = True, in_key_attr="in_keys", out_key_attr="out_keys"
+    ):
+        self.mode = mode
+        self.in_key_attr = in_key_attr
+        self.out_key_attr = out_key_attr
+        self._called = False
+
+    def clone(self) -> set_skip_existing:
+        # override this method if your children class takes __init__ parameters
+        out = self.__class__(self.mode)
+        out._called = self._called
+        return out
+
+    def __call__(self, func: Callable):
+
+        self._called = True
+
+        # sanity check
+        for i, key in enumerate(inspect.signature(func).parameters):
+            if i == 0:
+                # skip self
+                continue
+            if key != "tensordict":
+                raise RuntimeError(
+                    "the first argument of the wrapped function must be "
+                    "named 'tensordict'."
+                )
+            break
+
+        @functools.wraps(func)
+        def wrapper(_self, tensordict, *args: Any, **kwargs: Any) -> Any:
+            in_keys = getattr(_self, self.in_key_attr)
+            out_keys = getattr(_self, self.out_key_attr)
+            # we use skip_existing to allow users to override the mode internally
+            if (
+                skip_existing()
+                and all(key in tensordict.keys(True) for key in out_keys)
+                and not any(key in out_keys for key in in_keys)
+            ):
+                return tensordict
+            return func(_self, tensordict, *args, **kwargs)
+
+        return super().__call__(wrapper)
+
+    def __enter__(self) -> None:
+        global _SKIP_EXISTING
+        self.prev = _SKIP_EXISTING
+        if self.mode is not None:
+            _SKIP_EXISTING = self.mode
+        elif not self._called:
+            raise RuntimeError(
+                f"It seems you are using {self.__class__.__name__} as a decorator with ``None`` input. "
+                f"This behaviour is not allowed."
+            )
+
+    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
+        global _SKIP_EXISTING
+        _SKIP_EXISTING = self.prev
+
+
+def skip_existing():
+    """Returns whether or not existing entries in a tensordict should be re-computed by a module."""
+    return _SKIP_EXISTING
+
+
+def _rebuild_buffer(data, requires_grad, backward_hooks):
+    buffer = Buffer(data, requires_grad)
+    # NB: This line exists only for backwards compatibility; the
+    # general expectation is that backward_hooks is an empty
+    # OrderedDict.  See Note [Don't serialize hooks]
+    buffer._backward_hooks = backward_hooks
+
+    return buffer
+
+
+class Buffer(torch.Tensor, metaclass=_ParameterMeta):
+    r"""A kind of Tensor that is to be considered a module buffer.
+
+    Args:
+        data (Tensor): buffer tensor.
+        requires_grad (bool, optional): if the buffer requires gradient. See
+            :ref:`locally-disable-grad-doc` for more details. Default: `False`
+    """
+
+    def __new__(cls, data=None, requires_grad=False):
+        if data is None:
+            data = torch.empty(0)
+        if type(data) is torch.Tensor or type(data) is Buffer:
+            # For ease of BC maintenance, keep this path for standard Tensor.
+            # Eventually (tm), we should change the behavior for standard Tensor to match.
+            return torch.Tensor._make_subclass(cls, data, requires_grad)
+
+        # Path for custom tensors: set a flag on the instance to indicate parameter-ness.
+        t = data.detach().requires_grad_(requires_grad)
+        t._is_buffer = True
+        return t
+
+    def __deepcopy__(self, memo):
+        if id(self) in memo:
+            return memo[id(self)]
+        else:
+            result = type(self)(
+                self.data.clone(memory_format=torch.preserve_format), self.requires_grad
+            )
+            memo[id(self)] = result
+            return result
+
+    def __repr__(self):
+        return "Buffer containing:\n" + super(Buffer, self).__repr__()
+
+    def __reduce_ex__(self, proto):
+        # See Note [Don't serialize hooks]
+        return (
+            torch._utils._rebuild_parameter,
+            (self.data, self.requires_grad, OrderedDict()),
+        )
+
+    __torch_function__ = _disabled_torch_function_impl
```

## tensordict/nn/distributions/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from tensordict.nn.distributions import continuous, discrete
-from tensordict.nn.distributions.continuous import *
-from tensordict.nn.distributions.discrete import *
-
-distributions_maps = {
-    distribution_class.lower(): eval(distribution_class)
-    for distribution_class in (*continuous.__all__, *discrete.__all__)
-}
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from tensordict.nn.distributions import continuous, discrete
+from tensordict.nn.distributions.continuous import *
+from tensordict.nn.distributions.discrete import *
+
+distributions_maps = {
+    distribution_class.lower(): eval(distribution_class)
+    for distribution_class in (*continuous.__all__, *discrete.__all__)
+}
```

## tensordict/nn/distributions/continuous.py

 * *Ordering differences only*

```diff
@@ -1,199 +1,199 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-from numbers import Number
-from typing import Sequence
-
-import numpy as np
-
-import torch
-from tensordict.nn.utils import mappings
-from torch import distributions as D, nn
-
-__all__ = ["NormalParamExtractor", "NormalParamWrapper", "Delta"]
-
-# speeds up distribution construction
-D.Distribution.set_default_validate_args(False)
-
-
-class NormalParamWrapper(nn.Module):
-    """A wrapper for normal distribution parameters.
-
-    Args:
-        operator (nn.Module): operator whose output will be transformed_in in location and scale parameters
-        scale_mapping (str, optional): positive mapping function to be used with the std.
-            default = "biased_softplus_1.0" (i.e. softplus map with bias such that fn(0.0) = 1.0)
-            choices: "softplus", "exp", "relu", "biased_softplus_1";
-        scale_lb (Number, optional): The minimum value that the variance can take. Default is 1e-4.
-
-    Examples:
-        >>> from torch import nn
-        >>> import torch
-        >>> module = nn.Linear(3, 4)
-        >>> module_normal = NormalParamWrapper(module)
-        >>> tensor = torch.randn(3)
-        >>> loc, scale = module_normal(tensor)
-        >>> print(loc.shape, scale.shape)
-        torch.Size([2]) torch.Size([2])
-        >>> assert (scale > 0).all()
-        >>> # with modules that return more than one tensor
-        >>> module = nn.LSTM(3, 4)
-        >>> module_normal = NormalParamWrapper(module)
-        >>> tensor = torch.randn(4, 2, 3)
-        >>> loc, scale, others = module_normal(tensor)
-        >>> print(loc.shape, scale.shape)
-        torch.Size([4, 2, 2]) torch.Size([4, 2, 2])
-        >>> assert (scale > 0).all()
-
-    """
-
-    def __init__(
-        self,
-        operator: nn.Module,
-        scale_mapping: str = "biased_softplus_1.0",
-        scale_lb: Number = 1e-4,
-    ) -> None:
-        super().__init__()
-        self.operator = operator
-        self.scale_mapping = scale_mapping
-        self.scale_lb = scale_lb
-
-    def forward(self, *tensors: torch.Tensor) -> tuple[torch.Tensor, ...]:
-        net_output = self.operator(*tensors)
-        others = ()
-        if not isinstance(net_output, torch.Tensor):
-            net_output, *others = net_output
-        loc, scale = net_output.chunk(2, -1)
-        scale = mappings(self.scale_mapping)(scale).clamp_min(self.scale_lb)
-        return (loc, scale, *others)
-
-
-class NormalParamExtractor(nn.Module):
-    """A non-parametric nn.Module that splits its input into loc and scale parameters.
-
-    The scale parameters are mapped onto positive values using the specified ``scale_mapping``.
-
-    Args:
-        scale_mapping (str, optional): positive mapping function to be used with the std.
-            default = "biased_softplus_1.0" (i.e. softplus map with bias such that fn(0.0) = 1.0)
-            choices: "softplus", "exp", "relu", "biased_softplus_1";
-        scale_lb (Number, optional): The minimum value that the variance can take. Default is 1e-4.
-
-    Examples:
-        >>> import torch
-        >>> from tensordict.nn.distributions import NormalParamExtractor
-        >>> from torch import nn
-        >>> module = nn.Linear(3, 4)
-        >>> normal_params = NormalParamExtractor()
-        >>> tensor = torch.randn(3)
-        >>> loc, scale = normal_params(module(tensor))
-        >>> print(loc.shape, scale.shape)
-        torch.Size([2]) torch.Size([2])
-        >>> assert (scale > 0).all()
-        >>> # with modules that return more than one tensor
-        >>> module = nn.LSTM(3, 4)
-        >>> tensor = torch.randn(4, 2, 3)
-        >>> loc, scale, others = normal_params(*module(tensor))
-        >>> print(loc.shape, scale.shape)
-        torch.Size([4, 2, 2]) torch.Size([4, 2, 2])
-        >>> assert (scale > 0).all()
-
-    """
-
-    def __init__(
-        self,
-        scale_mapping: str = "biased_softplus_1.0",
-        scale_lb: Number = 1e-4,
-    ) -> None:
-        super().__init__()
-        self.scale_mapping = scale_mapping
-        self.scale_lb = scale_lb
-
-    def forward(self, *tensors: torch.Tensor) -> tuple[torch.Tensor, ...]:
-        tensor, *others = tensors
-        loc, scale = tensor.chunk(2, -1)
-        scale = mappings(self.scale_mapping)(scale).clamp_min(self.scale_lb)
-        return (loc, scale, *others)
-
-
-class Delta(D.Distribution):
-    """Delta distribution.
-
-    Args:
-        param (torch.Tensor): parameter of the delta distribution;
-        atol (number, optional): absolute tolerance to consider that a tensor matches the distribution parameter;
-            Default is 1e-6
-        rtol (number, optional): relative tolerance to consider that a tensor matches the distribution parameter;
-            Default is 1e-6
-        batch_shape (torch.Size, optional): batch shape;
-        event_shape (torch.Size, optional): shape of the outcome.
-
-    """
-
-    arg_constraints: dict = {}
-
-    def __init__(
-        self,
-        param: torch.Tensor,
-        atol: float = 1e-6,
-        rtol: float = 1e-6,
-        batch_shape: torch.Size | Sequence[int] | None = None,
-        event_shape: torch.Size | Sequence[int] | None = None,
-    ) -> None:
-        if batch_shape is None:
-            batch_shape = torch.Size([])
-        if event_shape is None:
-            event_shape = torch.Size([])
-        self.update(param)
-        self.atol = atol
-        self.rtol = rtol
-        if not len(batch_shape) and not len(event_shape):
-            batch_shape = param.shape[:-1]
-            event_shape = param.shape[-1:]
-        super().__init__(batch_shape=batch_shape, event_shape=event_shape)
-
-    def update(self, param: torch.Tensor) -> None:
-        self.param = param
-
-    def _is_equal(self, value: torch.Tensor) -> torch.Tensor:
-        param = self.param.expand_as(value)
-        is_equal = abs(value - param) < self.atol + self.rtol * abs(param)
-        for i in range(-1, -len(self.event_shape) - 1, -1):
-            is_equal = is_equal.all(i)
-        return is_equal
-
-    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
-        is_equal = self._is_equal(value)
-        out = torch.zeros_like(is_equal, dtype=value.dtype)
-        out.masked_fill_(is_equal, np.inf)
-        out.masked_fill_(~is_equal, -np.inf)
-        return out
-
-    @torch.no_grad()
-    def sample(
-        self,
-        sample_shape: torch.Size | Sequence[int] | None = None,
-    ) -> torch.Tensor:
-        if sample_shape is None:
-            sample_shape = torch.Size([])
-        return self.param.expand(*sample_shape, *self.param.shape)
-
-    def rsample(
-        self,
-        sample_shape: torch.Size | Sequence[int] | None = None,
-    ) -> torch.Tensor:
-        if sample_shape is None:
-            sample_shape = torch.Size([])
-        return self.param.expand(*sample_shape, *self.param.shape)
-
-    @property
-    def mode(self) -> torch.Tensor:
-        return self.param
-
-    @property
-    def mean(self) -> torch.Tensor:
-        return self.param
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+from numbers import Number
+from typing import Sequence
+
+import numpy as np
+
+import torch
+from tensordict.nn.utils import mappings
+from torch import distributions as D, nn
+
+__all__ = ["NormalParamExtractor", "NormalParamWrapper", "Delta"]
+
+# speeds up distribution construction
+D.Distribution.set_default_validate_args(False)
+
+
+class NormalParamWrapper(nn.Module):
+    """A wrapper for normal distribution parameters.
+
+    Args:
+        operator (nn.Module): operator whose output will be transformed_in in location and scale parameters
+        scale_mapping (str, optional): positive mapping function to be used with the std.
+            default = "biased_softplus_1.0" (i.e. softplus map with bias such that fn(0.0) = 1.0)
+            choices: "softplus", "exp", "relu", "biased_softplus_1";
+        scale_lb (Number, optional): The minimum value that the variance can take. Default is 1e-4.
+
+    Examples:
+        >>> from torch import nn
+        >>> import torch
+        >>> module = nn.Linear(3, 4)
+        >>> module_normal = NormalParamWrapper(module)
+        >>> tensor = torch.randn(3)
+        >>> loc, scale = module_normal(tensor)
+        >>> print(loc.shape, scale.shape)
+        torch.Size([2]) torch.Size([2])
+        >>> assert (scale > 0).all()
+        >>> # with modules that return more than one tensor
+        >>> module = nn.LSTM(3, 4)
+        >>> module_normal = NormalParamWrapper(module)
+        >>> tensor = torch.randn(4, 2, 3)
+        >>> loc, scale, others = module_normal(tensor)
+        >>> print(loc.shape, scale.shape)
+        torch.Size([4, 2, 2]) torch.Size([4, 2, 2])
+        >>> assert (scale > 0).all()
+
+    """
+
+    def __init__(
+        self,
+        operator: nn.Module,
+        scale_mapping: str = "biased_softplus_1.0",
+        scale_lb: Number = 1e-4,
+    ) -> None:
+        super().__init__()
+        self.operator = operator
+        self.scale_mapping = scale_mapping
+        self.scale_lb = scale_lb
+
+    def forward(self, *tensors: torch.Tensor) -> tuple[torch.Tensor, ...]:
+        net_output = self.operator(*tensors)
+        others = ()
+        if not isinstance(net_output, torch.Tensor):
+            net_output, *others = net_output
+        loc, scale = net_output.chunk(2, -1)
+        scale = mappings(self.scale_mapping)(scale).clamp_min(self.scale_lb)
+        return (loc, scale, *others)
+
+
+class NormalParamExtractor(nn.Module):
+    """A non-parametric nn.Module that splits its input into loc and scale parameters.
+
+    The scale parameters are mapped onto positive values using the specified ``scale_mapping``.
+
+    Args:
+        scale_mapping (str, optional): positive mapping function to be used with the std.
+            default = "biased_softplus_1.0" (i.e. softplus map with bias such that fn(0.0) = 1.0)
+            choices: "softplus", "exp", "relu", "biased_softplus_1";
+        scale_lb (Number, optional): The minimum value that the variance can take. Default is 1e-4.
+
+    Examples:
+        >>> import torch
+        >>> from tensordict.nn.distributions import NormalParamExtractor
+        >>> from torch import nn
+        >>> module = nn.Linear(3, 4)
+        >>> normal_params = NormalParamExtractor()
+        >>> tensor = torch.randn(3)
+        >>> loc, scale = normal_params(module(tensor))
+        >>> print(loc.shape, scale.shape)
+        torch.Size([2]) torch.Size([2])
+        >>> assert (scale > 0).all()
+        >>> # with modules that return more than one tensor
+        >>> module = nn.LSTM(3, 4)
+        >>> tensor = torch.randn(4, 2, 3)
+        >>> loc, scale, others = normal_params(*module(tensor))
+        >>> print(loc.shape, scale.shape)
+        torch.Size([4, 2, 2]) torch.Size([4, 2, 2])
+        >>> assert (scale > 0).all()
+
+    """
+
+    def __init__(
+        self,
+        scale_mapping: str = "biased_softplus_1.0",
+        scale_lb: Number = 1e-4,
+    ) -> None:
+        super().__init__()
+        self.scale_mapping = scale_mapping
+        self.scale_lb = scale_lb
+
+    def forward(self, *tensors: torch.Tensor) -> tuple[torch.Tensor, ...]:
+        tensor, *others = tensors
+        loc, scale = tensor.chunk(2, -1)
+        scale = mappings(self.scale_mapping)(scale).clamp_min(self.scale_lb)
+        return (loc, scale, *others)
+
+
+class Delta(D.Distribution):
+    """Delta distribution.
+
+    Args:
+        param (torch.Tensor): parameter of the delta distribution;
+        atol (number, optional): absolute tolerance to consider that a tensor matches the distribution parameter;
+            Default is 1e-6
+        rtol (number, optional): relative tolerance to consider that a tensor matches the distribution parameter;
+            Default is 1e-6
+        batch_shape (torch.Size, optional): batch shape;
+        event_shape (torch.Size, optional): shape of the outcome.
+
+    """
+
+    arg_constraints: dict = {}
+
+    def __init__(
+        self,
+        param: torch.Tensor,
+        atol: float = 1e-6,
+        rtol: float = 1e-6,
+        batch_shape: torch.Size | Sequence[int] | None = None,
+        event_shape: torch.Size | Sequence[int] | None = None,
+    ) -> None:
+        if batch_shape is None:
+            batch_shape = torch.Size([])
+        if event_shape is None:
+            event_shape = torch.Size([])
+        self.update(param)
+        self.atol = atol
+        self.rtol = rtol
+        if not len(batch_shape) and not len(event_shape):
+            batch_shape = param.shape[:-1]
+            event_shape = param.shape[-1:]
+        super().__init__(batch_shape=batch_shape, event_shape=event_shape)
+
+    def update(self, param: torch.Tensor) -> None:
+        self.param = param
+
+    def _is_equal(self, value: torch.Tensor) -> torch.Tensor:
+        param = self.param.expand_as(value)
+        is_equal = abs(value - param) < self.atol + self.rtol * abs(param)
+        for i in range(-1, -len(self.event_shape) - 1, -1):
+            is_equal = is_equal.all(i)
+        return is_equal
+
+    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
+        is_equal = self._is_equal(value)
+        out = torch.zeros_like(is_equal, dtype=value.dtype)
+        out.masked_fill_(is_equal, np.inf)
+        out.masked_fill_(~is_equal, -np.inf)
+        return out
+
+    @torch.no_grad()
+    def sample(
+        self,
+        sample_shape: torch.Size | Sequence[int] | None = None,
+    ) -> torch.Tensor:
+        if sample_shape is None:
+            sample_shape = torch.Size([])
+        return self.param.expand(*sample_shape, *self.param.shape)
+
+    def rsample(
+        self,
+        sample_shape: torch.Size | Sequence[int] | None = None,
+    ) -> torch.Tensor:
+        if sample_shape is None:
+            sample_shape = torch.Size([])
+        return self.param.expand(*sample_shape, *self.param.shape)
+
+    @property
+    def mode(self) -> torch.Tensor:
+        return self.param
+
+    @property
+    def mean(self) -> torch.Tensor:
+        return self.param
```

## tensordict/nn/distributions/discrete.py

 * *Ordering differences only*

```diff
@@ -1,87 +1,87 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-from typing import Sequence
-
-import torch
-from torch import distributions as D
-
-__all__ = [
-    "OneHotCategorical",
-]
-
-
-def _treat_categorical_params(
-    params: torch.Tensor | None = None,
-) -> torch.Tensor | None:
-    if params is None:
-        return None
-    if params.shape[-1] == 1:
-        params = params[..., 0]
-    return params
-
-
-def rand_one_hot(values: torch.Tensor, do_softmax: bool = True) -> torch.Tensor:
-    if do_softmax:
-        values = values.softmax(-1)
-    out = values.cumsum(-1) > torch.rand_like(values[..., :1])
-    out = (out.cumsum(-1) == 1).to(torch.long)
-    return out
-
-
-class OneHotCategorical(D.Categorical):
-    """One-hot categorical distribution.
-
-    This class behaves excacly as torch.distributions.Categorical except that it reads and produces one-hot encodings
-    of the discrete tensors.
-
-    """
-
-    num_params: int = 1
-
-    def __init__(
-        self,
-        logits: torch.Tensor | None = None,
-        probs: torch.Tensor | None = None,
-        **kwargs,
-    ) -> None:
-        logits = _treat_categorical_params(logits)
-        probs = _treat_categorical_params(probs)
-        super().__init__(probs=probs, logits=logits, **kwargs)
-
-    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
-        return super().log_prob(value.argmax(dim=-1))
-
-    @property
-    def mode(self) -> torch.Tensor:
-        if hasattr(self, "logits"):
-            return (self.logits == self.logits.max(-1, True)[0]).to(torch.long)
-        else:
-            return (self.probs == self.probs.max(-1, True)[0]).to(torch.long)
-
-    def sample(
-        self,
-        sample_shape: torch.Size | Sequence[int] | None = None,
-    ) -> torch.Tensor:
-        if sample_shape is None:
-            sample_shape = torch.Size([])
-        out = super().sample(sample_shape=sample_shape)
-        out = torch.nn.functional.one_hot(out, self.logits.shape[-1]).to(torch.long)
-        return out
-
-    def rsample(
-        self,
-        sample_shape: torch.Size | Sequence[int] | None = None,
-    ) -> torch.Tensor:
-        if sample_shape is None:
-            sample_shape = torch.Size([])
-        d = D.relaxed_categorical.RelaxedOneHotCategorical(
-            1.0, probs=self.probs, logits=self.logits
-        )
-        out = d.rsample(sample_shape)
-        out.data.copy_((out == out.max(-1)[0].unsqueeze(-1)).to(out.dtype))
-        return out
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+from typing import Sequence
+
+import torch
+from torch import distributions as D
+
+__all__ = [
+    "OneHotCategorical",
+]
+
+
+def _treat_categorical_params(
+    params: torch.Tensor | None = None,
+) -> torch.Tensor | None:
+    if params is None:
+        return None
+    if params.shape[-1] == 1:
+        params = params[..., 0]
+    return params
+
+
+def rand_one_hot(values: torch.Tensor, do_softmax: bool = True) -> torch.Tensor:
+    if do_softmax:
+        values = values.softmax(-1)
+    out = values.cumsum(-1) > torch.rand_like(values[..., :1])
+    out = (out.cumsum(-1) == 1).to(torch.long)
+    return out
+
+
+class OneHotCategorical(D.Categorical):
+    """One-hot categorical distribution.
+
+    This class behaves excacly as torch.distributions.Categorical except that it reads and produces one-hot encodings
+    of the discrete tensors.
+
+    """
+
+    num_params: int = 1
+
+    def __init__(
+        self,
+        logits: torch.Tensor | None = None,
+        probs: torch.Tensor | None = None,
+        **kwargs,
+    ) -> None:
+        logits = _treat_categorical_params(logits)
+        probs = _treat_categorical_params(probs)
+        super().__init__(probs=probs, logits=logits, **kwargs)
+
+    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
+        return super().log_prob(value.argmax(dim=-1))
+
+    @property
+    def mode(self) -> torch.Tensor:
+        if hasattr(self, "logits"):
+            return (self.logits == self.logits.max(-1, True)[0]).to(torch.long)
+        else:
+            return (self.probs == self.probs.max(-1, True)[0]).to(torch.long)
+
+    def sample(
+        self,
+        sample_shape: torch.Size | Sequence[int] | None = None,
+    ) -> torch.Tensor:
+        if sample_shape is None:
+            sample_shape = torch.Size([])
+        out = super().sample(sample_shape=sample_shape)
+        out = torch.nn.functional.one_hot(out, self.logits.shape[-1]).to(torch.long)
+        return out
+
+    def rsample(
+        self,
+        sample_shape: torch.Size | Sequence[int] | None = None,
+    ) -> torch.Tensor:
+        if sample_shape is None:
+            sample_shape = torch.Size([])
+        d = D.relaxed_categorical.RelaxedOneHotCategorical(
+            1.0, probs=self.probs, logits=self.logits
+        )
+        out = d.rsample(sample_shape)
+        out.data.copy_((out == out.max(-1)[0].unsqueeze(-1)).to(out.dtype))
+        return out
```

## tensordict/nn/distributions/truncated_normal.py

 * *Ordering differences only*

```diff
@@ -1,190 +1,190 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-# from https://github.com/toshas/torch_truncnorm
-
-from __future__ import annotations
-
-import math
-from numbers import Number
-from typing import Sequence
-
-import torch
-from torch.distributions import constraints, Distribution
-from torch.distributions.utils import broadcast_all
-
-CONST_SQRT_2 = math.sqrt(2)
-CONST_INV_SQRT_2PI = 1 / math.sqrt(2 * math.pi)
-CONST_INV_SQRT_2 = 1 / math.sqrt(2)
-CONST_LOG_INV_SQRT_2PI = math.log(CONST_INV_SQRT_2PI)
-CONST_LOG_SQRT_2PI_E = 0.5 * math.log(2 * math.pi * math.e)
-
-
-class TruncatedStandardNormal(Distribution):
-    """Truncated Standard Normal distribution.
-
-    Source: https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
-    """
-
-    arg_constraints = {
-        "a": constraints.real,
-        "b": constraints.real,
-    }
-    has_rsample = True
-    eps = 1e-6
-
-    def __init__(
-        self,
-        a: Number | torch.Tensor,
-        b: Number | torch.Tensor,
-        validate_args: bool | None = None,
-    ) -> None:
-        self.a, self.b = broadcast_all(a, b)
-        if isinstance(a, Number) and isinstance(b, Number):
-            batch_shape = torch.Size()
-        else:
-            batch_shape = self.a.size()
-        super().__init__(batch_shape, validate_args=validate_args)
-        if self.a.dtype != self.b.dtype:
-            raise ValueError("Truncation bounds types are different")
-        if any((self.a >= self.b).view(-1).tolist()):
-            raise ValueError("Incorrect truncation range")
-        # eps = torch.finfo(self.a.dtype).eps * 10
-        eps = self.eps
-        self._dtype_min_gt_0 = eps
-        self._dtype_max_lt_1 = 1 - eps
-        self._little_phi_a = self._little_phi(self.a)
-        self._little_phi_b = self._little_phi(self.b)
-        self._big_phi_a = self._big_phi(self.a)
-        self._big_phi_b = self._big_phi(self.b)
-        self._Z = (self._big_phi_b - self._big_phi_a).clamp(eps, 1 - eps)
-        self._log_Z = self._Z.log()
-        little_phi_coeff_a = torch.nan_to_num(self.a, nan=math.nan)
-        little_phi_coeff_b = torch.nan_to_num(self.b, nan=math.nan)
-        self._lpbb_m_lpaa_d_Z = (
-            self._little_phi_b * little_phi_coeff_b
-            - self._little_phi_a * little_phi_coeff_a
-        ) / self._Z
-        self._mean = -(self._little_phi_b - self._little_phi_a) / self._Z
-        self._variance = (
-            1
-            - self._lpbb_m_lpaa_d_Z
-            - ((self._little_phi_b - self._little_phi_a) / self._Z) ** 2
-        )
-        self._entropy = CONST_LOG_SQRT_2PI_E + self._log_Z - 0.5 * self._lpbb_m_lpaa_d_Z
-
-    @constraints.dependent_property
-    def support(self) -> constraints.Constraints:
-        return constraints.interval(self.a, self.b)
-
-    @property
-    def mean(self) -> torch.Tensor:
-        return self._mean
-
-    @property
-    def variance(self) -> torch.Tensor:
-        return self._variance
-
-    @property
-    def entropy(self) -> torch.Tensor:
-        return self._entropy
-
-    @property
-    def auc(self) -> torch.Tensor:
-        return self._Z
-
-    @staticmethod
-    def _little_phi(x: torch.Tensor) -> torch.Tensor:
-        return (-(x**2) * 0.5).exp() * CONST_INV_SQRT_2PI
-
-    def _big_phi(self, x: torch.Tensor) -> torch.Tensor:
-        phi = 0.5 * (1 + (x * CONST_INV_SQRT_2).erf())
-        return phi.clamp(self.eps, 1 - self.eps)
-
-    @staticmethod
-    def _inv_big_phi(x: torch.Tensor) -> torch.Tensor:
-        return CONST_SQRT_2 * (2 * x - 1).erfinv()
-
-    def cdf(self, value: torch.Tensor) -> torch.Tensor:
-        if self._validate_args:
-            self._validate_sample(value)
-        return ((self._big_phi(value) - self._big_phi_a) / self._Z).clamp(0, 1)
-
-    def icdf(self, value: torch.Tensor) -> torch.Tensor:
-        y = self._big_phi_a + value * self._Z
-        y = y.clamp(self.eps, 1 - self.eps)
-        return self._inv_big_phi(y)
-
-    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
-        if self._validate_args:
-            self._validate_sample(value)
-        return CONST_LOG_INV_SQRT_2PI - self._log_Z - (value**2) * 0.5
-
-    def rsample(
-        self,
-        sample_shape: torch.Size | Sequence[int] | None = None,
-    ) -> torch.Tensor:
-        if sample_shape is None:
-            sample_shape = torch.Size([])
-        shape = self._extended_shape(sample_shape)
-        p = torch.empty(shape, device=self.a.device).uniform_(
-            self._dtype_min_gt_0, self._dtype_max_lt_1
-        )
-        return self.icdf(p)
-
-
-class TruncatedNormal(TruncatedStandardNormal):
-    """Truncated Normal distribution.
-
-    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
-    """
-
-    has_rsample = True
-
-    def __init__(
-        self,
-        loc: Number | torch.Tensor,
-        scale: Number | torch.Tensor,
-        a: Number | torch.Tensor,
-        b: Number | torch.Tensor,
-        validate_args: bool | None = None,
-    ) -> None:
-        scale = scale.clamp_min(self.eps)
-        self.loc, self.scale, a, b = broadcast_all(loc, scale, a, b)
-        self._non_std_a = a
-        self._non_std_b = b
-        a = (a - self.loc) / self.scale
-        b = (b - self.loc) / self.scale
-        super().__init__(a, b, validate_args=validate_args)
-        self._log_scale = self.scale.log()
-        self._mean = self._mean * self.scale + self.loc
-        self._variance = self._variance * self.scale**2
-        self._entropy += self._log_scale
-
-    def _to_std_rv(self, value: torch.Tensor) -> torch.Tensor:
-        return (value - self.loc) / self.scale
-
-    def _from_std_rv(self, value: torch.Tensor) -> torch.Tensor:
-        return value * self.scale + self.loc
-
-    def cdf(self, value: torch.Tensor) -> torch.Tensor:
-        return super().cdf(self._to_std_rv(value))
-
-    def icdf(self, value: torch.Tensor) -> torch.Tensor:
-        sample = self._from_std_rv(super().icdf(value))
-
-        # clamp data but keep gradients
-        sample_clip = torch.stack(
-            [sample.detach(), self._non_std_a.detach().expand_as(sample)], 0
-        ).max(0)[0]
-        sample_clip = torch.stack(
-            [sample_clip, self._non_std_b.detach().expand_as(sample)], 0
-        ).min(0)[0]
-        sample.data.copy_(sample_clip)
-        return sample
-
-    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
-        value = self._to_std_rv(value)
-        return super().log_prob(value) - self._log_scale
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+# from https://github.com/toshas/torch_truncnorm
+
+from __future__ import annotations
+
+import math
+from numbers import Number
+from typing import Sequence
+
+import torch
+from torch.distributions import constraints, Distribution
+from torch.distributions.utils import broadcast_all
+
+CONST_SQRT_2 = math.sqrt(2)
+CONST_INV_SQRT_2PI = 1 / math.sqrt(2 * math.pi)
+CONST_INV_SQRT_2 = 1 / math.sqrt(2)
+CONST_LOG_INV_SQRT_2PI = math.log(CONST_INV_SQRT_2PI)
+CONST_LOG_SQRT_2PI_E = 0.5 * math.log(2 * math.pi * math.e)
+
+
+class TruncatedStandardNormal(Distribution):
+    """Truncated Standard Normal distribution.
+
+    Source: https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
+    """
+
+    arg_constraints = {
+        "a": constraints.real,
+        "b": constraints.real,
+    }
+    has_rsample = True
+    eps = 1e-6
+
+    def __init__(
+        self,
+        a: Number | torch.Tensor,
+        b: Number | torch.Tensor,
+        validate_args: bool | None = None,
+    ) -> None:
+        self.a, self.b = broadcast_all(a, b)
+        if isinstance(a, Number) and isinstance(b, Number):
+            batch_shape = torch.Size()
+        else:
+            batch_shape = self.a.size()
+        super().__init__(batch_shape, validate_args=validate_args)
+        if self.a.dtype != self.b.dtype:
+            raise ValueError("Truncation bounds types are different")
+        if any((self.a >= self.b).view(-1).tolist()):
+            raise ValueError("Incorrect truncation range")
+        # eps = torch.finfo(self.a.dtype).eps * 10
+        eps = self.eps
+        self._dtype_min_gt_0 = eps
+        self._dtype_max_lt_1 = 1 - eps
+        self._little_phi_a = self._little_phi(self.a)
+        self._little_phi_b = self._little_phi(self.b)
+        self._big_phi_a = self._big_phi(self.a)
+        self._big_phi_b = self._big_phi(self.b)
+        self._Z = (self._big_phi_b - self._big_phi_a).clamp(eps, 1 - eps)
+        self._log_Z = self._Z.log()
+        little_phi_coeff_a = torch.nan_to_num(self.a, nan=math.nan)
+        little_phi_coeff_b = torch.nan_to_num(self.b, nan=math.nan)
+        self._lpbb_m_lpaa_d_Z = (
+            self._little_phi_b * little_phi_coeff_b
+            - self._little_phi_a * little_phi_coeff_a
+        ) / self._Z
+        self._mean = -(self._little_phi_b - self._little_phi_a) / self._Z
+        self._variance = (
+            1
+            - self._lpbb_m_lpaa_d_Z
+            - ((self._little_phi_b - self._little_phi_a) / self._Z) ** 2
+        )
+        self._entropy = CONST_LOG_SQRT_2PI_E + self._log_Z - 0.5 * self._lpbb_m_lpaa_d_Z
+
+    @constraints.dependent_property
+    def support(self) -> constraints.Constraints:
+        return constraints.interval(self.a, self.b)
+
+    @property
+    def mean(self) -> torch.Tensor:
+        return self._mean
+
+    @property
+    def variance(self) -> torch.Tensor:
+        return self._variance
+
+    @property
+    def entropy(self) -> torch.Tensor:
+        return self._entropy
+
+    @property
+    def auc(self) -> torch.Tensor:
+        return self._Z
+
+    @staticmethod
+    def _little_phi(x: torch.Tensor) -> torch.Tensor:
+        return (-(x**2) * 0.5).exp() * CONST_INV_SQRT_2PI
+
+    def _big_phi(self, x: torch.Tensor) -> torch.Tensor:
+        phi = 0.5 * (1 + (x * CONST_INV_SQRT_2).erf())
+        return phi.clamp(self.eps, 1 - self.eps)
+
+    @staticmethod
+    def _inv_big_phi(x: torch.Tensor) -> torch.Tensor:
+        return CONST_SQRT_2 * (2 * x - 1).erfinv()
+
+    def cdf(self, value: torch.Tensor) -> torch.Tensor:
+        if self._validate_args:
+            self._validate_sample(value)
+        return ((self._big_phi(value) - self._big_phi_a) / self._Z).clamp(0, 1)
+
+    def icdf(self, value: torch.Tensor) -> torch.Tensor:
+        y = self._big_phi_a + value * self._Z
+        y = y.clamp(self.eps, 1 - self.eps)
+        return self._inv_big_phi(y)
+
+    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
+        if self._validate_args:
+            self._validate_sample(value)
+        return CONST_LOG_INV_SQRT_2PI - self._log_Z - (value**2) * 0.5
+
+    def rsample(
+        self,
+        sample_shape: torch.Size | Sequence[int] | None = None,
+    ) -> torch.Tensor:
+        if sample_shape is None:
+            sample_shape = torch.Size([])
+        shape = self._extended_shape(sample_shape)
+        p = torch.empty(shape, device=self.a.device).uniform_(
+            self._dtype_min_gt_0, self._dtype_max_lt_1
+        )
+        return self.icdf(p)
+
+
+class TruncatedNormal(TruncatedStandardNormal):
+    """Truncated Normal distribution.
+
+    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
+    """
+
+    has_rsample = True
+
+    def __init__(
+        self,
+        loc: Number | torch.Tensor,
+        scale: Number | torch.Tensor,
+        a: Number | torch.Tensor,
+        b: Number | torch.Tensor,
+        validate_args: bool | None = None,
+    ) -> None:
+        scale = scale.clamp_min(self.eps)
+        self.loc, self.scale, a, b = broadcast_all(loc, scale, a, b)
+        self._non_std_a = a
+        self._non_std_b = b
+        a = (a - self.loc) / self.scale
+        b = (b - self.loc) / self.scale
+        super().__init__(a, b, validate_args=validate_args)
+        self._log_scale = self.scale.log()
+        self._mean = self._mean * self.scale + self.loc
+        self._variance = self._variance * self.scale**2
+        self._entropy += self._log_scale
+
+    def _to_std_rv(self, value: torch.Tensor) -> torch.Tensor:
+        return (value - self.loc) / self.scale
+
+    def _from_std_rv(self, value: torch.Tensor) -> torch.Tensor:
+        return value * self.scale + self.loc
+
+    def cdf(self, value: torch.Tensor) -> torch.Tensor:
+        return super().cdf(self._to_std_rv(value))
+
+    def icdf(self, value: torch.Tensor) -> torch.Tensor:
+        sample = self._from_std_rv(super().icdf(value))
+
+        # clamp data but keep gradients
+        sample_clip = torch.stack(
+            [sample.detach(), self._non_std_a.detach().expand_as(sample)], 0
+        ).max(0)[0]
+        sample_clip = torch.stack(
+            [sample_clip, self._non_std_b.detach().expand_as(sample)], 0
+        ).min(0)[0]
+        sample.data.copy_(sample_clip)
+        return sample
+
+    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
+        value = self._to_std_rv(value)
+        return super().log_prob(value) - self._log_scale
```

## tensordict/nn/distributions/utils.py

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import torch
-from tensordict.utils import DeviceType
-from torch import distributions as D
-
-
-def _cast_device(
-    elt: torch.Tensor | float,
-    device: DeviceType,
-) -> torch.Tensor | float:
-    if isinstance(elt, torch.Tensor):
-        return elt.to(device)
-    return elt
-
-
-def _cast_transform_device(
-    transform: D.Transform | None,
-    device: DeviceType,
-) -> D.Transform | None:
-    if transform is None:
-        return transform
-    elif isinstance(transform, D.ComposeTransform):
-        for i, t in enumerate(transform.parts):
-            transform.parts[i] = _cast_transform_device(t, device)
-    elif isinstance(transform, D.Transform):
-        for attribute in dir(transform):
-            value = getattr(transform, attribute)
-            if isinstance(value, torch.Tensor):
-                setattr(transform, attribute, value.to(device))
-        return transform
-    else:
-        raise TypeError(
-            f"Cannot perform device casting for transform of type {type(transform)}"
-        )
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import torch
+from tensordict.utils import DeviceType
+from torch import distributions as D
+
+
+def _cast_device(
+    elt: torch.Tensor | float,
+    device: DeviceType,
+) -> torch.Tensor | float:
+    if isinstance(elt, torch.Tensor):
+        return elt.to(device)
+    return elt
+
+
+def _cast_transform_device(
+    transform: D.Transform | None,
+    device: DeviceType,
+) -> D.Transform | None:
+    if transform is None:
+        return transform
+    elif isinstance(transform, D.ComposeTransform):
+        for i, t in enumerate(transform.parts):
+            transform.parts[i] = _cast_transform_device(t, device)
+    elif isinstance(transform, D.Transform):
+        for attribute in dir(transform):
+            value = getattr(transform, attribute)
+            if isinstance(value, torch.Tensor):
+                setattr(transform, attribute, value.to(device))
+        return transform
+    else:
+        raise TypeError(
+            f"Cannot perform device casting for transform of type {type(transform)}"
+        )
```

## tensordict/prototype/__init__.py

 * *Ordering differences only*

```diff
@@ -1,12 +1,12 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-from tensordict.prototype.fx import symbolic_trace
-from tensordict.prototype.tensorclass import is_tensorclass, tensorclass
-
-__all__ = [
-    "is_tensorclass",
-    "symbolic_trace",
-    "tensorclass",
-]
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+from tensordict.prototype.fx import symbolic_trace
+from tensordict.prototype.tensorclass import is_tensorclass, tensorclass
+
+__all__ = [
+    "is_tensorclass",
+    "symbolic_trace",
+    "tensorclass",
+]
```

## tensordict/prototype/fx.py

 * *Ordering differences only*

```diff
@@ -1,198 +1,198 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import operator
-from itertools import filterfalse, tee
-from typing import Any, Callable, Iterable
-
-from tensordict._tensordict import _unravel_key_to_tuple
-from tensordict.nn import TensorDictModule, TensorDictSequential
-from tensordict.tensordict import TensorDictBase
-from tensordict.utils import NestedKey
-from torch import fx, nn
-
-__all__ = ["symbolic_trace"]
-
-
-class TDGraphModule(nn.Module):
-    """A graph module for TensorDict."""
-
-    def __init__(
-        self,
-        graph_module: fx.GraphModule,
-        out_keys: list[NestedKey],
-    ) -> None:
-        super().__init__()
-        self.out_keys = [_unravel_key_to_tuple(ok) for ok in out_keys]
-        self._gm = graph_module
-
-    def forward(
-        self,
-        tensordict: TensorDictBase,
-        tensordict_out: TensorDictBase | None = None,
-        **kwargs,
-    ) -> TensorDictBase:
-        outputs = self._gm(tensordict, **kwargs)
-
-        if tensordict_out is None:
-            tensordict_out = tensordict
-
-        for out_key, output in zip(self.out_keys, outputs):
-            if out_key != "_":
-                tensordict_out._set_tuple(
-                    out_key, output, inplace=False, validated=True
-                )
-
-        return tensordict_out
-
-    def __getattr__(self, name: str) -> Any:
-        try:
-            return super().__getattr__(name)
-        except AttributeError:
-            return getattr(self._gm, name)
-
-
-def symbolic_trace(td_module: TensorDictModule) -> TDGraphModule:
-    """A symbolic tracer for TensorDictModule."""
-    if isinstance(td_module, TensorDictSequential):
-        return _trace_tensordictsequential(td_module)
-    elif isinstance(td_module, TensorDictModule):
-        return _trace_tensordictmodule(td_module)
-    raise TypeError(f"Unsupported type {type(td_module)}")
-
-
-# cf. https://docs.python.org/3/library/itertools.html#itertools-recipes
-def _partition(
-    pred: Callable[..., bool], iterable: Iterable[Any]
-) -> tuple[Iterable[Any], Iterable[Any]]:
-    """Use a predicate to partition entries into false entries and true entries."""
-    # partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9
-    t1, t2 = tee(iterable)
-    return filterfalse(pred, t1), filter(pred, t2)
-
-
-def _parse_input_nodes(
-    in_keys: list[NestedKey], nodes, td: TensorDictBase, inputs: tuple[Any, ...], env
-):
-    for in_key, node in zip(in_keys, nodes):
-        if in_key in inputs:
-            new_node = inputs[in_key]
-        else:
-            output_proxy = operator.getitem(td, in_key)
-            new_node = output_proxy.node
-            inputs[in_key] = new_node
-        env[node.name] = new_node
-
-
-def _trace_tensordictmodule(td_module: TensorDictModule) -> TDGraphModule:
-    # this graph manipulation is based heavily on example in the PyTorch docs
-    # https://pytorch.org/docs/stable/fx.html#proxy-retracing
-
-    # trace the graph of the underlying module
-    graph = fx.Tracer().trace(td_module.module)
-
-    # create a new graph which we will populate from the old one
-    new_graph = fx.Graph()
-    env = {}
-
-    # create a new placeholder for the input tensordict
-    td = fx.Proxy(new_graph.placeholder("tensordict"))
-
-    node_iter = iter(graph.nodes)
-
-    # the first nodes, in order, are placeholders for the in_keys. We consume them and
-    # convert them to "call_function" nodes with target=operator.getitem.
-    _parse_input_nodes(td_module.in_keys, node_iter, td, {}, env)
-
-    # the remaining nodes we simply clone, pulling any arguments from the env
-    for node in node_iter:
-        new_node = new_graph.node_copy(node, lambda x: env[x.name])
-        env[node.name] = new_node
-
-    return TDGraphModule(
-        fx.GraphModule(td_module.module, new_graph), td_module.out_keys
-    )
-
-
-def _trace_tensordictsequential(td_sequential: TensorDictSequential) -> TDGraphModule:
-    # we track values previously read from / written to the tensordict by storing the
-    # nodes / proxy values in the inputs / outputs dictionaries
-    inputs = {}
-    outputs = {}
-    # env is a lookup for nodes in the new graph using names from the old graph
-    env = {}
-
-    new_graph = fx.Graph()
-    td = fx.Proxy(new_graph.placeholder("tensordict"))
-
-    for i, td_module in enumerate(td_sequential.module):
-        # trace the submodule
-        if isinstance(td_module, TensorDictSequential):
-            graph = _trace_tensordictsequential(td_module).graph
-            node_iter = iter(graph.nodes)
-            _td = next(node_iter)  # tensordict placeholder from submodule graph
-
-            # in the graph of TensorDictSequential, the getitem calls to the tensordict
-            # need not come first, so we partition nodes into getitem calls on the
-            # placeholder tensordict (input_nodes) and the remaining nodes
-            node_iter, input_nodes = _partition(
-                lambda node, _td=_td: (
-                    node.op == "call_function"
-                    and node.target == operator.getitem
-                    and node.args[0] == _td
-                ),
-                node_iter,
-            )
-            _parse_input_nodes(td_module.in_keys, input_nodes, td, inputs, env)
-
-        else:
-            graph = fx.Tracer().trace(td_module.module)
-            # in the trace of a regular nn.Module the placeholder nodes all come first,
-            # so we just consume them in order
-            node_iter = iter(graph.nodes)
-            _parse_input_nodes(td_module.in_keys, node_iter, td, inputs, env)
-
-        # clone the remaining nodes
-        for node in node_iter:
-            if node.op == "output":
-                # capture the outputs but don't clone the output node (this would
-                # result in prematurely returning intermediate values)
-
-                # need to unpack the args in the case that the submodule is itself a
-                # TensorDictSequential that returns a tuple of arguments
-                args = (
-                    node.args[0]
-                    if isinstance(td_module, TensorDictSequential)
-                    else node.args
-                )
-
-                # if the submodule has multiple outputs, args has structure
-                # ((out1, out2,),), so we need to do some extra unpacking
-                args = args[0] if isinstance(args[0], tuple) else args
-
-                for out_key, arg in zip(td_module.out_keys, args):
-                    # any outputs of submodules will need to be returned at the end
-                    outputs[out_key] = env[arg.name]
-                    # we also need to make outputs of submodules available as inputs
-                    # to subsequent submodules
-                    inputs[out_key] = env[arg.name]
-            else:
-                new_node = new_graph.node_copy(node, lambda x: env[x.name])
-                if new_node.op in ("call_module", "get_attr"):
-                    # since we traced the submodule in isolation, we need to patch the
-                    # targets of any calls to methods on the module or attribute access
-                    new_node.target = f"{i}.module.{new_node.target}"
-                    new_node.name = f"_{i}_{new_node.name}"
-                env[node.name] = new_node
-
-    # finally we add a new output node that collects all of the output values from
-    # submodules in the graph and returns them together
-    new_graph.output(tuple(outputs.values()))
-
-    return TDGraphModule(
-        fx.GraphModule(td_sequential.module, new_graph), tuple(outputs.keys())
-    )
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import operator
+from itertools import filterfalse, tee
+from typing import Any, Callable, Iterable
+
+from tensordict._tensordict import _unravel_key_to_tuple
+from tensordict.nn import TensorDictModule, TensorDictSequential
+from tensordict.tensordict import TensorDictBase
+from tensordict.utils import NestedKey
+from torch import fx, nn
+
+__all__ = ["symbolic_trace"]
+
+
+class TDGraphModule(nn.Module):
+    """A graph module for TensorDict."""
+
+    def __init__(
+        self,
+        graph_module: fx.GraphModule,
+        out_keys: list[NestedKey],
+    ) -> None:
+        super().__init__()
+        self.out_keys = [_unravel_key_to_tuple(ok) for ok in out_keys]
+        self._gm = graph_module
+
+    def forward(
+        self,
+        tensordict: TensorDictBase,
+        tensordict_out: TensorDictBase | None = None,
+        **kwargs,
+    ) -> TensorDictBase:
+        outputs = self._gm(tensordict, **kwargs)
+
+        if tensordict_out is None:
+            tensordict_out = tensordict
+
+        for out_key, output in zip(self.out_keys, outputs):
+            if out_key != "_":
+                tensordict_out._set_tuple(
+                    out_key, output, inplace=False, validated=True
+                )
+
+        return tensordict_out
+
+    def __getattr__(self, name: str) -> Any:
+        try:
+            return super().__getattr__(name)
+        except AttributeError:
+            return getattr(self._gm, name)
+
+
+def symbolic_trace(td_module: TensorDictModule) -> TDGraphModule:
+    """A symbolic tracer for TensorDictModule."""
+    if isinstance(td_module, TensorDictSequential):
+        return _trace_tensordictsequential(td_module)
+    elif isinstance(td_module, TensorDictModule):
+        return _trace_tensordictmodule(td_module)
+    raise TypeError(f"Unsupported type {type(td_module)}")
+
+
+# cf. https://docs.python.org/3/library/itertools.html#itertools-recipes
+def _partition(
+    pred: Callable[..., bool], iterable: Iterable[Any]
+) -> tuple[Iterable[Any], Iterable[Any]]:
+    """Use a predicate to partition entries into false entries and true entries."""
+    # partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9
+    t1, t2 = tee(iterable)
+    return filterfalse(pred, t1), filter(pred, t2)
+
+
+def _parse_input_nodes(
+    in_keys: list[NestedKey], nodes, td: TensorDictBase, inputs: tuple[Any, ...], env
+):
+    for in_key, node in zip(in_keys, nodes):
+        if in_key in inputs:
+            new_node = inputs[in_key]
+        else:
+            output_proxy = operator.getitem(td, in_key)
+            new_node = output_proxy.node
+            inputs[in_key] = new_node
+        env[node.name] = new_node
+
+
+def _trace_tensordictmodule(td_module: TensorDictModule) -> TDGraphModule:
+    # this graph manipulation is based heavily on example in the PyTorch docs
+    # https://pytorch.org/docs/stable/fx.html#proxy-retracing
+
+    # trace the graph of the underlying module
+    graph = fx.Tracer().trace(td_module.module)
+
+    # create a new graph which we will populate from the old one
+    new_graph = fx.Graph()
+    env = {}
+
+    # create a new placeholder for the input tensordict
+    td = fx.Proxy(new_graph.placeholder("tensordict"))
+
+    node_iter = iter(graph.nodes)
+
+    # the first nodes, in order, are placeholders for the in_keys. We consume them and
+    # convert them to "call_function" nodes with target=operator.getitem.
+    _parse_input_nodes(td_module.in_keys, node_iter, td, {}, env)
+
+    # the remaining nodes we simply clone, pulling any arguments from the env
+    for node in node_iter:
+        new_node = new_graph.node_copy(node, lambda x: env[x.name])
+        env[node.name] = new_node
+
+    return TDGraphModule(
+        fx.GraphModule(td_module.module, new_graph), td_module.out_keys
+    )
+
+
+def _trace_tensordictsequential(td_sequential: TensorDictSequential) -> TDGraphModule:
+    # we track values previously read from / written to the tensordict by storing the
+    # nodes / proxy values in the inputs / outputs dictionaries
+    inputs = {}
+    outputs = {}
+    # env is a lookup for nodes in the new graph using names from the old graph
+    env = {}
+
+    new_graph = fx.Graph()
+    td = fx.Proxy(new_graph.placeholder("tensordict"))
+
+    for i, td_module in enumerate(td_sequential.module):
+        # trace the submodule
+        if isinstance(td_module, TensorDictSequential):
+            graph = _trace_tensordictsequential(td_module).graph
+            node_iter = iter(graph.nodes)
+            _td = next(node_iter)  # tensordict placeholder from submodule graph
+
+            # in the graph of TensorDictSequential, the getitem calls to the tensordict
+            # need not come first, so we partition nodes into getitem calls on the
+            # placeholder tensordict (input_nodes) and the remaining nodes
+            node_iter, input_nodes = _partition(
+                lambda node, _td=_td: (
+                    node.op == "call_function"
+                    and node.target == operator.getitem
+                    and node.args[0] == _td
+                ),
+                node_iter,
+            )
+            _parse_input_nodes(td_module.in_keys, input_nodes, td, inputs, env)
+
+        else:
+            graph = fx.Tracer().trace(td_module.module)
+            # in the trace of a regular nn.Module the placeholder nodes all come first,
+            # so we just consume them in order
+            node_iter = iter(graph.nodes)
+            _parse_input_nodes(td_module.in_keys, node_iter, td, inputs, env)
+
+        # clone the remaining nodes
+        for node in node_iter:
+            if node.op == "output":
+                # capture the outputs but don't clone the output node (this would
+                # result in prematurely returning intermediate values)
+
+                # need to unpack the args in the case that the submodule is itself a
+                # TensorDictSequential that returns a tuple of arguments
+                args = (
+                    node.args[0]
+                    if isinstance(td_module, TensorDictSequential)
+                    else node.args
+                )
+
+                # if the submodule has multiple outputs, args has structure
+                # ((out1, out2,),), so we need to do some extra unpacking
+                args = args[0] if isinstance(args[0], tuple) else args
+
+                for out_key, arg in zip(td_module.out_keys, args):
+                    # any outputs of submodules will need to be returned at the end
+                    outputs[out_key] = env[arg.name]
+                    # we also need to make outputs of submodules available as inputs
+                    # to subsequent submodules
+                    inputs[out_key] = env[arg.name]
+            else:
+                new_node = new_graph.node_copy(node, lambda x: env[x.name])
+                if new_node.op in ("call_module", "get_attr"):
+                    # since we traced the submodule in isolation, we need to patch the
+                    # targets of any calls to methods on the module or attribute access
+                    new_node.target = f"{i}.module.{new_node.target}"
+                    new_node.name = f"_{i}_{new_node.name}"
+                env[node.name] = new_node
+
+    # finally we add a new output node that collects all of the output values from
+    # submodules in the graph and returns them together
+    new_graph.output(tuple(outputs.values()))
+
+    return TDGraphModule(
+        fx.GraphModule(td_sequential.module, new_graph), tuple(outputs.keys())
+    )
```

## tensordict/prototype/tensorclass.py

 * *Ordering differences only*

```diff
@@ -1,25 +1,25 @@
-import warnings
-from functools import wraps
-
-from tensordict import (  # no_qa
-    is_tensorclass as is_tensorclass_true,
-    tensorclass as tensorclass_true,
-)
-
-
-@wraps(tensorclass_true)
-def tensorclass(*args, **kwargs):  # noqa: D103
-    warnings.warn(
-        "tensorclass is not a prototype anymore and can be imported directly from tensordict root.",
-        category=DeprecationWarning,
-    )
-    return tensorclass_true(*args, **kwargs)
-
-
-@wraps(is_tensorclass_true)
-def is_tensorclass(*args, **kwargs):  # noqa: D103
-    warnings.warn(
-        "is_tensorclass is not a prototype anymore and can be imported directly from tensordict root.",
-        category=DeprecationWarning,
-    )
-    return is_tensorclass_true(*args, **kwargs)
+import warnings
+from functools import wraps
+
+from tensordict import (  # no_qa
+    is_tensorclass as is_tensorclass_true,
+    tensorclass as tensorclass_true,
+)
+
+
+@wraps(tensorclass_true)
+def tensorclass(*args, **kwargs):  # noqa: D103
+    warnings.warn(
+        "tensorclass is not a prototype anymore and can be imported directly from tensordict root.",
+        category=DeprecationWarning,
+    )
+    return tensorclass_true(*args, **kwargs)
+
+
+@wraps(is_tensorclass_true)
+def is_tensorclass(*args, **kwargs):  # noqa: D103
+    warnings.warn(
+        "is_tensorclass is not a prototype anymore and can be imported directly from tensordict root.",
+        category=DeprecationWarning,
+    )
+    return is_tensorclass_true(*args, **kwargs)
```

## Comparing `tensordict_nightly-2023.8.7.dist-info/LICENSE` & `tensordict_nightly-2023.8.8.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-MIT License
-
-Copyright (c) Meta Platforms, Inc. and affiliates.
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-  SOFTWARE.
+MIT License
+
+Copyright (c) Meta Platforms, Inc. and affiliates.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+  SOFTWARE.
```

## Comparing `tensordict_nightly-2023.8.7.dist-info/METADATA` & `tensordict_nightly-2023.8.8.dist-info/METADATA`

 * *Files 22% similar despite different names*

```diff
@@ -1,385 +1,384 @@
-Metadata-Version: 2.1
-Name: tensordict-nightly
-Version: 2023.8.7
-Summary: UNKNOWN
-Home-page: https://github.com/pytorch-labs/tensordict
-Author: tensordict contributors
-Author-email: vmoens@fb.com
-License: BSD
-Platform: UNKNOWN
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Development Status :: 4 - Beta
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: torch
-Requires-Dist: numpy
-Requires-Dist: cloudpickle
-Provides-Extra: checkpointing
-Requires-Dist: torchsnapshot-nightly ; extra == 'checkpointing'
-Provides-Extra: h5
-Requires-Dist: h5py >=3.8 ; extra == 'h5'
-Provides-Extra: tests
-Requires-Dist: pytest ; extra == 'tests'
-Requires-Dist: pyyaml ; extra == 'tests'
-Requires-Dist: pytest-instafail ; extra == 'tests'
-Requires-Dist: pytest-rerunfailures ; extra == 'tests'
-Requires-Dist: pytest-benchmark ; extra == 'tests'
-
-<!--- BADGES: START --->
-<!---
-[![Documentation](https://img.shields.io/badge/Documentation-blue.svg?style=flat)](https://pytorch-labs.github.io/tensordict/)
---->
-[![Docs - GitHub.io](https://img.shields.io/static/v1?logo=github&style=flat&color=pink&label=docs&message=tensordict)][#docs-package]
-[![Benchmarks](https://img.shields.io/badge/Benchmarks-blue.svg)][#docs-package-benchmark]
-[![Python version](https://img.shields.io/pypi/pyversions/tensordict.svg)](https://www.python.org/downloads/)
-[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)][#github-license]
-<a href="https://pypi.org/project/tensordict"><img src="https://img.shields.io/pypi/v/tensordict" alt="pypi version"></a>
-<a href="https://pypi.org/project/tensordict-nightly"><img src="https://img.shields.io/pypi/v/tensordict-nightly?label=nightly" alt="pypi nightly version"></a>
-[![Downloads](https://static.pepy.tech/personalized-badge/tensordict?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads)][#pepy-package]
-[![Downloads](https://static.pepy.tech/personalized-badge/tensordict-nightly?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads%20(nightly))][#pepy-package-nightly]
-[![codecov](https://codecov.io/gh/pytorch-labs/tensordict/branch/main/graph/badge.svg?token=9QTUG6NAGQ)][#codecov-package]
-[![circleci](https://circleci.com/gh/pytorch-labs/tensordict.svg?style=shield)][#circleci-package]
-[![Conda - Platform](https://img.shields.io/conda/pn/conda-forge/tensordict?logo=anaconda&style=flat)][#conda-forge-package]
-[![Conda (channel only)](https://img.shields.io/conda/vn/conda-forge/tensordict?logo=anaconda&style=flat&color=orange)][#conda-forge-package]
-
-[#docs-package]: https://pytorch-labs.github.io/tensordict/
-[#docs-package-benchmark]: https://pytorch-labs.github.io/tensordict/dev/bench/
-[#github-license]: https://github.com/pytorch-labs/tensordict/blob/main/LICENSE
-[#pepy-package]: https://pepy.tech/project/tensordict
-[#pepy-package-nightly]: https://pepy.tech/project/tensordict-nightly
-[#codecov-package]: https://codecov.io/gh/pytorch-labs/tensordict
-[#circleci-package]: https://circleci.com/gh/pytorch-labs/tensordict
-[#conda-forge-package]: https://anaconda.org/conda-forge/tensordict
-
-<!--- BADGES: END --->
-
-# TensorDict
-
-[**Installation**](#installation) | [**General features**](#general) |
-[**Tensor-like features**](#tensor-like-features) |  [**Distributed capabilities**](#distributed-capabilities) |
-[**TensorDict for functional programming using FuncTorch**](#tensordict-for-functional-programming-using-functorch) |
-[**Lazy preallocation**](#lazy-preallocation) | [**Nesting TensorDicts**](#nesting-tensordicts) | [**TensorClass**](#tensorclass)
-
-`TensorDict` is a dictionary-like class that inherits properties from tensors,
-such as indexing, shape operations, casting to device or point-to-point communication
-in distributed settings.
-
-The main purpose of TensorDict is to make code-bases more _readable_ and _modular_ by abstracting away tailored operations:
-```python
-for i, tensordict in enumerate(dataset):
-    # the model reads and writes tensordicts
-    tensordict = model(tensordict)
-    loss = loss_module(tensordict)
-    loss.backward()
-    optimizer.step()
-    optimizer.zero_grad()
-```
-With this level of abstraction, one can recycle a training loop for highly heterogeneous task.
-Each individual step of the training loop (data collection and transform, model prediction, loss computation etc.)
-can be tailored to the use case at hand without impacting the others.
-For instance, the above example can be easily used across classification and segmentation tasks, among many others.
-
-## Features
-
-### General
-
-A tensordict is primarily defined by its `batch_size` (or `shape`) and its key-value pairs:
-```python
->>> from tensordict import TensorDict
->>> import torch
->>> tensordict = TensorDict({
-...     "key 1": torch.ones(3, 4, 5),
-...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
-... }, batch_size=[3, 4])
-```
-The `batch_size` and the first dimensions of each of the tensors must be compliant.
-The tensors can be of any dtype and device. Optionally, one can restrict a tensordict to
-live on a dedicated device, which will send each tensor that is written there:
-```python
->>> tensordict = TensorDict({
-...     "key 1": torch.ones(3, 4, 5),
-...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
-... }, batch_size=[3, 4], device="cuda:0")
->>> tensordict["key 3"] = torch.randn(3, 4, device="cpu")
->>> assert tensordict["key 3"].device is torch.device("cuda:0")
-```
-
-### Tensor-like features
-
-TensorDict objects can be indexed exactly like tensors. The resulting of indexing
-a TensorDict is another TensorDict containing tensors indexed along the required dimension:
-```python
->>> tensordict = TensorDict({
-...     "key 1": torch.ones(3, 4, 5),
-...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
-... }, batch_size=[3, 4])
->>> sub_tensordict = tensordict[..., :2]
->>> assert sub_tensordict.shape == torch.Size([3, 2])
->>> assert sub_tensordict["key 1"].shape == torch.Size([3, 2, 5])
-```
-
-Similarly, one can build tensordicts by stacking or concatenating single tensordicts:
-```python
->>> tensordicts = [TensorDict({
-...     "key 1": torch.ones(3, 4, 5),
-...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
-... }, batch_size=[3, 4]) for _ in range(2)]
->>> stack_tensordict = torch.stack(tensordicts, 1)
->>> assert stack_tensordict.shape == torch.Size([3, 2, 4])
->>> assert stack_tensordict["key 1"].shape == torch.Size([3, 2, 4, 5])
->>> cat_tensordict = torch.cat(tensordicts, 0)
->>> assert cat_tensordict.shape == torch.Size([6, 4])
->>> assert cat_tensordict["key 1"].shape == torch.Size([6, 4, 5])
-```
-
-TensorDict instances can also be reshaped, viewed, squeezed and unsqueezed:
-```python
->>> tensordict = TensorDict({
-...     "key 1": torch.ones(3, 4, 5),
-...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
-... }, batch_size=[3, 4])
->>> print(tensordict.view(-1))
-torch.Size([12])
->>> print(tensordict.reshape(-1))
-torch.Size([12])
->>> print(tensordict.unsqueeze(-1))
-torch.Size([3, 4, 1])
-```
-
-One can also send tensordict from device to device, place them in shared memory,
-clone them, update them in-place or not, split them, unbind them, expand them etc.
-
-If a functionality is missing, it is easy to call it using `apply()` or `apply_()`:
-```python
-tensordict_uniform = tensordict.apply(lambda tensor: tensor.uniform_())
-```
-### Distributed capabilities
-
-Complex data structures can be cumbersome to synchronize in distributed settings.
-`tensordict` solves that problem with synchronous and asynchronous helper methods
-such as `recv`, `irecv`, `send` and `isend` that behave like their `torch.distributed`
-counterparts:
-```python
->>> # on all workers
->>> data = TensorDict({"a": torch.zeros(()), ("b", "c"): torch.ones(())}, [])
->>> # on worker 1
->>> data.isend(dst=0)
->>> # on worker 0
->>> data.irecv(src=1)
-```
-
-When nodes share a common scratch space, the
-[`MemmapTensor` backend](https://pytorch-labs.github.io/tensordict/tutorials/tensordict_memory.html)
-can be used
-to seamlessly send, receive and read a huge amount of data.
-
-### TensorDict for functional programming using FuncTorch
-
-We also provide an API to use TensorDict in conjunction with [FuncTorch](https://pytorch.org/functorch).
-For instance, TensorDict makes it easy to concatenate model weights to do model ensembling:
-```python
->>> from torch import nn
->>> from tensordict import TensorDict
->>> from tensordict.nn import make_functional
->>> import torch
->>> from torch import vmap
->>> layer1 = nn.Linear(3, 4)
->>> layer2 = nn.Linear(4, 4)
->>> model = nn.Sequential(layer1, layer2)
->>> # we represent the weights hierarchically
->>> weights1 = TensorDict(layer1.state_dict(), []).unflatten_keys(".")
->>> weights2 = TensorDict(layer2.state_dict(), []).unflatten_keys(".")
->>> params = make_functional(model)
->>> assert (params == TensorDict({"0": weights1, "1": weights2}, [])).all()
->>> # Let's use our functional module
->>> x = torch.randn(10, 3)
->>> out = model(x, params=params)  # params is the last arg (or kwarg)
->>> # an ensemble of models: we stack params along the first dimension...
->>> params_stack = torch.stack([params, params], 0)
->>> # ... and use it as an input we'd like to pass through the model
->>> y = vmap(model, (None, 0))(x, params_stack)
->>> print(y.shape)
-torch.Size([2, 10, 4])
-```
-
-Moreover, tensordict modules are compatible with `torch.fx` and `torch.compile`,
-which means that you can get the best of both worlds: a codebase that is
-both readable and future-proof as well as efficient and portable!
-
-
-### Lazy preallocation
-
-Pre-allocating tensors can be cumbersome and hard to scale if the list of preallocated
-items varies according to the script configuration. TensorDict solves this in an elegant way.
-Assume you are working with a function `foo() -> TensorDict`, e.g.
-```python
-def foo():
-    tensordict = TensorDict({}, batch_size=[])
-    tensordict["a"] = torch.randn(3)
-    tensordict["b"] = TensorDict({"c": torch.zeros(2)}, batch_size=[])
-    return tensordict
-```
-and you would like to call this function repeatedly. You could do this in two ways.
-The first would simply be to stack the calls to the function:
-```python
-tensordict = torch.stack([foo() for _ in range(N)])
-```
-However, you could also choose to preallocate the tensordict:
-```python
-tensordict = TensorDict({}, batch_size=[N])
-for i in range(N):
-    tensordict[i] = foo()
-```
-which also results in a tensordict (when `N = 10`)
-```
-TensorDict(
-    fields={
-        a: Tensor(torch.Size([10, 3]), dtype=torch.float32),
-        b: TensorDict(
-            fields={
-                c: Tensor(torch.Size([10, 2]), dtype=torch.float32)},
-            batch_size=torch.Size([10]),
-            device=None,
-            is_shared=False)},
-    batch_size=torch.Size([10]),
-    device=None,
-    is_shared=False)
-```
-When `i==0`, your empty tensordict will automatically be populated with empty tensors
-of batch-size `N`. After that, updates will be written in-place.
-Note that this would also work with a shuffled series of indices (pre-allocation does
-not require you to go through the tensordict in an ordered fashion).
-
-
-### Nesting TensorDicts
-
-It is possible to nest tensordict. The only requirement is that the sub-tensordict should be indexable
-under the parent tensordict, i.e. its batch size should match (but could be longer than) the parent
-batch size.
-
-We can switch easily between hierarchical and flat representations.
-For instance, the following code will result in a single-level tensordict with keys `"key 1"` and `"key 2.sub-key"`:
-```python
->>> tensordict = TensorDict({
-...     "key 1": torch.ones(3, 4, 5),
-...     "key 2": TensorDict({"sub-key": torch.randn(3, 4, 5, 6)}, batch_size=[3, 4, 5])
-... }, batch_size=[3, 4])
->>> tensordict_flatten = tensordict.flatten_keys(separator=".")
-```
-
-Accessing nested tensordicts can be achieved with a single index:
-```python
->>> sub_value = tensordict["key 2", "sub-key"]
-```
-
-## TensorClass
-
-Content flexibility comes at the cost of predictability.
-In some cases, developers may be looking for data structure with a more explicit behavior.
-`tensordict` provides a `dataclass`-like decorator that allows for the creation of custom dataclasses that support
-the tensordict operations:
-```python
->>> from tensordict.prototype import tensorclass
->>> import torch
->>>
->>> @tensorclass
-... class MyData:
-...    image: torch.Tensor
-...    mask: torch.Tensor
-...    label: torch.Tensor
-...
-...    def mask_image(self):
-...        return self.image[self.mask.expand_as(self.image)].view(*self.batch_size, -1)
-...
-...    def select_label(self, label):
-...        return self[self.label == label]
-...
->>> images = torch.randn(100, 3, 64, 64)
->>> label = torch.randint(10, (100,))
->>> mask = torch.zeros(1, 64, 64, dtype=torch.bool).bernoulli_().expand(100, 1, 64, 64)
->>>
->>> data = MyData(images, mask, label=label, batch_size=[100])
->>>
->>> print(data.select_label(1))
-MyData(
-    image=Tensor(torch.Size([11, 3, 64, 64]), dtype=torch.float32),
-    label=Tensor(torch.Size([11]), dtype=torch.int64),
-    mask=Tensor(torch.Size([11, 1, 64, 64]), dtype=torch.bool),
-    batch_size=torch.Size([11]),
-    device=None,
-    is_shared=False)
->>> print(data.mask_image().shape)
-torch.Size([100, 6117])
->>> print(data.reshape(10, 10))
-MyData(
-    image=Tensor(torch.Size([10, 10, 3, 64, 64]), dtype=torch.float32),
-    label=Tensor(torch.Size([10, 10]), dtype=torch.int64),
-    mask=Tensor(torch.Size([10, 10, 1, 64, 64]), dtype=torch.bool),
-    batch_size=torch.Size([10, 10]),
-    device=None,
-    is_shared=False)
-```
-As this example shows, one can write a specific data structures with dedicated methods while still enjoying the TensorDict
-artifacts such as shape operations (e.g. reshape or permutations), data manipulation (indexing, `cat` and `stack`) or calling
-arbitrary functions through the `apply` method (and many more).
-
-Tensorclasses support nesting and, in fact, all the TensorDict features.
-
-
-## Installation
-
-**With Pip**:
-
-To install the latest stable version of tensordict, simply run
-
-```bash
-pip install tensordict
-```
-
-This will work with Python 3.7 and upward as well as PyTorch 1.12 and upward.
-
-To enjoy the latest features, one can use
-
-```bash
-pip install tensordict-nightly
-```
-
-**With Conda**:
-
-Install `tensordict` from `conda-forge` channel.
-
-```sh
-conda install -c conda-forge tensordict
-```
-
-
-## Citation
-
-If you're using TensorDict, please refer to this BibTeX entry to cite this work:
-```
-@misc{bou2023torchrl,
-      title={TorchRL: A data-driven decision-making library for PyTorch}, 
-      author={Albert Bou and Matteo Bettini and Sebastian Dittert and Vikash Kumar and Shagun Sodhani and Xiaomeng Yang and Gianni De Fabritiis and Vincent Moens},
-      year={2023},
-      eprint={2306.00577},
-      archivePrefix={arXiv},
-      primaryClass={cs.LG}
-}
-```
-
-## Disclaimer
-
-TensorDict is at the *beta*-stage, meaning that there may be bc-breaking changes introduced, but 
-they should come with a warranty.
-Hopefully these should not happen too often, as the current roadmap mostly 
-involves adding new features and building compatibility with the broader
-PyTorch ecosystem.
-
-## License
-
-TensorDict is licensed under the MIT License. See [LICENSE](LICENSE) for details.
-
-
+Metadata-Version: 2.1
+Name: tensordict-nightly
+Version: 2023.8.8
+Summary: UNKNOWN
+Home-page: https://github.com/pytorch-labs/tensordict
+Author: tensordict contributors
+Author-email: vmoens@fb.com
+License: BSD
+Platform: UNKNOWN
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Development Status :: 4 - Beta
+Description-Content-Type: text/markdown
+Requires-Dist: torch
+Requires-Dist: numpy
+Requires-Dist: cloudpickle
+Provides-Extra: checkpointing
+Requires-Dist: torchsnapshot-nightly ; extra == 'checkpointing'
+Provides-Extra: h5
+Requires-Dist: h5py (>=3.8) ; extra == 'h5'
+Provides-Extra: tests
+Requires-Dist: pytest ; extra == 'tests'
+Requires-Dist: pyyaml ; extra == 'tests'
+Requires-Dist: pytest-instafail ; extra == 'tests'
+Requires-Dist: pytest-rerunfailures ; extra == 'tests'
+Requires-Dist: pytest-benchmark ; extra == 'tests'
+
+<!--- BADGES: START --->
+<!---
+[![Documentation](https://img.shields.io/badge/Documentation-blue.svg?style=flat)](https://pytorch-labs.github.io/tensordict/)
+--->
+[![Docs - GitHub.io](https://img.shields.io/static/v1?logo=github&style=flat&color=pink&label=docs&message=tensordict)][#docs-package]
+[![Benchmarks](https://img.shields.io/badge/Benchmarks-blue.svg)][#docs-package-benchmark]
+[![Python version](https://img.shields.io/pypi/pyversions/tensordict.svg)](https://www.python.org/downloads/)
+[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)][#github-license]
+<a href="https://pypi.org/project/tensordict"><img src="https://img.shields.io/pypi/v/tensordict" alt="pypi version"></a>
+<a href="https://pypi.org/project/tensordict-nightly"><img src="https://img.shields.io/pypi/v/tensordict-nightly?label=nightly" alt="pypi nightly version"></a>
+[![Downloads](https://static.pepy.tech/personalized-badge/tensordict?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads)][#pepy-package]
+[![Downloads](https://static.pepy.tech/personalized-badge/tensordict-nightly?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads%20(nightly))][#pepy-package-nightly]
+[![codecov](https://codecov.io/gh/pytorch-labs/tensordict/branch/main/graph/badge.svg?token=9QTUG6NAGQ)][#codecov-package]
+[![circleci](https://circleci.com/gh/pytorch-labs/tensordict.svg?style=shield)][#circleci-package]
+[![Conda - Platform](https://img.shields.io/conda/pn/conda-forge/tensordict?logo=anaconda&style=flat)][#conda-forge-package]
+[![Conda (channel only)](https://img.shields.io/conda/vn/conda-forge/tensordict?logo=anaconda&style=flat&color=orange)][#conda-forge-package]
+
+[#docs-package]: https://pytorch-labs.github.io/tensordict/
+[#docs-package-benchmark]: https://pytorch-labs.github.io/tensordict/dev/bench/
+[#github-license]: https://github.com/pytorch-labs/tensordict/blob/main/LICENSE
+[#pepy-package]: https://pepy.tech/project/tensordict
+[#pepy-package-nightly]: https://pepy.tech/project/tensordict-nightly
+[#codecov-package]: https://codecov.io/gh/pytorch-labs/tensordict
+[#circleci-package]: https://circleci.com/gh/pytorch-labs/tensordict
+[#conda-forge-package]: https://anaconda.org/conda-forge/tensordict
+
+<!--- BADGES: END --->
+
+# TensorDict
+
+[**Installation**](#installation) | [**General features**](#general) |
+[**Tensor-like features**](#tensor-like-features) |  [**Distributed capabilities**](#distributed-capabilities) |
+[**TensorDict for functional programming using FuncTorch**](#tensordict-for-functional-programming-using-functorch) |
+[**Lazy preallocation**](#lazy-preallocation) | [**Nesting TensorDicts**](#nesting-tensordicts) | [**TensorClass**](#tensorclass)
+
+`TensorDict` is a dictionary-like class that inherits properties from tensors,
+such as indexing, shape operations, casting to device or point-to-point communication
+in distributed settings.
+
+The main purpose of TensorDict is to make code-bases more _readable_ and _modular_ by abstracting away tailored operations:
+```python
+for i, tensordict in enumerate(dataset):
+    # the model reads and writes tensordicts
+    tensordict = model(tensordict)
+    loss = loss_module(tensordict)
+    loss.backward()
+    optimizer.step()
+    optimizer.zero_grad()
+```
+With this level of abstraction, one can recycle a training loop for highly heterogeneous task.
+Each individual step of the training loop (data collection and transform, model prediction, loss computation etc.)
+can be tailored to the use case at hand without impacting the others.
+For instance, the above example can be easily used across classification and segmentation tasks, among many others.
+
+## Features
+
+### General
+
+A tensordict is primarily defined by its `batch_size` (or `shape`) and its key-value pairs:
+```python
+>>> from tensordict import TensorDict
+>>> import torch
+>>> tensordict = TensorDict({
+...     "key 1": torch.ones(3, 4, 5),
+...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
+... }, batch_size=[3, 4])
+```
+The `batch_size` and the first dimensions of each of the tensors must be compliant.
+The tensors can be of any dtype and device. Optionally, one can restrict a tensordict to
+live on a dedicated device, which will send each tensor that is written there:
+```python
+>>> tensordict = TensorDict({
+...     "key 1": torch.ones(3, 4, 5),
+...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
+... }, batch_size=[3, 4], device="cuda:0")
+>>> tensordict["key 3"] = torch.randn(3, 4, device="cpu")
+>>> assert tensordict["key 3"].device is torch.device("cuda:0")
+```
+
+### Tensor-like features
+
+TensorDict objects can be indexed exactly like tensors. The resulting of indexing
+a TensorDict is another TensorDict containing tensors indexed along the required dimension:
+```python
+>>> tensordict = TensorDict({
+...     "key 1": torch.ones(3, 4, 5),
+...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
+... }, batch_size=[3, 4])
+>>> sub_tensordict = tensordict[..., :2]
+>>> assert sub_tensordict.shape == torch.Size([3, 2])
+>>> assert sub_tensordict["key 1"].shape == torch.Size([3, 2, 5])
+```
+
+Similarly, one can build tensordicts by stacking or concatenating single tensordicts:
+```python
+>>> tensordicts = [TensorDict({
+...     "key 1": torch.ones(3, 4, 5),
+...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
+... }, batch_size=[3, 4]) for _ in range(2)]
+>>> stack_tensordict = torch.stack(tensordicts, 1)
+>>> assert stack_tensordict.shape == torch.Size([3, 2, 4])
+>>> assert stack_tensordict["key 1"].shape == torch.Size([3, 2, 4, 5])
+>>> cat_tensordict = torch.cat(tensordicts, 0)
+>>> assert cat_tensordict.shape == torch.Size([6, 4])
+>>> assert cat_tensordict["key 1"].shape == torch.Size([6, 4, 5])
+```
+
+TensorDict instances can also be reshaped, viewed, squeezed and unsqueezed:
+```python
+>>> tensordict = TensorDict({
+...     "key 1": torch.ones(3, 4, 5),
+...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
+... }, batch_size=[3, 4])
+>>> print(tensordict.view(-1))
+torch.Size([12])
+>>> print(tensordict.reshape(-1))
+torch.Size([12])
+>>> print(tensordict.unsqueeze(-1))
+torch.Size([3, 4, 1])
+```
+
+One can also send tensordict from device to device, place them in shared memory,
+clone them, update them in-place or not, split them, unbind them, expand them etc.
+
+If a functionality is missing, it is easy to call it using `apply()` or `apply_()`:
+```python
+tensordict_uniform = tensordict.apply(lambda tensor: tensor.uniform_())
+```
+### Distributed capabilities
+
+Complex data structures can be cumbersome to synchronize in distributed settings.
+`tensordict` solves that problem with synchronous and asynchronous helper methods
+such as `recv`, `irecv`, `send` and `isend` that behave like their `torch.distributed`
+counterparts:
+```python
+>>> # on all workers
+>>> data = TensorDict({"a": torch.zeros(()), ("b", "c"): torch.ones(())}, [])
+>>> # on worker 1
+>>> data.isend(dst=0)
+>>> # on worker 0
+>>> data.irecv(src=1)
+```
+
+When nodes share a common scratch space, the
+[`MemmapTensor` backend](https://pytorch-labs.github.io/tensordict/tutorials/tensordict_memory.html)
+can be used
+to seamlessly send, receive and read a huge amount of data.
+
+### TensorDict for functional programming using FuncTorch
+
+We also provide an API to use TensorDict in conjunction with [FuncTorch](https://pytorch.org/functorch).
+For instance, TensorDict makes it easy to concatenate model weights to do model ensembling:
+```python
+>>> from torch import nn
+>>> from tensordict import TensorDict
+>>> from tensordict.nn import make_functional
+>>> import torch
+>>> from torch import vmap
+>>> layer1 = nn.Linear(3, 4)
+>>> layer2 = nn.Linear(4, 4)
+>>> model = nn.Sequential(layer1, layer2)
+>>> # we represent the weights hierarchically
+>>> weights1 = TensorDict(layer1.state_dict(), []).unflatten_keys(".")
+>>> weights2 = TensorDict(layer2.state_dict(), []).unflatten_keys(".")
+>>> params = make_functional(model)
+>>> assert (params == TensorDict({"0": weights1, "1": weights2}, [])).all()
+>>> # Let's use our functional module
+>>> x = torch.randn(10, 3)
+>>> out = model(x, params=params)  # params is the last arg (or kwarg)
+>>> # an ensemble of models: we stack params along the first dimension...
+>>> params_stack = torch.stack([params, params], 0)
+>>> # ... and use it as an input we'd like to pass through the model
+>>> y = vmap(model, (None, 0))(x, params_stack)
+>>> print(y.shape)
+torch.Size([2, 10, 4])
+```
+
+Moreover, tensordict modules are compatible with `torch.fx` and `torch.compile`,
+which means that you can get the best of both worlds: a codebase that is
+both readable and future-proof as well as efficient and portable!
+
+
+### Lazy preallocation
+
+Pre-allocating tensors can be cumbersome and hard to scale if the list of preallocated
+items varies according to the script configuration. TensorDict solves this in an elegant way.
+Assume you are working with a function `foo() -> TensorDict`, e.g.
+```python
+def foo():
+    tensordict = TensorDict({}, batch_size=[])
+    tensordict["a"] = torch.randn(3)
+    tensordict["b"] = TensorDict({"c": torch.zeros(2)}, batch_size=[])
+    return tensordict
+```
+and you would like to call this function repeatedly. You could do this in two ways.
+The first would simply be to stack the calls to the function:
+```python
+tensordict = torch.stack([foo() for _ in range(N)])
+```
+However, you could also choose to preallocate the tensordict:
+```python
+tensordict = TensorDict({}, batch_size=[N])
+for i in range(N):
+    tensordict[i] = foo()
+```
+which also results in a tensordict (when `N = 10`)
+```
+TensorDict(
+    fields={
+        a: Tensor(torch.Size([10, 3]), dtype=torch.float32),
+        b: TensorDict(
+            fields={
+                c: Tensor(torch.Size([10, 2]), dtype=torch.float32)},
+            batch_size=torch.Size([10]),
+            device=None,
+            is_shared=False)},
+    batch_size=torch.Size([10]),
+    device=None,
+    is_shared=False)
+```
+When `i==0`, your empty tensordict will automatically be populated with empty tensors
+of batch-size `N`. After that, updates will be written in-place.
+Note that this would also work with a shuffled series of indices (pre-allocation does
+not require you to go through the tensordict in an ordered fashion).
+
+
+### Nesting TensorDicts
+
+It is possible to nest tensordict. The only requirement is that the sub-tensordict should be indexable
+under the parent tensordict, i.e. its batch size should match (but could be longer than) the parent
+batch size.
+
+We can switch easily between hierarchical and flat representations.
+For instance, the following code will result in a single-level tensordict with keys `"key 1"` and `"key 2.sub-key"`:
+```python
+>>> tensordict = TensorDict({
+...     "key 1": torch.ones(3, 4, 5),
+...     "key 2": TensorDict({"sub-key": torch.randn(3, 4, 5, 6)}, batch_size=[3, 4, 5])
+... }, batch_size=[3, 4])
+>>> tensordict_flatten = tensordict.flatten_keys(separator=".")
+```
+
+Accessing nested tensordicts can be achieved with a single index:
+```python
+>>> sub_value = tensordict["key 2", "sub-key"]
+```
+
+## TensorClass
+
+Content flexibility comes at the cost of predictability.
+In some cases, developers may be looking for data structure with a more explicit behavior.
+`tensordict` provides a `dataclass`-like decorator that allows for the creation of custom dataclasses that support
+the tensordict operations:
+```python
+>>> from tensordict.prototype import tensorclass
+>>> import torch
+>>>
+>>> @tensorclass
+... class MyData:
+...    image: torch.Tensor
+...    mask: torch.Tensor
+...    label: torch.Tensor
+...
+...    def mask_image(self):
+...        return self.image[self.mask.expand_as(self.image)].view(*self.batch_size, -1)
+...
+...    def select_label(self, label):
+...        return self[self.label == label]
+...
+>>> images = torch.randn(100, 3, 64, 64)
+>>> label = torch.randint(10, (100,))
+>>> mask = torch.zeros(1, 64, 64, dtype=torch.bool).bernoulli_().expand(100, 1, 64, 64)
+>>>
+>>> data = MyData(images, mask, label=label, batch_size=[100])
+>>>
+>>> print(data.select_label(1))
+MyData(
+    image=Tensor(torch.Size([11, 3, 64, 64]), dtype=torch.float32),
+    label=Tensor(torch.Size([11]), dtype=torch.int64),
+    mask=Tensor(torch.Size([11, 1, 64, 64]), dtype=torch.bool),
+    batch_size=torch.Size([11]),
+    device=None,
+    is_shared=False)
+>>> print(data.mask_image().shape)
+torch.Size([100, 6117])
+>>> print(data.reshape(10, 10))
+MyData(
+    image=Tensor(torch.Size([10, 10, 3, 64, 64]), dtype=torch.float32),
+    label=Tensor(torch.Size([10, 10]), dtype=torch.int64),
+    mask=Tensor(torch.Size([10, 10, 1, 64, 64]), dtype=torch.bool),
+    batch_size=torch.Size([10, 10]),
+    device=None,
+    is_shared=False)
+```
+As this example shows, one can write a specific data structures with dedicated methods while still enjoying the TensorDict
+artifacts such as shape operations (e.g. reshape or permutations), data manipulation (indexing, `cat` and `stack`) or calling
+arbitrary functions through the `apply` method (and many more).
+
+Tensorclasses support nesting and, in fact, all the TensorDict features.
+
+
+## Installation
+
+**With Pip**:
+
+To install the latest stable version of tensordict, simply run
+
+```bash
+pip install tensordict
+```
+
+This will work with Python 3.7 and upward as well as PyTorch 1.12 and upward.
+
+To enjoy the latest features, one can use
+
+```bash
+pip install tensordict-nightly
+```
+
+**With Conda**:
+
+Install `tensordict` from `conda-forge` channel.
+
+```sh
+conda install -c conda-forge tensordict
+```
+
+
+## Citation
+
+If you're using TensorDict, please refer to this BibTeX entry to cite this work:
+```
+@misc{bou2023torchrl,
+      title={TorchRL: A data-driven decision-making library for PyTorch}, 
+      author={Albert Bou and Matteo Bettini and Sebastian Dittert and Vikash Kumar and Shagun Sodhani and Xiaomeng Yang and Gianni De Fabritiis and Vincent Moens},
+      year={2023},
+      eprint={2306.00577},
+      archivePrefix={arXiv},
+      primaryClass={cs.LG}
+}
+```
+
+## Disclaimer
+
+TensorDict is at the *beta*-stage, meaning that there may be bc-breaking changes introduced, but 
+they should come with a warranty.
+Hopefully these should not happen too often, as the current roadmap mostly 
+involves adding new features and building compatibility with the broader
+PyTorch ecosystem.
+
+## License
+
+TensorDict is licensed under the MIT License. See [LICENSE](LICENSE) for details.
+
+
```

### html2text {}

```diff
@@ -1,26 +1,25 @@
-Metadata-Version: 2.1 Name: tensordict-nightly Version: 2023.8.7 Summary:
+Metadata-Version: 2.1 Name: tensordict-nightly Version: 2023.8.8 Summary:
 UNKNOWN Home-page: https://github.com/pytorch-labs/tensordict Author:
 tensordict contributors Author-email: vmoens@fb.com License: BSD Platform:
 UNKNOWN Classifier: Programming Language :: Python :: 3.7 Classifier:
 Programming Language :: Python :: 3.8 Classifier: Programming Language ::
 Python :: 3.9 Classifier: Programming Language :: Python :: 3.10 Classifier:
-Development Status :: 4 - Beta Description-Content-Type: text/markdown License-
-File: LICENSE Requires-Dist: torch Requires-Dist: numpy Requires-Dist:
-cloudpickle Provides-Extra: checkpointing Requires-Dist: torchsnapshot-nightly
-; extra == 'checkpointing' Provides-Extra: h5 Requires-Dist: h5py >=3.8 ; extra
-== 'h5' Provides-Extra: tests Requires-Dist: pytest ; extra == 'tests'
-Requires-Dist: pyyaml ; extra == 'tests' Requires-Dist: pytest-instafail ;
-extra == 'tests' Requires-Dist: pytest-rerunfailures ; extra == 'tests'
-Requires-Dist: pytest-benchmark ; extra == 'tests'   [![Docs - GitHub.io]
-(https://img.shields.io/static/
-v1?logo=github&style=flat&color=pink&label=docs&message=tensordict)][#docs-
-package] [![Benchmarks](https://img.shields.io/badge/Benchmarks-blue.svg)]
-[#docs-package-benchmark] [![Python version](https://img.shields.io/pypi/
-pyversions/tensordict.svg)](https://www.python.org/downloads/) [![GitHub
+Development Status :: 4 - Beta Description-Content-Type: text/markdown
+Requires-Dist: torch Requires-Dist: numpy Requires-Dist: cloudpickle Provides-
+Extra: checkpointing Requires-Dist: torchsnapshot-nightly ; extra ==
+'checkpointing' Provides-Extra: h5 Requires-Dist: h5py (>=3.8) ; extra == 'h5'
+Provides-Extra: tests Requires-Dist: pytest ; extra == 'tests' Requires-Dist:
+pyyaml ; extra == 'tests' Requires-Dist: pytest-instafail ; extra == 'tests'
+Requires-Dist: pytest-rerunfailures ; extra == 'tests' Requires-Dist: pytest-
+benchmark ; extra == 'tests'   [![Docs - GitHub.io](https://img.shields.io/
+static/v1?logo=github&style=flat&color=pink&label=docs&message=tensordict)]
+[#docs-package] [![Benchmarks](https://img.shields.io/badge/Benchmarks-
+blue.svg)][#docs-package-benchmark] [![Python version](https://img.shields.io/
+pypi/pyversions/tensordict.svg)](https://www.python.org/downloads/) [![GitHub
 license](https://img.shields.io/badge/license-MIT-blue.svg)][#github-license]
 [pypi_version] [pypi_nightly_version] [![Downloads](https://static.pepy.tech/
 personalized-badge/
 tensordict?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads)]
 [#pepy-package] [![Downloads](https://static.pepy.tech/personalized-badge/
 tensordict-
 nightly?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads%20
```

