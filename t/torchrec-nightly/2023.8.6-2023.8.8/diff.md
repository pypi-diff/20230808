# Comparing `tmp/torchrec_nightly-2023.8.6-py39-none-any.whl.zip` & `tmp/torchrec_nightly-2023.8.8-py39-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,150 +1,150 @@
-Zip file size: 373252 bytes, number of entries: 148
--rw-r--r--  2.0 unx      811 b- defN 23-Aug-06 11:17 torchrec/__init__.py
--rw-r--r--  2.0 unx     1638 b- defN 23-Aug-06 11:17 torchrec/streamable.py
--rw-r--r--  2.0 unx      854 b- defN 23-Aug-06 11:17 torchrec/types.py
--rw-r--r--  2.0 unx     1153 b- defN 23-Aug-06 11:17 torchrec/datasets/__init__.py
--rw-r--r--  2.0 unx    41469 b- defN 23-Aug-06 11:17 torchrec/datasets/criteo.py
--rw-r--r--  2.0 unx     4548 b- defN 23-Aug-06 11:17 torchrec/datasets/movielens.py
--rw-r--r--  2.0 unx     6539 b- defN 23-Aug-06 11:17 torchrec/datasets/random.py
--rw-r--r--  2.0 unx    10909 b- defN 23-Aug-06 11:17 torchrec/datasets/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Aug-06 11:17 torchrec/datasets/scripts/__init__.py
--rw-r--r--  2.0 unx     2448 b- defN 23-Aug-06 11:17 torchrec/datasets/scripts/contiguous_preproc_criteo.py
--rw-r--r--  2.0 unx     2847 b- defN 23-Aug-06 11:17 torchrec/datasets/scripts/npy_preproc_criteo.py
--rw-r--r--  2.0 unx     3077 b- defN 23-Aug-06 11:17 torchrec/datasets/scripts/shuffle_preproc_criteo.py
--rw-r--r--  2.0 unx        0 b- defN 23-Aug-06 11:17 torchrec/datasets/test_utils/__init__.py
--rw-r--r--  2.0 unx     5308 b- defN 23-Aug-06 11:17 torchrec/datasets/test_utils/criteo_test_utils.py
--rw-r--r--  2.0 unx     1945 b- defN 23-Aug-06 11:17 torchrec/distributed/__init__.py
--rw-r--r--  2.0 unx    37307 b- defN 23-Aug-06 11:17 torchrec/distributed/batched_embedding_kernel.py
--rw-r--r--  2.0 unx     2069 b- defN 23-Aug-06 11:17 torchrec/distributed/collective_utils.py
--rw-r--r--  2.0 unx     4988 b- defN 23-Aug-06 11:17 torchrec/distributed/comm.py
--rw-r--r--  2.0 unx    56136 b- defN 23-Aug-06 11:17 torchrec/distributed/comm_ops.py
--rw-r--r--  2.0 unx    36889 b- defN 23-Aug-06 11:17 torchrec/distributed/dist_data.py
--rw-r--r--  2.0 unx    32295 b- defN 23-Aug-06 11:17 torchrec/distributed/embedding.py
--rw-r--r--  2.0 unx     4947 b- defN 23-Aug-06 11:17 torchrec/distributed/embedding_kernel.py
--rw-r--r--  2.0 unx    30169 b- defN 23-Aug-06 11:17 torchrec/distributed/embedding_lookup.py
--rw-r--r--  2.0 unx    19035 b- defN 23-Aug-06 11:17 torchrec/distributed/embedding_sharding.py
--rw-r--r--  2.0 unx    36853 b- defN 23-Aug-06 11:17 torchrec/distributed/embedding_tower_sharding.py
--rw-r--r--  2.0 unx    15386 b- defN 23-Aug-06 11:17 torchrec/distributed/embedding_types.py
--rw-r--r--  2.0 unx    37310 b- defN 23-Aug-06 11:17 torchrec/distributed/embeddingbag.py
--rw-r--r--  2.0 unx     7373 b- defN 23-Aug-06 11:17 torchrec/distributed/fbgemm_qcomm_codec.py
--rw-r--r--  2.0 unx     6219 b- defN 23-Aug-06 11:17 torchrec/distributed/fp_embeddingbag.py
--rw-r--r--  2.0 unx     5243 b- defN 23-Aug-06 11:17 torchrec/distributed/fused_embedding.py
--rw-r--r--  2.0 unx     5080 b- defN 23-Aug-06 11:17 torchrec/distributed/fused_embeddingbag.py
--rw-r--r--  2.0 unx     2271 b- defN 23-Aug-06 11:17 torchrec/distributed/fused_params.py
--rw-r--r--  2.0 unx     3807 b- defN 23-Aug-06 11:17 torchrec/distributed/grouped_position_weighted.py
--rw-r--r--  2.0 unx    10997 b- defN 23-Aug-06 11:17 torchrec/distributed/mc_embeddingbag.py
--rw-r--r--  2.0 unx    19750 b- defN 23-Aug-06 11:17 torchrec/distributed/model_parallel.py
--rw-r--r--  2.0 unx    20748 b- defN 23-Aug-06 11:17 torchrec/distributed/quant_embedding.py
--rw-r--r--  2.0 unx    15112 b- defN 23-Aug-06 11:17 torchrec/distributed/quant_embedding_kernel.py
--rw-r--r--  2.0 unx    12621 b- defN 23-Aug-06 11:17 torchrec/distributed/quant_embeddingbag.py
--rw-r--r--  2.0 unx    13928 b- defN 23-Aug-06 11:17 torchrec/distributed/quant_state.py
--rw-r--r--  2.0 unx     9261 b- defN 23-Aug-06 11:17 torchrec/distributed/shard.py
--rw-r--r--  2.0 unx    19906 b- defN 23-Aug-06 11:17 torchrec/distributed/sharding_plan.py
--rw-r--r--  2.0 unx    48752 b- defN 23-Aug-06 11:17 torchrec/distributed/train_pipeline.py
--rw-r--r--  2.0 unx    26441 b- defN 23-Aug-06 11:17 torchrec/distributed/types.py
--rw-r--r--  2.0 unx    15470 b- defN 23-Aug-06 11:17 torchrec/distributed/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Aug-06 11:17 torchrec/distributed/composable/__init__.py
--rw-r--r--  2.0 unx     3207 b- defN 23-Aug-06 11:17 torchrec/distributed/composable/table_batched_embedding_slice.py
--rw-r--r--  2.0 unx     1025 b- defN 23-Aug-06 11:17 torchrec/distributed/planner/__init__.py
--rw-r--r--  2.0 unx     3135 b- defN 23-Aug-06 11:17 torchrec/distributed/planner/constants.py
--rw-r--r--  2.0 unx    11430 b- defN 23-Aug-06 11:17 torchrec/distributed/planner/enumerators.py
--rw-r--r--  2.0 unx    12642 b- defN 23-Aug-06 11:17 torchrec/distributed/planner/partitioners.py
--rw-r--r--  2.0 unx      835 b- defN 23-Aug-06 11:17 torchrec/distributed/planner/perf_models.py
--rw-r--r--  2.0 unx    13803 b- defN 23-Aug-06 11:17 torchrec/distributed/planner/planners.py
--rw-r--r--  2.0 unx    11236 b- defN 23-Aug-06 11:17 torchrec/distributed/planner/proposers.py
--rw-r--r--  2.0 unx    41291 b- defN 23-Aug-06 11:17 torchrec/distributed/planner/shard_estimators.py
--rw-r--r--  2.0 unx    24082 b- defN 23-Aug-06 11:17 torchrec/distributed/planner/stats.py
--rw-r--r--  2.0 unx    12858 b- defN 23-Aug-06 11:17 torchrec/distributed/planner/storage_reservations.py
--rw-r--r--  2.0 unx    14582 b- defN 23-Aug-06 11:17 torchrec/distributed/planner/types.py
--rw-r--r--  2.0 unx     1431 b- defN 23-Aug-06 11:17 torchrec/distributed/planner/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Aug-06 11:17 torchrec/distributed/sharding/__init__.py
--rw-r--r--  2.0 unx     2479 b- defN 23-Aug-06 11:17 torchrec/distributed/sharding/cw_sequence_sharding.py
--rw-r--r--  2.0 unx    12945 b- defN 23-Aug-06 11:17 torchrec/distributed/sharding/cw_sharding.py
--rw-r--r--  2.0 unx     2802 b- defN 23-Aug-06 11:17 torchrec/distributed/sharding/dp_sequence_sharding.py
--rw-r--r--  2.0 unx     7681 b- defN 23-Aug-06 11:17 torchrec/distributed/sharding/dp_sharding.py
--rw-r--r--  2.0 unx     7640 b- defN 23-Aug-06 11:17 torchrec/distributed/sharding/rw_sequence_sharding.py
--rw-r--r--  2.0 unx    17911 b- defN 23-Aug-06 11:17 torchrec/distributed/sharding/rw_sharding.py
--rw-r--r--  2.0 unx     3627 b- defN 23-Aug-06 11:17 torchrec/distributed/sharding/sequence_sharding.py
--rw-r--r--  2.0 unx     7632 b- defN 23-Aug-06 11:17 torchrec/distributed/sharding/tw_sequence_sharding.py
--rw-r--r--  2.0 unx    16097 b- defN 23-Aug-06 11:17 torchrec/distributed/sharding/tw_sharding.py
--rw-r--r--  2.0 unx     1284 b- defN 23-Aug-06 11:17 torchrec/distributed/sharding/twcw_sharding.py
--rw-r--r--  2.0 unx    19871 b- defN 23-Aug-06 11:17 torchrec/distributed/sharding/twrw_sharding.py
--rw-r--r--  2.0 unx        0 b- defN 23-Aug-06 11:17 torchrec/distributed/test_utils/__init__.py
--rw-r--r--  2.0 unx    11237 b- defN 23-Aug-06 11:17 torchrec/distributed/test_utils/infer_utils.py
--rw-r--r--  2.0 unx     4868 b- defN 23-Aug-06 11:17 torchrec/distributed/test_utils/multi_process.py
--rw-r--r--  2.0 unx    34993 b- defN 23-Aug-06 11:17 torchrec/distributed/test_utils/test_model.py
--rw-r--r--  2.0 unx    11197 b- defN 23-Aug-06 11:17 torchrec/distributed/test_utils/test_model_parallel.py
--rw-r--r--  2.0 unx    25310 b- defN 23-Aug-06 11:17 torchrec/distributed/test_utils/test_model_parallel_base.py
--rw-r--r--  2.0 unx    15367 b- defN 23-Aug-06 11:17 torchrec/distributed/test_utils/test_sharding.py
--rw-r--r--  2.0 unx      422 b- defN 23-Aug-06 11:17 torchrec/fx/__init__.py
--rw-r--r--  2.0 unx     6451 b- defN 23-Aug-06 11:17 torchrec/fx/tracer.py
--rw-r--r--  2.0 unx     4524 b- defN 23-Aug-06 11:17 torchrec/fx/utils.py
--rw-r--r--  2.0 unx     1223 b- defN 23-Aug-06 11:17 torchrec/inference/__init__.py
--rw-r--r--  2.0 unx     3614 b- defN 23-Aug-06 11:17 torchrec/inference/client.py
--rw-r--r--  2.0 unx     3957 b- defN 23-Aug-06 11:17 torchrec/inference/model_packager.py
--rw-r--r--  2.0 unx     8068 b- defN 23-Aug-06 11:17 torchrec/inference/modules.py
--rw-r--r--  2.0 unx     3797 b- defN 23-Aug-06 11:17 torchrec/inference/state_dict_transform.py
--rw-r--r--  2.0 unx        0 b- defN 23-Aug-06 11:17 torchrec/metrics/__init__.py
--rw-r--r--  2.0 unx     4168 b- defN 23-Aug-06 11:17 torchrec/metrics/accuracy.py
--rw-r--r--  2.0 unx    12549 b- defN 23-Aug-06 11:17 torchrec/metrics/auc.py
--rw-r--r--  2.0 unx     3703 b- defN 23-Aug-06 11:17 torchrec/metrics/calibration.py
--rw-r--r--  2.0 unx     3465 b- defN 23-Aug-06 11:17 torchrec/metrics/ctr.py
--rw-r--r--  2.0 unx     3836 b- defN 23-Aug-06 11:17 torchrec/metrics/mae.py
--rw-r--r--  2.0 unx    17990 b- defN 23-Aug-06 11:17 torchrec/metrics/metric_module.py
--rw-r--r--  2.0 unx     6796 b- defN 23-Aug-06 11:17 torchrec/metrics/metrics_config.py
--rw-r--r--  2.0 unx     3731 b- defN 23-Aug-06 11:17 torchrec/metrics/metrics_namespace.py
--rw-r--r--  2.0 unx     3904 b- defN 23-Aug-06 11:17 torchrec/metrics/model_utils.py
--rw-r--r--  2.0 unx     4631 b- defN 23-Aug-06 11:17 torchrec/metrics/mse.py
--rw-r--r--  2.0 unx     5605 b- defN 23-Aug-06 11:17 torchrec/metrics/multiclass_recall.py
--rw-r--r--  2.0 unx     8735 b- defN 23-Aug-06 11:17 torchrec/metrics/ndcg.py
--rw-r--r--  2.0 unx     6811 b- defN 23-Aug-06 11:17 torchrec/metrics/ne.py
--rw-r--r--  2.0 unx    33554 b- defN 23-Aug-06 11:17 torchrec/metrics/rec_metric.py
--rw-r--r--  2.0 unx    10490 b- defN 23-Aug-06 11:17 torchrec/metrics/recall_session.py
--rw-r--r--  2.0 unx     6057 b- defN 23-Aug-06 11:17 torchrec/metrics/throughput.py
--rw-r--r--  2.0 unx    11160 b- defN 23-Aug-06 11:17 torchrec/metrics/tower_qps.py
--rw-r--r--  2.0 unx     2867 b- defN 23-Aug-06 11:17 torchrec/metrics/weighted_avg.py
--rw-r--r--  2.0 unx    16441 b- defN 23-Aug-06 11:17 torchrec/metrics/test_utils/__init__.py
--rw-r--r--  2.0 unx      913 b- defN 23-Aug-06 11:17 torchrec/models/__init__.py
--rw-r--r--  2.0 unx    11410 b- defN 23-Aug-06 11:17 torchrec/models/deepfm.py
--rw-r--r--  2.0 unx    29965 b- defN 23-Aug-06 11:17 torchrec/models/dlrm.py
--rw-r--r--  2.0 unx        0 b- defN 23-Aug-06 11:17 torchrec/models/experimental/__init__.py
--rw-r--r--  2.0 unx     9825 b- defN 23-Aug-06 11:17 torchrec/models/experimental/test_transformerdlrm.py
--rw-r--r--  2.0 unx     7434 b- defN 23-Aug-06 11:17 torchrec/models/experimental/transformerdlrm.py
--rw-r--r--  2.0 unx     1179 b- defN 23-Aug-06 11:17 torchrec/modules/__init__.py
--rw-r--r--  2.0 unx     1456 b- defN 23-Aug-06 11:17 torchrec/modules/activation.py
--rw-r--r--  2.0 unx    14636 b- defN 23-Aug-06 11:17 torchrec/modules/crossnet.py
--rw-r--r--  2.0 unx     8415 b- defN 23-Aug-06 11:17 torchrec/modules/deepfm.py
--rw-r--r--  2.0 unx     6468 b- defN 23-Aug-06 11:17 torchrec/modules/embedding_configs.py
--rw-r--r--  2.0 unx    14021 b- defN 23-Aug-06 11:17 torchrec/modules/embedding_modules.py
--rw-r--r--  2.0 unx     4858 b- defN 23-Aug-06 11:17 torchrec/modules/embedding_tower.py
--rw-r--r--  2.0 unx    12360 b- defN 23-Aug-06 11:17 torchrec/modules/feature_processor.py
--rw-r--r--  2.0 unx     4964 b- defN 23-Aug-06 11:17 torchrec/modules/feature_processor_.py
--rw-r--r--  2.0 unx     4960 b- defN 23-Aug-06 11:17 torchrec/modules/fp_embedding_modules.py
--rw-r--r--  2.0 unx    31545 b- defN 23-Aug-06 11:17 torchrec/modules/fused_embedding_modules.py
--rw-r--r--  2.0 unx    10533 b- defN 23-Aug-06 11:17 torchrec/modules/lazy_extension.py
--rw-r--r--  2.0 unx     3143 b- defN 23-Aug-06 11:17 torchrec/modules/managed_collision_modules.py
--rw-r--r--  2.0 unx     5124 b- defN 23-Aug-06 11:17 torchrec/modules/mc_embedding_modules.py
--rw-r--r--  2.0 unx     6309 b- defN 23-Aug-06 11:17 torchrec/modules/mlp.py
--rw-r--r--  2.0 unx     3897 b- defN 23-Aug-06 11:17 torchrec/modules/utils.py
--rw-r--r--  2.0 unx     1639 b- defN 23-Aug-06 11:17 torchrec/optim/__init__.py
--rw-r--r--  2.0 unx     2012 b- defN 23-Aug-06 11:17 torchrec/optim/apply_optimizer_in_backward.py
--rw-r--r--  2.0 unx     1569 b- defN 23-Aug-06 11:17 torchrec/optim/clipping.py
--rw-r--r--  2.0 unx     1353 b- defN 23-Aug-06 11:17 torchrec/optim/fused.py
--rw-r--r--  2.0 unx    16069 b- defN 23-Aug-06 11:17 torchrec/optim/keyed.py
--rw-r--r--  2.0 unx     4420 b- defN 23-Aug-06 11:17 torchrec/optim/optimizers.py
--rw-r--r--  2.0 unx     7405 b- defN 23-Aug-06 11:17 torchrec/optim/rowwise_adagrad.py
--rw-r--r--  2.0 unx     4865 b- defN 23-Aug-06 11:17 torchrec/optim/warmup.py
--rw-r--r--  2.0 unx      560 b- defN 23-Aug-06 11:17 torchrec/optim/test_utils/__init__.py
--rw-r--r--  2.0 unx     1140 b- defN 23-Aug-06 11:17 torchrec/quant/__init__.py
--rw-r--r--  2.0 unx    26618 b- defN 23-Aug-06 11:17 torchrec/quant/embedding_modules.py
--rw-r--r--  2.0 unx     4292 b- defN 23-Aug-06 11:17 torchrec/quant/utils.py
--rw-r--r--  2.0 unx     1163 b- defN 23-Aug-06 11:17 torchrec/sparse/__init__.py
--rw-r--r--  2.0 unx    56427 b- defN 23-Aug-06 11:17 torchrec/sparse/jagged_tensor.py
--rw-r--r--  2.0 unx     1430 b- defN 23-Aug-06 11:17 torchrec/sparse/test_utils/__init__.py
--rw-r--r--  2.0 unx     5661 b- defN 23-Aug-06 11:17 torchrec/test_utils/__init__.py
--rw-r--r--  2.0 unx     1530 b- defN 23-Aug-06 11:22 torchrec_nightly-2023.8.6.dist-info/LICENSE
--rw-r--r--  2.0 unx     5011 b- defN 23-Aug-06 11:22 torchrec_nightly-2023.8.6.dist-info/METADATA
--rw-r--r--  2.0 unx       93 b- defN 23-Aug-06 11:22 torchrec_nightly-2023.8.6.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 23-Aug-06 11:22 torchrec_nightly-2023.8.6.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    13819 b- defN 23-Aug-06 11:22 torchrec_nightly-2023.8.6.dist-info/RECORD
-148 files, 1543456 bytes uncompressed, 351152 bytes compressed:  77.2%
+Zip file size: 374619 bytes, number of entries: 148
+-rw-r--r--  2.0 unx      811 b- defN 23-Aug-08 11:17 torchrec/__init__.py
+-rw-r--r--  2.0 unx     1638 b- defN 23-Aug-08 11:17 torchrec/streamable.py
+-rw-r--r--  2.0 unx      854 b- defN 23-Aug-08 11:17 torchrec/types.py
+-rw-r--r--  2.0 unx     1153 b- defN 23-Aug-08 11:17 torchrec/datasets/__init__.py
+-rw-r--r--  2.0 unx    41469 b- defN 23-Aug-08 11:17 torchrec/datasets/criteo.py
+-rw-r--r--  2.0 unx     4548 b- defN 23-Aug-08 11:17 torchrec/datasets/movielens.py
+-rw-r--r--  2.0 unx     6539 b- defN 23-Aug-08 11:17 torchrec/datasets/random.py
+-rw-r--r--  2.0 unx    10909 b- defN 23-Aug-08 11:17 torchrec/datasets/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:17 torchrec/datasets/scripts/__init__.py
+-rw-r--r--  2.0 unx     2448 b- defN 23-Aug-08 11:17 torchrec/datasets/scripts/contiguous_preproc_criteo.py
+-rw-r--r--  2.0 unx     2847 b- defN 23-Aug-08 11:17 torchrec/datasets/scripts/npy_preproc_criteo.py
+-rw-r--r--  2.0 unx     3077 b- defN 23-Aug-08 11:17 torchrec/datasets/scripts/shuffle_preproc_criteo.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:17 torchrec/datasets/test_utils/__init__.py
+-rw-r--r--  2.0 unx     5308 b- defN 23-Aug-08 11:17 torchrec/datasets/test_utils/criteo_test_utils.py
+-rw-r--r--  2.0 unx     1945 b- defN 23-Aug-08 11:17 torchrec/distributed/__init__.py
+-rw-r--r--  2.0 unx    37307 b- defN 23-Aug-08 11:17 torchrec/distributed/batched_embedding_kernel.py
+-rw-r--r--  2.0 unx     2069 b- defN 23-Aug-08 11:17 torchrec/distributed/collective_utils.py
+-rw-r--r--  2.0 unx     4988 b- defN 23-Aug-08 11:17 torchrec/distributed/comm.py
+-rw-r--r--  2.0 unx    56136 b- defN 23-Aug-08 11:17 torchrec/distributed/comm_ops.py
+-rw-r--r--  2.0 unx    36889 b- defN 23-Aug-08 11:17 torchrec/distributed/dist_data.py
+-rw-r--r--  2.0 unx    32295 b- defN 23-Aug-08 11:17 torchrec/distributed/embedding.py
+-rw-r--r--  2.0 unx     4947 b- defN 23-Aug-08 11:17 torchrec/distributed/embedding_kernel.py
+-rw-r--r--  2.0 unx    30169 b- defN 23-Aug-08 11:17 torchrec/distributed/embedding_lookup.py
+-rw-r--r--  2.0 unx    19035 b- defN 23-Aug-08 11:17 torchrec/distributed/embedding_sharding.py
+-rw-r--r--  2.0 unx    36853 b- defN 23-Aug-08 11:17 torchrec/distributed/embedding_tower_sharding.py
+-rw-r--r--  2.0 unx    15386 b- defN 23-Aug-08 11:17 torchrec/distributed/embedding_types.py
+-rw-r--r--  2.0 unx    37310 b- defN 23-Aug-08 11:17 torchrec/distributed/embeddingbag.py
+-rw-r--r--  2.0 unx     7373 b- defN 23-Aug-08 11:17 torchrec/distributed/fbgemm_qcomm_codec.py
+-rw-r--r--  2.0 unx     6219 b- defN 23-Aug-08 11:17 torchrec/distributed/fp_embeddingbag.py
+-rw-r--r--  2.0 unx     5243 b- defN 23-Aug-08 11:17 torchrec/distributed/fused_embedding.py
+-rw-r--r--  2.0 unx     5080 b- defN 23-Aug-08 11:17 torchrec/distributed/fused_embeddingbag.py
+-rw-r--r--  2.0 unx     2271 b- defN 23-Aug-08 11:17 torchrec/distributed/fused_params.py
+-rw-r--r--  2.0 unx     3807 b- defN 23-Aug-08 11:17 torchrec/distributed/grouped_position_weighted.py
+-rw-r--r--  2.0 unx    10997 b- defN 23-Aug-08 11:17 torchrec/distributed/mc_embeddingbag.py
+-rw-r--r--  2.0 unx    19750 b- defN 23-Aug-08 11:17 torchrec/distributed/model_parallel.py
+-rw-r--r--  2.0 unx    26936 b- defN 23-Aug-08 11:17 torchrec/distributed/quant_embedding.py
+-rw-r--r--  2.0 unx    15112 b- defN 23-Aug-08 11:17 torchrec/distributed/quant_embedding_kernel.py
+-rw-r--r--  2.0 unx    12621 b- defN 23-Aug-08 11:17 torchrec/distributed/quant_embeddingbag.py
+-rw-r--r--  2.0 unx    13928 b- defN 23-Aug-08 11:17 torchrec/distributed/quant_state.py
+-rw-r--r--  2.0 unx     9261 b- defN 23-Aug-08 11:17 torchrec/distributed/shard.py
+-rw-r--r--  2.0 unx    19906 b- defN 23-Aug-08 11:17 torchrec/distributed/sharding_plan.py
+-rw-r--r--  2.0 unx    48752 b- defN 23-Aug-08 11:17 torchrec/distributed/train_pipeline.py
+-rw-r--r--  2.0 unx    26441 b- defN 23-Aug-08 11:17 torchrec/distributed/types.py
+-rw-r--r--  2.0 unx    15470 b- defN 23-Aug-08 11:17 torchrec/distributed/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:17 torchrec/distributed/composable/__init__.py
+-rw-r--r--  2.0 unx     3207 b- defN 23-Aug-08 11:17 torchrec/distributed/composable/table_batched_embedding_slice.py
+-rw-r--r--  2.0 unx     1025 b- defN 23-Aug-08 11:17 torchrec/distributed/planner/__init__.py
+-rw-r--r--  2.0 unx     3135 b- defN 23-Aug-08 11:17 torchrec/distributed/planner/constants.py
+-rw-r--r--  2.0 unx    11430 b- defN 23-Aug-08 11:17 torchrec/distributed/planner/enumerators.py
+-rw-r--r--  2.0 unx    12642 b- defN 23-Aug-08 11:17 torchrec/distributed/planner/partitioners.py
+-rw-r--r--  2.0 unx      835 b- defN 23-Aug-08 11:17 torchrec/distributed/planner/perf_models.py
+-rw-r--r--  2.0 unx    13803 b- defN 23-Aug-08 11:17 torchrec/distributed/planner/planners.py
+-rw-r--r--  2.0 unx    11236 b- defN 23-Aug-08 11:17 torchrec/distributed/planner/proposers.py
+-rw-r--r--  2.0 unx    41291 b- defN 23-Aug-08 11:17 torchrec/distributed/planner/shard_estimators.py
+-rw-r--r--  2.0 unx    24082 b- defN 23-Aug-08 11:17 torchrec/distributed/planner/stats.py
+-rw-r--r--  2.0 unx    12858 b- defN 23-Aug-08 11:17 torchrec/distributed/planner/storage_reservations.py
+-rw-r--r--  2.0 unx    14582 b- defN 23-Aug-08 11:17 torchrec/distributed/planner/types.py
+-rw-r--r--  2.0 unx     1431 b- defN 23-Aug-08 11:17 torchrec/distributed/planner/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:17 torchrec/distributed/sharding/__init__.py
+-rw-r--r--  2.0 unx     4784 b- defN 23-Aug-08 11:17 torchrec/distributed/sharding/cw_sequence_sharding.py
+-rw-r--r--  2.0 unx    12945 b- defN 23-Aug-08 11:17 torchrec/distributed/sharding/cw_sharding.py
+-rw-r--r--  2.0 unx     2802 b- defN 23-Aug-08 11:17 torchrec/distributed/sharding/dp_sequence_sharding.py
+-rw-r--r--  2.0 unx     7681 b- defN 23-Aug-08 11:17 torchrec/distributed/sharding/dp_sharding.py
+-rw-r--r--  2.0 unx     7640 b- defN 23-Aug-08 11:17 torchrec/distributed/sharding/rw_sequence_sharding.py
+-rw-r--r--  2.0 unx    17911 b- defN 23-Aug-08 11:17 torchrec/distributed/sharding/rw_sharding.py
+-rw-r--r--  2.0 unx     3627 b- defN 23-Aug-08 11:17 torchrec/distributed/sharding/sequence_sharding.py
+-rw-r--r--  2.0 unx     7632 b- defN 23-Aug-08 11:17 torchrec/distributed/sharding/tw_sequence_sharding.py
+-rw-r--r--  2.0 unx    16097 b- defN 23-Aug-08 11:17 torchrec/distributed/sharding/tw_sharding.py
+-rw-r--r--  2.0 unx     1284 b- defN 23-Aug-08 11:17 torchrec/distributed/sharding/twcw_sharding.py
+-rw-r--r--  2.0 unx    19871 b- defN 23-Aug-08 11:17 torchrec/distributed/sharding/twrw_sharding.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:17 torchrec/distributed/test_utils/__init__.py
+-rw-r--r--  2.0 unx    11237 b- defN 23-Aug-08 11:17 torchrec/distributed/test_utils/infer_utils.py
+-rw-r--r--  2.0 unx     4868 b- defN 23-Aug-08 11:17 torchrec/distributed/test_utils/multi_process.py
+-rw-r--r--  2.0 unx    34993 b- defN 23-Aug-08 11:17 torchrec/distributed/test_utils/test_model.py
+-rw-r--r--  2.0 unx    11197 b- defN 23-Aug-08 11:17 torchrec/distributed/test_utils/test_model_parallel.py
+-rw-r--r--  2.0 unx    25310 b- defN 23-Aug-08 11:17 torchrec/distributed/test_utils/test_model_parallel_base.py
+-rw-r--r--  2.0 unx    15367 b- defN 23-Aug-08 11:17 torchrec/distributed/test_utils/test_sharding.py
+-rw-r--r--  2.0 unx      422 b- defN 23-Aug-08 11:17 torchrec/fx/__init__.py
+-rw-r--r--  2.0 unx     6451 b- defN 23-Aug-08 11:17 torchrec/fx/tracer.py
+-rw-r--r--  2.0 unx     4524 b- defN 23-Aug-08 11:17 torchrec/fx/utils.py
+-rw-r--r--  2.0 unx     1223 b- defN 23-Aug-08 11:17 torchrec/inference/__init__.py
+-rw-r--r--  2.0 unx     3614 b- defN 23-Aug-08 11:17 torchrec/inference/client.py
+-rw-r--r--  2.0 unx     3957 b- defN 23-Aug-08 11:17 torchrec/inference/model_packager.py
+-rw-r--r--  2.0 unx     8068 b- defN 23-Aug-08 11:17 torchrec/inference/modules.py
+-rw-r--r--  2.0 unx     3797 b- defN 23-Aug-08 11:17 torchrec/inference/state_dict_transform.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:17 torchrec/metrics/__init__.py
+-rw-r--r--  2.0 unx     4168 b- defN 23-Aug-08 11:17 torchrec/metrics/accuracy.py
+-rw-r--r--  2.0 unx    12549 b- defN 23-Aug-08 11:17 torchrec/metrics/auc.py
+-rw-r--r--  2.0 unx     3703 b- defN 23-Aug-08 11:17 torchrec/metrics/calibration.py
+-rw-r--r--  2.0 unx     3465 b- defN 23-Aug-08 11:17 torchrec/metrics/ctr.py
+-rw-r--r--  2.0 unx     3836 b- defN 23-Aug-08 11:17 torchrec/metrics/mae.py
+-rw-r--r--  2.0 unx    17990 b- defN 23-Aug-08 11:17 torchrec/metrics/metric_module.py
+-rw-r--r--  2.0 unx     6796 b- defN 23-Aug-08 11:17 torchrec/metrics/metrics_config.py
+-rw-r--r--  2.0 unx     3731 b- defN 23-Aug-08 11:17 torchrec/metrics/metrics_namespace.py
+-rw-r--r--  2.0 unx     3904 b- defN 23-Aug-08 11:17 torchrec/metrics/model_utils.py
+-rw-r--r--  2.0 unx     4631 b- defN 23-Aug-08 11:17 torchrec/metrics/mse.py
+-rw-r--r--  2.0 unx     5605 b- defN 23-Aug-08 11:17 torchrec/metrics/multiclass_recall.py
+-rw-r--r--  2.0 unx     8735 b- defN 23-Aug-08 11:17 torchrec/metrics/ndcg.py
+-rw-r--r--  2.0 unx     6811 b- defN 23-Aug-08 11:17 torchrec/metrics/ne.py
+-rw-r--r--  2.0 unx    33554 b- defN 23-Aug-08 11:17 torchrec/metrics/rec_metric.py
+-rw-r--r--  2.0 unx    10490 b- defN 23-Aug-08 11:17 torchrec/metrics/recall_session.py
+-rw-r--r--  2.0 unx     6057 b- defN 23-Aug-08 11:17 torchrec/metrics/throughput.py
+-rw-r--r--  2.0 unx    11160 b- defN 23-Aug-08 11:17 torchrec/metrics/tower_qps.py
+-rw-r--r--  2.0 unx     2867 b- defN 23-Aug-08 11:17 torchrec/metrics/weighted_avg.py
+-rw-r--r--  2.0 unx    16441 b- defN 23-Aug-08 11:17 torchrec/metrics/test_utils/__init__.py
+-rw-r--r--  2.0 unx      913 b- defN 23-Aug-08 11:17 torchrec/models/__init__.py
+-rw-r--r--  2.0 unx    11410 b- defN 23-Aug-08 11:17 torchrec/models/deepfm.py
+-rw-r--r--  2.0 unx    29965 b- defN 23-Aug-08 11:17 torchrec/models/dlrm.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-08 11:17 torchrec/models/experimental/__init__.py
+-rw-r--r--  2.0 unx     9825 b- defN 23-Aug-08 11:17 torchrec/models/experimental/test_transformerdlrm.py
+-rw-r--r--  2.0 unx     7434 b- defN 23-Aug-08 11:17 torchrec/models/experimental/transformerdlrm.py
+-rw-r--r--  2.0 unx     1179 b- defN 23-Aug-08 11:17 torchrec/modules/__init__.py
+-rw-r--r--  2.0 unx     1456 b- defN 23-Aug-08 11:17 torchrec/modules/activation.py
+-rw-r--r--  2.0 unx    14636 b- defN 23-Aug-08 11:17 torchrec/modules/crossnet.py
+-rw-r--r--  2.0 unx     8415 b- defN 23-Aug-08 11:17 torchrec/modules/deepfm.py
+-rw-r--r--  2.0 unx     6468 b- defN 23-Aug-08 11:17 torchrec/modules/embedding_configs.py
+-rw-r--r--  2.0 unx    14021 b- defN 23-Aug-08 11:17 torchrec/modules/embedding_modules.py
+-rw-r--r--  2.0 unx     4858 b- defN 23-Aug-08 11:17 torchrec/modules/embedding_tower.py
+-rw-r--r--  2.0 unx    12360 b- defN 23-Aug-08 11:17 torchrec/modules/feature_processor.py
+-rw-r--r--  2.0 unx     4964 b- defN 23-Aug-08 11:17 torchrec/modules/feature_processor_.py
+-rw-r--r--  2.0 unx     4960 b- defN 23-Aug-08 11:17 torchrec/modules/fp_embedding_modules.py
+-rw-r--r--  2.0 unx    31545 b- defN 23-Aug-08 11:17 torchrec/modules/fused_embedding_modules.py
+-rw-r--r--  2.0 unx    10533 b- defN 23-Aug-08 11:17 torchrec/modules/lazy_extension.py
+-rw-r--r--  2.0 unx     3143 b- defN 23-Aug-08 11:17 torchrec/modules/managed_collision_modules.py
+-rw-r--r--  2.0 unx     5124 b- defN 23-Aug-08 11:17 torchrec/modules/mc_embedding_modules.py
+-rw-r--r--  2.0 unx     6309 b- defN 23-Aug-08 11:17 torchrec/modules/mlp.py
+-rw-r--r--  2.0 unx     3897 b- defN 23-Aug-08 11:17 torchrec/modules/utils.py
+-rw-r--r--  2.0 unx     1639 b- defN 23-Aug-08 11:17 torchrec/optim/__init__.py
+-rw-r--r--  2.0 unx     2012 b- defN 23-Aug-08 11:17 torchrec/optim/apply_optimizer_in_backward.py
+-rw-r--r--  2.0 unx     1569 b- defN 23-Aug-08 11:17 torchrec/optim/clipping.py
+-rw-r--r--  2.0 unx     1353 b- defN 23-Aug-08 11:17 torchrec/optim/fused.py
+-rw-r--r--  2.0 unx    16069 b- defN 23-Aug-08 11:17 torchrec/optim/keyed.py
+-rw-r--r--  2.0 unx     4420 b- defN 23-Aug-08 11:17 torchrec/optim/optimizers.py
+-rw-r--r--  2.0 unx     7405 b- defN 23-Aug-08 11:17 torchrec/optim/rowwise_adagrad.py
+-rw-r--r--  2.0 unx     4865 b- defN 23-Aug-08 11:17 torchrec/optim/warmup.py
+-rw-r--r--  2.0 unx      560 b- defN 23-Aug-08 11:17 torchrec/optim/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1140 b- defN 23-Aug-08 11:17 torchrec/quant/__init__.py
+-rw-r--r--  2.0 unx    26618 b- defN 23-Aug-08 11:17 torchrec/quant/embedding_modules.py
+-rw-r--r--  2.0 unx     4292 b- defN 23-Aug-08 11:17 torchrec/quant/utils.py
+-rw-r--r--  2.0 unx     1163 b- defN 23-Aug-08 11:17 torchrec/sparse/__init__.py
+-rw-r--r--  2.0 unx    56427 b- defN 23-Aug-08 11:17 torchrec/sparse/jagged_tensor.py
+-rw-r--r--  2.0 unx     1430 b- defN 23-Aug-08 11:17 torchrec/sparse/test_utils/__init__.py
+-rw-r--r--  2.0 unx     5661 b- defN 23-Aug-08 11:17 torchrec/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1530 b- defN 23-Aug-08 11:22 torchrec_nightly-2023.8.8.dist-info/LICENSE
+-rw-r--r--  2.0 unx     5011 b- defN 23-Aug-08 11:22 torchrec_nightly-2023.8.8.dist-info/METADATA
+-rw-r--r--  2.0 unx       93 b- defN 23-Aug-08 11:22 torchrec_nightly-2023.8.8.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 23-Aug-08 11:22 torchrec_nightly-2023.8.8.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    13819 b- defN 23-Aug-08 11:22 torchrec_nightly-2023.8.8.dist-info/RECORD
+148 files, 1551949 bytes uncompressed, 352519 bytes compressed:  77.3%
```

## zipnote {}

```diff
@@ -423,23 +423,23 @@
 
 Filename: torchrec/sparse/test_utils/__init__.py
 Comment: 
 
 Filename: torchrec/test_utils/__init__.py
 Comment: 
 
-Filename: torchrec_nightly-2023.8.6.dist-info/LICENSE
+Filename: torchrec_nightly-2023.8.8.dist-info/LICENSE
 Comment: 
 
-Filename: torchrec_nightly-2023.8.6.dist-info/METADATA
+Filename: torchrec_nightly-2023.8.8.dist-info/METADATA
 Comment: 
 
-Filename: torchrec_nightly-2023.8.6.dist-info/WHEEL
+Filename: torchrec_nightly-2023.8.8.dist-info/WHEEL
 Comment: 
 
-Filename: torchrec_nightly-2023.8.6.dist-info/top_level.txt
+Filename: torchrec_nightly-2023.8.8.dist-info/top_level.txt
 Comment: 
 
-Filename: torchrec_nightly-2023.8.6.dist-info/RECORD
+Filename: torchrec_nightly-2023.8.8.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## torchrec/distributed/quant_embedding.py

```diff
@@ -2,16 +2,17 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 
+from collections import defaultdict, deque
 from dataclasses import dataclass
-from typing import Any, Dict, List, Optional, Type
+from typing import Any, cast, Dict, List, Optional, Tuple, Type
 
 import torch
 from fbgemm_gpu.split_table_batched_embeddings_ops_inference import (
     IntNBitTableBatchedEmbeddingBagsCodegen,
 )
 from torch import nn
 from torchrec.distributed.embedding import (
@@ -31,23 +32,26 @@
     FUSED_PARAM_QUANT_STATE_DICT_SPLIT_SCALE_BIAS,
     FUSED_PARAM_REGISTER_TBE_BOOL,
     get_tbes_to_register_from_iterable,
     is_fused_param_quant_state_dict_split_scale_bias,
     is_fused_param_register_tbe,
 )
 from torchrec.distributed.quant_state import ShardedQuantEmbeddingModuleState
+from torchrec.distributed.sharding.cw_sequence_sharding import (
+    InferCwSequenceEmbeddingSharding,
+)
 from torchrec.distributed.sharding.rw_sequence_sharding import (
     InferRwSequenceEmbeddingSharding,
 )
 from torchrec.distributed.sharding.rw_sharding import InferRwSparseFeaturesDist
 from torchrec.distributed.sharding.sequence_sharding import InferSequenceShardingContext
 from torchrec.distributed.sharding.tw_sequence_sharding import (
     InferTwSequenceEmbeddingSharding,
 )
-from torchrec.distributed.types import ParameterSharding, ShardingEnv
+from torchrec.distributed.types import ParameterSharding, ShardingEnv, ShardMetadata
 from torchrec.modules.embedding_configs import (
     data_type_to_sparse_type,
     dtype_to_data_type,
     EmbeddingConfig,
 )
 from torchrec.quant.embedding_modules import (
     EmbeddingCollection as QuantEmbeddingCollection,
@@ -83,14 +87,16 @@
     InferSequenceShardingContext,
     KJTList,
     List[torch.Tensor],
     List[torch.Tensor],
 ]:
     if sharding_type == ShardingType.TABLE_WISE.value:
         return InferTwSequenceEmbeddingSharding(sharding_infos, env, device)
+    elif sharding_type == ShardingType.COLUMN_WISE.value:
+        return InferCwSequenceEmbeddingSharding(sharding_infos, env, device)
     elif sharding_type == ShardingType.ROW_WISE.value:
         return InferRwSequenceEmbeddingSharding(sharding_infos, env, device)
     else:
         raise ValueError(f"Sharding type not supported {sharding_type}")
 
 
 @torch.fx.wrap
@@ -163,71 +169,158 @@
             values=embs_split_per_key[i],
             lengths=lengths_list[i],
             weights=values_list[i] if need_indices else None,
         )
     return ret
 
 
+def _construct_jagged_tensors_cw(
+    embeddings: List[torch.Tensor],
+    features: KJTList,
+    embedding_names_per_rank: List[List[str]],
+    need_indices: bool,
+    features_to_permute_indices: Dict[str, torch.Tensor],
+) -> Dict[str, JaggedTensor]:
+    ret: Dict[str, JaggedTensor] = {}
+    stride = features[0].stride()
+    lengths_lists: List[List[torch.Tensor]] = []
+    embeddings_lists: List[List[torch.Tensor]] = []
+    values_lists: List[List[torch.Tensor]] = []
+    for i in range(len(features)):
+        embedding = embeddings[i]
+        feature = features[i]
+        lengths_lists.append(torch.unbind(feature.lengths().view(-1, stride), dim=0))
+        embeddings_lists.append(
+            list(torch.split(embedding, feature.length_per_key(), dim=0))
+        )
+    if need_indices:
+        for i in range(len(features)):
+            feature = features[i]
+            values_lists.append(
+                list(torch.split(feature.values(), feature.length_per_key()))
+            )
+
+    key_to_feature_coordinates: Dict[str, List[Tuple[int, int]]] = {}
+    for rank, embedding_names in enumerate(embedding_names_per_rank):
+        for idx_in_rank, embedding_name in enumerate(embedding_names):
+            if embedding_name not in key_to_feature_coordinates:
+                key_to_feature_coordinates[embedding_name] = torch.jit.annotate(
+                    List[Tuple[int, int]], []
+                )
+            key_to_feature_coordinates[embedding_name].append((rank, idx_in_rank))
+
+    for key, coordinates in key_to_feature_coordinates.items():
+        permuted_coordinates: List[Tuple[int, int]] = coordinates
+
+        if key in features_to_permute_indices:
+            permuted_coordinates = [(-1, -1)] * len(coordinates)
+            permute_indices: List[int] = features_to_permute_indices[key].tolist()
+            for i, permute_idx in enumerate(permute_indices):
+                permuted_coordinates[i] = coordinates[permute_idx]
+
+        rank0, idx_in_rank0 = permuted_coordinates[0]
+        ret[key] = JaggedTensor(
+            lengths=lengths_lists[rank0][idx_in_rank0],
+            values=torch.cat(
+                [
+                    embeddings_lists[rank][idx_in_rank]
+                    for rank, idx_in_rank in permuted_coordinates
+                ],
+                dim=1,
+            ),
+            weights=values_lists[rank0][idx_in_rank0] if need_indices else None,
+        )
+    return ret
+
+
 def _construct_jagged_tensors(
+    sharding_type: str,
     embeddings: List[torch.Tensor],
     features: KJTList,
     embedding_names_per_rank: List[List[str]],
     features_before_input_dist: KeyedJaggedTensor,
     need_indices: bool,
-    unbucketize_tensor: Optional[torch.Tensor],
-    features_to_permute_indices: Dict[str, torch.Tensor],
+    rw_unbucketize_tensor: Optional[torch.Tensor],
+    cw_features_to_permute_indices: Dict[str, torch.Tensor],
 ) -> Dict[str, JaggedTensor]:
-    if unbucketize_tensor is not None:
-        # RW sharding
+
+    # Validating sharding type and parameters
+    valid_sharding_types = [
+        ShardingType.ROW_WISE.value,
+        ShardingType.COLUMN_WISE.value,
+        ShardingType.TABLE_WISE.value,
+    ]
+    if sharding_type not in valid_sharding_types:
+        raise ValueError(f"Unknown sharding type {sharding_type}")
+
+    if sharding_type == ShardingType.ROW_WISE.value and rw_unbucketize_tensor is None:
+        raise ValueError("rw_unbucketize_tensor is required for row-wise sharding")
+
+    if (
+        sharding_type == ShardingType.ROW_WISE.value
+        and rw_unbucketize_tensor is not None
+    ):
         return _construct_jagged_tensors_rw(
             embeddings,
             features_before_input_dist,
             need_indices,
-            unbucketize_tensor,
+            rw_unbucketize_tensor,
         )
-
-    return _construct_jagged_tensors_tw(embeddings, features, need_indices)
+    elif sharding_type == ShardingType.COLUMN_WISE.value:
+        return _construct_jagged_tensors_cw(
+            embeddings,
+            features,
+            embedding_names_per_rank,
+            need_indices,
+            cw_features_to_permute_indices,
+        )
+    else:  # sharding_type == ShardingType.TABLE_WISE.value
+        return _construct_jagged_tensors_tw(embeddings, features, need_indices)
 
 
 @torch.fx.wrap
 def output_jt_dict(
+    sharding_types: List[str],
     emb_per_sharding: List[List[torch.Tensor]],
     features_per_sharding: List[KJTList],
     embedding_names_per_rank_per_sharding: List[List[List[str]]],
     need_indices: bool,
     features_before_input_dist_per_sharding: List[KeyedJaggedTensor],
     features_to_permute_indices: Dict[str, torch.Tensor],
     unbucketize_tensors: List[torch.Tensor],
     unbucketize_tensor_idxs_per_sharding: List[int],
 ) -> Dict[str, JaggedTensor]:
     jt_dict: Dict[str, JaggedTensor] = {}
     for (
+        sharding_type,
         emb_sharding,
         features_sharding,
         embedding_names_per_rank,
         unbucketize_tensor_idx,
         features_before_input_dist,
     ) in zip(
+        sharding_types,
         emb_per_sharding,
         features_per_sharding,
         embedding_names_per_rank_per_sharding,
         unbucketize_tensor_idxs_per_sharding,
         features_before_input_dist_per_sharding,
     ):
         jt_dict.update(
             _construct_jagged_tensors(
+                sharding_type=sharding_type,
                 embeddings=emb_sharding,
                 features=features_sharding,
                 embedding_names_per_rank=embedding_names_per_rank,
                 features_before_input_dist=features_before_input_dist,
                 need_indices=need_indices,
-                unbucketize_tensor=unbucketize_tensors[unbucketize_tensor_idx]
+                rw_unbucketize_tensor=unbucketize_tensors[unbucketize_tensor_idx]
                 if unbucketize_tensor_idx != -1
                 else None,
-                features_to_permute_indices=features_to_permute_indices,
+                cw_features_to_permute_indices=features_to_permute_indices,
             )
         )
     return jt_dict
 
 
 class ShardedQuantEmbeddingCollection(
     ShardedQuantEmbeddingModuleState[
@@ -276,14 +369,25 @@
         self._embedding_names_per_rank_per_sharding: List[List[List[str]]] = []
         for sharding in self._sharding_type_to_sharding.values():
             self._embedding_names_per_sharding.append(sharding.embedding_names())
             self._embedding_names_per_rank_per_sharding.append(
                 sharding.embedding_names_per_rank()
             )
         self._features_to_permute_indices: Dict[str, torch.Tensor] = {}
+        if ShardingType.COLUMN_WISE.value in self._sharding_type_to_sharding:
+            sharding = self._sharding_type_to_sharding[ShardingType.COLUMN_WISE.value]
+            # CW partition must be same for all CW sharded parameters
+            self._local_embedding_dim = cast(
+                ShardMetadata, sharding.embedding_shard_metadata()[0]
+            ).shard_sizes[1]
+            self._features_to_permute_indices = (
+                self._generate_permute_indices_per_feature(
+                    module.embedding_configs(), table_name_to_parameter_sharding
+                )
+            )
 
         self._device = device
         self._input_dists: List[nn.Module] = []
         self._lookups: List[nn.Module] = []
         self._create_lookups(fused_params, device)
         self._output_dists: List[nn.Module] = []
 
@@ -338,14 +442,45 @@
                 for key in lookup_state_dict:
                     if key.endswith(".weight"):
                         table_name = key[: -len(".weight")]
                         self.embeddings[table_name].register_buffer(
                             "weight", lookup_state_dict[key]
                         )
 
+    def _generate_permute_indices_per_feature(
+        self,
+        embedding_configs: List[EmbeddingConfig],
+        table_name_to_parameter_sharding: Dict[str, ParameterSharding],
+    ) -> Dict[str, torch.Tensor]:
+        ret: Dict[str, torch.Tensor] = {}
+        shared_feature: Dict[str, bool] = {}
+        for table in embedding_configs:
+            for feature_name in table.feature_names:
+                if feature_name not in shared_feature:
+                    shared_feature[feature_name] = False
+                else:
+                    shared_feature[feature_name] = True
+
+        for table in embedding_configs:
+            sharding = table_name_to_parameter_sharding[table.name]
+            if sharding.sharding_type != ShardingType.COLUMN_WISE.value:
+                continue
+            ranks = cast(List[int], sharding.ranks)
+            rank_to_indices = defaultdict(deque)
+            for i, rank in enumerate(sorted(ranks)):
+                rank_to_indices[rank].append(i)
+            permute_indices = [rank_to_indices[rank].popleft() for rank in ranks]
+            tensor = torch.tensor(permute_indices, dtype=torch.int64)
+            for feature_name in table.feature_names:
+                if shared_feature[feature_name]:
+                    ret[feature_name + "@" + table.name] = tensor
+                else:
+                    ret[feature_name] = tensor
+        return ret
+
     def _create_input_dist(
         self,
         input_feature_names: List[str],
         device: torch.device,
         input_dist_device: Optional[torch.device] = None,
     ) -> None:
         feature_names: List[str] = []
@@ -427,26 +562,31 @@
                         unbucketize_permute_tensor=input_dist.unbucketize_permute_tensor
                         if isinstance(input_dist, InferRwSparseFeaturesDist)
                         else None,
                     )
                 )
         return ListOfKJTList(ret)
 
+    def _embedding_dim_for_sharding_type(self, sharding_type: str) -> int:
+        return (
+            self._local_embedding_dim
+            if sharding_type == ShardingType.COLUMN_WISE.value
+            else self._embedding_dim
+        )
+
     def compute(
         self, ctx: EmbeddingCollectionContext, dist_input: ListOfKJTList
     ) -> List[List[torch.Tensor]]:
         ret: List[List[torch.Tensor]] = []
 
-        for lookup, features in zip(
-            self._lookups,
-            dist_input,
+        for lookup, features, sharding_type in zip(
+            self._lookups, dist_input, self._sharding_type_to_sharding.keys()
         ):
-            ret.append(
-                [o.view(-1, self._embedding_dim) for o in lookup.forward(features)]
-            )
+            embedding_dim = self._embedding_dim_for_sharding_type(sharding_type)
+            ret.append([o.view(-1, embedding_dim) for o in lookup.forward(features)])
         return ret
 
     # pyre-ignore
     def output_dist(
         self, ctx: EmbeddingCollectionContext, output: List[List[torch.Tensor]]
     ) -> Dict[str, JaggedTensor]:
         emb_per_sharding: List[List[torch.Tensor]] = []
@@ -474,14 +614,15 @@
                     len(unbucketize_tensors) - 1
                 )
             features_before_input_dist_per_sharding.append(
                 # pyre-ignore
                 sharding_ctx.features_before_input_dist
             )
         return output_jt_dict(
+            sharding_types=list(self._sharding_type_to_sharding.keys()),
             emb_per_sharding=emb_per_sharding,
             features_per_sharding=features_per_sharding,
             embedding_names_per_rank_per_sharding=self._embedding_names_per_rank_per_sharding,
             need_indices=self._need_indices,
             features_before_input_dist_per_sharding=features_before_input_dist_per_sharding,
             unbucketize_tensor_idxs_per_sharding=unbucketize_tensor_idxs_per_sharding,
             unbucketize_tensors=unbucketize_tensors,
```

## torchrec/distributed/sharding/cw_sequence_sharding.py

```diff
@@ -1,28 +1,38 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
-from typing import Any, Dict, Optional
+from typing import Any, Dict, List, Optional
 
 import torch
-from torchrec.distributed.embedding_lookup import GroupedEmbeddingsLookup
+from torchrec.distributed.dist_data import SeqEmbeddingsAllToOne
+from torchrec.distributed.embedding_lookup import (
+    GroupedEmbeddingsLookup,
+    InferGroupedEmbeddingsLookup,
+)
 from torchrec.distributed.embedding_sharding import (
     BaseEmbeddingDist,
     BaseEmbeddingLookup,
     BaseSparseFeaturesDist,
 )
-from torchrec.distributed.embedding_types import BaseGroupedFeatureProcessor
+from torchrec.distributed.embedding_types import BaseGroupedFeatureProcessor, KJTList
 from torchrec.distributed.sharding.cw_sharding import BaseCwEmbeddingSharding
-from torchrec.distributed.sharding.sequence_sharding import SequenceShardingContext
+from torchrec.distributed.sharding.sequence_sharding import (
+    InferSequenceShardingContext,
+    SequenceShardingContext,
+)
 from torchrec.distributed.sharding.tw_sequence_sharding import TwSequenceEmbeddingDist
-from torchrec.distributed.sharding.tw_sharding import TwSparseFeaturesDist
+from torchrec.distributed.sharding.tw_sharding import (
+    InferTwSparseFeaturesDist,
+    TwSparseFeaturesDist,
+)
 from torchrec.sparse.jagged_tensor import KeyedJaggedTensor
 
 
 class CwSequenceEmbeddingSharding(
     BaseCwEmbeddingSharding[
         SequenceShardingContext, KeyedJaggedTensor, torch.Tensor, torch.Tensor
     ]
@@ -62,7 +72,72 @@
         assert self._pg is not None
         return TwSequenceEmbeddingDist(
             self._pg,
             self.features_per_rank(),
             device if device is not None else self._device,
             qcomm_codecs_registry=self.qcomm_codecs_registry,
         )
+
+
+class InferCwSequenceEmbeddingSharding(
+    BaseCwEmbeddingSharding[
+        InferSequenceShardingContext, KJTList, List[torch.Tensor], List[torch.Tensor]
+    ]
+):
+    def create_input_dist(
+        self, device: Optional[torch.device] = None
+    ) -> BaseSparseFeaturesDist[KJTList]:
+        return InferTwSparseFeaturesDist(
+            features_per_rank=self.features_per_rank(),
+            world_size=self._world_size,
+            device=device if device is not None else self._device,
+        )
+
+    def create_lookup(
+        self,
+        device: Optional[torch.device] = None,
+        fused_params: Optional[Dict[str, Any]] = None,
+        feature_processor: Optional[BaseGroupedFeatureProcessor] = None,
+    ) -> BaseEmbeddingLookup[KJTList, List[torch.Tensor]]:
+        return InferGroupedEmbeddingsLookup(
+            grouped_configs_per_rank=self._grouped_embedding_configs_per_rank,
+            world_size=self._world_size,
+            fused_params=fused_params,
+            device=device if device is not None else self._device,
+        )
+
+    def create_output_dist(
+        self, device: Optional[torch.device] = None
+    ) -> BaseEmbeddingDist[
+        InferSequenceShardingContext, List[torch.Tensor], List[torch.Tensor]
+    ]:
+        device = device if device is not None else self._device
+        assert device is not None
+
+        dist_out = InferCwSequenceEmbeddingDist(
+            device,
+            self._world_size,
+        )
+        return dist_out
+
+
+class InferCwSequenceEmbeddingDist(
+    BaseEmbeddingDist[
+        InferSequenceShardingContext, List[torch.Tensor], List[torch.Tensor]
+    ]
+):
+    def __init__(
+        self,
+        device: torch.device,
+        world_size: int,
+    ) -> None:
+        super().__init__()
+        self._dist: SeqEmbeddingsAllToOne = SeqEmbeddingsAllToOne(
+            device=device, world_size=world_size
+        )
+
+    def forward(
+        self,
+        local_embs: List[torch.Tensor],
+        sharding_ctx: Optional[InferSequenceShardingContext] = None,
+    ) -> List[torch.Tensor]:
+        return self._dist.forward(local_embs)
```

## Comparing `torchrec_nightly-2023.8.6.dist-info/LICENSE` & `torchrec_nightly-2023.8.8.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `torchrec_nightly-2023.8.6.dist-info/METADATA` & `torchrec_nightly-2023.8.8.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: torchrec-nightly
-Version: 2023.8.6
+Version: 2023.8.8
 Summary: Pytorch domain library for recommendation systems
 Home-page: https://github.com/pytorch/torchrec
 Author: TorchRec Team
 Author-email: packages@pytorch.org
 License: BSD-3
 Keywords: pytorch,recommendation systems,sharding
 Classifier: Development Status :: 4 - Beta
```

## Comparing `torchrec_nightly-2023.8.6.dist-info/RECORD` & `torchrec_nightly-2023.8.8.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -29,15 +29,15 @@
 torchrec/distributed/fp_embeddingbag.py,sha256=mM4Dnb1_nE5pH-VqwCs0DBYhi87VxjWC8JlMC3sa1qY,6219
 torchrec/distributed/fused_embedding.py,sha256=1VJeW5Dl7EFMvyOfhBvDKZlp39GYucBo8vNFJY2alFI,5243
 torchrec/distributed/fused_embeddingbag.py,sha256=tG_BrUlCdsek87jgHPKbxg3-z13sAgSWPNRBArf2_ss,5080
 torchrec/distributed/fused_params.py,sha256=YEbH5KUphcSWkwLi-JzXUR9SRDm2OykFxP46CJj6nTM,2271
 torchrec/distributed/grouped_position_weighted.py,sha256=q-QE0U306BiPkXIAlJGIQ80EUDZj-FXTbWwjz3EyvLI,3807
 torchrec/distributed/mc_embeddingbag.py,sha256=3_cm7HF53Lj12DJ271yfN1cbjbdxxYRpBCQUWySNsWc,10997
 torchrec/distributed/model_parallel.py,sha256=VV9Vsyas0VKNHr8sX9pq1iT_xJ2b9xjenY3pYAY5HYw,19750
-torchrec/distributed/quant_embedding.py,sha256=9hTS4oUYkoCEHInYO7r0IQjfAJi4fsw5WNiTOHGUJuM,20748
+torchrec/distributed/quant_embedding.py,sha256=a3mm2vlsFXe5SqlaBU9BXD_b4GASmGN6CoZbBx9QT_Q,26936
 torchrec/distributed/quant_embedding_kernel.py,sha256=VsROr4bXBkYysS0H7NnlZzF7IvGgZFtgws5jsnPf6g4,15112
 torchrec/distributed/quant_embeddingbag.py,sha256=SJPY-nL95LKozfT8iR4SHt1BNLx4OcoAfFvqKzy1Gf4,12621
 torchrec/distributed/quant_state.py,sha256=PDQ7qUwhFt-q1WvTkoSbWE3rotiFOYyKNFsQ5vJQ36U,13928
 torchrec/distributed/shard.py,sha256=4Dr5ixWCoMEFEuL5WN4fL2gIdl9wmSUjZWsiF-kdCdQ,9261
 torchrec/distributed/sharding_plan.py,sha256=xYM8l3JpqURxfZwFPwQtSTrfg9qM3dp20PAYEGROHfI,19906
 torchrec/distributed/train_pipeline.py,sha256=weqXkMGeL-O4WVlQ_hQ03pfiiGOaGG5ISKp5K2-gBvQ,48752
 torchrec/distributed/types.py,sha256=a7kNk8HdIQjyBTyq0rmBY7wn3CyGW7hk3rDC9XzWvAU,26441
@@ -53,15 +53,15 @@
 torchrec/distributed/planner/proposers.py,sha256=tfhI0DavpRYfY4UN5diXBMl8fWHIEm8wNco5R6yBIGI,11236
 torchrec/distributed/planner/shard_estimators.py,sha256=QTW2OdGRaBnOp79xRiUz5W3O9oxS9m4wlAqP-goHCi4,41291
 torchrec/distributed/planner/stats.py,sha256=nSlFy12f2mhuCbuGZ-jzCi8-L0AXMF7-1BOZN0BVSwo,24082
 torchrec/distributed/planner/storage_reservations.py,sha256=bq2uRVTgkz1BxJghW9d_Wl08YfQgomfukZXiRVdf2Os,12858
 torchrec/distributed/planner/types.py,sha256=FHe0shRgJJ8Kvqnb9IpNV7Aphqsga2wcZ6dXTmI_M7s,14582
 torchrec/distributed/planner/utils.py,sha256=4dD84n7rGUGfCT8tbtPVVNN-iFBnDpb5eyyPxvWru3k,1431
 torchrec/distributed/sharding/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-torchrec/distributed/sharding/cw_sequence_sharding.py,sha256=N5qRxCxDkhT0ZcOAUBDTuhQTAz3_I7B1cN_pFeleBbk,2479
+torchrec/distributed/sharding/cw_sequence_sharding.py,sha256=10AYo-Bvy07NznCcqPj7eFrSQ4jos4TkcOuuXwnDrrM,4784
 torchrec/distributed/sharding/cw_sharding.py,sha256=K6Nc8JaQxENSaOPv4rMZytW6FE7wGlKxXj78RZOx6QE,12945
 torchrec/distributed/sharding/dp_sequence_sharding.py,sha256=zQmERguEGVd5IAbmVsIhqdKXb0f2g8tQamN838qVvfM,2802
 torchrec/distributed/sharding/dp_sharding.py,sha256=HSedwv3zCoi-WGElzucyqDMolrviB6tyqs1jN-6RTEY,7681
 torchrec/distributed/sharding/rw_sequence_sharding.py,sha256=xhgtuuzXgj29k2CRFMHaYDXhvdf2v5C7uoyCSP0yH-A,7640
 torchrec/distributed/sharding/rw_sharding.py,sha256=a7tTyVTMA_8NzlZhWCoJbrfv8nqowUnTMkpFO-BmI_Q,17911
 torchrec/distributed/sharding/sequence_sharding.py,sha256=Df7Rodg34MDUSbQBYGHLnUCoNT-6fKhnbrE-FQPRy9E,3627
 torchrec/distributed/sharding/tw_sequence_sharding.py,sha256=vEjhuY38hhtkYGHtzvIOvK3wLN1B9IGNpVjrlnAVhi8,7632
@@ -137,12 +137,12 @@
 torchrec/quant/__init__.py,sha256=A6NIA6ztq6iP1JTLRLNzlgnCcd-LaN8efnxGub3Ii4A,1140
 torchrec/quant/embedding_modules.py,sha256=-_5_HaoNpt3AmL7fapO2Wj2r6WPjbq2VvATNmGhrai0,26618
 torchrec/quant/utils.py,sha256=rcyo5LDcLK49VLs6ZFxOHeutblWZunDAM_T-0NsraDE,4292
 torchrec/sparse/__init__.py,sha256=dLqSye4Jo6obnNNTUKdPDxPQb9sL2U4weemSn-DjpYk,1163
 torchrec/sparse/jagged_tensor.py,sha256=i7bqXkDMZOEW1eXiluYq1bdYFxOgUDGqYtI7vIaYUBs,56427
 torchrec/sparse/test_utils/__init__.py,sha256=BLxfGKJvwjjCiQM64O5wGAA_Cea0sG-buw9lTDWuqug,1430
 torchrec/test_utils/__init__.py,sha256=JncJcXS4N3gI7-fsizQ2-qiWM6MhIrpvskF_9gDf0Go,5661
-torchrec_nightly-2023.8.6.dist-info/LICENSE,sha256=e0Eotbf_rHOYPuEUlppIbvwy4SN98CZnl_hqwvbDA4Q,1530
-torchrec_nightly-2023.8.6.dist-info/METADATA,sha256=AZW-Oa_IE9A56qGAT44B3QSpJfvr54Xzjsi5MCKK8gY,5011
-torchrec_nightly-2023.8.6.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
-torchrec_nightly-2023.8.6.dist-info/top_level.txt,sha256=LoLcTAPLj_7x62AuyYmhEVBcx2WJ1Z1Nrknv0Jnk_gQ,9
-torchrec_nightly-2023.8.6.dist-info/RECORD,,
+torchrec_nightly-2023.8.8.dist-info/LICENSE,sha256=e0Eotbf_rHOYPuEUlppIbvwy4SN98CZnl_hqwvbDA4Q,1530
+torchrec_nightly-2023.8.8.dist-info/METADATA,sha256=KdR-M-Tk1Ce3yUlWVguHUClK4l_wTRl8Q-ME8R8jMAM,5011
+torchrec_nightly-2023.8.8.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
+torchrec_nightly-2023.8.8.dist-info/top_level.txt,sha256=LoLcTAPLj_7x62AuyYmhEVBcx2WJ1Z1Nrknv0Jnk_gQ,9
+torchrec_nightly-2023.8.8.dist-info/RECORD,,
```

